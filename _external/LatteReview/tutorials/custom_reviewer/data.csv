Title,Abstract,Year of publication,DOI,Clinical Application,is_CT
(18)F-FDG PET/CT Uptake Classification in Lymphoma and Lung Cancer by Using Deep Convolutional Neural Networks,"Background Fluorine 18 ((18)F)-fluorodeoxyglucose (FDG) PET/CT is a routine tool for staging patients with lymphoma and lung cancer. Purpose To evaluate configurations of deep convolutional neural networks (CNNs) to localize and classify uptake patterns of whole-body (18)F-FDG PET/CT images in patients with lung cancer and lymphoma. Materials and Methods This was a retrospective analysis of consecutive patients with lung cancer or lymphoma referred to a single center from August 2011 to August 2013. Two nuclear medicine experts manually delineated foci with increased (18)F-FDG uptake, specified the anatomic location, and classified these findings as suspicious for tumor or metastasis or nonsuspicious. By using these expert readings as the reference standard, a CNN was developed to detect foci positive for (18)F-FDG uptake, predict the anatomic location, and determine the expert classification. Examinations were divided into independent training (60%), validation (20%), and test (20%) subsets. Results This study included 629 patients (mean age, 52.2 years ± 20.4 [standard deviation]; 394 men). There were 302 patients with lung cancer and 327 patients with lymphoma. For the test set (123 patients; 10 782 foci), the CNN areas under the receiver operating characteristic curve (AUCs) for determining hypermetabolic (18)F-FDG PET/CT foci that were suspicious for cancer versus nonsuspicious by using the five input features were as follows: CT alone, 0.78 (95% confidence interval [CI]: 0.72, 0.83); (18)F-FDG PET alone, 0.97 (95% CI: 0.97, 0.98); (18)F-FDG PET/CT, 0.98 (95% CI: 0.97, 0.99); (18)F-FDG PET/CT maximum intensity projection (MIP), 0.98 (95% CI: 0.98, 0.99); and (18)F-FDG PET/CT MIP atlas, 0.99 (95% CI: 0.98, 1.00). The combination of (18)F-FDG PET and CT information improved overall classification accuracy (AUC, 0.975 vs 0.981, respectively; P < .001). Anatomic localization accuracy of the CNN was 2543 of 2639 (96.4%; 95% CI: 95.5%, 97.1%) for body part, 2292 of 2639 (86.9%; 95% CI: 85.3%, 88.5%) for region (ie, organ), and 2149 of 2639 (81.4%; 95% CI: 79.3%-83.5%) for subregion. Conclusion The fully automated anatomic localization and classification of fluorine 18-fluorodeoxyglucose PET uptake patterns in foci suspicious and nonsuspicious for cancer in patients with lung cancer and lymphoma by using a convolutional neural network is feasible and achieves high diagnostic performance when both CT and PET images are used. © RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Froelich and Salavati in this issue.",2020,10.1148/radiol.2019191114,diagnosis,True
(18)F-FDG-PET/CT Whole-Body Imaging Lung Tumor Diagnostic Model: An Ensemble E-ResNet-NRC with Divided Sample Space,"Under the background of (18)F-FDG-PET/CT multimodal whole-body imaging for lung tumor diagnosis, for the problems of network degradation and high dimension features during convolutional neural network (CNN) training, beginning with the perspective of dividing sample space, an E-ResNet-NRC (ensemble ResNet nonnegative representation classifier) model is proposed in this paper. The model includes the following steps: (1) Parameters of a pretrained ResNet model are initialized using transfer learning. (2) Samples are divided into three different sample spaces (CT, PET, and PET/CT) based on the differences in multimodal medical images PET/CT, and ROI of the lesion was extracted. (3) The ResNet neural network was used to extract ROI features and obtain feature vectors. (4) Individual classifier ResNet-NRC was constructed with nonnegative representation NRC at a fully connected layer. (5) Ensemble classifier E-ResNet-NRC was constructed using the ""relative majority voting method."" Finally, two network models, AlexNet and ResNet-50, and three classification algorithms, nearest neighbor classification algorithm (NNC), softmax, and nonnegative representation classification algorithm (NRC), were combined to compare with the E-ResNet-NRC model in this paper. The experimental results show that the overall classification performance of the Ensemble E-ResNet-NRC model is better than the individual ResNet-NRC, and specificity and sensitivity are more higher; the E-ResNet-NRC has better robustness and generalization ability.",2021,10.1155/2021/8865237,diagnosis,True
[(123)I]Metaiodobenzylguanidine (MIBG) Cardiac Scintigraphy and Automated Classification Techniques in Parkinsonian Disorders,"PURPOSE: To provide reliable and reproducible heart/mediastinum (H/M) ratio cut-off values for parkinsonian disorders using two machine learning techniques, Support Vector Machines (SVM) and Random Forest (RF) classifier, applied to [(123)I]MIBG cardiac scintigraphy. PROCEDURES: We studied 85 subjects, 50 with idiopathic Parkinson's disease, 26 with atypical Parkinsonian syndromes (P), and 9 with essential tremor (ET). All patients underwent planar early and delayed cardiac scintigraphy after [(123)I]MIBG (111 MBq) intravenous injection. Images were evaluated both qualitatively and quantitatively; the latter by the early and delayed H/M ratio obtained from regions of interest (ROIt(1) and ROIt(2)) drawn on planar images. SVM and RF classifiers were finally used to obtain the correct cut-off value. RESULTS: SVM and RF produced excellent classification performances: SVM classifier achieved perfect classification and RF also attained very good accuracy. The better cut-off for H/M value was 1.55 since it remains the same for both ROIt(1) and ROIt(2.) This value allowed to correctly classify PD from P and ET: patients with H/M ratio less than 1.55 were classified as PD while those with values higher than 1.55 were considered as affected by parkinsonism and/or ET. No difference was found when early or late H/M ratio were considered separately thus suggesting that a single early evaluation could be sufficient to obtain the final diagnosis. CONCLUSIONS: Our results evidenced that the use of SVM and CT permitted to define the better cut-off value for H/M ratios both in early and in delayed phase thus underlining the role of [(123)I]MIBG cardiac scintigraphy and the effectiveness of H/M ratio in differentiating PD from other parkinsonism or ET. Moreover, early scans alone could be used for a reliable diagnosis since no difference was found between early and late. Definitely, a larger series of cases is needed to confirm this data.",2020,10.1007/s11307-019-01406-6,diagnosis,False
[Formula: see text]: deep learning-based radiomics for the time-to-event outcome prediction in lung cancer,"Hand-crafted radiomics has been used for developing models in order to predict time-to-event clinical outcomes in patients with lung cancer. Hand-crafted features, however, are pre-defined and extracted without taking the desired target into account. Furthermore, accurate segmentation of the tumor is required for development of a reliable predictive model, which may be objective and a time-consuming task. To address these drawbacks, we propose a deep learning-based radiomics model for the time-to-event outcome prediction, referred to as DRTOP that takes raw images as inputs, and calculates the image-based risk of death or recurrence, for each patient. Our experiments on an in-house dataset of 132 lung cancer patients show that the obtained image-based risks are significant predictors of the time-to-event outcomes. Computed Tomography (CT)-based features are predictors of the overall survival (OS), with the hazard ratio (HR) of 1.35, distant control (DC), with HR of 1.06, and local control (LC), with HR of 2.66. The Positron Emission Tomography (PET)-based features are predictors of OS and recurrence free survival (RFS), with hazard ratios of 1.67 and 1.18, respectively. The concordance indices of [Formula: see text], [Formula: see text], and [Formula: see text] for predicting the OS, DC, and RFS show that the deep learning-based radiomics model is as accurate or better in predicting predefined clinical outcomes compared to hand-crafted radiomics, with concordance indices of [Formula: see text], [Formula: see text], and [Formula: see text], for predicting the OS, DC, and RFS, respectively. Deep learning-based radiomics has the potential to offer complimentary predictive information in the personalized management of lung cancer patients.",2020,10.1038/s41598-020-69106-8,prognosis,True
3-D Convolutional Neural Networks for Automatic Detection of Pulmonary Nodules in Chest CT,"Deep two-dimensional (2-D) convolutional neural networks (CNNs) have been remarkably successful in producing record-breaking results in a variety of computer vision tasks. It is possible to extend CNNs to three dimensions using 3-D kernels to make them suitable for volumetric medical imaging data such as CT or MRI, but this increases the processing time as well as the required number of training samples (due to the higher number of parameters that need to be learned). In this paper, we address both of these issues for a 3-D CNN implementation through the development of a two-stage computer-aided detection system for automatic detection of pulmonary nodules. The first stage consists of a 3-D fully convolutional network for fast screening and generation of candidate suspicious regions. The second stage consists of an ensemble of 3-D CNNs trained using extensive transformations applied to both the positive and negative patches to augment the training set. To enable the second stage classifiers to learn differently, they are trained on false positive patches obtained from the screening model using different thresholds on their associated scores as well as different augmentation types. The networks in the second stage are averaged together to produce the final classification score for each candidate patch. Using this procedure, our overall nodule detection system called DeepMed is fast and can achieve 91% sensitivity at 2 false positives per scan on cases from the LIDC dataset.",2019,10.1109/jbhi.2018.2879449,diagnosis,True
3D CNN with Visual Insights for Early Detection of Lung Cancer Using Gradient-Weighted Class Activation,"The 3D convolutional neural network is able to make use of the full nonlinear 3D context information of lung nodule detection from the DICOM (Digital Imaging and Communications in Medicine) images, and the Gradient Class Activation has shown to be useful for tailoring classification tasks and localization interpretation for fine-grained features and visual explanation for the internal working. Gradient-weighted class activation plays a crucial role for clinicians and radiologists in terms of trusting and adopting the model. Practitioners not only rely on a model that can provide high precision but also really want to gain the respect of radiologists. So, in this paper, we explored the lung nodule classification using the improvised 3D AlexNet with lightweight architecture. Our network employed the full nature of the multiview network strategy. We have conducted the binary classification (benign and malignant) on computed tomography (CT) images from the LUNA 16 database conglomerate and database image resource initiative. The results obtained are through the 10-fold cross-validation. Experimental results have shown that the proposed lightweight architecture achieved a superior classification accuracy of 97.17% on LUNA 16 dataset when compared with existing classification algorithms and low-dose CT scan images as well.",2021,10.1155/2021/6695518,diagnosis,True
3D deep learning based classification of pulmonary ground glass opacity nodules with automatic segmentation,"Classifying ground-glass lung nodules (GGNs) into atypical adenomatous hyperplasia (AAH), adenocarcinoma in situ (AIS), minimally invasive adenocarcinoma (MIA), and invasive adenocarcinoma (IAC) on diagnostic CT images is important to evaluate the therapy options for lung cancer patients. In this paper, we propose a joint deep learning model where the segmentation can better facilitate the classification of pulmonary GGNs. Based on our observation that masking the nodule to train the model results in better lesion classification, we propose to build a cascade architecture with both segmentation and classification networks. The segmentation model works as a trainable preprocessing module to provide the classification-guided 'attention' weight map to the raw CT data to achieve better diagnosis performance. We evaluate our proposed model and compare with other baseline models for 4 clinically significant nodule classification tasks, defined by a combination of pathology types, using 4 classification metrics: Accuracy, Average F1 Score, Matthews Correlation Coefficient (MCC), and Area Under the Receiver Operating Characteristic Curve (AUC). Experimental results show that the proposed method outperforms other baseline models on all the diagnostic classification tasks.",2021,10.1016/j.compmedimag.2020.101814,diagnosis,True
3D deep learning for detecting pulmonary nodules in CT scans,"OBJECTIVE: To demonstrate and test the validity of a novel deep-learning-based system for the automated detection of pulmonary nodules. MATERIALS AND METHODS: The proposed system uses 2 3D deep learning models, 1 for each of the essential tasks of computer-aided nodule detection: candidate generation and false positive reduction. A total of 888 scans from the LIDC-IDRI dataset were used for training and evaluation. RESULTS: Results for candidate generation on the test data indicated a detection rate of 94.77% with 30.39 false positives per scan, while the test results for false positive reduction exhibited a sensitivity of 94.21% with 1.789 false positives per scan. The overall system detection rate on the test data was 89.29% with 1.789 false positives per scan. DISCUSSION: An extensive and rigorous validation was conducted to assess the performance of the proposed system. The system demonstrated a novel combination of 3D deep neural network architectures and demonstrates the use of deep learning for both candidate generation and false positive reduction to be evaluated with a substantial test dataset. The results strongly support the ability of deep learning pulmonary nodule detection systems to generalize to unseen data. The source code and trained model weights have been made available. CONCLUSION: A novel deep-neural-network-based pulmonary nodule detection system is demonstrated and validated. The results provide comparison of the proposed deep-learning-based system over other similar systems based on performance.",2018,10.1093/jamia/ocy098,diagnosis,True
3D Deep Learning from CT Scans Predicts Tumor Invasiveness of Subcentimeter Pulmonary Adenocarcinomas,,2018,"10.1158/0008-5472.Can-18-0696: Identification of early-stage pulmonary adenocarcinomas before surgery, especially in cases of subcentimeter cancers, would be clinically important and could provide guidance to clinical decision making. In this study, we developed a deep learning system based on 3D convolutional neural networks and multitask learning, which automatically predicts tumor invasiveness, together with 3D nodule segmentation masks. The system processes a 3D nodule-centered patch of preprocessed CT and learns a deep representation of a given nodule without the need for any additional information. A dataset of 651 nodules with manually segmented voxel-wise masks and pathological labels of atypical adenomatous hyperplasia (AAH), adenocarcinomas in situ (AIS), minimally invasive adenocarcinoma (MIA), and invasive pulmonary adenocarcinoma (IA) was used in this study. We trained and validated our deep learning system on 523 nodules and tested its performance on 128 nodules. An observer study with 2 groups of radiologists, 2 senior and 2 junior, was also investigated. We merged AAH and AIS into one single category AAH-AIS, comprising a 3-category classification in our study. The proposed deep learning system achieved better classification performance than the radiologists; in terms of 3-class weighted average F1 score, the model achieved 63.3% while the radiologists achieved 55.6%, 56.6%, 54.3%, and 51.0%, respectively. These results suggest that deep learning methods improve the yield of discriminative results and hold promise in the CADx application domain, which could help doctors work efficiently and facilitate the application of precision medicine. SIGNIFICANCE: Machine learning tools are beginning to be implemented for clinical applications. This study represents an important milestone for this emerging technology, which could improve therapy selection for patients with lung cancer.",diagnosis,True
3D Kinect Camera Scheme with Time-Series Deep-Learning Algorithms for Classification and Prediction of Lung Tumor Motility,"This paper proposes a time-series deep-learning 3D Kinect camera scheme to classify the respiratory phases with a lung tumor and predict the lung tumor displacement. Specifically, the proposed scheme is driven by two time-series deep-learning algorithmic models: the respiratory-phase classification model and the regression-based prediction model. To assess the performance of the proposed scheme, the classification and prediction models were tested with four categories of datasets: patient-based datasets with regular and irregular breathing patterns; and pseudopatient-based datasets with regular and irregular breathing patterns. In this study, 'pseudopatients' refer to a dynamic thorax phantom with a lung tumor programmed with varying breathing patterns and breaths per minute. The total accuracy of the respiratory-phase classification model was 100%, 100%, 100%, and 92.44% for the four dataset categories, with a corresponding mean squared error (MSE), mean absolute error (MAE), and coefficient of determination (R(2)) of 1.2-1.6%, 0.65-0.8%, and 0.97-0.98, respectively. The results demonstrate that the time-series deep-learning classification and regression-based prediction models can classify the respiratory phases and predict the lung tumor displacement with high accuracy. Essentially, the novelty of this research lies in the use of a low-cost 3D Kinect camera with time-series deep-learning algorithms in the medical field to efficiently classify the respiratory phase and predict the lung tumor displacement.",2022,10.3390/s22082918,prognosis,True
3D multi-scale deep convolutional neural networks for pulmonary nodule detection,"With the rapid development of big data and artificial intelligence technology, computer-aided pulmonary nodule detection based on deep learning has achieved some successes. However, the sizes of pulmonary nodules vary greatly, and the pulmonary nodules have visual similarity with structures such as blood vessels and shadows around pulmonary nodules, which make the quick and accurate detection of pulmonary nodules in CT image still a challenging task. In this paper, we propose two kinds of 3D multi-scale deep convolution neural networks for nodule candidate detection and false positive reduction respectively. Among them, the nodule candidate detection network consists of two parts: 1) the backbone network part Res2SENet, which is used to extract multi-scale feature information of pulmonary nodules, it is composed of the multi-scale Res2Net modules of multiple available receptive fields at a granular level and the squeeze-and-excitation units; 2) the detection part, which uses a region proposal network structure to determine region candidates, and introduces context enhancement module and spatial attention module to improve detection performance. The false positive reduction network, also composed of the multi-scale Res2Net modules and the squeeze-and-excitation units, can further classify the nodule candidates generated by the nodule candidate detection network and screen out the ground truth positive nodules. Finally, the prediction probability generated by the nodule candidate detection network is weighted average with the prediction probability generated by the false positive reduction network to obtain the final results. The experimental results on the publicly available LUNA16 dataset showed that the proposed method has a superior ability to detect pulmonary nodules in CT images.",2021,10.1371/journal.pone.0244406,diagnosis,True
"3D multi-scale, multi-task, and multi-label deep learning for prediction of lymph node metastasis in T1 lung adenocarcinoma patients' CT images","The diagnosis of preoperative lymph node (LN) metastasis is crucial to evaluate possible therapy options for T1 lung adenocarcinoma patients. Radiologists preoperatively diagnose LN metastasis by evaluating signs related to LN metastasis, like spiculation or lobulation of pulmonary nodules in CT images. However, this type of evaluation is subjective and time-consuming, which may result in poor consistency and low efficiency of diagnoses. In this study, a 3D Multi-scale, Multi-task, and Multi-label classification network (3M-CN) was proposed to predict LN metastasis, as well as evaluate multiple related signs of pulmonary nodules in order to improve the accuracy of LN metastasis prediction. The following key approaches were adapted for this method. First, a multi-scale feature fusion module was proposed to aggregate the features from different levels for which different labels be best modeled at different levels; second, an auxiliary segmentation task was applied to force the model to focus more on the nodule region and less on surrounding unrelated structures; and third, a cross-modal integration module called the refine layer was designed to integrate the related risk factors into the model to further improve its confidence level. The 3M-CN was trained using data from 401 cases and then validated on both internal and external datasets, which consisted of 100 cases and 53 cases, respectively. The proposed 3M-CN model was then compared with existing state-of-the-art methods for prediction of LN metastasis. The proposed model outperformed other methods, achieving the best performance with AUCs of 0.945 and 0.948 in the internal and external test datasets, respectively. The proposed model not only obtain strong generalization, but greatly enhance the interpretability of the deep learning model, increase doctors' confidence in the model results, conform to doctors' diagnostic process, and may also be transferable to the diagnosis of other diseases.",2021,10.1016/j.compmedimag.2021.101987,prognosis,True
3D multi-view convolutional neural networks for lung nodule classification,"The 3D convolutional neural network (CNN) is able to make full use of the spatial 3D context information of lung nodules, and the multi-view strategy has been shown to be useful for improving the performance of 2D CNN in classifying lung nodules. In this paper, we explore the classification of lung nodules using the 3D multi-view convolutional neural networks (MV-CNN) with both chain architecture and directed acyclic graph architecture, including 3D Inception and 3D Inception-ResNet. All networks employ the multi-view-one-network strategy. We conduct a binary classification (benign and malignant) and a ternary classification (benign, primary malignant and metastatic malignant) on Computed Tomography (CT) images from Lung Image Database Consortium and Image Database Resource Initiative database (LIDC-IDRI). All results are obtained via 10-fold cross validation. As regards the MV-CNN with chain architecture, results show that the performance of 3D MV-CNN surpasses that of 2D MV-CNN by a significant margin. Finally, a 3D Inception network achieved an error rate of 4.59% for the binary classification and 7.70% for the ternary classification, both of which represent superior results for the corresponding task. We compare the multi-view-one-network strategy with the one-view-one-network strategy. The results reveal that the multi-view-one-network strategy can achieve a lower error rate than the one-view-one-network strategy.",2017,10.1371/journal.pone.0188290,diagnosis,True
3D virtual histopathology of cardiac tissue from Covid-19 patients based on phase-contrast X-ray tomography,"For the first time, we have used phase-contrast X-ray tomography to characterize the three-dimensional (3d) structure of cardiac tissue from patients who succumbed to Covid-19. By extending conventional histopathological examination by a third dimension, the delicate pathological changes of the vascular system of severe Covid-19 progressions can be analyzed, fully quantified and compared to other types of viral myocarditis and controls. To this end, cardiac samples with a cross-section of 3.5mm were scanned at a laboratory setup as well as at a parallel beam setup at a synchrotron radiation facility the synchrotron in a parallel beam configuration. The vascular network was segmented by a deep learning architecture suitable for 3d datasets (V-net), trained by sparse manual annotations. Pathological alterations of vessels, concerning the variation of diameters and the amount of small holes, were observed, indicative of elevated occurrence of intussusceptive angiogenesis, also confirmed by high-resolution cone beam X-ray tomography and scanning electron microscopy. Furthermore, we implemented a fully automated analysis of the tissue structure in the form of shape measures based on the structure tensor. The corresponding distributions show that the histopathology of Covid-19 differs from both influenza and typical coxsackie virus myocarditis.",2021,10.7554/eLife.71359,diagnosis,False
3D-MCN: A 3D Multi-scale Capsule Network for Lung Nodule Malignancy Prediction,"Despite the advances in automatic lung cancer malignancy prediction, achieving high accuracy remains challenging. Existing solutions are mostly based on Convolutional Neural Networks (CNNs), which require a large amount of training data. Most of the developed CNN models are based only on the main nodule region, without considering the surrounding tissues. Obtaining high sensitivity is challenging with lung nodule malignancy prediction. Moreover, the interpretability of the proposed techniques should be a consideration when the end goal is to utilize the model in a clinical setting. Capsule networks (CapsNets) are new and revolutionary machine learning architectures proposed to overcome shortcomings of CNNs. Capitalizing on the success of CapsNet in biomedical domains, we propose a novel model for lung tumor malignancy prediction. The proposed framework, referred to as the 3D Multi-scale Capsule Network (3D-MCN), is uniquely designed to benefit from: (i) 3D inputs, providing information about the nodule in 3D; (ii) Multi-scale input, capturing the nodule's local features, as well as the characteristics of the surrounding tissues, and; (iii) CapsNet-based design, being capable of dealing with a small number of training samples. The proposed 3D-MCN architecture predicted lung nodule malignancy with a high accuracy of 93.12%, sensitivity of 94.94%, area under the curve (AUC) of 0.9641, and specificity of 90% when tested on the LIDC-IDRI dataset. When classifying patients as having a malignant condition (i.e., at least one malignant nodule is detected) or not, the proposed model achieved an accuracy of 83%, and a sensitivity and specificity of 84% and 81% respectively.",2020,10.1038/s41598-020-64824-5,diagnosis,True
4D radiomics: impact of 4D-CBCT image quality on radiomic analysis,"PURPOSE: To investigate the impact of 4D-CBCT image quality on radiomic analysis and the efficacy of using deep learning based image enhancement to improve the accuracy of radiomic features of 4D-CBCT. MATERIAL AND METHODS: In this study, 4D-CT data from 16 lung cancer patients were obtained. Digitally reconstructed radiographs (DRRs) were simulated from the 4D-CT, and then used to reconstruct 4D CBCT using the conventional FDK (Feldkamp et al 1984 J. Opt. Soc. Am. A 1 612-9) algorithm. Different projection numbers (i.e. 72, 120, 144, 180) and projection angle distributions (i.e. evenly distributed and unevenly distributed using angles from real 4D-CBCT scans) were simulated to generate the corresponding 4D-CBCT. A deep learning model (TecoGAN) was trained on 10 patients and validated on 3 patients to enhance the 4D-CBCT image quality to match with the corresponding ground-truth 4D-CT. The remaining 3 patients with different tumor sizes were used for testing. The radiomic features in 6 different categories, including histogram, GLCM, GLRLM, GLSZM, NGTDM, and wavelet, were extracted from the gross tumor volumes of each phase of original 4D-CBCT, enhanced 4D-CBCT, and 4D-CT. The radiomic features in 4D-CT were used as the ground-truth to evaluate the errors of the radiomic features in the original 4D-CBCT and enhanced 4D-CBCT. Errors in the original 4D-CBCT demonstrated the impact of image quality on radiomic features. Comparison between errors in the original 4D-CBCT and enhanced 4D-CBCT demonstrated the efficacy of using deep learning to improve the radiomic feature accuracy. RESULTS: 4D-CBCT image quality can substantially affect the accuracy of the radiomic features, and the degree of impact is feature-dependent. The deep learning model was able to enhance the anatomical details and edge information in the 4D-CBCT as well as removing other image artifacts. This enhancement of image quality resulted in reduced errors for most radiomic features. The average reduction of radiomics errors for 3 patients are 20.0%, 31.4%, 36.7%, 50.0%, 33.6% and 11.3% for histogram, GLCM, GLRLM, GLSZM, NGTDM and Wavelet features. And the error reduction was more significant for patients with larger tumors. The findings were consistent across different respiratory phases, projection numbers, and angle distributions. CONCLUSIONS: The study demonstrated that 4D-CBCT image quality has a significant impact on the radiomic analysis. The deep learning-based augmentation technique proved to be an effective approach to enhance 4D-CBCT image quality to improve the accuracy of radiomic analysis.",2021,10.1088/1361-6560/abd668,diagnosis,True
4S-DT: Self-Supervised Super Sample Decomposition for Transfer Learning With Application to COVID-19 Detection,"Due to the high availability of large-scale annotated image datasets, knowledge transfer from pretrained models showed outstanding performance in medical image classification. However, building a robust image classification model for datasets with data irregularity or imbalanced classes can be a very challenging task, especially in the medical imaging domain. In this article, we propose a novel deep convolutional neural network, which we called self-supervised super sample decomposition for transfer learning (4S-DT) model. The 4S-DT encourages a coarse-to-fine transfer learning from large-scale image recognition tasks to a specific chest X-ray image classification task using a generic self-supervised sample decomposition approach. Our main contribution is a novel self-supervised learning mechanism guided by a super sample decomposition of unlabeled chest X-ray images. 4S-DT helps in improving the robustness of knowledge transformation via a downstream learning strategy with a class-decomposition (CD) layer to simplify the local structure of the data. The 4S-DT can deal with any irregularities in the image dataset by investigating its class boundaries using a downstream CD mechanism. We used 50000 unlabeled chest X-ray images to achieve our coarse-to-fine transfer learning with an application to COVID-19 detection, as an exemplar. The 4S-DT has achieved a high accuracy of 99.8% on the larger of the two datasets used in the experimental study and an accuracy of 97.54% on the smaller dataset, which was enriched by augmented images, out of which all real COVID-19 cases were detected.",2021,10.1109/tnnls.2021.3082015,diagnosis,False
A 2D-3D hybrid convolutional neural network for lung lobe auto-segmentation on standard slice thickness computed tomography of patients receiving radiotherapy,"BACKGROUND: Accurate segmentation of lung lobe on routine computed tomography (CT) images of locally advanced stage lung cancer patients undergoing radiotherapy can help radiation oncologists to implement lobar-level treatment planning, dose assessment and efficacy prediction. We aim to establish a novel 2D-3D hybrid convolutional neural network (CNN) to provide reliable lung lobe auto-segmentation results in the clinical setting. METHODS: We retrospectively collected and evaluated thorax CT scans of 105 locally advanced non-small-cell lung cancer (NSCLC) patients treated at our institution from June 2019 to August 2020. The CT images were acquired with 5 mm slice thickness. Two CNNs were used for lung lobe segmentation, a 3D CNN for extracting 3D contextual information and a 2D CNN for extracting texture information. Contouring quality was evaluated using six quantitative metrics and visual evaluation was performed to assess the clinical acceptability. RESULTS: For the 35 cases in the test group, Dice Similarity Coefficient (DSC) of all lung lobes contours exceeded 0.75, which met the pass criteria of the segmentation result. Our model achieved high performances with DSC as high as 0.9579, 0.9479, 0.9507, 0.9484, and 0.9003 for left upper lobe (LUL), left lower lobe (LLL), right upper lobe (RUL), right lower lobe (RLL), and right middle lobe (RML), respectively. The proposed model resulted in accuracy, sensitivity, and specificity of 99.57, 98.23, 99.65 for LUL; 99.6, 96.14, 99.76 for LLL; 99.67, 96.13, 99.81 for RUL; 99.72, 92.38, 99.83 for RML; 99.58, 96.03, 99.78 for RLL, respectively. Clinician's visual assessment showed that 164/175 lobe contours met the requirements for clinical use, only 11 contours need manual correction. CONCLUSIONS: Our 2D-3D hybrid CNN model achieved accurate automatic segmentation of lung lobes on conventional slice-thickness CT of locally advanced lung cancer patients, and has good clinical practicability.",2021,10.1186/s12938-021-00932-1,diagnosis,True
A 3-D Riesz-Covariance Texture Model for Prediction of Nodule Recurrence in Lung CT,"This paper proposes a novel imaging biomarker of lung cancer relapse from 3-D texture analysis of CT images. Three-dimensional morphological nodular tissue properties are described in terms of 3-D Riesz-wavelets. The responses of the latter are aggregated within nodular regions by means of feature covariances, which leverage rich intra- and inter-variations of the feature space dimensions. When compared to the classical use of the average for feature aggregation, feature covariances preserve spatial co-variations between features. The obtained Riesz-covariance descriptors lie on a manifold governed by Riemannian geometry allowing geodesic measurements and differentiations. The latter property is incorporated both into a kernel for support vector machines (SVM) and a manifold-aware sparse regularized classifier. The effectiveness of the presented models is evaluated on a dataset of 110 patients with non-small cell lung carcinoma (NSCLC) and cancer recurrence information. Disease recurrence within a timeframe of 12 months could be predicted with an accuracy of 81.3-82.7%. The anatomical location of recurrence could be discriminated between local, regional and distant failure with an accuracy of 78.3-93.3%. The obtained results open novel research perspectives by revealing the importance of the nodular regions used to build the predictive models.",2016,10.1109/tmi.2016.2591921,prognosis,True
A 3D image segmentation for lung cancer using V.Net architecture based deep convolutional networks,"Lung segmentation of chest CT scan is utilised to identify lung cancer and this step is also critical in other diagnostic pathways. Therefore, powerful algorithms to accomplish this accurate segmentation task are highly needed in the medical imaging domain, where the tumours are required to be segmented with the lung parenchyma. Also, the lung parenchyma needs to be detached from the tumour regions that are often confused with the lung tissue. Recently, lung semantic segmentation is more suitable to allocate each pixel in the image to a predefined class based on fully convolutional networks (FCNs). In this paper, CT cancer scans from the Task06_Lung database were applied to FCN that was inspired by V.Net architecture for efficiently selecting a region of interest (ROI) using the 3D segmentation. This lung database is segregated into 64 training images and 32 testing images. The proposed system is generalised by three steps including data preprocessing, data augmentation and neural network based on the V-Net model. Then, it was evaluated by dice score coefficient (DSC) to calculate the ratio of the segmented image and the ground truth image. This proposed system outperformed other previous schemes for 3D lung segmentation with an average DCS of 80% for ROI and 98% for surrounding lung tissues. Moreover, this system demonstrated that 3D views of lung tumours in CT images precisely carried tumour estimation and robust lung segmentation.",2021,10.1080/03091902.2021.1905895,diagnosis,True
A 3D Probabilistic Deep Learning System for Detection and Diagnosis of Lung Cancer Using Low-Dose CT Scans,"We introduce a new computer aided detection and diagnosis system for lung cancer screening with low-dose CT scans that produces meaningful probability assessments. Our system is based entirely on 3D convolutional neural networks and achieves state-of-the-art performance for both lung nodule detection and malignancy classification tasks on the publicly available LUNA16 and Kaggle Data Science Bowl challenges. While nodule detection systems are typically designed and optimized on their own, we find that it is important to consider the coupling between detection and diagnosis components. Exploiting this coupling allows us to develop an end-to-end system that has higher and more robust performance and eliminates the need for a nodule detection false positive reduction stage. Furthermore, we characterize model uncertainty in our deep learning systems, a first for lung CT analysis, and show that we can use this to provide well-calibrated classification probabilities for both nodule detection and patient malignancy diagnosis. These calibrated probabilities informed by model uncertainty can be used for subsequent risk-based decision making towards diagnostic interventions or disease treatments, as we demonstrate using a probability-based patient referral strategy to further improve our results.",2020,10.1109/tmi.2019.2947595,diagnosis,True
A 3D-CNN model with CT-based parametric response mapping for classifying COPD subjects,"Chronic obstructive pulmonary disease (COPD) is a respiratory disorder involving abnormalities of lung parenchymal morphology with different severities. COPD is assessed by pulmonary-function tests and computed tomography-based approaches. We introduce a new classification method for COPD grouping based on deep learning and a parametric-response mapping (PRM) method. We extracted parenchymal functional variables of functional small airway disease percentage (fSAD%) and emphysema percentage (Emph%) with an image registration technique, being provided as input parameters of 3D convolutional neural network (CNN). The integrated 3D-CNN and PRM (3D-cPRM) achieved a classification accuracy of 89.3% and a sensitivity of 88.3% in five-fold cross-validation. The prediction accuracy of the proposed 3D-cPRM exceeded those of the 2D model and traditional 3D CNNs with the same neural network, and was comparable to that of 2D pretrained PRM models. We then applied a gradient-weighted class activation mapping (Grad-CAM) that highlights the key features in the CNN learning process. Most of the class-discriminative regions appeared in the upper and middle lobes of the lung, consistent with the regions of elevated fSAD% and Emph% in COPD subjects. The 3D-cPRM successfully represented the parenchymal abnormalities in COPD and matched the CT-based diagnosis of COPD.",2021,10.1038/s41598-020-79336-5,diagnosis,True
A Bayesian approach to tissue-fraction estimation for oncological PET segmentation,"Tumor segmentation in oncological PET is challenging, a major reason being the partial-volume effects (PVEs) that arise due to low system resolution and finite voxel size. The latter results in tissue-fraction effects (TFEs), i.e. voxels contain a mixture of tissue classes. Conventional segmentation methods are typically designed to assign each image voxel as belonging to a certain tissue class. Thus, these methods are inherently limited in modeling TFEs. To address the challenge of accounting for PVEs, and in particular, TFEs, we propose a Bayesian approach to tissue-fraction estimation for oncological PET segmentation. Specifically, this Bayesian approach estimates the posterior mean of the fractional volume that the tumor occupies within each image voxel. The proposed method, implemented using a deep-learning-based technique, was first evaluated using clinically realistic 2D simulation studies with known ground truth, in the context of segmenting the primary tumor in PET images of patients with lung cancer. The evaluation studies demonstrated that the method accurately estimated the tumor-fraction areas and significantly outperformed widely used conventional PET segmentation methods, including a U-net-based method, on the task of segmenting the tumor. In addition, the proposed method was relatively insensitive to PVEs and yielded reliable tumor segmentation for different clinical-scanner configurations. The method was then evaluated using clinical images of patients with stage IIB/III non-small cell lung cancer from ACRIN 6668/RTOG 0235 multi-center clinical trial. Here, the results showed that the proposed method significantly outperformed all other considered methods and yielded accurate tumor segmentation on patient images with Dice similarity coefficient (DSC) of 0.82 (95% CI: 0.78, 0.86). In particular, the method accurately segmented relatively small tumors, yielding a high DSC of 0.77 for the smallest segmented cross-section of 1.30 cm(2). Overall, this study demonstrates the efficacy of the proposed method to accurately segment tumors in PET images.",2021,10.1088/1361-6560/ac01f4,diagnosis,False
A bilinear convolutional neural network for lung nodules classification on CT images,"PURPOSE: Lung cancer is the most frequent cancer worldwide and is the leading cause of cancer-related deaths. Its early detection and treatment at the stage of a lung nodule improve the prognosis. In this study was proposed a new classification approach named bilinear convolutional neural network (BCNN) for the classification of lung nodules on CT images. METHODS: Convolutional neural network (CNN) is considered as the leading model in deep learning and is highly recommended for the design of computer-aided diagnosis systems thanks to its promising results on medical image analysis. The proposed BCNN scheme consists of two-stream CNNs (VGG16 and VGG19) as feature extractors followed by a support vector machine (SVM) classifier for false positive reduction. Series of experiments are performed by introducing the bilinear vector features extracted from three BCNN combinations into various types of SVMs that we adopted instead of the original softmax to determine the most suitable classifier for our study. RESULTS: The method performance was evaluated on 3186 images from the public LUNA16 database. We found that the BCNN [VGG16, VGG19] combination with and without SVM surpassed the [VGG16]2 and [VGG19]2 architectures, achieved an accuracy rate of 91.99% against 91.84% and 90.58%, respectively, and an area under the curve (AUC) rate of 95.9% against 94.8% and 94%, respectively. CONCLUSION: The proposed method improved the outcomes of conventional CNN-based architectures and showed promising and satisfying results, compared to other works, with an affordable complexity. We believe that the proposed BCNN can be used as an assessment tool for radiologists to make a precise analysis of lung nodules and an early diagnosis of lung cancers.",2021,10.1007/s11548-020-02283-z,diagnosis,True
A biomarker basing on radiomics for the prediction of overall survival in non-small cell lung cancer patients,"BACKGROUND: This study aimed at predicting the survival status on non-small cell lung cancer patients with the phenotypic radiomics features obtained from the CT images. METHODS: A total of 186 patients' CT images were used for feature extraction via Pyradiomics. The minority group was balanced via SMOTE method. The final dataset was randomized into training set (n = 223) and validation set (n = 75) with the ratio of 3:1. Multiple random forest models were trained applying hyperparameters grid search with 10-fold cross-validation using precision or recall as evaluation standard. Then a decision threshold was searched on the selected model. The final model was evaluated through ROC curve and prediction accuracy. RESULTS: From those segmented images of 186 patients, 1218 features were obtained via feature extraction. The preferred model was selected with recall as evaluation standard and the optimal decision threshold was set 0.56. The model had a prediction accuracy of 89.33% and the AUC score was 0.9296. CONCLUSION: A hyperparameters tuning random forest classifier had greater performance in predicting the survival status of non-small cell lung cancer patients, which could be taken for an automated classifier promising to stratify patients.",2018,10.1186/s12931-018-0887-8,diagnosis,True
A CAD system for pulmonary nodule prediction based on deep three-dimensional convolutional neural networks and ensemble learning,"BACKGROUND: Detection of pulmonary nodules is an important aspect of an automatic detection system. Incomputer-aided diagnosis (CAD) systems, the ability to detect pulmonary nodules is highly important, which plays an important role in the diagnosis and early treatment of lung cancer. Currently, the detection of pulmonary nodules depends mainly on doctor experience, which varies. This paper aims to address the challenge of pulmonary nodule detection more effectively. METHODS: A method for detecting pulmonary nodules based on an improved neural network is presented in this paper. Nodules are clusters of tissue with a diameter of 3 mm to 30 mm in the pulmonary parenchyma. Because pulmonary nodules are similar to other lung structures and have a low density, false positive nodules often occur. Thus, our team proposed an improved convolutional neural network (CNN) framework to detect nodules. First, a nonsharpening mask is used to enhance the nodules in computed tomography (CT) images; then, CT images of 512×512 pixels are segmented into smaller images of 96×96 pixels. Second, in the 96×96 pixel images which contain or exclude pulmonary nodules, the plaques corresponding to positive and negative samples are segmented. Third, CT images segmented into 96×96 pixels are down-sampled to 64×64 and 32×32 size respectively. Fourth, an improved fusion neural network structure is constructed that consists of three three-dimensional convolutional neural networks, designated as CNN-1, CNN-2, and CNN-3, to detect false positive pulmonary nodules. The networks' input sizes are 32×32×32, 64×64×64, and 96×96×96 and include 5, 7, and 9 layers, respectively. Finally, we use the AdaBoost classifier to fuse the results of CNN-1, CNN-2, and CNN-3. We call this new neural network framework the Amalgamated-Convolutional Neural Network (A-CNN) and use it to detect pulmonary nodules. FINDINGS: Our team trained A-CNN using the LUNA16 and Ali Tianchi datasets and evaluated its performance using the LUNA16 dataset. We discarded nodules less than 5mm in diameter. When the average number of false positives per scan was 0.125 and 0.25, the sensitivity of A-CNN reached as high as 81.7% and 85.1%, respectively.",2019,10.1371/journal.pone.0219369,diagnosis,True
A cascade and heterogeneous neural network for CT pulmonary nodule detection and its evaluation on both phantom and patient data,"Screening of pulmonary nodules in computed tomography (CT) is crucial for early diagnosis and treatment of lung cancer. Although computer-aided diagnosis (CAD) systems have been designed to assist radiologists to detect nodules, fully automated detection is still challenging due to variations in nodule size, shape, and density. In this paper, we first propose a fully automated nodule detection method using a cascade and heterogeneous neural network trained on chest CT images of 12155 patients, then evaluate the performance by using phantom (828 CT images) and clinical datasets (2640 CT images) scanned with different imaging parameters. The nodule detection network employs two feature pyramid networks (FPNs) and a classification network (BasicNet). The first FPN is trained to achieve high sensitivity for nodule detection, and the second FPN refines the candidates for false positive reduction (FPR). Then, a BasicNet is combined with the second FPR to classify the candidates into either nodules or non-nodules for the final refinement. This study investigates the performance of nodule detection of solid and ground-glass nodules in phantom and patient data scanned with different imaging parameters. The results show that the detection of the solid nodules is robust to imaging parameters, and for GGO detection, reconstruction methods ""iDose4-YA"" and ""STD-YA"" achieve better performance. For thin-slice images, higher performance is achieved across different nodule sizes with reconstruction method ""iDose4-STD"". For 5 mm slice thickness, the best choice is the reconstruction method ""iDose4-YA"" for larger nodules (>5 mm). Overall, the reconstruction method ""iDose4-YA"" is suggested to achieve the best balanced results for both solid and GGO nodules.",2021,10.1016/j.compmedimag.2021.101889,diagnosis,True
A Cascade-SEME network for COVID-19 detection in chest x-ray images,"PURPOSE: The worldwide spread of the SARS-CoV-2 virus poses unprecedented challenges to medical resources and infection prevention and control measures around the world. In this case, a rapid and effective detection method for COVID-19 can not only relieve the pressure of the medical system but find and isolate patients in time, to a certain extent, slow down the development of the epidemic. In this paper, we propose a method that can quickly and accurately diagnose whether pneumonia is viral pneumonia, and classify viral pneumonia in a fine-grained way to diagnose COVID-19. METHODS: We proposed a Cascade Squeeze-Excitation and Moment Exchange (Cascade-SEME) framework that can effectively detect COVID-19 cases by evaluating the chest x-ray images, where SE is the structure we designed in the network which has attention mechanism, and ME is a method for image enhancement from feature dimension. The framework integrates a model for a coarse level detection of virus cases among other forms of lung infection, and a model for fine-grained categorisation of pneumonia types identifying COVID-19 cases. In addition, a Regional Learning approach is proposed to mitigate the impact of non-lesion features on network training. The network output is also visualised, highlighting the likely areas of lesion, to assist experts' assessment and diagnosis of COVID-19. RESULTS: Three datasets were used: a set of Chest x-ray Images for Classification with bacterial pneumonia, viral pneumonia and normal chest x-rays, a COVID chest x-ray dataset with COVID-19, and a Lung Segmentation dataset containing 1000 chest x-rays with masks in the lung region. We evaluated all the models on the test set. The results shows the proposed SEME structure significantly improves the performance of the models: in the task of pneumonia infection type diagnosis, the sensitivity, specificity, accuracy and F1 score of ResNet50 with SEME structure are significantly improved in each category, and the accuracy and AUC of the whole test set are also enhanced; in the detection task of COVID-19, the evaluation results shows that when SEME structure was added to the task, the sensitivities, specificities, accuracy and F1 scores of ResNet50 and DenseNet169 are improved. Although the sensitivities and specificities are not significantly promoted, SEME well balanced these two significant indicators. Regional learning also plays an important role. Experiments show that Regional Learning can effectively correct the impact of non-lesion features on the network, which can be seen in the Grad-CAM method. CONCLUSIONS: Experiments show that after the application of SEME structure in the network, the performance of SEME-ResNet50 and SEME-DenseNet169 in both two datasets show a clear enhancement. And the proposed regional learning method effectively directs the network's attention to focus on relevant pathological regions in the lung radiograph, ensuring the performance of the proposed framework even when a small training set is used. The visual interpretation step using Grad-CAM finds that the region of attention on radiographs of different types of pneumonia are located in different regions of the lungs.",2021,10.1002/mp.14711,diagnosis,False
A cascaded dual-pathway residual network for lung nodule segmentation in CT images,"It is difficult to obtain an accurate segmentation due to the variety of lung nodules in computed tomography (CT) images. In this study, we propose a data-driven model, called the Cascaded Dual-Pathway Residual Network (CDP-ResNet) to improve the segmentation of lung nodules in the CT images. Our approach incorporates the multi-view and multi-scale features of different nodules from CT images. The proposed residual block based dual-path network extracts local features and rich contextual information of lung nodules. In addition, we designed an improved weighted sampling strategy to select training samples based on the edge. The proposed method was extensively evaluated on an LIDC dataset, which contains 986 nodules. Experimental results show that the CDP-ResNet achieves superior segmentation performance with an average DICE score (standard deviation) of 81.58% (11.05) on the LIDC dataset. Moreover, we compared our results with those of four radiologists on the same dataset. The comparison shows that the CDP-ResNet is slightly better than human experts in terms of segmentation accuracy. Meanwhile, the proposed segmentation method outperforms existing methods.",2019,10.1016/j.ejmp.2019.06.003,diagnosis,True
A comparative analysis of eleven neural networks architectures for small datasets of lung images of COVID-19 patients toward improved clinical decisions,"The 2019 novel severe acute respiratory syndrome coronavirus 2-SARS-CoV2, commonly known as COVID-19, is a highly infectious disease that has endangered the health of many people around the world. COVID-19, which infects the lungs, is often diagnosed and managed using X-ray or computed tomography (CT) images. For such images, rapid and accurate classification and diagnosis can be performed using deep learning methods that are trained using existing neural network models. However, at present, there is no standardized method or uniform evaluation metric for image classification, which makes it difficult to compare the strengths and weaknesses of different neural network models. This paper used eleven well-known convolutional neural networks, including VGG-16, ResNet-18, ResNet-50, DenseNet-121, DenseNet-169, Inception-v3, Inception-v4, SqueezeNet, MobileNet, ShuffeNet, and EfficientNet-b0, to classify and distinguish COVID-19 and non-COVID-19 lung images. These eleven models were applied to different batch sizes and epoch cases, and their overall performance was compared and discussed. The results of this study can provide decision support in guiding research on processing and analyzing small medical datasets to understand which model choices can yield better outcomes in lung image classification, diagnosis, disease management and patient care.",2021,10.1016/j.compbiomed.2021.104887,diagnosis,True
A comparison between manual and artificial intelligence-based automatic positioning in CT imaging for COVID-19 patients,"OBJECTIVE: To analyze and compare the imaging workflow, radiation dose, and image quality for COVID-19 patients examined using either the conventional manual positioning (MP) method or an AI-based automatic positioning (AP) method. MATERIALS AND METHODS: One hundred twenty-seven adult COVID-19 patients underwent chest CT scans on a CT scanner using the same scan protocol except with the manual positioning (MP group) for the initial scan and an AI-based automatic positioning method (AP group) for the follow-up scan. Radiation dose, patient positioning time, and off-center distance of the two groups were recorded and compared. Image noise and signal-to-noise ratio (SNR) were assessed by three experienced radiologists and were compared between the two groups. RESULTS: The AP operation was successful for all patients in the AP group and reduced the total positioning time by 28% compared with the MP group. Compared with the MP group, the AP group had significantly less patient off-center distance (AP 1.56 cm ± 0.83 vs. MP 4.05 cm ± 2.40, p < 0.001) and higher proportion of positioning accuracy (AP 99% vs. MP 92%), resulting in 16% radiation dose reduction (AP 6.1 mSv ± 1.3 vs. MP 7.3 mSv ± 1.2, p < 0.001) and 9% image noise reduction in erector spinae and lower noise and higher SNR for lesions in the pulmonary peripheral areas. CONCLUSION: The AI-based automatic positioning and centering in CT imaging is a promising new technique for reducing radiation dose and optimizing imaging workflow and image quality in imaging the chest. KEY POINTS: • The AI-based automatic positioning (AP) operation was successful for all patients in our study. • AP method reduced the total positioning time by 28% compared with the manual positioning (MP). • AP method had less patient off-center distance and higher proportion of positioning accuracy than MP method, resulting in 16% radiation dose reduction and 9% image noise reduction in erector spinae.",2021,10.1007/s00330-020-07629-4,diagnosis,True
A comparison of machine learning methods for predicting recurrence and death after curative-intent radiotherapy for non-small cell lung cancer: Development and validation of multivariable clinical prediction models,"BACKGROUND: Surveillance is universally recommended for non-small cell lung cancer (NSCLC) patients treated with curative-intent radiotherapy. High-quality evidence to inform optimal surveillance strategies is lacking. Machine learning demonstrates promise in accurate outcome prediction for a variety of health conditions. The purpose of this study was to utilise readily available patient, tumour, and treatment data to develop, validate and externally test machine learning models for predicting recurrence, recurrence-free survival (RFS) and overall survival (OS) at 2 years from treatment. METHODS: A retrospective, multicentre study of patients receiving curative-intent radiotherapy for NSCLC was undertaken. A total of 657 patients from 5 hospitals were eligible for inclusion. Data pre-processing derived 34 features for predictive modelling. Combinations of 8 feature reduction methods and 10 machine learning classification algorithms were compared, producing risk-stratification models for predicting recurrence, RFS and OS. Models were compared with 10-fold cross validation and an external test set and benchmarked against TNM-stage and performance status. Youden Index was derived from validation set ROC curves to distinguish high and low risk groups and Kaplan-Meier analyses performed. FINDINGS: Median follow-up time was 852 days. Parameters were well matched across training-validation and external test sets: Mean age was 73 and 71 respectively, and recurrence, RFS and OS rates at 2 years were 43% vs 34%, 54% vs 47% and 54% vs 47% respectively. The respective validation and test set AUCs were as follows: 1) RFS: 0·682 (0·575-0·788) and 0·681 (0·597-0·766), 2) Recurrence: 0·687 (0·582-0·793) and 0·722 (0·635-0·81), and 3) OS: 0·759 (0·663-0·855) and 0·717 (0·634-0·8). Our models were superior to TNM stage and performance status in predicting recurrence and OS. INTERPRETATION: This robust and ready to use machine learning method, validated and externally tested, sets the stage for future clinical trials entailing quantitative personalised risk-stratification and surveillance following curative-intent radiotherapy for NSCLC. FUNDING: A full list of funding bodies that contributed to this study can be found in the Acknowledgements section.",2022,10.1016/j.ebiom.2022.103911,prognosis,True
A comparison of the fusion model of deep learning neural networks with human observation for lung nodule detection and classification,"OBJECTIVES: To compare the diagnostic performance of a newly developed artificial intelligence (AI) algorithm derived from the fusion of convolution neural networks (CNN) versus human observers in the estimation of malignancy risk in pulmonary nodules. METHODS: The study population consists of 158 nodules from 158 patients. All nodules (81 benign and 77 malignant) were determined to be malignant or benign by a radiologist based on pathologic assessment and/or follow-up imaging. Two radiologists and an AI platform analyzed the nodules based on the Lung-RADS classification. The two observers also noted the size, location, and morphologic features of the nodules. An intraclass correlation coefficient was calculated for both observers and the AI; ROC curve analysis was performed to determine diagnostic performances. RESULTS: Nodule size, presence of spiculation, and presence of fat were significantly different between the malignant and benign nodules (p < 0.001, for all three). Eighteen (11.3%) nodules were not detected and analyzed by the AI. Observer 1, observer 2, and the AI had an AUC of 0.917 ± 0.023, 0.870 ± 0.033, and 0.790 ± 0.037 in the ROC analysis of malignity probability, respectively. The observers were in almost perfect agreement for localization, nodule size, and lung-RADS classification [κ (95% CI)=0.984 (0.961-1.000), 0.978 (0.970-0.984), and 0.924 (0.878-0.970), respectively]. CONCLUSION: The performance of the fusion AI algorithm in estimating the risk of malignancy was slightly lower than the performance of the observers. Fusion AI algorithms might be applied in an assisting role, especially for inexperienced radiologists. ADVANCES IN KNOWLEDGE: In this study, we proposed a fusion model using four state-of-art object detectors for lung nodule detection and discrimination. The use of fusion of deep learning neural networks might be used in a supportive role for radiologists when interpreting lung nodule discrimination.",2021,10.1259/bjr.20210222,diagnosis,True
A comprehensive study on classification of COVID-19 on computed tomography with pretrained convolutional neural networks,"The use of imaging data has been reported to be useful for rapid diagnosis of COVID-19. Although computed tomography (CT) scans show a variety of signs caused by the viral infection, given a large amount of images, these visual features are difficult and can take a long time to be recognized by radiologists. Artificial intelligence methods for automated classification of COVID-19 on CT scans have been found to be very promising. However, current investigation of pretrained convolutional neural networks (CNNs) for COVID-19 diagnosis using CT data is limited. This study presents an investigation on 16 pretrained CNNs for classification of COVID-19 using a large public database of CT scans collected from COVID-19 patients and non-COVID-19 subjects. The results show that, using only 6 epochs for training, the CNNs achieved very high performance on the classification task. Among the 16 CNNs, DenseNet-201, which is the deepest net, is the best in terms of accuracy, balance between sensitivity and specificity, [Formula: see text] score, and area under curve. Furthermore, the implementation of transfer learning with the direct input of whole image slices and without the use of data augmentation provided better classification rates than the use of data augmentation. Such a finding alleviates the task of data augmentation and manual extraction of regions of interest on CT images, which are adopted by current implementation of deep-learning models for COVID-19 classification.",2020,10.1038/s41598-020-74164-z,diagnosis,True
A computer-aided diagnosis approach for emphysema recognition in chest radiography,"The purpose of this work is twofold: (i) to develop a CAD system for the assessment of emphysema by digital chest radiography and (ii) to test it against CT imaging. The system is based on the analysis of the shape of lung silhouette as imaged in standard chest examination. Postero-anterior and lateral views are processed to extract the contours of the lung fields automatically. Subsequently, the shape of lung silhouettes is described by polyline approximation and the computed feature-set processed by a neural network to estimate the probability of emphysema. Images of radiographic studies from 225 patients were collected and properly annotated to build an experimental dataset named EMPH. Each patient had undergone a standard two-views chest radiography and CT for diagnostic purposes. In addition, the images (247) from JSRT dataset were used to evaluate lung segmentation in postero-anterior view. System performances were assessed by: (i) analyzing the quality of the automatic segmentation of the lung silhouette against manual tracing and (ii) measuring the capabilities of emphysema recognition. As to step i, on JSRT dataset, we obtained overlap percentage (Ω) 92.7±3.3%, Dice Similarity Coefficient (DSC) 95.5±3.7% and average contour distance (ACD) 1.73±0.87 mm. On EMPH dataset we had Ω=93.1±2.9%, DSC=96.1±3.5% and ACD=1.62±0.92 mm, for the postero-anterior view, while we had Ω=94.5±4.6%, DSC=91.0±6.3% and ACD=2.22±0.86 mm, for the lateral view. As to step ii, accuracy of emphysema recognition was 95.4%, with sensitivity and specificity 94.5% and 96.1% respectively. According to experimental results our system allows reliable and inexpensive recognition of emphysema on digital chest radiography.",2013,10.1016/j.medengphy.2012.03.011,diagnosis,True
A computer-aided diagnosis system for the classification of COVID-19 and non-COVID-19 pneumonia on chest X-ray images by integrating CNN with sparse autoencoder and feed forward neural network,"Several infectious diseases have affected the lives of many people and have caused great dilemmas all over the world. COVID-19 was declared a pandemic caused by a newly discovered virus named Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) by the World Health Organisation in 2019. RT-PCR is considered the golden standard for COVID-19 detection. Due to the limited RT-PCR resources, early diagnosis of the disease has become a challenge. Radiographic images such as Ultrasound, CT scans, X-rays can be used for the detection of the deathly disease. Developing deep learning models using radiographic images for detecting COVID-19 can assist in countering the outbreak of the virus. This paper presents a computer-aided detection model utilizing chest X-ray images for combating the pandemic. Several pre-trained networks and their combinations have been used for developing the model. The method uses features extracted from pre-trained networks along with Sparse autoencoder for dimensionality reduction and a Feed Forward Neural Network (FFNN) for the detection of COVID-19. Two publicly available chest X-ray image datasets, consisting of 504 COVID-19 images and 542 non-COVID-19 images, have been combined to train the model. The method was able to achieve an accuracy of 0.9578 and an AUC of 0.9821, using the combination of InceptionResnetV2 and Xception. Experiments have proved that the accuracy of the model improves with the usage of sparse autoencoder as the dimensionality reduction technique.",2022,10.1016/j.compbiomed.2021.105134,diagnosis,False
A cross-modal 3D deep learning for accurate lymph node metastasis prediction in clinical stage T1 lung adenocarcinoma,"OBJECTIVES: The evaluation of lymph node (LN) status by radiologists based on preoperative computed tomography (CT) lacks high precision for early lung cancer patients; erroneous evaluations result in inappropriate therapeutic plans and increase the risk of complications. This study aims to develop a cross-modal 3D neural network based on CT images and prior clinical knowledge for accurate prediction of LN metastasis in clinical stage T1 lung adenocarcinoma. PATIENTS AND METHODS: Five hundred one lung adenocarcinoma patients with clinical stage T1 were enrolled. Data including: corresponding 3D nodule-centered patches of CT; prior clinical features; and pathological labels of LN status were obtained. We proposed a cross-modal deep learning system, which can successfully incorporate prior clinical knowledge and CT images into a 3D neural network to predict LN metastasis. We trained and validated our system with 401 cases and tested its performance with 100 cases. The result was compared with that of the logistic regression integration model, the single deep learning model without prior clinical knowledge integration, radiomics method, and manual evaluation by radiologists. RESULTS: The model proposed DensePriNet achieved an AUC of 0.926, which is significantly higher than the logistic regression integration model (0.904) single deep learning model (0.880), and radiomics method (0.891). The Matthews Correlation Coefficient (MCC) of DensePriNet (0.705) was significantly higher than manual classification by one senior radiologist (0.534) and one junior radiologist (0.416), respectively. CONCLUSION: The performance of the single deep learning method is significantly higher than the radiomics method and the radiologists, and integration of prior clinical knowledge into the deep learning model enhance the diagnostic precision of LN status and facilitate the application of precision medicine.",2020,10.1016/j.lungcan.2020.04.014,diagnosis,True
A CT-based deep learning model for subsolid pulmonary nodules to distinguish minimally invasive adenocarcinoma and invasive adenocarcinoma,"OBJECTIVE: To develop and validate a deep learning nomogram (DLN) model constructed from non-contrast computed tomography (CT) images for discriminating minimally invasive adenocarcinoma (MIA) from invasive adenocarcinoma (IAC) in patients with subsolid pulmonary nodules (SSPNs). MATERIALS AND METHODS: In total, 365 consecutive patients who presented with SSPNs and were pathologically diagnosed with MIA or IAC after surgery, were recruited from two medical institutions from 2016 to 2019. Deep learning features were selected from preoperative CT images using convolutional neural network. Deep learning signature (DLS) was developed via the least absolute shrinkage and selection operator (LASSO). New DLN integrating clinical variables, subjective CT findings, and DLS was constructed. The diagnostic efficiency and discriminative capability were analyzed using the receiver operating characteristic method and decision curve analysis (DCA). RESULTS: In total, 18 deep learning features with non-zero coefficients were enrolled to develop the DLS, which was statistically different between the MIA and IAC groups. Independent predictors of DLS and lobulated sharp were used to build the DLN. The areas under the curves of the DLN were 0.889 (95% confidence interval (CI): 0.824-0.936), 0.915 (95% CI: 0.846-0.959), and 0.914 (95% CI: 0.848-0.958) in the training, internal validation, and external validation cohorts, respectively. After stratification analysis and DCA, the DLN showed potential generalization ability. CONCLUSION: The DLN incorporating the DLS and subjective CT findings have strong potential to distinguish MIA from IAC in patients with SSPNs, and will facilitate the suitable treatment method selection for the management of SSPNs.",2021,10.1016/j.ejrad.2021.110041,diagnosis,True
A deep 3D residual CNN for false-positive reduction in pulmonary nodule detection,"PURPOSE: The automatic detection of pulmonary nodules using CT scans improves the efficiency of lung cancer diagnosis, and false-positive reduction plays a significant role in the detection. In this paper, we focus on the false-positive reduction task and propose an effective method for this task. METHODS: We construct a deep 3D residual CNN (convolution neural network) to reduce false-positive nodules from candidate nodules. The proposed network is much deeper than the traditional 3D CNNs used in medical image processing. Specifically, in the network, we design a spatial pooling and cropping (SPC) layer to extract multilevel contextual information of CT data. Moreover, we employ an online hard sample selection strategy in the training process to make the network better fit hard samples (e.g., nodules with irregular shapes). RESULTS: Our method is evaluated on 888 CT scans from the dataset of the LUNA16 Challenge. The free-response receiver operating characteristic (FROC) curve shows that the proposed method achieves a high detection performance. CONCLUSIONS: Our experiments confirm that our method is robust and that the SPC layer helps increase the prediction accuracy. Additionally, the proposed method can easily be extended to other 3D object detection tasks in medical image processing.",2018,10.1002/mp.12846,diagnosis,True
A deep learning algorithm using CT images to screen for Corona virus disease (COVID-19),"OBJECTIVE: The outbreak of Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-COV-2) has caused more than 26 million cases of Corona virus disease (COVID-19) in the world so far. To control the spread of the disease, screening large numbers of suspected cases for appropriate quarantine and treatment are a priority. Pathogenic laboratory testing is typically the gold standard, but it bears the burden of significant false negativity, adding to the urgent need of alternative diagnostic methods to combat the disease. Based on COVID-19 radiographic changes in CT images, this study hypothesized that artificial intelligence methods might be able to extract specific graphical features of COVID-19 and provide a clinical diagnosis ahead of the pathogenic test, thus saving critical time for disease control. METHODS: We collected 1065 CT images of pathogen-confirmed COVID-19 cases along with those previously diagnosed with typical viral pneumonia. We modified the inception transfer-learning model to establish the algorithm, followed by internal and external validation. RESULTS: The internal validation achieved a total accuracy of 89.5% with a specificity of 0.88 and sensitivity of 0.87. The external testing dataset showed a total accuracy of 79.3% with a specificity of 0.83 and sensitivity of 0.67. In addition, in 54 COVID-19 images, the first two nucleic acid test results were negative, and 46 were predicted as COVID-19 positive by the algorithm, with an accuracy of 85.2%. CONCLUSION: These results demonstrate the proof-of-principle for using artificial intelligence to extract radiological features for timely and accurate COVID-19 diagnosis. KEY POINTS: • The study evaluated the diagnostic performance of a deep learning algorithm using CT images to screen for COVID-19 during the influenza season. • As a screening method, our model achieved a relatively high sensitivity on internal and external CT image datasets. • The model was used to distinguish between COVID-19 and other typical viral pneumonia, both of which have quite similar radiologic characteristics.",2021,10.1007/s00330-021-07715-1,diagnosis,True
A deep learning approach to characterize 2019 coronavirus disease (COVID-19) pneumonia in chest CT images,"OBJECTIVES: To utilize a deep learning model for automatic detection of abnormalities in chest CT images from COVID-19 patients and compare its quantitative determination performance with radiological residents. METHODS: A deep learning algorithm consisted of lesion detection, segmentation, and location was trained and validated in 14,435 participants with chest CT images and definite pathogen diagnosis. The algorithm was tested in a non-overlapping dataset of 96 confirmed COVID-19 patients in three hospitals across China during the outbreak. Quantitative detection performance of the model was compared with three radiological residents with two experienced radiologists' reading reports as reference standard by assessing the accuracy, sensitivity, specificity, and F1 score. RESULTS: Of 96 patients, 88 had pneumonia lesions on CT images and 8 had no abnormities on CT images. For per-patient basis, the algorithm showed superior sensitivity of 1.00 (95% confidence interval (CI) 0.95, 1.00) and F1 score of 0.97 in detecting lesions from CT images of COVID-19 pneumonia patients. While for per-lung lobe basis, the algorithm achieved a sensitivity of 0.96 (95% CI 0.94, 0.98) and a slightly inferior F1 score of 0.86. The median volume of lesions calculated by algorithm was 40.10 cm(3). An average running speed of 20.3 s ± 5.8 per case demonstrated the algorithm was much faster than the residents in assessing CT images (all p < 0.017). The deep learning algorithm can also assist radiologists make quicker diagnosis (all p < 0.0001) with superior diagnostic performance. CONCLUSIONS: The algorithm showed excellent performance in detecting COVID-19 pneumonia on chest CT images compared with resident radiologists. KEY POINTS: • The higher sensitivity of deep learning model in detecting COVID-19 pneumonia were found compared with radiological residents on a per-lobe and per-patient basis. • The deep learning model improves diagnosis efficiency by shortening processing time. • The deep learning model can automatically calculate the volume of the lesions and whole lung.",2020,10.1007/s00330-020-07044-9,diagnosis,True
A deep learning approach using effective preprocessing techniques to detect COVID-19 from chest CT-scan and X-ray images,"Coronavirus disease-19 (COVID-19) is a severe respiratory viral disease first reported in late 2019 that has spread worldwide. Although some wealthy countries have made significant progress in detecting and containing this disease, most underdeveloped countries are still struggling to identify COVID-19 cases in large populations. With the rising number of COVID-19 cases, there are often insufficient COVID-19 diagnostic kits and related resources in such countries. However, other basic diagnostic resources often do exist, which motivated us to develop Deep Learning models to assist clinicians and radiologists to provide prompt diagnostic support to the patients. In this study, we have developed a deep learning-based COVID-19 case detection model trained with a dataset consisting of chest CT scans and X-ray images. A modified ResNet50V2 architecture was employed as deep learning architecture in the proposed model. The dataset utilized to train the model was collected from various publicly available sources and included four class labels: confirmed COVID-19, normal controls and confirmed viral and bacterial pneumonia cases. The aggregated dataset was preprocessed through a sharpening filter before feeding the dataset into the proposed model. This model attained an accuracy of 96.452% for four-class cases (COVID-19/Normal/Bacterial pneumonia/Viral pneumonia), 97.242% for three-class cases (COVID-19/Normal/Bacterial pneumonia) and 98.954% for two-class cases (COVID-19/Viral pneumonia) using chest X-ray images. The model acquired a comprehensive accuracy of 99.012% for three-class cases (COVID-19/Normal/Community-acquired pneumonia) and 99.99% for two-class cases (Normal/COVID-19) using CT-scan images of the chest. This high accuracy presents a new and potentially important resource to enable radiologists to identify and rapidly diagnose COVID-19 cases with only basic but widely available equipment.",2021,10.1016/j.compbiomed.2021.105014,diagnosis,True
A deep learning integrated radiomics model for identification of coronavirus disease 2019 using computed tomography,"Since its first outbreak, Coronavirus Disease 2019 (COVID-19) has been rapidly spreading worldwide and caused a global pandemic. Rapid and early detection is essential to contain COVID-19. Here, we first developed a deep learning (DL) integrated radiomics model for end-to-end identification of COVID-19 using CT scans and then validated its clinical feasibility. We retrospectively collected CT images of 386 patients (129 with COVID-19 and 257 with other community-acquired pneumonia) from three medical centers to train and externally validate the developed models. A pre-trained DL algorithm was utilized to automatically segment infected lesions (ROIs) on CT images which were used for feature extraction. Five feature selection methods and four machine learning algorithms were utilized to develop radiomics models. Trained with features selected by L1 regularized logistic regression, classifier multi-layer perceptron (MLP) demonstrated the optimal performance with AUC of 0.922 (95% CI 0.856-0.988) and 0.959 (95% CI 0.910-1.000), the same sensitivity of 0.879, and specificity of 0.900 and 0.887 on internal and external testing datasets, which was equivalent to the senior radiologist in a reader study. Additionally, diagnostic time of DL-MLP was more efficient than radiologists (38 s vs 5.15 min). With an adequate performance for identifying COVID-19, DL-MLP may help in screening of suspected cases.",2021,10.1038/s41598-021-83237-6,diagnosis,True
A Deep Learning Prognosis Model Help Alert for COVID-19 Patients at High-Risk of Death: A Multi-Center Study,"Since its outbreak in December 2019, the persistent coronavirus disease (COVID-19) became a global health emergency. It is imperative to develop a prognostic tool to identify high-risk patients and assist in the formulation of treatment plans. We retrospectively collected 366 severe or critical COVID-19 patients from four centers, including 70 patients who died within 14 days (labeled as high-risk patients) since their initial CT scan and 296 who survived more than 14 days or were cured (labeled as low-risk patients). We developed a 3D densely connected convolutional neural network (termed De-COVID19-Net) to predict the probability of COVID-19 patients belonging to the high-risk or low-risk group, combining CT and clinical information. The area under the curve (AUC) and other evaluation techniques were used to assess our model. The De-COVID19-Net yielded an AUC of 0.952 (95% confidence interval, 0.928-0.977) on the training set and 0.943 (0.904-0.981) on the test set. The stratified analyses indicated that our model's performance is independent of age, sex, and with/without chronic diseases. The Kaplan-Meier analysis revealed that our model could significantly categorize patients into high-risk and low-risk groups (p < 0.001). In conclusion, De-COVID19-Net can non-invasively predict whether a patient will die shortly based on the patient's initial CT scan with an impressive performance, which indicated that it could be used as a potential prognosis tool to alert high-risk patients and intervene in advance.",2020,10.1109/jbhi.2020.3034296,prognosis,True
A Deep Learning Radiomics Model to Identify Poor Outcome in COVID-19 Patients With Underlying Health Conditions: A Multicenter Study,"OBJECTIVE: Coronavirus disease 2019 (COVID-19) has caused considerable morbidity and mortality, especially in patients with underlying health conditions. A precise prognostic tool to identify poor outcomes among such cases is desperately needed. METHODS: Total 400 COVID-19 patients with underlying health conditions were retrospectively recruited from 4 centers, including 54 dead cases (labeled as poor outcomes) and 346 patients discharged or hospitalized for at least 7 days since initial CT scan. Patients were allocated to a training set (n = 271), a test set (n = 68), and an external test set (n = 61). We proposed an initial CT-derived hybrid model by combining a 3D-ResNet10 based deep learning model and a quantitative 3D radiomics model to predict the probability of COVID-19 patients reaching poor outcome. The model performance was assessed by area under the receiver operating characteristic curve (AUC), survival analysis, and subgroup analysis. RESULTS: The hybrid model achieved AUCs of 0.876 (95% confidence interval: 0.752-0.999) and 0.864 (0.766-0.962) in test and external test sets, outperforming other models. The survival analysis verified the hybrid model as a significant risk factor for mortality (hazard ratio, 2.049 [1.462-2.871], P < 0.001) that could well stratify patients into high-risk and low-risk of reaching poor outcomes (P < 0.001). CONCLUSION: The hybrid model that combined deep learning and radiomics could accurately identify poor outcomes in COVID-19 patients with underlying health conditions from initial CT scans. The great risk stratification ability could help alert risk of death and allow for timely surveillance plans.",2021,10.1109/jbhi.2021.3076086,prognosis,True
A deep learning- and CT image-based prognostic model for the prediction of survival in non-small cell lung cancer,"OBJECTIVE: To assist clinicians in arranging personalized treatment, planning follow-up programs and extending survival times for non-small cell lung cancer (NSCLC) patients, a method of deep learning combined with computed tomography (CT) imaging for survival prediction was designed. METHODS: Data were collected from 484 patients from four research centers. The data from 344 patients were utilized to build the A_CNN survival prognosis model to classify 2-year overall survival time ranges (730 days cut-off). Data from 140 patients, including independent internal and external test sets, were utilized for model testing. First, a series of preprocessing techniques were used to process the original CT images and generate training and test data sets from the axial, coronal, and sagittal planes. Second, the structure of the A_CNN model was designed based on asymmetric convolution, bottleneck blocks, the uniform cross-entropy (UC) loss function, and other advanced techniques. After that, the A_CNN model was trained, and numerous comparative experiments were designed to obtain the best prognostic survival model. Last, the model performance was evaluated, and the predicted survival curves were analyzed. RESULTS: The A_CNN survival prognosis model yielded a high patient-level accuracy of 88.8%, a patch-level accuracy of 82.9%, and an area under the receiver operating characteristic (ROC) curve (AUC) of 0.932. When tested on an external data set, the maximum patient-level accuracy was 80.0%. CONCLUSIONS: The results suggest that using a deep learning method can improve prognosis in patients with NSCLC and has important application value in establishing individualized prognostic models.",2021,10.1002/mp.15302,prognosis,True
A deep learning-based dual-omics prediction model for radiation pneumonitis,"PURPOSE: Radiation pneumonitis (RP) is the main source of toxicity in thoracic radiotherapy. This study proposed a deep learning-based dual-omics model, which aims to improve the RP prediction performance by integrating more data points and exploring the data in greater depth. MATERIALS AND METHODS: The bimodality data were the original dose (OD) distribution and the ventilation image (VI) derived from four-dimensional computed tomography (4DCT). The functional dose (FD) distribution was obtained by weighting OD with VI. A pre-trained three-dimensional convolution (C3D) network was used to extract the features from FD, VI, and OD. The extracted features were then filtered and selected using entropy-based methods. The prediction models were constructed with four most commonly used binary classifiers. Cross-validation, bootstrap, and nested sampling methods were adopted in the process of training and hyper-tuning. RESULTS: Data from 217 thoracic cancer patients treated with radiotherapy were used to train and validate the prediction model. The 4DCT-based VI showed the inhomogeneous pulmonary function of the lungs. More than half of the extracted features were singular (of none-zero value for few patients), which were eliminated to improve the stability of the model. The area under curve (AUC) of the dual-omics model was 0.874 (95% confidence interval: 0.871-0.877), and the AUC of the single-omics model was 0.780 (0.775-0.785, VI) and 0.810 (0.804-0.811, OD), respectively. CONCLUSIONS: The dual-omics outperformed single-omics for RP prediction, which can be contributed to: (1) using more data points; (2) exploring the data in greater depth; and (3) incorporating of the bimodality data.",2021,10.1002/mp.15079,diagnosis,True
A deep residual learning network for predicting lung adenocarcinoma manifesting as ground-glass nodule on CT images,"OBJECTIVE: To develop a deep learning-based artificial intelligence (AI) scheme for predicting the likelihood of the ground-glass nodule (GGN) detected on CT images being invasive adenocarcinoma (IA) and also compare the accuracy of this AI scheme with that of two radiologists. METHODS: First, we retrospectively collected 828 histopathologically confirmed GGNs of 644 patients from two centers. Among them, 209 GGNs are confirmed IA and 619 are non-IA, including 409 adenocarcinomas in situ and 210 minimally invasive adenocarcinomas. Second, we applied a series of pre-preprocessing techniques, such as image resampling, rescaling and cropping, and data augmentation, to process original CT images and generate new training and testing images. Third, we built an AI scheme based on a deep convolutional neural network by using a residual learning architecture and batch normalization technique. Finally, we conducted an observer study and compared the prediction performance of the AI scheme with that of two radiologists using an independent dataset with 102 GGNs. RESULTS: The new AI scheme yielded an area under the receiver operating characteristic curve (AUC) of 0.92 ± 0.03 in classifying between IA and non-IA GGNs, which is equivalent to the senior radiologist's performance (AUC 0.92 ± 0.03) and higher than the score of the junior radiologist (AUC 0.90 ± 0.03). The Kappa value of two sets of subjective prediction scores generated by two radiologists is 0.6. CONCLUSIONS: The study result demonstrates using an AI scheme to improve the performance in predicting IA, which can help improve the development of a more effective personalized cancer treatment paradigm. KEY POINTS: • The feasibility of using a deep learning method to predict the likelihood of the ground-glass nodule being invasive adenocarcinoma. • Residual learning-based CNN model improves the performance in classifying between IA and non-IA nodules. • Artificial intelligence (AI) scheme yields higher performance than radiologists in predicting invasive adenocarcinoma.",2020,10.1007/s00330-019-06533-w,diagnosis,True
"A deep-learning pipeline for the diagnosis and discrimination of viral, non-viral and COVID-19 pneumonia from chest X-ray images","Common lung diseases are first diagnosed using chest X-rays. Here, we show that a fully automated deep-learning pipeline for the standardization of chest X-ray images, for the visualization of lesions and for disease diagnosis can identify viral pneumonia caused by coronavirus disease 2019 (COVID-19) and assess its severity, and can also discriminate between viral pneumonia caused by COVID-19 and other types of pneumonia. The deep-learning system was developed using a heterogeneous multicentre dataset of 145,202 images, and tested retrospectively and prospectively with thousands of additional images across four patient cohorts and multiple countries. The system generalized across settings, discriminating between viral pneumonia, other types of pneumonia and the absence of disease with areas under the receiver operating characteristic curve (AUCs) of 0.94-0.98; between severe and non-severe COVID-19 with an AUC of 0.87; and between COVID-19 pneumonia and other viral or non-viral pneumonia with AUCs of 0.87-0.97. In an independent set of 440 chest X-rays, the system performed comparably to senior radiologists and improved the performance of junior radiologists. Automated deep-learning systems for the assessment of pneumonia could facilitate early intervention and provide support for clinical decision-making.",2021,10.1038/s41551-021-00704-1,diagnosis,False
A depthwise separable dense convolutional network with convolution block attention module for COVID-19 diagnosis on CT scans,"Coronavirus disease 2019 (COVID-19) has caused more than 3 million deaths and infected more than 170 million individuals all over the world. Rapid identification of patients with COVID-19 is the key to control transmission and prevent depletion of hospitals. Several networks have been proposed to assist radiologists in diagnosing COVID-19 based on CT scans. However, CTs used in these studies are unavailable for other researchers to do deeper extensions due to privacy concerns. Furthermore, these networks are too heavy-weighted to satisfy the general trend applying on a computationally limited platform. In this paper, we aim to solve these two problems. Firstly, we establish an available dataset COVID-CTx, which contains 828 CT scans positive for COVID-19 across 324 patient cases from three open access data repositories. To our knowledge, it has the largest number of publicly available COVID-19 positive cases compared to other public datasets. Secondly, we propose a light-weighted hybrid neural network: Depthwise Separable Dense Convolutional Network with Convolution Block Attention Module (AM-SdenseNet). AM-SdenseNet synergistically integrates Convolutional Block Attention Module with depthwise separable convolutions to learn powerful feature representations while reducing the parameters to overcome the overfitting problem. Through experiments, we demonstrate the superior performance of our proposed AM-SdenseNet compared with several state-of-the-art baselines. The excellent performance of AM-SdenseNet can improve the speed and accuracy of COVID-19 diagnosis, which is extremely useful to control the spreading of infection.",2021,10.1016/j.compbiomed.2021.104837,diagnosis,True
A fast neural network approach to predict lung tumor motion during respiration for radiation therapy applications,"During radiotherapy treatment for thoracic and abdomen cancers, for example, lung cancers, respiratory motion moves the target tumor and thus badly affects the accuracy of radiation dose delivery into the target. A real-time image-guided technique can be used to monitor such lung tumor motion for accurate dose delivery, but the system latency up to several hundred milliseconds for repositioning the radiation beam also affects the accuracy. In order to compensate the latency, neural network prediction technique with real-time retraining can be used. We have investigated real-time prediction of 3D time series of lung tumor motion on a classical linear model, perceptron model, and on a class of higher-order neural network model that has more attractive attributes regarding its optimization convergence and computational efficiency. The implemented static feed-forward neural architectures are compared when using gradient descent adaptation and primarily the Levenberg-Marquardt batch algorithm as the ones of the most common and most comprehensible learning algorithms. The proposed technique resulted in fast real-time retraining, so the total computational time on a PC platform was equal to or even less than the real treatment time. For one-second prediction horizon, the proposed techniques achieved accuracy less than one millimeter of 3D mean absolute error in one hundred seconds of total treatment time.",2015,10.1155/2015/489679,treatment,True
A fully automated noncontrast CT 3-D reconstruction algorithm enabled accurate anatomical demonstration for lung segmentectomy,"BACKGROUND: Three-dimensional reconstruction of chest computerized tomography (CT) excels in intuitively demonstrating anatomical patterns for pulmonary segmentectomy. However, current methods are labor-intensive and rely on contrast CT. We hereby present a novel fully automated reconstruction algorithm based on noncontrast CT and assess its performance both independently and in combination with surgeons. METHODS: A retrospective pilot study was performed. Patients between May 2020 to August 2020 who underwent segmentectomy in our single institution were enrolled. Noncontrast CTs were used for reconstruction. In the first part of the study, the accuracy of the demonstration of anatomical variants by either automated or manual reconstruction algorithm were compared to surgical observation, respectively. In the second part of the study, we tested the accuracy of the identification of anatomical variants by four independent attendees who reviewed 3-D reconstruction in combination with CT scans. RESULTS: A total of 20 cases were enrolled in this study. All segments were represented in this study with two left S1-3, two left S4 + 5, one left S6, five left basal segmentectomies, one right S1, three right S2, 1 right S2b + 3a, one right S3, two right S6 and two right basal segmentectomies. The median time consumption for the automated reconstruction was 280 (205-324) s. Accurate vessel and bronchial detection were achieved in 85% by the AI approach and 80% by Mimics, p = 1.00. The accuracy of vessel classification was 80 and 95% by AI and manual approaches, respectively, p = 0.34. In real-world application, the accuracy of the identification of anatomical variant by thoracic surgeons was 85% by AI+CT, and the median time consumption was 2 (1-3) min. CONCLUSIONS: The AI reconstruction algorithm overcame defects of traditional methods and is valuable in surgical planning for segmentectomy. With the AI reconstruction, surgeons may achieve high identification accuracy of anatomical patterns in a short time frame.",2022,10.1111/1759-7714.14322,treatment,True
A fully automatic deep learning system for COVID-19 diagnostic and prognostic analysis,"Coronavirus disease 2019 (COVID-19) has spread globally, and medical resources become insufficient in many regions. Fast diagnosis of COVID-19 and finding high-risk patients with worse prognosis for early prevention and medical resource optimisation is important. Here, we proposed a fully automatic deep learning system for COVID-19 diagnostic and prognostic analysis by routinely used computed tomography.We retrospectively collected 5372 patients with computed tomography images from seven cities or provinces. Firstly, 4106 patients with computed tomography images were used to pre-train the deep learning system, making it learn lung features. Following this, 1266 patients (924 with COVID-19 (471 had follow-up for >5 days) and 342 with other pneumonia) from six cities or provinces were enrolled to train and externally validate the performance of the deep learning system.In the four external validation sets, the deep learning system achieved good performance in identifying COVID-19 from other pneumonia (AUC 0.87 and 0.88, respectively) and viral pneumonia (AUC 0.86). Moreover, the deep learning system succeeded to stratify patients into high- and low-risk groups whose hospital-stay time had significant difference (p=0.013 and p=0.014, respectively). Without human assistance, the deep learning system automatically focused on abnormal areas that showed consistent characteristics with reported radiological findings.Deep learning provides a convenient tool for fast screening of COVID-19 and identifying potential high-risk patients, which may be helpful for medical resource optimisation and early prevention before patients show severe symptoms.",2020,10.1183/13993003.00775-2020,diagnosis,True
A Generalized Deep Learning-Based Diagnostic System for Early Diagnosis of Various Types of Pulmonary Nodules,"A novel framework for the classification of lung nodules using computed tomography scans is proposed in this article. To get an accurate diagnosis of the detected lung nodules, the proposed framework integrates the following 2 groups of features: (1) appearance features modeled using the higher order Markov Gibbs random field model that has the ability to describe the spatial inhomogeneities inside the lung nodule and (2) geometric features that describe the shape geometry of the lung nodules. The novelty of this article is to accurately model the appearance of the detected lung nodules using a new developed seventh-order Markov Gibbs random field model that has the ability to model the existing spatial inhomogeneities for both small and large detected lung nodules, in addition to the integration with the extracted geometric features. Finally, a deep autoencoder classifier is fed by the above 2 feature groups to distinguish between the malignant and benign nodules. To evaluate the proposed framework, we used the publicly available data from the Lung Image Database Consortium. We used a total of 727 nodules that were collected from 467 patients. The proposed system demonstrates the promise to be a valuable tool for the detection of lung cancer evidenced by achieving a nodule classification accuracy of 91.20%.",2018,10.1177/1533033818798800,diagnosis,True
A homological approach to a mathematical definition of pulmonary fibrosis and emphysema on computed tomography,"Three-dimensional imaging is essential to evaluate local abnormalities and understand structure-function relationships in an organ. However, quantifiable and interpretable methods to localize abnormalities remain unestablished. Visual assessments are prone to bias, machine learning methods depend on training images, and the underlying decision principle is usually difficult to interpret. Here, we developed a homological approach to mathematically define emphysema and fibrosis in the lungs on computed tomography (CT). With the use of persistent homology, the density of homological features, including connected components, tunnels, and voids, was extracted from the volumetric CT scans of lung diseases. A pair of CT values at which each homological feature appeared (birth) and disappeared (death) was computed by sweeping the threshold levels from higher to lower CT values. Consequently, fibrosis and emphysema were defined as voxels with dense voids having a longer lifetime (birth-death difference) and voxels with dense connected components having a lower birth, respectively. In an independent dataset including subjects with idiopathic pulmonary fibrosis (IPF), chronic obstructive pulmonary disease (COPD), and combined pulmonary fibrosis and emphysema (CPFE), the proposed definition enabled accurate segmentation with comparable quality to deep learning in terms of Dice coefficients. Persistent homology-defined fibrosis was closely associated with physiological abnormalities such as impaired diffusion capacity and long-term mortality in subjects with IPF and CPFE, and persistent homology-defined emphysema was associated with impaired diffusion capacity in subjects with COPD. The present persistent homology-based evaluation of structural abnormalities could help explore the clinical and physiological impacts of structural changes and morphological mechanisms of disease progression.NEW & NOTEWORTHY This study proposes a homological approach to mathematically define a three-dimensional texture feature of emphysema and fibrosis on chest computed tomography using persistent homology. The proposed definition enabled accurate segmentation with comparable quality to deep learning while offering higher interpretability than deep learning-based methods.",2021,10.1152/japplphysiol.00150.2021,diagnosis,True
A human-computer collaboration for COVID-19 differentiation: combining a radiomics model with deep learning and human auditing,"BACKGROUND: This study aimed to build a radiomics model with deep learning (DL) and human auditing and examine its diagnostic value in differentiating between coronavirus disease 2019 (COVID-19) and community-acquired pneumonia (CAP). METHODS: Forty-three COVID-19 patients, whose diagnoses had been confirmed with reverse-transcriptase polymerase-chain-reaction (RT-PCR) tests, and 60 CAP patients, whose diagnoses had been confirmed with sputum cultures, were enrolled in this retrospective study. The candidate regions of interest (ROIs) on the computed tomography (CT) images of the 103 patients were determined using a DL-based segmentation model powered by transfer learning. These ROIs were manually audited and corrected by 3 radiologists (with an average of 12 years of experience; range 6-17 years) to check the segmentation acceptance for the radiomics analysis. ROI-derived radiomics features were subsequently extracted to build the classification model and processed using 4 different algorithms (L1 regularization, Lasso, Ridge, and Z test) and 4 classifiers, including the logistic regression (LR), multi-layer perceptron (MLP), support vector machine (SVM), and extreme Gradient Boosting (XGboost). A receiver operating characteristic curve (ROC) analysis was conducted to evaluate the performance of the model. RESULTS: Quantitative CT measurements derived from human-audited segmentation results showed that COVID-19 patients had significantly decreased numbers of infected lobes compared to patients in the CAP group {median [interquartile range (IQR)]: 4 [3, 4] and 4 [4, 5]; P=0.031}. The infected percentage (%) of the whole lung was significantly more elevated in the CAP group [6.40 (2.77, 11.11)] than the COVID-19 group [1.83 (0.65, 4.42); P<0.001], and the same trend applied to each lobe, except for the superior lobe of the right lung [1.81 (0.09, 5.28) for COVID-19 vs. 1.32 (0.14, 7.02) for CAP; P=0.649]. Additionally, the highest proportion of infected lesions were observed in the CT value range of (-470, -370) Hounsfield units (HU) in the COVID-19 group. Conversely, the CAP group had a value range of (30, 60) HU. Radiomic model using corrected ROIs exhibited the highest area under ROC (AUC) of 0.990 [95% confidence interval (CI): 0.962-1.000] using Lasso for feature selection and MLP for classification. CONCLUSIONS: The proposed radiomics model based on human-audited segmentation made accurate differential diagnoses of COVID-19 and CAP. The quantification of CT measurements derived from DL could potentially be used as effective biomarkers in current clinical practice.",2021,10.21037/apm-20-2625,diagnosis,True
A hybrid CNN feature model for pulmonary nodule malignancy risk differentiation,"The malignancy risk differentiation of pulmonary nodule is one of the most challenge tasks of computer-aided diagnosis (CADx). Most recently reported CADx methods or schemes based on texture and shape estimation have shown relatively satisfactory on differentiating the risk level of malignancy among the nodules detected in lung cancer screening. However, the existing CADx schemes tend to detect and analyze characteristics of pulmonary nodules from a statistical perspective according to local features only. Enlightened by the currently prevailing learning ability of convolutional neural network (CNN), which simulates human neural network for target recognition and our previously research on texture features, we present a hybrid model that takes into consideration of both global and local features for pulmonary nodule differentiation using the largest public database founded by the Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI). By comparing three types of CNN models in which two of them were newly proposed by us, we observed that the multi-channel CNN model yielded the best discrimination in capacity of differentiating malignancy risk of the nodules based on the projection of distributions of extracted features. Moreover, CADx scheme using the new multi-channel CNN model outperformed our previously developed CADx scheme using the 3D texture feature analysis method, which increased the computed area under a receiver operating characteristic curve (AUC) from 0.9441 to 0.9702.",2018,10.3233/xst-17302,diagnosis,True
A Hybrid Convolutional Neural Network Model for Diagnosis of COVID-19 Using Chest X-ray Images,"COVID-19 declared as a pandemic that has a faster rate of infection and has impacted the lives and the country's economy due to forced lockdowns. Its detection using RT-PCR is required long time and due to which its infection has grown exponentially. This creates havoc for the shortage of testing kits in many countries. This work has proposed a new image processing-based technique for the health care systems named ""C19D-Net"", to detect ""COVID-19"" infection from ""Chest X-Ray"" (XR) images, which can help radiologists to improve their accuracy of detection COVID-19. The proposed system extracts deep learning (DL) features by applying the InceptionV4 architecture and Multiclass SVM classifier to classify and detect COVID-19 infection into four different classes. The dataset of 1900 Chest XR images has been collected from two publicly accessible databases. Images are pre-processed with proper scaling and regular feeding to the proposed model for accuracy attainments. Extensive tests are conducted with the proposed model (""C19D-Net"") and it has succeeded to achieve the highest COVID-19 detection accuracy as 96.24% for 4-classes, 95.51% for three-classes, and 98.1% for two-classes. The proposed method has outperformed well in expressions of ""precision"", ""accuracy"", ""F1-score"" and ""recall"" in comparison with most of the recent previously published methods. As a result, for the present situation of COVID-19, the proposed ""C19D-Net"" can be employed in places where test kits are in short supply, to help the radiologists to improve their accuracy of detection of COVID-19 patients through XR-Images.",2021,10.3390/ijerph182212191,diagnosis,False
A joint ROI extraction filter for computer aided lung nodule detection,"Extraction of regions of interest plays an important rule in computer aided lung nodules detection. However, because of the complex background and structure, accurate and robust extraction of ROIs in medical image still remains a problem. Aim at this problem, a two-stage operations joint filter: Hessian-LoB, is proposed. The first stage is blobs (which being taken as candidate ROIs) detection and the second stage is ROIs extraction. In the first stage, the derivatives of a Hessian matrix at multiple scales are convolved with input images to localize blobs. Then in the second stage, Laplacian of bilateral filter (LoB) is convolved with the detected blobs to extract the final ROIs. Experiments show that the proposed filter can deal with images with noise and low brightness contrast, and is effectively in ROI extraction for lung nodule detection.",2015,10.3233/bme-151448,diagnosis,True
A large margin piecewise linear classifier with fusion of deep features in the diagnosis of COVID-19,"The world has experienced epidemics of coronavirus infections several times over the last two decades. Recent studies have shown that using medical imaging techniques can be useful in developing an automatic computer-aided diagnosis system to detect pandemic diseases with high accuracy at an early stage. In this study, a large margin piecewise linear classifier was developed to diagnose COVID-19 compared to a wide range of viral pneumonia, including SARS and MERS, using chest x-ray images. In the proposed method, a preprocessing pipeline was employed. Moreover, deep pre- and post-rectified linear unit (ReLU) features were extracted using the well-known VGG-Net19, which was fine-tuned to optimize transfer learning. Afterward, the canonical correlation analysis was performed for feature fusion, and fused deep features were passed into the LMPL classifier. The introduced method reached the highest performance in comparison with related state-of-the-art methods for two different schemes (normal, COVID-19, and typical viral pneumonia) and (COVID-19, SARS, and MERS pneumonia) with 99.39% and 98.86% classification accuracy, respectively.",2021,10.1016/j.compbiomed.2021.104927,diagnosis,False
A Lightweight Multi-Section CNN for Lung Nodule Classification and Malignancy Estimation,"The size and shape of a nodule are the essential indicators of malignancy in lung cancer diagnosis. However, effectively capturing the nodule's structural information from CT scans in a computer-aided system is a challenging task. Unlike previous models that proposed computationally intensive deep ensemble models or three-dimensional CNN models, we propose a lightweight, multiple view sampling based multi-section CNN architecture. The model obtains a nodule's cross sections from multiple view angles and encodes the nodule's volumetric information into a compact representation by aggregating information from its different cross sections via a view pooling layer. The compact feature is subsequently used for the task of nodule classification. The method does not require the nodule's spatial annotation and works directly on the cross sections generated from volume enclosing the nodule. We evaluated the proposed method on lung image database consortium (LIDC) and image database resource initiative (IDRI) dataset. It achieved the state-of-the-art performance with a mean 93.18% classification accuracy. The architecture could also be used to select the representative cross sections determining the nodule's malignancy that facilitates in the interpretation of results. Because of being lightweight, the model could be ported to mobile devices, which brings the power of artificial intelligence (AI) driven application directly into the practitioner's hand.",2019,10.1109/jbhi.2018.2879834,prognosis,True
A Machine Learning Model Based on PET/CT Radiomics and Clinical Characteristics Predicts Tumor Immune Profiles in Non-Small Cell Lung Cancer: A Retrospective Multicohort Study,"BACKGROUND: The tumor immune microenvironment (TIME) phenotypes have been reported to mainly impact the efficacy of immunotherapy. Given the increasing use of immunotherapy in cancers, knowing an individual's TIME phenotypes could be helpful in screening patients who are more likely to respond to immunotherapy. Our study intended to establish, validate, and apply a machine learning model to predict TIME profiles in non-small cell lung cancer (NSCLC) by using (18)F-FDG PET/CT radiomics and clinical characteristics. METHODS: The RNA-seq data of 1145 NSCLC patients from The Cancer Genome Atlas (TCGA) cohort were analyzed. Then, 221 NSCLC patients from Daping Hospital (DPH) cohort received(18)F-FDG PET/CT scans before treatment and CD8 expression of the tumor samples were tested. The Artificial Intelligence Kit software was used to extract radiomic features of PET/CT images and develop a radiomics signature. The models were established by radiomics, clinical features, and radiomics-clinical combination, respectively, the performance of which was calculated by receiver operating curves (ROCs) and compared by DeLong test. Moreover, based on radiomics score (Rad-score) and clinical features, a nomogram was established. Finally, we applied the combined model to evaluate TIME phenotypes of NSCLC patients in The Cancer Imaging Archive (TCIA) cohort (n = 39). RESULTS: TCGA data showed CD8 expression could represent the TIME profiles in NSCLC. In DPH cohort, PET/CT radiomics model outperformed CT model (AUC: 0.907 vs. 0.861, P = 0.0314) to predict CD8 expression. Further, PET/CT radiomics-clinical combined model (AUC = 0.932) outperformed PET/CT radiomics model (AUC = 0.907, P = 0.0326) or clinical model (AUC = 0.868, P = 0.0036) to predict CD8 expression. In the TCIA cohort, the predicted CD8-high group had significantly higher immune scores and more activated immune pathways than the predicted CD8-low group (P = 0.0421). CONCLUSION: Our study indicates that (18)F-FDG PET/CT radiomics-clinical combined model could be a clinically practical method to non-invasively detect the tumor immune status in NSCLCs.",2022,10.3389/fimmu.2022.859323,prognosis,True
A machine learning-based framework for diagnosis of COVID-19 from chest X-ray images,"Corona virus disease (COVID-19) acknowledged as a pandemic by the WHO and mankind all over the world is vulnerable to this virus. Alternative tools are needed that can help in diagnosis of the coronavirus. Researchers of this article investigated the potential of machine learning methods for automatic diagnosis of corona virus with high accuracy from X-ray images. Two most commonly used classifiers were selected: logistic regression (LR) and convolutional neural networks (CNN). The main reason was to make the system fast and efficient. Moreover, a dimensionality reduction approach was also investigated based on principal component analysis (PCA) to further speed up the learning process and improve the classification accuracy by selecting the highly discriminate features. The deep learning-based methods demand large amount of training samples compared to conventional approaches, yet adequate amount of labelled training samples was not available for COVID-19 X-ray images. Therefore, data augmentation technique using generative adversarial network (GAN) was employed to further increase the training samples and reduce the overfitting problem. We used the online available dataset and incorporated GAN to have 500 X-ray images in total for this study. Both CNN and LR showed encouraging results for COVID-19 patient identification. The LR and CNN models showed 95.2-97.6% overall accuracy without PCA and 97.6-100% with PCA for positive cases identification, respectively.",2021,10.1007/s12539-020-00403-6,diagnosis,False
A machine learning-based real-time tumor tracking system for fluoroscopic gating of lung radiotherapy,"To improve respiratory-gated radiotherapy accuracy, we developed a machine learning approach for markerless tumor tracking and evaluated it using lung cancer patient data. Digitally reconstructed radiography (DRR) datasets were generated using planning 4DCT data. Tumor positions were selected on respective DRR images to place the GTV center of gravity in the center of each DRR. DRR subimages around the tumor regions were cropped so that the subimage size was defined by tumor size. Training data were then classified into two groups: positive (including tumor) and negative (not including tumor) samples. Machine learning parameters were optimized by the extremely randomized tree method. For the tracking stage, a machine learning algorithm was generated to provide a tumor likelihood map using fluoroscopic images. Prior probability tumor positions were also calculated using the previous two frames. Tumor position was then estimated by calculating maximum probability on the tumor likelihood map and prior probability tumor positions. We acquired treatment planning 4DCT images in eight patients. Digital fluoroscopic imaging systems on either side of the vertical irradiation port allowed fluoroscopic image acquisition during treatment delivery. Each fluoroscopic dataset was acquired at 15 frames per second. We evaluated the tracking accuracy and computation times. Tracking positional accuracy averaged over all patients was 1.03 ± 0.34 mm (mean ± standard deviation, Euclidean distance) and 1.76 ± 0.71 mm ([Formula: see text] percentile). Computation time was 28.66 ± 1.89 ms/frame averaged over all frames. Our markerless algorithm successfully estimated tumor position in real time.",2020,10.1088/1361-6560/ab79c5,treatment,True
A Machine-Learning Approach Using PET-Based Radiomics to Predict the Histological Subtypes of Lung Cancer,"PURPOSE: We sought to distinguish lung adenocarcinoma (ADC) from squamous cell carcinoma using a machine-learning algorithm with PET-based radiomic features. METHODS: A total of 396 patients with 210 ADCs and 186 squamous cell carcinomas who underwent FDG PET/CT prior to treatment were retrospectively analyzed. Four clinical features (age, sex, tumor size, and smoking status) and 40 radiomic features were investigated in terms of lung ADC subtype prediction. Radiomic features were extracted from the PET images of segmented tumors using the LIFEx package. The clinical and radiomic features were ranked, and a subset of useful features was selected based on Gini coefficient scores in terms of associations with histological class. The areas under the receiver operating characteristic curves (AUCs) of classifications afforded by several machine-learning algorithms (random forest, neural network, naive Bayes, logistic regression, and a support vector machine) were compared and validated via random sampling. RESULTS: We developed and validated a PET-based radiomic model predicting the histological subtypes of lung cancer. Sex, SUVmax, gray-level zone length nonuniformity, gray-level nonuniformity for zone, and total lesion glycolysis were the 5 best predictors of lung ADC. The logistic regression model outperformed all other classifiers (AUC = 0.859, accuracy = 0.769, F1 score = 0.774, precision = 0.804, recall = 0.746) followed by the neural network model (AUC = 0.854, accuracy = 0.772, F1 score = 0.777, precision = 0.807, recall = 0.750). CONCLUSIONS: A machine-learning approach successfully identified the histological subtypes of lung cancer. A PET-based radiomic features may help clinicians improve the histopathologic diagnosis in a noninvasive manner.",2019,10.1097/rlu.0000000000002810,diagnosis,True
A Method for Optimal Detection of Lung Cancer Based on Deep Learning Optimized by Marine Predators Algorithm,"Lung cancer is the uncontrolled growth of cells in the lung that are made up of two spongy organs located in the chest. These cells may penetrate outside the lungs in a process called metastasis and spread to tissues and organs in the body. In this paper, using image processing, deep learning, and metaheuristic, an optimal methodology is proposed for early detection of this cancer. Here, we design a new convolutional neural network for this purpose. Marine predators algorithm is also used for optimal arrangement and better network accuracy. The method finally applied to RIDER dataset, and the results are compared with some pretrained deep networks, including CNN ResNet-18, GoogLeNet, AlexNet, and VGG-19. Final results showed higher results of the proposed method toward the compared techniques. The results showed that the proposed MPA-based method with 93.4% accuracy, 98.4% sensitivity, and 97.1% specificity provides the highest efficiency with the least error (1.6) toward the other state of the art methods.",2021,10.1155/2021/3694723,diagnosis,True
A method to combine target volume data from 3D and 4D planned thoracic radiotherapy patient cohorts for machine learning applications,"BACKGROUND AND PURPOSE: The gross tumour volume (GTV) is predictive of clinical outcome and consequently features in many machine-learned models. 4D-planning, however, has prompted substitution of the GTV with the internal gross target volume (iGTV). We present and validate a method to synthesise GTV data from the iGTV, allowing the combination of 3D and 4D planned patient cohorts for modelling. MATERIAL AND METHODS: Expert delineations in 40 non-small cell lung cancer patients were used to develop linear fit and erosion methods to synthesise the GTV volume and shape. Quality was assessed using Dice Similarity Coefficients (DSC) and closest point measurements; by calculating dosimetric features; and by assessing the quality of random forest models built on patient populations with and without synthetic GTVs. RESULTS: Volume estimates were within the magnitudes of inter-observer delineation variability. Shape comparisons produced mean DSCs of 0.8817 and 0.8584 for upper and lower lobe cases, respectively. A model trained on combined true and synthetic data performed significantly better than models trained on GTV alone, or combined GTV and iGTV data. CONCLUSIONS: Accurate synthesis of GTV size from the iGTV permits the combination of lung cancer patient cohorts, facilitating machine learning applications in thoracic radiotherapy.",2018,10.1016/j.radonc.2017.11.015,treatment,True
A model based on CT radiomic features for predicting RT-PCR becoming negative in coronavirus disease 2019 (COVID-19) patients,"BACKGROUND: Coronavirus disease 2019 (COVID-19) has emerged as a global pandemic. According to the diagnosis and treatment guidelines of China, negative reverse transcription-polymerase chain reaction (RT-PCR) is the key criterion for discharging COVID-19 patients. However, repeated RT-PCR tests lead to medical waste and prolonged hospital stays for COVID-19 patients during the recovery period. Our purpose is to assess a model based on chest computed tomography (CT) radiomic features and clinical characteristics to predict RT-PCR negativity during clinical treatment. METHODS: From February 10 to March 10, 2020, 203 mild COVID-19 patients in Fangcang Shelter Hospital were retrospectively included (training: n = 141; testing: n = 62), and clinical characteristics were collected. Lung abnormalities on chest CT images were segmented with a deep learning algorithm. CT quantitative features and radiomic features were automatically extracted. Clinical characteristics and CT quantitative features were compared between RT-PCR-negative and RT-PCR-positive groups. Univariate logistic regression and Spearman correlation analyses identified the strongest features associated with RT-PCR negativity, and a multivariate logistic regression model was established. The diagnostic performance was evaluated for both cohorts. RESULTS: The RT-PCR-negative group had a longer time interval from symptom onset to CT exams than the RT-PCR-positive group (median 23 vs. 16 days, p < 0.001). There was no significant difference in the other clinical characteristics or CT quantitative features. In addition to the time interval from symptom onset to CT exams, nine CT radiomic features were selected for the model. ROC curve analysis revealed AUCs of 0.811 and 0.812 for differentiating the RT-PCR-negative group, with sensitivity/specificity of 0.765/0.625 and 0.784/0.600 in the training and testing datasets, respectively. CONCLUSION: The model combining CT radiomic features and clinical data helped predict RT-PCR negativity during clinical treatment, indicating the proper time for RT-PCR retesting.",2020,10.1186/s12880-020-00521-z,diagnosis,True
A model for the effective COVID-19 identification in uncertainty environment using primary symptoms and CT scans,"The rapid spread of the COVID-19 virus around the world poses a real threat to public safety. Some COVID-19 symptoms are similar to other viral chest diseases, which makes it challenging to develop models for effective detection of COVID-19 infection. This article advocates a model to differentiate between COVID-19 and other four viral chest diseases under uncertainty environment using the viruses primary symptoms and CT scans. The proposed model is based on a plithogenic set, which provides higher accurate evaluation results in an uncertain environment. The proposed model employs the best-worst method (BWM) and the technique in order of preference by similarity to ideal solution (TOPSIS). Besides, this study discusses how smart Internet of Things technology can assist medical staff in monitoring the spread of COVID-19. Experimental evaluation of the proposed model was conducted on five different chest diseases. Evaluation results demonstrate that the proposed model effectiveness in detecting the COVID-19 in all five cases achieving detection accuracy of up to 98%.",2020,10.1177/1460458220952918,diagnosis,True
A multi-center study of COVID-19 patient prognosis using deep learning-based CT image analysis and electronic health records,"PURPOSE: As of August 30th, there were in total 25.1 million confirmed cases and 845 thousand deaths caused by coronavirus disease of 2019 (COVID-19) worldwide. With overwhelming demands on medical resources, patient stratification based on their risks is essential. In this multi-center study, we built prognosis models to predict severity outcomes, combining patients' electronic health records (EHR), which included vital signs and laboratory data, with deep learning- and CT-based severity prediction. METHOD: We first developed a CT segmentation network using datasets from multiple institutions worldwide. Two biomarkers were extracted from the CT images: total opacity ratio (TOR) and consolidation ratio (CR). After obtaining TOR and CR, further prognosis analysis was conducted on datasets from INSTITUTE-1, INSTITUTE-2 and INSTITUTE-3. For each data cohort, generalized linear model (GLM) was applied for prognosis prediction. RESULTS: For the deep learning model, the correlation coefficient of the network prediction and manual segmentation was 0.755, 0.919, and 0.824 for the three cohorts, respectively. The AUC (95 % CI) of the final prognosis models was 0.85(0.77,0.92), 0.93(0.87,0.98), and 0.86(0.75,0.94) for INSTITUTE-1, INSTITUTE-2 and INSTITUTE-3 cohorts, respectively. Either TOR or CR exist in all three final prognosis models. Age, white blood cell (WBC), and platelet (PLT) were chosen predictors in two cohorts. Oxygen saturation (SpO2) was a chosen predictor in one cohort. CONCLUSION: The developed deep learning method can segment lung infection regions. Prognosis results indicated that age, SpO2, CT biomarkers, PLT, and WBC were the most important prognostic predictors of COVID-19 in our prognosis model.",2021,10.1016/j.ejrad.2021.109583,diagnosis,True
A multistage discriminative model for tumor and lymph node detection in thoracic images,"Analysis of primary lung tumors and disease in regional lymph nodes is important for lung cancer staging, and an automated system that can detect both types of abnormalities will be helpful for clinical routine. In this paper, we present a new method to automatically detect both tumors and abnormal lymph nodes simultaneously from positron emission tomography-computed tomography thoracic images. We perform the detection in a multistage approach, by first detecting all potential abnormalities, then differentiate between tumors and lymph nodes, and finally refine the detected tumors for false positive reduction. Each stage is designed with a discriminative model based on support vector machines and conditional random fields, exploiting intensity, spatial and contextual features. The method is designed to handle a wide and complex variety of abnormal patterns found in clinical datasets, consisting of different spatial contexts of tumors and abnormal lymph nodes. We evaluated the proposed method thoroughly on clinical datasets, and encouraging results were obtained.",2012,10.1109/tmi.2012.2185057,diagnosis,True
A new method based on MTANNs for cutting down false-positives: an evaluation on different versions of commercial pulmonary nodule detection CAD software,"One of the major problems for computer-aided pulmonary nodule detection in chest radiographs is that a high false-positive (FP) rate exists. In an effort to overcome this problem, a new method based on the MTANN (Massive Training Artificial Neural Network) is proposed in this paper. An MTANN comprises a multi-layer neural network where a linear function rather than a sigmoid function is used as its activity function in the output layer. In this work, a mixture of multiple MTANNs were employed rather than only a single MTANN. 50 MTANNs for 50 different types of FPs were prepared firstly. Then, several effective MTANNs that had higher performances were selected to construct the MTANNs mixture. Finally, the outputs of the multiple MTANNs were combined with a mixing neural network to reduce various different types of FPs. The performance of this MTANNs mixture in FPs reduction is validated on three different versions of commercial CAD software with a validation database consisting of 52 chest radiographs. Experimental results demonstrate that the proposed MTANN approach is useful in cutting down FPs in different CAD software for detecting pulmonary nodules in chest radiographs.",2014,10.3233/bme-141102,diagnosis,False
A new method of detecting pulmonary nodules with PET/CT based on an improved watershed algorithm,"BACKGROUND: Integrated 18F-fluorodeoxyglucose positron emission tomography/computed tomography (18F-FDG PET/CT) is widely performed for staging solitary pulmonary nodules (SPNs). However, the diagnostic efficacy of SPNs based on PET/CT is not optimal. Here, we propose a method of detection based on PET/CT that can differentiate malignant and benign SPNs with few false-positives. METHOD: Our proposed method combines the features of positron-emission tomography (PET) and computed tomography (CT). A dynamic threshold segmentation method was used to identify lung parenchyma in CT images and suspicious areas in PET images. Then, an improved watershed method was used to mark suspicious areas on the CT image. Next, the support vector machine (SVM) method was used to classify SPNs based on textural features of CT images and metabolic features of PET images to validate the proposed method. RESULTS: Our proposed method was more efficient than traditional methods and methods based on the CT or PET features alone (sensitivity 95.6%; average of 2.9 false positives per scan).",2015,10.1371/journal.pone.0123694,diagnosis,True
A New Optimal Diagnosis System for Coronavirus (COVID-19) Diagnosis Based on Archimedes Optimization Algorithm on Chest X-Ray Images,"The new coronavirus, COVID-19, has affected people all over the world. Coronaviruses are a large group of viruses that can infect animals and humans and cause respiratory distress; these discomforts may be as mild as a cold or as severe as pneumonia. Correct detection of this disease can help to avoid its spreading increasingly. In this paper, a new CAD-based approach is suggested for the optimal diagnosis of this disease from chest X-ray images. The proposed method starts with a min-max normalization to scale all data into a normal scale, and then, histogram equalization is performed to improve the quality of the image before main processing. Afterward, 18 different features are extracted from the image. To decrease the method difficulty, the minimum features are selected based on a metaheuristic called Archimedes optimization algorithm (AOA). The model is then implemented on three datasets, and its results are compared with four other state-of-the-art methods. The final results indicated that the proposed method with 86% accuracy and 96% precision has the highest balance between accuracy and reliability with the compared methods as a diagnostic system for COVID-19.",2021,10.1155/2021/7788491,diagnosis,False
A Noise-Robust Framework for Automatic Segmentation of COVID-19 Pneumonia Lesions From CT Images,"Segmentation of pneumonia lesions from CT scans of COVID-19 patients is important for accurate diagnosis and follow-up. Deep learning has a potential to automate this task but requires a large set of high-quality annotations that are difficult to collect. Learning from noisy training labels that are easier to obtain has a potential to alleviate this problem. To this end, we propose a novel noise-robust framework to learn from noisy labels for the segmentation task. We first introduce a noise-robust Dice loss that is a generalization of Dice loss for segmentation and Mean Absolute Error (MAE) loss for robustness against noise, then propose a novel COVID-19 Pneumonia Lesion segmentation network (COPLE-Net) to better deal with the lesions with various scales and appearances. The noise-robust Dice loss and COPLE-Net are combined with an adaptive self-ensembling framework for training, where an Exponential Moving Average (EMA) of a student model is used as a teacher model that is adaptively updated by suppressing the contribution of the student to EMA when the student has a large training loss. The student model is also adaptive by learning from the teacher only when the teacher outperforms the student. Experimental results showed that: (1) our noise-robust Dice loss outperforms existing noise-robust loss functions, (2) the proposed COPLE-Net achieves higher performance than state-of-the-art image segmentation networks, and (3) our framework with adaptive self-ensembling significantly outperforms a standard training process and surpasses other noise-robust training approaches in the scenario of learning from noisy labels for COVID-19 pneumonia lesion segmentation.",2020,10.1109/tmi.2020.3000314,diagnosis,True
A Noninvasive Multianalytical Approach for Lung Cancer Diagnosis of Patients with Pulmonary Nodules,"Addressing the high false-positive rate of conventional low-dose computed tomography (LDCT) for lung cancer diagnosis, the efficacy of incorporating blood-based noninvasive testing for assisting practicing clinician's decision making in diagnosis of pulmonary nodules (PNs) is investigated. In this prospective observative study, next generation sequencing- (NGS-) based cell-free DNA (cfDNA) mutation profiling, NGS-based cfDNA methylation profiling, and blood-based protein cancer biomarker testing are performed for patients with PNs, who are diagnosed as high-risk patients through LDCT and subsequently undergo surgical resections, with tissue sections pathologically examined and classified. Using pathological classification as the gold standard, statistical and machine learning methods are used to select molecular markers associated with tissue's malignant classification based on a 98-patient discovery cohort (28 benign and 70 malignant), and to construct an integrative multianalytical model for tissue malignancy prediction. Predictive models based on individual testing platforms have shown varying levels of performance, while their final integrative model produces an area under the receiver operating characteristic curve (AUC) of 0.85. The model's performance is further confirmed on a 29-patient independent validation cohort (14 benign and 15 malignant, with power > 0.90), reproducing AUC of 0.86, which translates to an overall sensitivity of 80% and specificity of 85.7%.",2021,10.1002/advs.202100104,diagnosis,True
A novel adaptive momentum method for medical image classification using convolutional neural network,"BACKGROUND: AI for medical diagnosis has made a tremendous impact by applying convolutional neural networks (CNNs) to medical image classification and momentum plays an essential role in stochastic gradient optimization algorithms for accelerating or improving training convolutional neural networks. In traditional optimizers in CNNs, the momentum is usually weighted by a constant. However, tuning hyperparameters for momentum can be computationally complex. In this paper, we propose a novel adaptive momentum for fast and stable convergence. METHOD: Applying adaptive momentum rate proposes increasing or decreasing based on every epoch's error changes, and it eliminates the need for momentum hyperparameter optimization. We tested the proposed method with 3 different datasets: REMBRANDT Brain Cancer, NIH Chest X-ray, COVID-19 CT scan. We compared the performance of a novel adaptive momentum optimizer with Stochastic gradient descent (SGD) and other adaptive optimizers such as Adam and RMSprop. RESULTS: Proposed method improves SGD performance by reducing classification error from 6.12 to 5.44%, and it achieved the lowest error and highest accuracy compared with other optimizers. To strengthen the outcomes of this study, we investigated the performance comparison for the state-of-the-art CNN architectures with adaptive momentum. The results shows that the proposed method achieved the highest with 95% compared to state-of-the-art CNN architectures while using the same dataset. The proposed method improves convergence performance by reducing classification error and achieves high accuracy compared with other optimizers.",2022,10.1186/s12880-022-00755-z,diagnosis,True
A novel approach for the automated segmentation and volume quantification of cardiac fats on computed tomography,"The deposits of fat on the surroundings of the heart are correlated to several health risk factors such as atherosclerosis, carotid stiffness, coronary artery calcification, atrial fibrillation and many others. These deposits vary unrelated to obesity, which reinforces its direct segmentation for further quantification. However, manual segmentation of these fats has not been widely deployed in clinical practice due to the required human workload and consequential high cost of physicians and technicians. In this work, we propose a unified method for an autonomous segmentation and quantification of two types of cardiac fats. The segmented fats are termed epicardial and mediastinal, and stand apart from each other by the pericardium. Much effort was devoted to achieve minimal user intervention. The proposed methodology mainly comprises registration and classification algorithms to perform the desired segmentation. We compare the performance of several classification algorithms on this task, including neural networks, probabilistic models and decision tree algorithms. Experimental results of the proposed methodology have shown that the mean accuracy regarding both epicardial and mediastinal fats is 98.5% (99.5% if the features are normalized), with a mean true positive rate of 98.0%. In average, the Dice similarity index was equal to 97.6%.",2016,10.1016/j.cmpb.2015.09.017,diagnosis,True
"A Novel Block Imaging Technique Using Nine Artificial Intelligence Models for COVID-19 Disease Classification, Characterization and Severity Measurement in Lung Computed Tomography Scans on an Italian Cohort","Computer Tomography (CT) is currently being adapted for visualization of COVID-19 lung damage. Manual classification and characterization of COVID-19 may be biased depending on the expert's opinion. Artificial Intelligence has recently penetrated COVID-19, especially deep learning paradigms. There are nine kinds of classification systems in this study, namely one deep learning-based CNN, five kinds of transfer learning (TL) systems namely VGG16, DenseNet121, DenseNet169, DenseNet201 and MobileNet, three kinds of machine-learning (ML) systems, namely artificial neural network (ANN), decision tree (DT), and random forest (RF) that have been designed for classification of COVID-19 segmented CT lung against Controls. Three kinds of characterization systems were developed namely (a) Block imaging for COVID-19 severity index (CSI); (b) Bispectrum analysis; and (c) Block Entropy. A cohort of Italian patients with 30 controls (990 slices) and 30 COVID-19 patients (705 slices) was used to test the performance of three types of classifiers. Using K10 protocol (90% training and 10% testing), the best accuracy and AUC was for DCNN and RF pairs were 99.41 ± 5.12%, 0.991 (p < 0.0001), and 99.41 ± 0.62%, 0.988 (p < 0.0001), respectively, followed by other ML and TL classifiers. We show that diagnostics odds ratio (DOR) was higher for DL compared to ML, and both, Bispecturm and Block Entropy shows higher values for COVID-19 patients. CSI shows an association with Ground Glass Opacities (0.9146, p < 0.0001). Our hypothesis holds true that deep learning shows superior performance compared to machine learning models. Block imaging is a powerful novel approach for pinpointing COVID-19 severity and is clinically validated.",2021,10.1007/s10916-021-01707-w,diagnosis,True
A Novel Computer-Aided Diagnosis Scheme on Small Annotated Set: G2C-CAD,"PURPOSE: Computer-aided diagnosis (CAD) can aid in improving diagnostic level; however, the main problem currently faced by CAD is that it cannot obtain sufficient labeled samples. To solve this problem, in this study, we adopt a generative adversarial network (GAN) approach and design a semisupervised learning algorithm, named G2C-CAD. METHODS: From the National Cancer Institute (NCI) Lung Image Database Consortium (LIDC) dataset, we extracted four types of pulmonary nodule sign images closely related to lung cancer: noncentral calcification, lobulation, spiculation, and nonsolid/ground-glass opacity (GGO) texture, obtaining a total of 3,196 samples. In addition, we randomly selected 2,000 non-lesion image blocks as negative samples. We split the data 90% for training and 10% for testing. We designed a DCGAN generative adversarial framework and trained it on the small sample set. We also trained our designed CNN-based fuzzy Co-forest on the labeled small sample set and obtained a preliminary classifier. Then, coupled with the simulated unlabeled samples generated by the trained DCGAN, we conducted iterative semisupervised learning, which continually improved the classification performance of the fuzzy Co-forest until the termination condition was reached. Finally, we tested the fuzzy Co-forest and compared its performance with that of a C4.5 random decision forest and the G2C-CAD system without the fuzzy scheme, using ROC and confusion matrix for evaluation. RESULTS: Four different types of lung cancer-related signs were used in the classification experiment: noncentral calcification, lobulation, spiculation, and nonsolid/ground-glass opacity (GGO) texture, along with negative image samples. For these five classes, the G2C-CAD system obtained AUCs of 0.946, 0.912, 0.908, 0.887, and 0.939, respectively. The average accuracy of G2C-CAD exceeded that of the C4.5 random decision tree by 14%. G2C-CAD also obtained promising test results on the LISS signs dataset; its AUCs for GGO, lobulation, spiculation, pleural indentation, and negative image samples were 0.972, 0.964, 0.941, 0.967, and 0.953, respectively. CONCLUSION: The experimental results show that G2C-CAD is an appropriate method for addressing the problem of insufficient labeled samples in the medical image analysis field. Moreover, our system can be used to establish a training sample library for CAD classification diagnosis, which is important for future medical image analysis.",2019,10.1155/2019/6425963,diagnosis,True
A Novel COVID-19 Diagnosis Support System Using the Stacking Approach and Transfer Learning Technique on Chest X-Ray Images,"COVID-19 is an infectious disease-causing flu-like respiratory problem with various symptoms such as cough or fever, which in severe cases can cause pneumonia. The aim of this paper is to develop a rapid and accurate medical diagnosis support system to detect COVID-19 in chest X-ray images using a stacking approach combining transfer learning techniques and KNN algorithm for selection of the best model. In deep learning, we have multiple approaches for building a classification system for analyzing radiographic images. In this work, we used the transfer learning technique. This approach makes it possible to store and use the knowledge acquired from a pretrained convolutional neural network to solve a new problem. To ensure the robustness of the proposed system for diagnosing patients with COVID-19 using X-ray images, we used a machine learning method called the stacking approach to combine the performances of the many transfer learning-based models. The generated model was trained on a dataset containing four classes, namely, COVID-19, tuberculosis, viral pneumonia, and normal cases. The dataset used was collected from a six-source dataset of X-ray images. To evaluate the performance of the proposed system, we used different common evaluation measures. Our proposed system achieves an extremely good accuracy of 99.23% exceeding many previous related studies.",2021,10.1155/2021/9437538,diagnosis,True
A Novel CT-Based Radiomics Features Analysis for Identification and Severity Staging of COPD,"RATIONALE AND OBJECTIVES: To evaluate the role of radiomics based on Chest Computed Tomography (CT) in the identification and severity staging of chronic obstructive pulmonary disease (COPD). MATERIALS AND METHODS: This retrospective analysis included 322 participants (249 COPD patients and 73 control subjects). In total, 1395 chest CT-based radiomics features were extracted from each participant's CT images. Three feature selection methods, including variance threshold, Select K Best method, and least absolute shrinkage and selection operator (LASSO), and two classification methods, including support vector machine (SVM) and logistic regression (LR), were used as identification and severity classification of COPD. Performance was compared by AUC, accuracy, sensitivity, specificity, precision, and F1-score. RESULTS: 38 and 10 features were selected to construct radiomics models to detect and stage COPD, respectively. For COPD identification, SVM classifier achieved AUCs of 0.992 and 0.970, while LR classifier achieved AUCs of 0.993 and 0.972 in the training set and test set, respectively. For the severity staging of COPD, the mentioned two machine learning classifiers can better differentiate less severity (GOLD1 + GOLD2) group from greater severity (GOLD3 + GOLD4) group. The AUCs of SVM and LR is 0.907 and 0.903 in the training set, and that of 0.799 and 0.797 in the test set. CONCLUSION: The present study showed that the novel radiomics approach based on chest CT images that can be used for COPD identification and severity classification, and the constructed radiomics model demonstrated acceptable performance.",2022,10.1016/j.acra.2022.01.004,combined,True
A novel deep learning approach to extract Chinese clinical entities for lung cancer screening and staging,"BACKGROUND: Computed tomography (CT) reports record a large volume of valuable information about patients' conditions and the interpretations of radiology images from radiologists, which can be used for clinical decision-making and further academic study. However, the free-text nature of clinical reports is a critical barrier to use this data more effectively. In this study, we investigate a novel deep learning method to extract entities from Chinese CT reports for lung cancer screening and TNM staging. METHODS: The proposed approach presents a new named entity recognition algorithm, namely the BERT-based-BiLSTM-Transformer network (BERT-BTN) with pre-training, to extract clinical entities for lung cancer screening and staging. Specifically, instead of traditional word embedding methods, BERT is applied to learn the deep semantic representations of characters. Following the long short-term memory layer, a Transformer layer is added to capture the global dependencies between characters. Besides, pre-training technique is employed to alleviate the problem of insufficient labeled data. RESULTS: We verify the effectiveness of the proposed approach on a clinical dataset containing 359 CT reports collected from the Department of Thoracic Surgery II of Peking University Cancer Hospital. The experimental results show that the proposed approach achieves an 85.96% macro-F1 score under exact match scheme, which improves the performance by 1.38%, 1.84%, 3.81%,4.29%,5.12%,5.29% and 8.84% compared to BERT-BTN, BERT-LSTM, BERT-fine-tune, BERT-Transformer, FastText-BTN, FastText-BiLSTM and FastText-Transformer, respectively. CONCLUSIONS: In this study, we developed a novel deep learning method, i.e., BERT-BTN with pre-training, to extract the clinical entities from Chinese CT reports. The experimental results indicate that the proposed approach can efficiently recognize various clinical entities about lung cancer screening and staging, which shows the potential for further clinical decision-making and academic research.",2021,10.1186/s12911-021-01575-x,combined,True
A Novel Deep Learning Network and Its Application for Pulmonary Nodule Segmentation,"Pulmonary nodules are the early manifestation of lung cancer, which appear as circular shadow of no more than 3 cm on the computed tomography (CT) image. Accurate segmentation of the contours of pulmonary nodules can help doctors improve the efficiency of diagnosis. Deep learning has achieved great success in computer vision. In this study, we propose a novel network for pulmonary nodule segmentation from CT images based on U-NET. The proposed network has two merits: one is that it introduces dense connection to transfer and utilize features. Additionally, the problem of gradient disappearance can be avoided. The second is that it introduces a new loss function which is tolerance on the pixels near the borders of the nodule. Experimental results show that the proposed network at least achieves 1% improvement compared with other state-of-art networks in terms of different criteria.",2022,10.1155/2022/7124902,diagnosis,True
A novel deep learning-based quantification of serial chest computed tomography in Coronavirus Disease 2019 (COVID-19),"This study aims to explore and compare a novel deep learning-based quantification with the conventional semi-quantitative computed tomography (CT) scoring for the serial chest CT scans of COVID-19. 95 patients with confirmed COVID-19 and a total of 465 serial chest CT scans were involved, including 61 moderate patients (moderate group, 319 chest CT scans) and 34 severe patients (severe group, 146 chest CT scans). Conventional CT scoring and deep learning-based quantification were performed for all chest CT scans for two study goals: (1) Correlation between these two estimations; (2) Exploring the dynamic patterns using these two estimations between moderate and severe groups. The Spearman's correlation coefficient between these two estimation methods was 0.920 (p < 0.001). predicted pulmonary involvement (CT score and percent of pulmonary lesions calculated using deep learning-based quantification) increased more rapidly and reached a higher peak on 23rd days from symptom onset in severe group, which reached a peak on 18th days in moderate group with faster absorption of the lesions. The deep learning-based quantification for COVID-19 showed a good correlation with the conventional CT scoring and demonstrated a potential benefit in the estimation of disease severities of COVID-19.",2021,10.1038/s41598-020-80261-w,prognosis,True
A novel deep neuroevolution-based image classification method to diagnose coronavirus disease (COVID-19),"COVID-19 has had a detrimental impact on normal activities, public safety, and the global financial system. To identify the presence of this disease within communities and to commence the management of infected patients early, positive cases should be diagnosed as quickly as possible. New results from X-ray imaging indicate that images provide key information about COVID-19. Advanced deep-learning (DL) models can be applied to X-ray radiological images to accurately diagnose this disease and to mitigate the effects of a shortage of skilled medical personnel in rural areas. However, the performance of DL models strongly depends on the methodology used to design their architectures. Therefore, deep neuroevolution (DNE) techniques are introduced to automatically design DL architectures accurately. In this paper, a new paradigm is proposed for the automated diagnosis of COVID-19 from chest X-ray images using a novel two-stage improved DNE Algorithm. The proposed DNE framework is evaluated on a real-world dataset and the results demonstrate that it provides the highest classification performance in terms of different evaluation metrics.",2021,10.1016/j.compbiomed.2021.104994,diagnosis,False
A Novel Framework Based on Deep Learning and ANOVA Feature Selection Method for Diagnosis of COVID-19 Cases from Chest X-Ray Images,"Background and Objective. The new coronavirus disease (known as COVID-19) was first identified in Wuhan and quickly spread worldwide, wreaking havoc on the economy and people's everyday lives. As the number of COVID-19 cases is rapidly increasing, a reliable detection technique is needed to identify affected individuals and care for them in the early stages of COVID-19 and reduce the virus's transmission. The most accessible method for COVID-19 identification is Reverse Transcriptase-Polymerase Chain Reaction (RT-PCR); however, it is time-consuming and has false-negative results. These limitations encouraged us to propose a novel framework based on deep learning that can aid radiologists in diagnosing COVID-19 cases from chest X-ray images. Methods. In this paper, a pretrained network, DenseNet169, was employed to extract features from X-ray images. Features were chosen by a feature selection method, i.e., analysis of variance (ANOVA), to reduce computations and time complexity while overcoming the curse of dimensionality to improve accuracy. Finally, selected features were classified by the eXtreme Gradient Boosting (XGBoost). The ChestX-ray8 dataset was employed to train and evaluate the proposed method. Results and Conclusion. The proposed method reached 98.72% accuracy for two-class classification (COVID-19, No-findings) and 92% accuracy for multiclass classification (COVID-19, No-findings, and Pneumonia). The proposed method's precision, recall, and specificity rates on two-class classification were 99.21%, 93.33%, and 100%, respectively. Also, the proposed method achieved 94.07% precision, 88.46% recall, and 100% specificity for multiclass classification. The experimental results show that the proposed framework outperforms other methods and can be helpful for radiologists in the diagnosis of COVID-19 cases.",2022,10.1155/2022/4694567,diagnosis,False
A Novel Hybrid Feature Extraction Model for Classification on Pulmonary Nodules,"In this paper an improved Computer Aided Design system can offer a second opinion to radiologists on early diagnosis of pulmonary nodules on CT (Computer Tomography) images. A Deep Convolutional Neural Network (DCNN) method is used for feature extraction and hybridize as combination of Convolutional Neural Network (CNN), Histogram of Oriented Gradient (HOG), Extended Histogram of Oriented Gradients (ExHOG) and Local Binary Pattern (LBP). A combination of shape, texture, scaling, rotation, translation features extracted using HOG, LBP and CNN. The Homogeneous descriptors used to extract the feature of lung images from Lung Image Database Consortium (LIDC) are given to classifiers Support Vector Machine (SVM), K-Nearest Neighbour (KNN), Decision Tree and Random Forest to classify nodules and non-nodules. Experimental results demonstrate the effectiveness of the proposed method in terms of accuracy which gives best result than the competing methods.",2019,10.31557/apjcp.2019.20.2.457,diagnosis,True
A Novel Machine Learning-derived Radiomic Signature of the Whole Lung Differentiates Stable From Progressive COVID-19 Infection: A Retrospective Cohort Study,"OBJECTIVE: This study aimed to use the radiomics signatures of a machine learning-based tool to evaluate the prognosis of patients with coronavirus disease 2019 (COVID-19) infection. METHODS: The clinical and imaging data of 64 patients with confirmed diagnoses of COVID-19 were retrospectively selected and divided into a stable group and a progressive group according to the data obtained from the ongoing treatment process. Imaging features from whole-lung images from baseline computed tomography (CT) scans were extracted and dimensionality reduction was performed. Support vector machines were used to construct radiomics signatures and to compare differences between the 2 groups. We also compared the differences of signature scores in the clinical, laboratory, and CT image feature subgroups and finally analyzed the correlation between the radiomics features of the constructed signature and the other features including clinical, laboratory, and CT imaging features. RESULTS: The signature has a good classification effect for the stable group and the progressive group, with area under curve, sensitivity, and specificity of 0.833, 80.95%, and 74.42%, respectively. Signature score differences in laboratory and CT imaging features between subgroups were not statistically significant (P>0.05); cough was negatively correlated with GLCM Entropy_angle 90_offset4 (r=-0.578), but was positively correlated with ShortRunEmphhasis_AllDirect_offset4_SD (r=0.454); C-reactive protein was positively correlated with Cluster Prominence_ AllDirect_offset 4_ SD (r=0.47). CONCLUSION: The radiomics signature of the whole lung based on machine learning may reveal the changes of lung microstructure in the early stage and help to indicate the progression of the disease.",2020,10.1097/rti.0000000000000544,prognosis,True
A Novel Multicolor-thresholding Auto-detection Method to Detect the Location and Severity of Inflammation in Confirmed SARS-COV-2 Cases using Chest X-Ray Images,"OBJECTIVES: Since late 2019, Coronavirus Disease 2019 (COVID-19) has spread around the world. It has been determined that the disease is very contagious and can cause Acute Respiratory Distress (ARD). Medical imaging has the potential to help identify, detect, and quantify the severity of this infection. This work seeks to develop a novel auto-detection technique for verified COVID-19 cases that can detect aberrant alterations in traditional X-ray pictures. METHODS: Nineteen separately colored layers were created from X-ray scans of patients diagnosed with COVID-19. Each layer represents objects that have a similar contrast and can be represented by a single color. In a single layer, objects with similar contrasts are formed. A single color image was created by extracting all the objects from all the layers. The prototype model could recognize a wide range of abnormal changes in the image texture based on color differentiation. This was true even when the contrast values of the detected unclear abnormalities varied slightly. RESULTS: The results indicate that the proposed novel method is 91% accurate in detecting and grading COVID-19 lung infections compared to the opinions of three experienced radiologists evaluating chest X-ray images. Additionally, the method can be used to determine the infection site and severity of the disease by categorizing X-rays into five severity levels. CONCLUSION: By comparing affected tissue to healthy tissue, the proposed COVID-19 auto-detection method can identify locations and indicate the severity of the disease, as well as predict where the disease may spread.",2022,10.2174/1573405617666210910150119,diagnosis,False
A novel multiple instance learning framework for COVID-19 severity assessment via data augmentation and self-supervised learning,"How to fast and accurately assess the severity level of COVID-19 is an essential problem, when millions of people are suffering from the pandemic around the world. Currently, the chest CT is regarded as a popular and informative imaging tool for COVID-19 diagnosis. However, we observe that there are two issues - weak annotation and insufficient data that may obstruct automatic COVID-19 severity assessment with CT images. To address these challenges, we propose a novel three-component method, i.e., 1) a deep multiple instance learning component with instance-level attention to jointly classify the bag and also weigh the instances, 2) a bag-level data augmentation component to generate virtual bags by reorganizing high confidential instances, and 3) a self-supervised pretext component to aid the learning process. We have systematically evaluated our method on the CT images of 229 COVID-19 cases, including 50 severe and 179 non-severe cases. Our method could obtain an average accuracy of 95.8%, with 93.6% sensitivity and 96.4% specificity, which outperformed previous works.",2021,10.1016/j.media.2021.101978,diagnosis,True
A novel technology to integrate imaging and clinical markers for non-invasive diagnosis of lung cancer,"This study presents a non-invasive, automated, clinical diagnostic system for early diagnosis of lung cancer that integrates imaging data from a single computed tomography scan and breath bio-markers obtained from a single exhaled breath to quickly and accurately classify lung nodules. CT imaging and breath volatile organic compounds data were collected from 47 patients. Spherical Harmonics-based shape features to quantify the shape complexity of the pulmonary nodules, 7th-Order Markov Gibbs Random Field based appearance model to describe the spatial non-homogeneities in the pulmonary nodule, and volumetric features (size) of pulmonary nodules were calculated from CT images. 27 VOCs in exhaled breath were captured by a micro-reactor approach and quantied using mass spectrometry. CT and breath markers were input into a deep-learning autoencoder classifier with a leave-one-subject-out cross validation for nodule classification. To mitigate the limitation of a small sample size and validate the methodology for individual markers, retrospective CT scans from 467 patients with 727 pulmonary nodules, and breath samples from 504 patients were analyzed. The CAD system achieved 97.8% accuracy, 97.3% sensitivity, 100% specificity, and 99.1% area under curve in classifying pulmonary nodules.",2021,10.1038/s41598-021-83907-5,diagnosis,True
A PET Radiomics Model to Predict Refractory Mediastinal Hodgkin Lymphoma,"First-order radiomic features, such as metabolic tumor volume (MTV) and total lesion glycolysis (TLG), are associated with disease progression in early-stage classical Hodgkin lymphoma (HL). We hypothesized that a model incorporating first- and second-order radiomic features would more accurately predict outcome than MTV or TLG alone. We assessed whether radiomic features extracted from baseline PET scans predicted relapsed or refractory disease status in a cohort of 251 patients with stage I-II HL who were managed at a tertiary cancer center. Models were developed and tested using a machine-learning algorithm. Features extracted from mediastinal sites were highly predictive of primary refractory disease. A model incorporating 5 of the most predictive features had an area under the curve (AUC) of 95.2% and total error rate of 1.8%. By comparison, the AUC was 78% for both MTV and TLG and was 65% for maximum standardize uptake value (SUV(max)). Furthermore, among the patients with refractory mediastinal disease, our model distinguished those who were successfully salvaged from those who ultimately died of HL. We conclude that our PET radiomic model may improve upfront stratification of early-stage HL patients with mediastinal disease and thus contribute to risk-adapted, individualized management.",2019,10.1038/s41598-018-37197-z,diagnosis,False
A physics-guided modular deep-learning based automated framework for tumor segmentation in PET,"An important need exists for reliable positron emission tomography (PET) tumor-segmentation methods for tasks such as PET-based radiation-therapy planning and reliable quantification of volumetric and radiomic features. To address this need, we propose an automated physics-guided deep-learning-based three-module framework to segment PET images on a per-slice basis. The framework is designed to help address the challenges of limited spatial resolution and lack of clinical training data with known ground-truth tumor boundaries in PET. The first module generates PET images containing highly realistic tumors with known ground-truth using a new stochastic and physics-based approach, addressing lack of training data. The second module trains a modified U-net using these images, helping it learn the tumor-segmentation task. The third module fine-tunes this network using a small-sized clinical dataset with radiologist-defined delineations as surrogate ground-truth, helping the framework learn features potentially missed in simulated tumors. The framework was evaluated in the context of segmenting primary tumors in (18)F-fluorodeoxyglucose (FDG)-PET images of patients with lung cancer. The framework's accuracy, generalizability to different scanners, sensitivity to partial volume effects (PVEs) and efficacy in reducing the number of training images were quantitatively evaluated using Dice similarity coefficient (DSC) and several other metrics. The framework yielded reliable performance in both simulated (DSC: 0.87 (95% confidence interval (CI): 0.86, 0.88)) and patient images (DSC: 0.73 (95% CI: 0.71, 0.76)), outperformed several widely used semi-automated approaches, accurately segmented relatively small tumors (smallest segmented cross-section was 1.83 cm(2)), generalized across five PET scanners (DSC: 0.74 (95% CI: 0.71, 0.76)), was relatively unaffected by PVEs, and required low training data (training with data from even 30 patients yielded DSC of 0.70 (95% CI: 0.68, 0.71)). In conclusion, the proposed automated physics-guided deep-learning-based PET-segmentation framework yielded reliable performance in delineating tumors in FDG-PET images of patients with lung cancer.",2020,10.1088/1361-6560/ab8535,treatment,False
A pilot study using kernelled support tensor machine for distant failure prediction in lung SBRT,"We developed a kernelled support tensor machine (KSTM)-based model with tumor tensors derived from pre-treatment PET and CT imaging as input to predict distant failure in early stage non-small cell lung cancer (NSCLC) treated with stereotactic body radiation therapy (SBRT). The patient cohort included 110 early stage NSCLC patients treated with SBRT, 25 of whom experienced failure at distant sites. Three-dimensional tumor tensors were constructed and used as input for the KSTM-based classifier. A KSTM iterative algorithm with a convergent proof was developed to train the weight vectors for every mode of the tensor for the classifier. In contrast to conventional radiomics approaches that rely on handcrafted imaging features, the KSTM-based classifier uses 3D imaging as input, taking full advantage of the imaging information. The KSTM-based classifier preserves the intrinsic 3D geometry structure of the medical images and the correlation in the original images and trains the classification hyper-plane in an adaptive feature tensor space. The KSTM-based predictive algorithm was compared with three conventional machine learning models and three radiomics approaches. For PET and CT, the KSTM-based predictive method achieved the highest prediction results among the seven methods investigated in this study based on 10-fold cross validation and independent testing.",2018,10.1016/j.media.2018.09.004,treatment,True
A pilot study: Quantify lung volume and emphysema extent directly from two-dimensional scout images,"PURPOSE: The potential to compute volume metrics of emphysema from planar scout images was investigated in this study. The successful implementation of this concept will have a wide impact in different fields, and specifically, maximize the diagnostic potential of the planar medical images. METHODS: We investigated our premise using a well-characterized chronic obstructive pulmonary disease (COPD) cohort. In this cohort, planar scout images from computed tomography (CT) scans were used to compute lung volume and percentage of emphysema. Lung volume and percentage of emphysema were quantified on the volumetric CT images and used as the ""ground truth"" for developing the models to compute the variables from the corresponding scout images. We trained two classical convolutional neural networks (CNNs), including VGG19 and InceptionV3, to compute lung volume and the percentage of emphysema from the scout images. The scout images (n = 1,446) were split into three subgroups: (1) training (n = 1,235), (2) internal validation (n = 99), and (3) independent test (n = 112) at the subject level in a ratio of 8:1:1. The mean absolute difference (MAD) and R-square (R2) were the performance metrics to evaluate the prediction performance of the developed models. RESULTS: The lung volumes and percentages of emphysema computed from a single planar scout image were significantly linear correlated with the measures quantified using volumetric CT images (VGG19: R2 = 0.934 for lung volume and R2 = 0.751 for emphysema percentage, and InceptionV3: R2 = 0.977 for lung volume and R2 = 0.775 for emphysema percentage). The mean absolute differences (MADs) for lung volume and percentage of emphysema were 0.302 ± 0.247L and 2.89 ± 2.58%, respectively, for VGG19, and 0.366 ± 0.287L and 3.19 ± 2.14, respectively, for InceptionV3. CONCLUSIONS: Our promising results demonstrated the feasibility of inferring volume metrics from planar images using CNNs.",2021,10.1002/mp.15019,diagnosis,True
A preliminary study of a photon dose calculation algorithm using a convolutional neural network,"The aim of dose calculation algorithm research is to improve the calculation accuracy while maximizing the calculation efficiency. In this study, the three-dimensional distribution of total energy release per unit mass (TERMA) and the electron density (ED) distribution are considered inputs in a method for calculating the three-dimensional dose distribution based on a convolutional neural network (CNN). Attempts are made to improve the efficiency of the collapsed cone convolution/superposition (CCCS) algorithm while providing an approach to improve the efficiency of other traditional dose calculation algorithms. Twelve sets of computed tomography (CT) images were employed for training. Data sets were generated by the CCCS algorithm with a random beam configuration. For each monoenergetic photon model, 7500 samples were generated for the training set, and 1500 samples were generated for the validation set. Training occurred for 0.5 MeV, 1 MeV, 2 MeV, 3 MeV, 4 MeV, 5 MeV, and 6 MeV monoenergetic photon models. To evaluate the usability under linac conditions, a comparison between CCCS and CNN-Dose was performed for the Mohan 6-MV spectrum for 12 additional new sets of CT images with different anatomies. A total of 1512 test samples were generated. For all anatomies, the mean value, 95% lower confidence limit (LCL) and 95% upper confidence limit (UCL) were 99.56%, 99.51% and 99.61%, respectively, at the 3%/2 mm criteria. The mean value, 95% LCL and 95% UCL were 98.57%, 98.46% and 98.67%, respectively, at the 2%/2 mm criteria. The results meet the relevant clinical requirements. In the proposed methods, the dose distribution of clinical energy can be obtained by TERMA, and the electronic density can be obtained with a CNN. This method can also be used for other traditional dose algorithms and displays potential in treatment planning, adaptive radiation therapy, and in vivo verification.",2020,10.1088/1361-6560/abb1d7,treatment,True
A prognostic analysis method for non-small cell lung cancer based on the computed tomography radiomics,"In order to assist doctors in arranging the postoperative treatments and re-examinations for non-small cell lung cancer (NSCLC) patients, this study was initiated to explore a prognostic analysis method for NSCLC based on computed tomography (CT) radiomics. The data of 173 NSCLC patients were collected retrospectively and the clinically meaningful 3-year survival was used as the predictive limit to predict the patient's prognosis survival time range. Firstly, lung tumors were segmented and the radiomics features were extracted. Secondly, the feature weighting algorithm was used to screen and optimize the extracted original feature data. Then, the selected feature data combining with the prognosis survival of patients were used to train machine learning classification models. Finally, a prognostic survival prediction model and radiomics prognostic factors were obtained to predict the prognosis survival time range of NSCLC patients. The classification accuracy rate under cross-validation was up to 88.7% in the prognosis survival analysis model. When verifying on an independent data set, the model also yielded a high prediction accuracy which is up to 79.6%. Inverse different moment, lobulation sign and angular second moment were NSCLC prognostic factors based on radiomics. This study proved that CT radiomics features could effectively assist doctors to make more accurate prognosis survival prediction for NSCLC patients, so as to help doctors to optimize treatment and re-examination for NSCLC patients to extend their survival time.",2020,10.1088/1361-6560/ab6e51,prognosis,True
A promising approach for screening pulmonary hypertension based on frontal chest radiographs using deep learning: A retrospective study,"BACKGROUND: To date, the missed diagnosis rate of pulmonary hypertension (PH) was high, and there has been limited development of a rapid, simple, and effective way to screen the disease. The purpose of this study is to develop a deep learning approach to achieve rapid detection of possible abnormalities in chest radiographs suggesting PH for screening patients suspected of PH. METHODS: We retrospectively collected frontal chest radiographs and the pulmonary artery systolic pressure (PASP) value measured by Doppler transthoracic echocardiography from 762 patients (357 healthy controls and 405 with PH) from three institutes in China from January 2013 to May 2019. The wohle sample comprised 762 images (641 for training, 80 for internal test, and 41 for external test). We firstly performed a 8-fold cross-validation on the 641 images selected for training (561 for pre-training, 80 for validation), then decided to tune learning rate to 0.0008 according to the best score on validation data. Finally, we used all the pre-training and validation data (561+80 = 641) to train our models (Resnet50, Xception, and Inception V3), evaluated them on internal and external test dataset to classify the images as having manifestations of PH or healthy according to the area under the receiver operating characteristic curve (AUC/ROC). After that, the three deep learning models were further used for prediction of PASP using regression algorithm. Moreover, we invited an experienced chest radiologist to classify the images in the test dataset as having PH or not, and compared the prediction accuracy performed by deep learing models with that of manual classification. RESULTS: The AUC performed by the best model (Inception V3) achieved 0.970 in the internal test, and slightly declined in the external test (0.967) when using deep learning algorithms to classify PH from normal based on chest X-rays. The mean absolute error (MAE) of the best model for prediction of PASP value was smaller in the internal test (7.45) compared to 9.95 in the external test. Manual classification of PH based on chest X-rays showed much lower AUCs compared to that performed by deep learning models both in the internal and external test. CONCLUSIONS: The present study used deep learning algorithms to classify abnormalities suggesting PH in chest radiographs with high accuracy and good generalizability. Once tested prospectively in clinical settings, the technology could provide a non-invasive and easy-to-use method to screen patients suspected of having PH.",2020,10.1371/journal.pone.0236378,diagnosis,False
A quantitative imaging biomarker for predicting disease-free-survival-associated histologic subgroups in lung adenocarcinoma,"OBJECTIVES: Classification of histologic subgroups has significant prognostic value for lung adenocarcinoma patients who undergo surgical resection. However, clinical histopathology assessment is generally performed on only a small portion of the overall tumor from biopsy or surgery. Our objective is to identify a noninvasive quantitative imaging biomarker (QIB) for the classification of histologic subgroups in lung adenocarcinoma patients. METHODS: We retrospectively collected and reviewed 1313 CT scans of patients with resected lung adenocarcinomas from two geographically distant institutions who were seen between January 2014 and October 2017. Three study cohorts, the training, internal validation, and external validation cohorts, were created, within which lung adenocarcinomas were divided into two disease-free-survival (DFS)-associated histologic subgroups, the mid/poor and good DFS groups. A comprehensive machine learning- and deep learning-based analytical system was adopted to identify reproducible QIBs and help to understand QIBs' significance. RESULTS: Intensity-Skewness, a QIB quantifying tumor density distribution, was identified as the optimal biomarker for predicting histologic subgroups. Intensity-Skewness achieved high AUCs (95% CI) of 0.849(0.813,0.881), 0.820(0.781,0.856) and 0.863(0.827,0.895) on the training, internal validation, and external validation cohorts, respectively. A criterion of Intensity-Skewness ≤ 1.5, which indicated high tumor density, showed high specificity of 96% (sensitivity 46%) and 99% (sensitivity 53%) on predicting the mid/poor DFS group in the training and external validation cohorts, respectively. CONCLUSIONS: A QIB derived from routinely acquired CT was able to predict lung adenocarcinoma histologic subgroups, providing a noninvasive method that could potentially benefit personalized treatment decision-making for lung cancer patients. KEY POINTS: • A noninvasive imaging biomarker, Intensity-Skewness, which described the distortion of pixel-intensity distribution within lesions on CT images, was identified as a biomarker to predict disease-free-survival-associated histologic subgroups in lung adenocarcinoma. • An Intensity-Skewness of ≤ 1.5 has high specificity in predicting the mid/poor disease-free survival histologic patient group in both the training cohort and the external validation cohort. • The Intensity-Skewness is a feature that can be automatically computed with high reproducibility and robustness.",2020,10.1007/s00330-020-06663-6,prognosis,True
A Radiogenomics Ensemble to Predict EGFR and KRAS Mutations in NSCLC,"Lung cancer causes more deaths globally than any other type of cancer. To determine the best treatment, detecting EGFR and KRAS mutations is of interest. However, non-invasive ways to obtain this information are not available. Furthermore, many times there is a lack of big enough relevant public datasets, so the performance of single classifiers is not outstanding. In this paper, an ensemble approach is applied to increase the performance of EGFR and KRAS mutation prediction using a small dataset. A new voting scheme, Selective Class Average Voting (SCAV), is proposed and its performance is assessed both for machine learning models and CNNs. For the EGFR mutation, in the machine learning approach, there was an increase in the sensitivity from 0.66 to 0.75, and an increase in AUC from 0.68 to 0.70. With the deep learning approach, an AUC of 0.846 was obtained, and with SCAV, the accuracy of the model was increased from 0.80 to 0.857. For the KRAS mutation, both in the machine learning models (0.65 to 0.71 AUC) and the deep learning models (0.739 to 0.778 AUC), a significant increase in performance was found. The results obtained in this work show how to effectively learn from small image datasets to predict EGFR and KRAS mutations, and that using ensembles with SCAV increases the performance of machine learning classifiers and CNNs. The results provide confidence that as large datasets become available, tools to augment clinical capabilities can be fielded.",2021,10.3390/tomography7020014,diagnosis,True
A radiomics approach for lung nodule detection in thoracic CT images based on the dynamic patterns of morphological variation,"OBJECTIVES: To propose and evaluate a set of radiomic features, called morphological dynamics features, for pulmonary nodule detection, which were rooted in the dynamic patterns of morphological variation and needless precise lesion segmentation. MATERIALS AND METHODS: Two datasets were involved, namely, university hospital (UH) and LIDC datasets, comprising 72 CT scans (360 nodules) and 888 CT scans (2230 nodules), respectively. Each nodule was annotated by multiple radiologists. Denoted the category of nodules identified by at least k radiologists as ALk. A nodule detection algorithm, called CAD-MD algorithm, was proposed based on the morphological dynamics radiomic features, characterizing a lesion by ten sets of the same features with different values extracted from ten different thresholding results. Each nodule candidate was classified by a two-level classifier, including ten decision trees and a random forest, respectively. The CAD-MD algorithm was compared with a deep learning approach, the N-Net, using the UH dataset. RESULTS: On the AL1 and AL2 of the UH dataset, the AUC of the AFROC curves were 0.777 and 0.851 for the CAD-MD algorithm and 0.478 and 0.472 for the N-Net, respectively. The CAD-MD algorithm achieved the sensitivities of 84.4% and 91.4% with 2.98 and 3.69 FPs/scan and the N-Net 74.4% and 80.7% with 3.90 and 4.49 FPs/scan, respectively. On the LIDC dataset, the CAD-MD algorithm attained the sensitivities of 87.6%, 89.2%, 92.2%, and 95.0% with 4 FPs/scan for AL1-AL4, respectively. CONCLUSION: The morphological dynamics radiomic features might serve as an effective set of radiomic features for lung nodule detection. KEY POINTS: • Texture features varied with such CT system settings as reconstruction kernels of CT images, CT scanner models, and parameter settings, and so on. • Shape and first-order statistics were shown to be the most robust features against variation in CT imaging parameters. • The morphological dynamics radiomic features, which mainly characterized the dynamic patterns of morphological variation, were shown to be effective for lung nodule detection.",2022,10.1007/s00330-021-08456-x,diagnosis,True
A radiomics study to predict invasive pulmonary adenocarcinoma appearing as pure ground-glass nodules,"AIM: To establish a machine-learning model to differentiate adenocarcinoma in situ (AIS) or minimally invasive adenocarcinoma (MIA) from invasive adenocarcinoma (IAC) appearing as pure ground-glass nodules (pGGNs). MATERIALS AND METHODS: This retrospective study enrolled 136 patients with histopathologically diagnosed with AIS, MIA, and IAC. All pGGNs were divided randomly into a training and a testing dataset at a ratio of 7 : 3. Radiomics features were extracted based on the unenhanced computed tomography (CT) images derived from the last preoperative CT examination of each patient. The F-test and least absolute shrinkage and selection operator (LASSO) logistic regression were applied to select the most valuable features to establish a support vector machine (SVM) model. The performance of the model was evaluated using the area under the receiver operating characteristic curve (AUROC), and the accuracy, sensitivity, and specificity were calculated to compare the diagnostic performance of radiologists and the SVM model. RESULTS: Six significant radiomics features were selected to develop the SVM model and showed excellent ability to differentiate AIS/MIA from IAC in both the training dataset (AUROC=0.950, 95% confidence interval [CI]: 0.886-0.984) and the testing dataset (AUROC=0.945, 95% CI: 0.826-0.992). Compared with two radiologists, the proposed model possessed significant advantages with higher accuracy (90.24% versus 75.61% and 80.49%), sensitivity (91.67% versus 50% and 75%), and specificity (89.66% versus 86.21% and 82.76%). CONCLUSION: A machine-learning model based on radiomics features exhibits superior diagnostic performance in differentiating AIS/MIA from IAC appearing as pGGNs.",2021,10.1016/j.crad.2020.10.005,diagnosis,True
A radiomics-boosted deep-learning model for COVID-19 and non-COVID-19 pneumonia classification using chest x-ray images,"PURPOSE: To develop a deep learning model design that integrates radiomics analysis for enhanced performance of COVID-19 and non-COVID-19 pneumonia detection using chest x-ray images. METHODS: As a novel radiomics approach, a 2D sliding kernel was implemented to map the impulse response of radiomic features throughout the entire chest x-ray image; thus, each feature is rendered as a 2D map in the same dimension as the x-ray image. Based on each of the three investigated deep neural network architectures, including VGG-16, VGG-19, and DenseNet-121, a pilot model was trained using x-ray images only. Subsequently, two radiomic feature maps (RFMs) were selected based on cross-correlation analysis in reference to the pilot model saliency map results. The radiomics-boosted model was then trained based on the same deep neural network architecture using x-ray images plus the selected RFMs as input. The proposed radiomics-boosted design was developed using 812 chest x-ray images with 262/288/262 COVID-19/non-COVID-19 pneumonia/healthy cases, and 649/163 cases were assigned as training-validation/independent test sets. For each model, 50 runs were trained with random assignments of training/validation cases following the 7:1 ratio in the training-validation set. Sensitivity, specificity, accuracy, and ROC curves together with area-under-the-curve (AUC) from all three deep neural network architectures were evaluated. RESULTS: After radiomics-boosted implementation, all three investigated deep neural network architectures demonstrated improved sensitivity, specificity, accuracy, and ROC AUC results in COVID-19 and healthy individual classifications. VGG-16 showed the largest improvement in COVID-19 classification ROC (AUC from 0.963 to 0.993), and DenseNet-121 showed the largest improvement in healthy individual classification ROC (AUC from 0.962 to 0.989). The reduced variations suggested improved robustness of the model to data partition. For the challenging non-COVID-19 pneumonia classification task, radiomics-boosted implementation of VGG-16 (AUC from 0.918 to 0.969) and VGG-19 (AUC from 0.964 to 0.970) improved ROC results, while DenseNet-121 showed a slight yet insignificant ROC performance reduction (AUC from 0.963 to 0.949). The achieved highest accuracy of COVID-19/non-COVID-19 pneumonia/healthy individual classifications were 0.973 (VGG-19)/0.936 (VGG-19)/ 0.933 (VGG-16), respectively. CONCLUSIONS: The inclusion of radiomic analysis in deep learning model design improved the performance and robustness of COVID-19/non-COVID-19 pneumonia/healthy individual classification, which holds great potential for clinical applications in the COVID-19 pandemic.",2022,10.1002/mp.15582,diagnosis,False
A Rapid Artificial Intelligence-Based Computer-Aided Diagnosis System for COVID-19 Classification from CT Images,"The excessive number of COVID-19 cases reported worldwide so far, supplemented by a high rate of false alarms in its diagnosis using the conventional polymerase chain reaction method, has led to an increased number of high-resolution computed tomography (CT) examinations conducted. The manual inspection of the latter, besides being slow, is susceptible to human errors, especially because of an uncanny resemblance between the CT scans of COVID-19 and those of pneumonia, and therefore demands a proportional increase in the number of expert radiologists. Artificial intelligence-based computer-aided diagnosis of COVID-19 using the CT scans has been recently coined, which has proven its effectiveness in terms of accuracy and computation time. In this work, a similar framework for classification of COVID-19 using CT scans is proposed. The proposed method includes four core steps: (i) preparing a database of three different classes such as COVID-19, pneumonia, and normal; (ii) modifying three pretrained deep learning models such as VGG16, ResNet50, and ResNet101 for the classification of COVID-19-positive scans; (iii) proposing an activation function and improving the firefly algorithm for feature selection; and (iv) fusing optimal selected features using descending order serial approach and classifying using multiclass supervised learning algorithms. We demonstrate that once this method is performed on a publicly available dataset, this system attains an improved accuracy of 97.9% and the computational time is almost 34 (sec).",2021,10.1155/2021/2560388,diagnosis,True
A rapid screening classifier for diagnosing COVID-19,"Rationale: Coronavirus disease 2019 (COVID-19) has caused a global pandemic. A classifier combining chest X-ray (CXR) with clinical features may serve as a rapid screening approach. Methods: The study included 512 patients with COVID-19 and 106 with influenza A/B pneumonia. A deep neural network (DNN) was applied, and deep features derived from CXR and clinical findings formed fused features for diagnosis prediction. Results: The clinical features of COVID-19 and influenza showed different patterns. Patients with COVID-19 experienced less fever, more diarrhea, and more salient hypercoagulability. Classifiers constructed using the clinical features or CXR had an area under the receiver operating curve (AUC) of 0.909 and 0.919, respectively. The diagnostic efficacy of the classifier combining the clinical features and CXR was dramatically improved and the AUC was 0.952 with 91.5% sensitivity and 81.2% specificity. Moreover, combined classifier was functional in both severe and non-serve COVID-19, with an AUC of 0.971 with 96.9% sensitivity in non-severe cases, which was on par with the computed tomography (CT)-based classifier, but had relatively inferior efficacy in severe cases compared to CT. In extension, we performed a reader study involving three experienced pulmonary physicians, artificial intelligence (AI) system demonstrated superiority in turn-around time and diagnostic accuracy compared with experienced pulmonary physicians. Conclusions: The classifier constructed using clinical and CXR features is efficient, economical, and radiation safe for distinguishing COVID-19 from influenza A/B pneumonia, serving as an ideal rapid screening tool during the COVID-19 pandemic.",2021,10.7150/ijbs.53982,diagnosis,False
"A Rapid, Accurate and Machine-Agnostic Segmentation and Quantification Method for CT-Based COVID-19 Diagnosis","COVID-19 has caused a global pandemic and become the most urgent threat to the entire world. Tremendous efforts and resources have been invested in developing diagnosis, prognosis and treatment strategies to combat the disease. Although nucleic acid detection has been mainly used as the gold standard to confirm this RNA virus-based disease, it has been shown that such a strategy has a high false negative rate, especially for patients in the early stage, and thus CT imaging has been applied as a major diagnostic modality in confirming positive COVID-19. Despite the various, urgent advances in developing artificial intelligence (AI)-based computer-aided systems for CT-based COVID-19 diagnosis, most of the existing methods can only perform classification, whereas the state-of-the-art segmentation method requires a high level of human intervention. In this paper, we propose a fully-automatic, rapid, accurate, and machine-agnostic method that can segment and quantify the infection regions on CT scans from different sources. Our method is founded upon two innovations: 1) the first CT scan simulator for COVID-19, by fitting the dynamic change of real patients' data measured at different time points, which greatly alleviates the data scarcity issue; and 2) a novel deep learning algorithm to solve the large-scene-small-object problem, which decomposes the 3D segmentation problem into three 2D ones, and thus reduces the model complexity by an order of magnitude and, at the same time, significantly improves the segmentation accuracy. Comprehensive experimental results over multi-country, multi-hospital, and multi-machine datasets demonstrate the superior performance of our method over the existing ones and suggest its important application value in combating the disease.",2020,10.1109/tmi.2020.3001810,diagnosis,True
A segmentation tool for pulmonary nodules in lung cancer screening: Testing and clinical usage,"PURPOSE: With the future goal of defining a large dataset based on low-dose CT with labelled pulmonary lesions for lung cancer screening (LCS) research, the aim of this work is to propose and evaluate into a clinical context a tool for semi-automatic segmentation able to facilitate the process of labels collection from a LCS study (COSMOS, Continuous Observation of SMOking Subjects). METHODS: Considering a preliminary set of manual annotations, a segmentation model based on a 2D-Unet was trained from scratch. Contour quality of the final 2D-Unet was assessed on an internal test set of manual annotations and on a subset of the public available LIDC dataset used as external test set. The tool for semi-automatic segmentation was then designed integrating the tested model into a Graphical User Interface. According to the opinion of two clinical users, the percentage of lesions properly contoured through the tool was quantified (Acceptance Rate, AR). The variability between segmentations derived by the two readers was estimated as mean percentage of difference (MPD) between the two sets of volumes and comparing the likelihood of malignancy derived from Volume Doubling Time (VDT). RESULTS: Performance in test sets were found similar (DICE ~ 0.75(0.15)). Accordingly, a good mean AR (80.1%) resulted from the two readers. Variability in terms of MPD was equal to 23.6% while 2.7% was the VDTs percentage of disagreement. CONCLUSIONS: A semi-automatic segmentation tool was developed and its applicability evaluated into a clinical context demonstrating the efficacy of the tool in facilitating the collection of labelled data.",2021,10.1016/j.ejmp.2021.08.011,diagnosis,True
A short-term follow-up CT based radiomics approach to predict response to immunotherapy in advanced non-small-cell lung cancer,"To develop a short-term follow-up CT-based radiomics approach to predict response to immunotherapy in advanced non-small-cell lung cancer (NSCLC) and investigate the prognostic value of radiomics features in predicting progression-free survival (PFS) and overall survival (OS). We first retrospectively collected 224 advanced NSCLC patients from two centers, and divided them into a primary cohort and two validation cohorts respectively. Then, we processed CT scans with a series of image preprocessing techniques namely, tumor segmentation, image resampling, feature extraction and normalization. To select the optimal features, we applied the feature ranking with recursive feature elimination method. After resampling the training dataset with a synthetic minority oversampling technique, we applied the support vector machine classifier to build a machine-learning-based classification model to predict response to immunotherapy. Finally, we used Kaplan-Meier (KM) survival analysis method to evaluate prognostic value of rad-score generated by CT-radiomics model. In two validation cohorts, the delta-radiomics model significantly improved the area under receiver operating characteristic curve from 0.64 and 0.52 to 0.82 and 0.87, respectively (P < .05). In sub-group analysis, pre- and delta-radiomics model yielded higher performance for adenocarcinoma (ADC) patients than squamous cell carcinoma (SCC) patients. Through the KM survival analysis, the rad-score of delta-radiomics model had a significant prognostic for PFS and OS in validation cohorts (P < .05). Our results demonstrated that (1) delta-radiomics model could improve the prediction performance, (2) radiomics model performed better on ADC patients than SCC patients, (3) delta-radiomics model had prognostic values in predicting PFS and OS of NSCLC patients.",2022,10.1080/2162402x.2022.2028962,prognosis,True
A Simple Method to Train the AI Diagnosis Model of Pulmonary Nodules,"BACKGROUND: The differential diagnosis of subcentimetre lung nodules with a diameter of less than 1 cm has always been one of the problems of imaging doctors and thoracic surgeons. We plan to create a deep learning model for the diagnosis of pulmonary nodules in a simple method. METHODS: Image data and pathological diagnosis of patients come from the First Affiliated Hospital of Zhejiang University School of Medicine from October 1, 2016, to October 1, 2019. After data preprocessing and data augmentation, the training set is used to train the model. The test set is used to evaluate the trained model. At the same time, the clinician will also diagnose the test set. RESULTS: A total of 2,295 images of 496 lung nodules and their corresponding pathological diagnosis were selected as a training set and test set. After data augmentation, the number of training set images reached 12,510 images, including 6,648 malignant nodular images and 5,862 benign nodular images. The area under the P-R curve of the trained model is 0.836 in the classification of malignant and benign nodules. The area under the ROC curve of the trained model is 0.896 (95% CI: 78.96%~100.18%), which is higher than that of three doctors. However, the P value is not less than 0.05. CONCLUSION: With the help of an automatic machine learning system, clinicians can create a deep learning pulmonary nodule pathology classification model without the help of deep learning experts. The diagnostic efficiency of this model is not inferior to that of the clinician.",2020,10.1155/2020/2812874,diagnosis,True
A simplified cluster model and a tool adapted for collaborative labeling of lung cancer CT scans,"BACKGROUND AND OBJECTIVE: Lung cancer is the most common type of cancer with a high mortality rate. Early detection using medical imaging is critically important for the long-term survival of the patients. Computer-aided diagnosis (CAD) tools can potentially reduce the number of incorrect interpretations of medical image data by radiologists. Datasets with adequate sample size, annotation, and truth are the dominant factors in developing and training effective CAD algorithms. The objective of this study was to produce a practical approach and a tool for the creation of medical image datasets. METHODS: The proposed model uses the modified maximum transverse diameter approach to mark a putative lung nodule. The modification involves the possibility to use a set of overlapping spheres of appropriate size to approximate the shape of the nodule. The algorithm embedded in the model also groups the marks made by different readers for the same lesion. We used the data of 536 randomly selected patients of Moscow outpatient clinics to create a dataset of standard-dose chest computed tomography (CT) scans utilizing the double-reading approach with arbitration. Six volunteer radiologists independently produced a report for each scan using the proposed model with the main focus on the detection of lesions with sizes ranging from 3 to 30 mm. After this, an arbitrator reviewed their marks and annotations. RESULTS: The maximum transverse diameter approach outperformed the alternative methods (3D box, ellipsoid, and complete outline construction) in a study of 10,000 computer-generated tumor models of different shapes in terms of accuracy and speed of nodule shape approximation. The markup and annotation of the CTLungCa-500 dataset revealed 72 studies containing no lung nodules. The remaining 464 CT scans contained 3151 lesions marked by at least one radiologist: 56%, 14%, and 29% of the lesions were malignant, benign, and non-nodular, respectively. 2887 lesions have the target size of 3-30 mm. Only 70 nodules were uniformly identified by all the six readers. An increase in the number of independent readers providing CT scans interpretations led to an accuracy increase associated with a decrease in agreement. The dataset markup process took three working weeks. CONCLUSIONS: The developed cluster model simplifies the collaborative and crowdsourced creation of image repositories and makes it time-efficient. Our proof-of-concept dataset provides a valuable source of annotated medical imaging data for training CAD algorithms aimed at early detection of lung nodules. The tool and the dataset are publicly available at https://github.com/Center-of-Diagnostics-and-Telemedicine/FAnTom.git and https://mosmed.ai/en/datasets/ct_lungcancer_500/, respectively.",2021,10.1016/j.cmpb.2021.106111,diagnosis,True
A stacked ensemble for the detection of COVID-19 with high recall and accuracy,"The main challenges for the automatic detection of the coronavirus disease (COVID-19) from computed tomography (CT) scans of an individual are: a lack of large datasets, ambiguity in the characteristics of COVID-19 and the detection techniques having low sensitivity (or recall). Hence, developing diagnostic techniques with high recall and automatic feature extraction using the available data are crucial for controlling the spread of COVID-19. This paper proposes a novel stacked ensemble capable of detecting COVID-19 from a patient's chest CT scans with high recall and accuracy. A systematic approach for designing a stacked ensemble from pre-trained computer vision models using transfer learning (TL) is presented. A novel diversity measure that results in the stacked ensemble with high recall and accuracy is proposed. The stacked ensemble proposed in this paper considers four pre-trained computer vision models: the visual geometry group (VGG)-19, residual network (ResNet)-101, densely connected convolutional network (DenseNet)-169 and wide residual network (WideResNet)-50-2. The proposed model was trained and evaluated with three different chest CT scans. As recall is more important than precision, the trade-offs between recall and precision were explored in relevance to COVID-19. The optimal recommended threshold values were found for each dataset.",2021,10.1016/j.compbiomed.2021.104608,diagnosis,True
A Super-Learner Model for Tumor Motion Prediction and Management in Radiation Therapy: Development and Feasibility Evaluation,"In cancer radiation therapy, large tumor motion due to respiration can lead to uncertainties in tumor target delineation and treatment delivery, thus making active motion management an essential step in thoracic and abdominal tumor treatment. In current practice, patients with tumor motion may be required to receive two sets of CT scans - the initial free-breathing 4-dimensional CT (4DCT) scan for tumor motion estimation and a second CT scan under appropriate motion management such as breath-hold or abdominal compression. The aim of this study is to assess the feasibility of a predictive model for tumor motion estimation in three-dimensional space based on machine learning algorithms. The model was developed based on sixteen imaging features extracted from non-4D diagnostic CT images and eleven clinical features extracted from the Electronic Health Record (EHR) database of 150 patients to characterize the lung tumor motion. A super-learner model was trained to combine four base machine learning models including the Random Forest, Multi-Layer Perceptron, LightGBM and XGBoost, the hyper-parameters of which were also optimized to obtain the best performance. The outputs of the super-learner model consist of tumor motion predictions in the Superior-Inferior (SI), Anterior-Posterior (AP) and Left-Right (LR) directions, and were compared against tumor motions measured in the free-breathing 4DCT scans. The accuracy of predictions was evaluated using Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) through ten rounds of independent tests. The MAE and RMSE of predictions in the SI direction were 1.23 mm and 1.70 mm; the MAE and RMSE of predictions in the AP direction were 0.81 mm and 1.19 mm, and the MAE and RMSE of predictions in the LR direction were 0.70 mm and 0.95 mm. In addition, the relative feature importance analysis demonstrated that the imaging features are of great importance in the tumor motion prediction compared to the clinical features. Our findings indicate that a super-learner model can accurately predict tumor motion ranges as measured in the 4DCT, and could provide a machine learning framework to assist radiation oncologists in determining the active motion management strategy for patients with large tumor motion.",2019,10.1038/s41598-019-51338-y,treatment,True
A Two-Stage Convolutional Neural Networks for Lung Nodule Detection,"Early detection of lung cancer is an effective way to improve the survival rate of patients. It is a critical step to have accurate detection of lung nodules in computed tomography (CT) images for the diagnosis of lung cancer. However, due to the heterogeneity of the lung nodules and the complexity of the surrounding environment, it is a challenge to develop a robust nodule detection method. In this study, we propose a two-stage convolutional neural networks (TSCNN) for lung nodule detection. The first stage based on the improved U-Net segmentation network is to establish an initial detection of lung nodules. During this stage, in order to obtain a high recall rate without introducing excessive false positive nodules, we propose a new sampling strategy for training. Simultaneously, a two-phase prediction method is also proposed in this stage. The second stage in the TSCNN architecture based on the proposed dual pooling structure is built into three 3D-CNN classification networks for false positive reduction. Since the network training requires a significant amount of training data, we designed a random mask as the data augmentation method in this study. Furthermore, we have improved the generalization ability of the false positive reduction model by means of ensemble learning. We verified the proposed architecture on the LUNA dataset in our experiments, which showed that the proposed TSCNN architecture did obtain competitive detection performance.",2020,10.1109/jbhi.2019.2963720,diagnosis,True
A Weakly-Supervised Framework for COVID-19 Classification and Lesion Localization From Chest CT,"Accurate and rapid diagnosis of COVID-19 suspected cases plays a crucial role in timely quarantine and medical treatment. Developing a deep learning-based model for automatic COVID-19 diagnosis on chest CT is helpful to counter the outbreak of SARS-CoV-2. A weakly-supervised deep learning framework was developed using 3D CT volumes for COVID-19 classification and lesion localization. For each patient, the lung region was segmented using a pre-trained UNet; then the segmented 3D lung region was fed into a 3D deep neural network to predict the probability of COVID-19 infectious; the COVID-19 lesions are localized by combining the activation regions in the classification network and the unsupervised connected components. 499 CT volumes were used for training and 131 CT volumes were used for testing. Our algorithm obtained 0.959 ROC AUC and 0.976 PR AUC. When using a probability threshold of 0.5 to classify COVID-positive and COVID-negative, the algorithm obtained an accuracy of 0.901, a positive predictive value of 0.840 and a very high negative predictive value of 0.982. The algorithm took only 1.93 seconds to process a single patient's CT volume using a dedicated GPU. Our weakly-supervised deep learning model can accurately predict the COVID-19 infectious probability and discover lesion regions in chest CT without the need for annotating the lesions for training. The easily-trained and high-performance deep learning algorithm provides a fast way to identify COVID-19 patients, which is beneficial to control the outbreak of SARS-CoV-2. The developed deep learning software is available at https://github.com/sydney0zq/covid-19-detection.",2020,10.1109/tmi.2020.2995965,diagnosis,True
A weighted rule based method for predicting malignancy of pulmonary nodules by nodule characteristics,"Predicting malignancy of solitary pulmonary nodules from computer tomography scans is a difficult and important problem in the diagnosis of lung cancer. This paper investigates the contribution of nodule characteristics in the prediction of malignancy. Using data from Lung Image Database Consortium (LIDC) database, we propose a weighted rule based classification approach for predicting malignancy of pulmonary nodules. LIDC database contains CT scans of nodules and information about nodule characteristics evaluated by multiple annotators. In the first step of our method, votes for nodule characteristics are obtained from ensemble classifiers by using image features. In the second step, votes and rules obtained from radiologist evaluations are used by a weighted rule based method to predict malignancy. The rule based method is constructed by using radiologist evaluations on previous cases. Correlations between malignancy and other nodule characteristics and agreement ratio of radiologists are considered in rule evaluation. To handle the unbalanced nature of LIDC, ensemble classifiers and data balancing methods are used. The proposed approach is compared with the classification methods trained on image features. Classification accuracy, specificity and sensitivity of classifiers are measured. The experimental results show that using nodule characteristics for malignancy prediction can improve classification results.",2015,10.1016/j.jbi.2015.05.011,diagnosis,True
AAR-LN-DQ: Automatic anatomy recognition based disease quantification in thoracic lymph node zones via FDG PET/CT images without Nodal Delineation,"PURPOSE: The derivation of quantitative information from medical images in a practical manner is essential for quantitative radiology (QR) to become a clinical reality, but still faces a major hurdle because of image segmentation challenges. With the goal of performing disease quantification in lymph node (LN) stations without explicit nodal delineation, this paper presents a novel approach for disease quantification (DQ) by automatic recognition of LN zones and detection of malignant lymph nodes within thoracic LN zones via positron emission tomography/computed tomography (PET/CT) images. Named AAR-LN-DQ, this approach decouples DQ methods from explicit nodal segmentation via an LN recognition strategy involving a novel globular filter and a deep neural network called SegNet. METHOD: The methodology consists of four main steps: (a) Building lymph node zone models by automatic anatomy recognition (AAR) method. It incorporates novel aspects of model building that relate to finding an optimal hierarchy for organs and lymph node zones in the thorax. (b) Recognizing lymph node zones by the built lymph node models. (c) Detecting pathologic LNs in the recognized zones by using a novel globular filter (g-filter) and a multi-level support vector machine (SVM) classifier. Here, we make use of the general globular shape of LNs to first localize them and then use a multi-level SVM classifier to identify pathologic LNs from among the LNs localized by the g-filter. Alternatively, we designed a deep neural network called SegNet which is trained to directly recognize pathologic nodes within AAR localized LN zones. (d) Disease quantification based on identified pathologic LNs within localized zones. A fuzzy disease map is devised to express the degree of disease burden at each voxel within the identified LNs to simultaneously handle several uncertain phenomena such as PET partial volume effects, uncertainty in localization of LNs, and gradation of disease content at the voxel level. We focused on the task of disease quantification in patients with lymphoma based on PET/CT acquisitions and devised a method of evaluation. Model building was carried out using 42 near-normal patient datasets via contrast-enhanced CT examinations of their thorax. PET/CT datasets from an additional 63 lymphoma patients were utilized for evaluating the AAR-LN-DQ methodology. We assess the accuracy of the three main processes involved in AAR-LN-DQ via fivefold cross validation: lymph node zone recognition, abnormal lymph node localization, and disease quantification. RESULTS: The recognition and scale error for LN zones were 12.28 mm ± 1.99 and 0.94 ± 0.02, respectively, on normal CT datasets. On abnormal PET/CT datasets, the sensitivity and specificity of pathologic LN recognition were 84.1% ± 0.115 and 98.5% ± 0.003, respectively, for the g-filter-SVM strategy, and 91.3% ± 0.110 and 96.1% ± 0.016, respectively, for the SegNet method. Finally, the mean absolute percent errors for disease quantification of the recognized abnormal LNs were 8% ± 0.09 and 14% ± 0.10 for the g-filter-SVM method and the best SegNet strategy, respectively. CONCLUSIONS: Accurate disease quantification on PET/CT images without performing explicit delineation of lymph nodes is feasible following lymph node zone and pathologic LN localization. It is very useful to perform LN zone recognition by AAR as this step can cover most (95.8%) of the abnormal LNs and drastically reduce the regions to search for abnormal LNs. This also improves the specificity of deep networks such as SegNet significantly. It is possible to utilize general shape information about LNs such as their globular nature via g-filter and to arrive at high recognition rates for abnormal LNs in conjunction with a traditional classifier such as SVM. Finally, the disease map concept is effective for estimating disease burden, irrespective of how the LNs are identified, to handle various uncertainties without having to address them explicitly one by one.",2020,10.1002/mp.14240,diagnosis,True
Accuracy of deep learning-based computed tomography diagnostic system for COVID-19: A consecutive sampling external validation cohort study,"Ali-M3, an artificial intelligence program, analyzes chest computed tomography (CT) and detects the likelihood of coronavirus disease (COVID-19) based on scores ranging from 0 to 1. However, Ali-M3 has not been externally validated. Our aim was to evaluate the accuracy of Ali-M3 for detecting COVID-19 and discuss its clinical value. We evaluated the external validity of Ali-M3 using sequential Japanese sampling data. In this retrospective cohort study, COVID-19 infection probabilities for 617 symptomatic patients were determined using Ali-M3. In 11 Japanese tertiary care facilities, these patients underwent reverse transcription-polymerase chain reaction (RT-PCR) testing. They also underwent chest CT to confirm a diagnosis of COVID-19. Of the 617 patients, 289 (46.8%) were RT-PCR-positive. The area under the curve (AUC) of Ali-M3 for predicting a COVID-19 diagnosis was 0.797 (95% confidence interval: 0.762‒0.833) and the goodness-of-fit was P = 0.156. With a cut-off probability of a diagnosis of COVID-19 by Ali-M3 set at 0.5, the sensitivity and specificity were 80.6% and 68.3%, respectively. A cut-off of 0.2 yielded a sensitivity and specificity of 89.2% and 43.2%, respectively. Among the 223 patients who required oxygen, the AUC was 0.825. Sensitivity at a cut-off of 0.5% and 0.2% was 88.7% and 97.9%, respectively. Although the sensitivity was lower when the days from symptom onset were fewer, the sensitivity increased for both cut-off values after 5 days. We evaluated Ali-M3 using external validation with symptomatic patient data from Japanese tertiary care facilities. As Ali-M3 showed sufficient sensitivity performance, despite a lower specificity performance, Ali-M3 could be useful in excluding a diagnosis of COVID-19.",2021,10.1371/journal.pone.0258760,diagnosis,True
Accurate auto-labeling of chest X-ray images based on quantitative similarity to an explainable AI model,"The inability to accurately, efficiently label large, open-access medical imaging datasets limits the widespread implementation of artificial intelligence models in healthcare. There have been few attempts, however, to automate the annotation of such public databases; one approach, for example, focused on labor-intensive, manual labeling of subsets of these datasets to be used to train new models. In this study, we describe a method for standardized, automated labeling based on similarity to a previously validated, explainable AI (xAI) model-derived-atlas, for which the user can specify a quantitative threshold for a desired level of accuracy (the probability-of-similarity, pSim metric). We show that our xAI model, by calculating the pSim values for each clinical output label based on comparison to its training-set derived reference atlas, can automatically label the external datasets to a user-selected, high level of accuracy, equaling or exceeding that of human experts. We additionally show that, by fine-tuning the original model using the automatically labelled exams for retraining, performance can be preserved or improved, resulting in a highly accurate, more generalized model.",2022,10.1038/s41467-022-29437-8,diagnosis,True
Accurate detection of COVID-19 using deep features based on X-Ray images and feature selection methods,"COVID-19 is a severe epidemic affecting the whole world. This epidemic, which has a high mortality rate, affects the health systems and the economies of countries significantly. Therefore, ending the epidemic is one of the most important priorities of all states. For this, automatic diagnosis and detection systems are very important to control the epidemic. In addition to the recommendation of the ""reverse transcription-polymerase chain reaction (RT-PCR)"" test, additional diagnosis and detection systems are required. Hence, based on the fact that the COVID-19 virus attacks the lungs, automatic diagnosis and detection systems developed using X-ray and CT images come to the fore. In this study, a high-performance detection system was implemented with three different CNN (ResNet50, ResNet101, InceptionResNetV2) models and X-ray images of three different classes (COVID-19, Normal, Pneumonia). The particle swarm optimization (PSO) algorithm and ant colony algorithm (ACO) was applied among the feature selection methods, and their performances were compared. The results were obtained using support vector machines (SVM) and a k-nearest neighbor (k-NN) classifier using the 10-fold cross-validation method. The highest overall accuracy performance was 99.83% with the SVM algorithm without feature selection. The highest performance was achieved after the feature selection process with the SVM + PSO method as 99.86%. As a result, higher performance with less computational load has been achieved by realizing the feature selection. Based on the high results obtained, it is thought that this study will benefit radiologists as a decision support system.",2021,10.1016/j.compbiomed.2021.104771,diagnosis,False
Accurate Screening of COVID-19 Using Attention-Based Deep 3D Multiple Instance Learning,"Automated Screening of COVID-19 from chest CT is of emergency and importance during the outbreak of SARS-CoV-2 worldwide in 2020. However, accurate screening of COVID-19 is still a massive challenge due to the spatial complexity of 3D volumes, the labeling difficulty of infection areas, and the slight discrepancy between COVID-19 and other viral pneumonia in chest CT. While a few pioneering works have made significant progress, they are either demanding manual annotations of infection areas or lack of interpretability. In this paper, we report our attempt towards achieving highly accurate and interpretable screening of COVID-19 from chest CT with weak labels. We propose an attention-based deep 3D multiple instance learning (AD3D-MIL) where a patient-level label is assigned to a 3D chest CT that is viewed as a bag of instances. AD3D-MIL can semantically generate deep 3D instances following the possible infection area. AD3D-MIL further applies an attention-based pooling approach to 3D instances to provide insight into each instance's contribution to the bag label. AD3D-MIL finally learns Bernoulli distributions of the bag-level labels for more accessible learning. We collected 460 chest CT examples: 230 CT examples from 79 patients with COVID-19, 100 CT examples from 100 patients with common pneumonia, and 130 CT examples from 130 people without pneumonia. A series of empirical studies show that our algorithm achieves an overall accuracy of 97.9%, AUC of 99.0%, and Cohen kappa score of 95.7%. These advantages endow our algorithm as an efficient assisted tool in the screening of COVID-19.",2020,10.1109/tmi.2020.2996256,diagnosis,True
Accurate segmentation for different types of lung nodules on CT images using improved U-Net convolutional network,"Since lung nodules on computed tomography images can have different shapes, contours, textures or locations and may be attached to neighboring blood vessels or pleural surfaces, accurate segmentation is still challenging. In this study, we propose an accurate segmentation method based on an improved U-Net convolutional network for different types of lung nodules on computed tomography images.The first phase is to segment lung parenchyma and correct the lung contour by applying α-hull algorithm. The second phase is to extract image pairs of patches containing lung nodules in the center and the corresponding ground truth and build an improved U-Net network with introduction of batch normalization.A large number of experiments manifest that segmentation performance of Dice loss has superior results than mean square error and Binary_crossentropy loss. The α-hull algorithm and batch normalization can improve the segmentation performance effectively. Our best result for Dice similar coefficient (0.8623) is also more competitive than other state-of-the-art segmentation algorithms.In order to segment different types of lung nodules accurately, we propose an improved U-Net network, which can improve the segmentation accuracy effectively. Moreover, this work also has practical value in helping radiologists segment lung nodules and diagnose lung cancer.",2021,10.1097/md.0000000000027491,diagnosis,True
"Accurately Differentiating Between Patients With COVID-19, Patients With Other Viral Infections, and Healthy Individuals: Multimodal Late Fusion Learning Approach","BACKGROUND: Effectively identifying patients with COVID-19 using nonpolymerase chain reaction biomedical data is critical for achieving optimal clinical outcomes. Currently, there is a lack of comprehensive understanding in various biomedical features and appropriate analytical approaches for enabling the early detection and effective diagnosis of patients with COVID-19. OBJECTIVE: We aimed to combine low-dimensional clinical and lab testing data, as well as high-dimensional computed tomography (CT) imaging data, to accurately differentiate between healthy individuals, patients with COVID-19, and patients with non-COVID viral pneumonia, especially at the early stage of infection. METHODS: In this study, we recruited 214 patients with nonsevere COVID-19, 148 patients with severe COVID-19, 198 noninfected healthy participants, and 129 patients with non-COVID viral pneumonia. The participants' clinical information (ie, 23 features), lab testing results (ie, 10 features), and CT scans upon admission were acquired and used as 3 input feature modalities. To enable the late fusion of multimodal features, we constructed a deep learning model to extract a 10-feature high-level representation of CT scans. We then developed 3 machine learning models (ie, k-nearest neighbor, random forest, and support vector machine models) based on the combined 43 features from all 3 modalities to differentiate between the following 4 classes: nonsevere, severe, healthy, and viral pneumonia. RESULTS: Multimodal features provided substantial performance gain from the use of any single feature modality. All 3 machine learning models had high overall prediction accuracy (95.4%-97.7%) and high class-specific prediction accuracy (90.6%-99.9%). CONCLUSIONS: Compared to the existing binary classification benchmarks that are often focused on single-feature modality, this study's hybrid deep learning-machine learning framework provided a novel and effective breakthrough for clinical applications. Our findings, which come from a relatively large sample size, and analytical workflow will supplement and assist with clinical decision support for current COVID-19 diagnostic methods and other clinical applications with high-dimensional multimodal biomedical features.",2021,10.2196/25535,diagnosis,True
Active Contour Based Segmentation and Classification for Pleura Diseases Based on Otsu’s Thresholding and Support Vector Machine (SVM),"Objective: Generally, lung cancer is the abnormal growth of cells that originates in one or both lungs. Finding the pulmonary nodule helps in the diagnosis of lung cancer in early stage and also increase the lifetime of the individual. Accurate segmentation of normal and abnormal portion in segmentation is challenging task in computer-aided diagnostics. Methods: The article proposes an innovative method to spot the cancer portion using Otsu’s segmentation algorithm. It is followed by a Support Vector Machine (SVM) classifier to classify the abnormal portion of the lung image. Results: The suggested methods use the Otsu’s thresholding and active contour based segmentation techniques to locate the affected lung nodule of CT images. The segmentation is followed by an SVM classifier in order to categorize the affected portion is normal or abnormal. The proposed method is suitable to provide good and accurate segmentation and classification results for complex images. Conclusion: The comparative analysis between the two segmentation methods along with SVM classifier was performed. A classification process based on active contour and SVM techniques provides better than Otsu’s segmentation for complex lung images.",2019,10.31557/apjcp.2019.20.1.167,diagnosis,True
ADA-COVID: Adversarial Deep Domain Adaptation-Based Diagnosis of COVID-19 from Lung CT Scans Using Triplet Embeddings,"Rapid diagnosis of COVID-19 with high reliability is essential in the early stages. To this end, recent research often uses medical imaging combined with machine vision methods to diagnose COVID-19. However, the scarcity of medical images and the inherent differences in existing datasets that arise from different medical imaging tools, methods, and specialists may affect the generalization of machine learning-based methods. Also, most of these methods are trained and tested on the same dataset, reducing the generalizability and causing low reliability of the obtained model in real-world applications. This paper introduces an adversarial deep domain adaptation-based approach for diagnosing COVID-19 from lung CT scan images, termed ADA-COVID. Domain adaptation-based training process receives multiple datasets with different input domains to generate domain-invariant representations for medical images. Also, due to the excessive structural similarity of medical images compared to other image data in machine vision tasks, we use the triplet loss function to generate similar representations for samples of the same class (infected cases). The performance of ADA-COVID is evaluated and compared with other state-of-the-art COVID-19 diagnosis algorithms. The obtained results indicate that ADA-COVID achieves classification improvements of at least 3%, 20%, 20%, and 11% in accuracy, precision, recall, and F(1) score, respectively, compared to the best results of competitors, even without directly training on the same data. The implementation source code of the ADA-COVID is publicly available at https://github.com/MehradAria/ADA-COVID.",2022,10.1155/2022/2564022,diagnosis,True
Adaptive Feature Selection Guided Deep Forest for COVID-19 Classification With Chest CT,"Chest computed tomography (CT) becomes an effective tool to assist the diagnosis of coronavirus disease-19 (COVID-19). Due to the outbreak of COVID-19 worldwide, using the computed-aided diagnosis technique for COVID-19 classification based on CT images could largely alleviate the burden of clinicians. In this paper, we propose an Adaptive Feature Selection guided Deep Forest (AFS-DF) for COVID-19 classification based on chest CT images. Specifically, we first extract location-specific features from CT images. Then, in order to capture the high-level representation of these features with the relatively small-scale data, we leverage a deep forest model to learn high-level representation of the features. Moreover, we propose a feature selection method based on the trained deep forest model to reduce the redundancy of features, where the feature selection could be adaptively incorporated with the COVID-19 classification model. We evaluated our proposed AFS-DF on COVID-19 dataset with 1495 patients of COVID-19 and 1027 patients of community acquired pneumonia (CAP). The accuracy (ACC), sensitivity (SEN), specificity (SPE), AUC, precision and F1-score achieved by our method are 91.79%, 93.05%, 89.95%, 96.35%, 93.10% and 93.07%, respectively. Experimental results on the COVID-19 dataset suggest that the proposed AFS-DF achieves superior performance in COVID-19 vs. CAP classification, compared with 4 widely used machine learning methods.",2020,10.1109/jbhi.2020.3019505,diagnosis,True
Added Value of Computer-aided CT Image Features for Early Lung Cancer Diagnosis with Small Pulmonary Nodules: A Matched Case-Control Study,"Purpose To test whether computer-aided diagnosis (CAD) approaches can increase the positive predictive value (PPV) and reduce the false-positive rate in lung cancer screening for small nodules compared with human reading by thoracic radiologists. Materials and Methods A matched case-control sample of low-dose computed tomography (CT) studies in 186 participants with 4-20-mm noncalcified lung nodules who underwent biopsy in the National Lung Screening Trial (NLST) was selected. Variables used for matching were age, sex, smoking status, chronic obstructive pulmonary disease status, body mass index, study year of the positive screening test, and screening results. Studies before lung biopsy were randomly split into a training set (70 cancers plus 70 benign controls) and a validation set (20 cancers plus 26 benign controls). Image features from within and outside dominant nodules were extracted. A CAD algorithm developed from the training set and a random forest classifier were applied to the validation set to predict biopsy outcomes. Receiver operating characteristic analysis was used to compare the prediction accuracy of CAD with the NLST investigator's diagnosis and readings from three experienced and board-certified thoracic radiologists who used contemporary clinical practice guidelines. Results In the validation cohort, the area under the receiver operating characteristic curve for CAD was 0.9154. By default, the sensitivity, specificity, and PPV of the NLST investigators were 1.00, 0.00, and 0.43, respectively. The sensitivity, specificity, PPV, and negative predictive value of CAD and the three radiologists' combined reading were 0.95, 0.88, 0.86, and 0.96 and 0.70, 0.69, 0.64, and 0.75, respectively. Conclusion CAD could increase PPV and reduce the false-positive rate in the early diagnosis of lung cancer. (©) RSNA, 2017 Online supplemental material is available for this article.",2018,10.1148/radiol.2017162725,diagnosis,True
Advanced Deep Learning Algorithms for Infectious Disease Modeling Using Clinical Data: A Case Study on COVID-19,"BACKGROUND: Dealing with the COVID-19 pandemic has been one of the most important objectives of many countries.Intently observing the growth dynamics of the cases is one way to accomplish the solution for the pandemic. INTRODUCTION: Infectious diseases are caused by a micro-organism/virus from another person or an animal. It causes difficulty at both the individual and collective levels. The ongoing episode of COVID-19 ailment, brought about by the new coronavirus first detected in Wuhan, China, and its quick spread far and wide revived the consideration of the world towards the impact of such plagues on an individual's everyday existence. We suggested that a basic structure be developed to work with the progressive examination of the development rate (cases/day) and development speed (cases/day2) of COVID-19 cases. METHODS: We attempt to exploit the effectiveness of advanced deep learning algorithms to predict the growth of infectious diseases based on time series data and classification based on symptoms text data and X-ray image data. The goal is to identify the nature of the phenomenon represented by the sequence of observations and forecasting. RESULTS: We concluded that our good habits and healthy lifestyle prevent the risk of COVID-19. We observed that by simply using masks in our daily lives, we could flatten the curve of increasing cases.Limiting human mobility resulted in a significant decrease in the development speed within a few days, a deceleration within two weeks, and a close to fixed development within six weeks. CONCLUSION: These outcomes authenticate that mass social isolation is a profoundly viable measure against the spread of SARS-CoV-2, as recently recommended. Aside from the research of country- by-country predominance, the proposed structure is useful for city, state, district, and discretionary region information, serving as a resource for screening COVID-19 cases in the area.",2022,10.2174/1573405617666210908125911,diagnosis,False
AI detection of mild COVID-19 pneumonia from chest CT scans,"OBJECTIVES: An artificial intelligence model was adopted to identify mild COVID-19 pneumonia from computed tomography (CT) volumes, and its diagnostic performance was then evaluated. METHODS: In this retrospective multicenter study, an atrous convolution-based deep learning model was established for the computer-assisted diagnosis of mild COVID-19 pneumonia. The dataset included 2087 chest CT exams collected from four hospitals between 1 January 2019 and 31 May 2020. The true positive rate, true negative rate, receiver operating characteristic curve, area under the curve (AUC) and convolutional feature map were used to evaluate the model. RESULTS: The proposed deep learning model was trained on 1538 patients and tested on an independent testing cohort of 549 patients. The overall sensitivity was 91.5% (195/213; p < 0.001, 95% CI: 89.2-93.9%), the overall specificity was 90.5% (304/336; p < 0.001, 95% CI: 88.0-92.9%) and the general AUC value was 0.955 (p < 0.001). CONCLUSIONS: A deep learning model can accurately detect COVID-19 and serve as an important supplement to the COVID-19 reverse transcription-polymerase chain reaction (RT-PCR) test. KEY POINTS: • The implementation of a deep learning model to identify mild COVID-19 pneumonia was confirmed to be effective and feasible. • The strategy of using a binary code instead of the region of interest label to identify mild COVID-19 pneumonia was verified. • This AI model can assist in the early screening of COVID-19 without interfering with normal clinical examinations.",2021,10.1007/s00330-021-07797-x,diagnosis,True
AI Lung Segmentation and Perfusion Analysis of Dual-Energy CT Can Help to Distinguish COVID-19 Infiltrates from Visually Similar Immunotherapy-Related Pneumonitis Findings and Can Optimize Radiological Workflows,"(1) To explore the potential impact of an AI dual-energy CT (DECT) prototype on decision making and workflows by investigating its capabilities to differentiate COVID-19 from immunotherapy-related pneumonitis. (2) Methods: From 3 April 2020 to 12 February 2021, DECT from biometrically matching patients with COVID-19, pneumonitis, and inconspicuous findings were selected from our clinical routine. Three blinded readers independently scored each pulmonary lobe analogous to CO-RADS. Inter-rater agreement was determined with an intraclass correlation coefficient (ICC). Averaged perfusion metrics per lobe (iodine uptake in mg, volume without vessels in ml, iodine concentration in mg/mL) were extracted using manual segmentation and an AI DECT prototype. A generalized linear mixed model was used to investigate metric validity and potential distinctions at equal CO-RADS scores. Multinomial regression measured the contribution ""Reader"", ""CO-RADS score"", and ""perfusion metrics"" to diagnosis. The time to diagnosis was measured for manual vs. AI segmentation. (3) Results: We included 105 patients (62 ± 13 years, mean BMI 27 ± 2). There were no significant differences between manually and AI-extracted perfusion metrics (p = 0.999). Regardless of the CO-RADS score, iodine uptake and concentration per lobe were significantly higher in COVID-19 than in pneumonitis (p < 0.001). In regression, iodine uptake had a greater contribution to diagnosis than CO-RADS scoring (Odds Ratio (OR) = 1.82 [95%CI 1.10-2.99] vs. OR = 0.20 [95%CI 0.14-0.29]). The AI prototype extracted the relevant perfusion metrics significantly faster than radiologists (10 ± 1 vs. 15 ± 2 min, p < 0.001). (4) Conclusions: The investigated AI prototype positively impacts decision making and workflows by extracting perfusion metrics that differentiate COVID-19 from visually similar pneumonitis significantly faster than radiologists.",2021,10.3390/tomography8010003,diagnosis,True
AI-based diagnosis of COVID-19 patients using X-ray scans with stochastic ensemble of CNNs,"According to the World Health Organization (WHO), novel coronavirus (COVID-19) is an infectious disease and has a significant social and economic impact. The main challenge in fighting against this disease is its scale. Due to the outbreak, medical facilities are under pressure due to case numbers. A quick diagnosis system is required to address these challenges. To this end, a stochastic deep learning model is proposed. The main idea is to constrain the deep-representations over a Gaussian prior to reinforce the discriminability in feature space. The model can work on chest X-ray or CT-scan images. It provides a fast diagnosis of COVID-19 and can scale seamlessly. The work presents a comprehensive evaluation of previously proposed approaches for X-ray based disease diagnosis. The approach works by learning a latent space over X-ray image distribution from the ensemble of state-of-the-art convolutional-nets, and then linearly regressing the predictions from an ensemble of classifiers which take the latent vector as input. We experimented with publicly available datasets having three classes: COVID-19, normal and pneumonia yielding an overall accuracy and AUC of 0.91 and 0.97, respectively. Moreover, for robust evaluation, experiments were performed on a large chest X-ray dataset to classify among Atelectasis, Effusion, Infiltration, Nodule, and Pneumonia classes. The results demonstrate that the proposed model has better understanding of the X-ray images which make the network more generic to be later used with other domains of medical image analysis.",2021,10.1007/s13246-021-01060-9,diagnosis,True
AI-based improvement in lung cancer detection on chest radiographs: results of a multi-reader study in NLST dataset,"OBJECTIVE: Assess if deep learning-based artificial intelligence (AI) algorithm improves reader performance for lung cancer detection on chest X-rays (CXRs). METHODS: This reader study included 173 images from cancer-positive patients (n = 98) and 346 images from cancer-negative patients (n = 196) selected from National Lung Screening Trial (NLST). Eight readers, including three radiology residents, and five board-certified radiologists, participated in the observer performance test. AI algorithm provided image-level probability of pulmonary nodule or mass on CXRs and a heatmap of detected lesions. Reader performance was compared with AUC, sensitivity, specificity, false-positives per image (FPPI), and rates of chest CT recommendations. RESULTS: With AI, the average sensitivity of readers for the detection of visible lung cancer increased for residents, but was similar for radiologists compared to that without AI (0.61 [95% CI, 0.55-0.67] vs. 0.72 [95% CI, 0.66-0.77], p = 0.016 for residents, and 0.76 [95% CI, 0.72-0.81] vs. 0.76 [95% CI, 0.72-0.81, p = 1.00 for radiologists), while false-positive findings per image (FPPI) was similar for residents, but decreased for radiologists (0.15 [95% CI, 0.11-0.18] vs. 0.12 [95% CI, 0.09-0.16], p = 0.13 for residents, and 0.24 [95% CI, 0.20-0.29] vs. 0.17 [95% CI, 0.13-0.20], p < 0.001 for radiologists). With AI, the average rate of chest CT recommendation in patients positive for visible cancer increased for residents, but was similar for radiologists (54.7% [95% CI, 48.2-61.2%] vs. 70.2% [95% CI, 64.2-76.2%], p < 0.001 for residents and 72.5% [95% CI, 68.0-77.1%] vs. 73.9% [95% CI, 69.4-78.3%], p = 0.68 for radiologists), while that in cancer-negative patients was similar for residents, but decreased for radiologists (11.2% [95% CI, 9.6-13.1%] vs. 9.8% [95% CI, 8.0-11.6%], p = 0.32 for residents and 16.4% [95% CI, 14.7-18.2%] vs. 11.7% [95% CI, 10.2-13.3%], p < 0.001 for radiologists). CONCLUSIONS: AI algorithm can enhance the performance of readers for the detection of lung cancers on chest radiographs when used as second reader. KEY POINTS: • Reader study in the NLST dataset shows that AI algorithm had sensitivity benefit for residents and specificity benefit for radiologists for the detection of visible lung cancer. • With AI, radiology residents were able to recommend more chest CT examinations (54.7% vs 70.2%, p < 0.001) for patients with visible lung cancer. • With AI, radiologists recommended significantly less proportion of unnecessary chest CT examinations (16.4% vs. 11.7%, p < 0.001) in cancer-negative patients.",2021,10.1007/s00330-021-08074-7,diagnosis,False
AI-Based Quantitative CT Analysis of Temporal Changes According to Disease Severity in COVID-19 Pneumonia,"OBJECTIVE: To quantitatively evaluate computed tomography (CT) parameters of coronavirus disease 2019 (COVID-19) pneumonia an artificial intelligence (AI)-based software in different clinical severity groups during the disease course. METHODS: From March 11 to April 15, 2020, 51 patients (age, 18-84 years; 28 men) diagnosed and hospitalized with COVID-19 pneumonia with a total of 116 CT scans were enrolled in the study. Patients were divided into mild (n = 12), moderate (n = 31), and severe (n = 8) groups based on clinical severity. An AI-based quantitative CT analysis, including lung volume, opacity score, opacity volume, percentage of opacity, and mean lung density, was performed in initial and follow-up CTs obtained at different time points. Receiver operating characteristic analysis was performed to find the diagnostic ability of quantitative CT parameters for discriminating severe from nonsevere pneumonia. RESULTS: In baseline assessment, the severe group had significantly higher opacity score, opacity volume, higher percentage of opacity, and higher mean lung density than the moderate group (all P ≤ 0.001). Through consecutive time points, the severe group had a significant decrease in lung volume (P = 0.006), a significant increase in total opacity score (P = 0.003), and percentage of opacity (P = 0.007). A significant increase in total opacity score was also observed for the mild group (P = 0.011). Residual opacities were observed in all groups. The involvement of more than 4 lobes (sensitivity, 100%; specificity, 65.26%), total opacity score greater than 4 (sensitivity, 100%; specificity, 64.21), total opacity volume greater than 337.4 mL (sensitivity, 80.95%; specificity, 84.21%), percentage of opacity greater than 11% (sensitivity, 80.95%; specificity, 88.42%), total high opacity volume greater than 10.5 mL (sensitivity, 95.24%; specificity, 66.32%), percentage of high opacity greater than 0.8% (sensitivity, 85.71%; specificity, 80.00%) and mean lung density HU greater than -705 HU (sensitivity, 57.14%; specificity, 90.53%) were related to severe pneumonia. CONCLUSIONS: An AI-based quantitative CT analysis is an objective tool in demonstrating disease severity and can also assist the clinician in follow-up by providing information about the disease course and prognosis according to different clinical severity groups.",2021,10.1097/rct.0000000000001224,diagnosis,True
ai-corona: Radiologist-assistant deep learning framework for COVID-19 diagnosis in chest CT scans,"The development of medical assisting tools based on artificial intelligence advances is essential in the global fight against COVID-19 outbreak and the future of medical systems. In this study, we introduce ai-corona, a radiologist-assistant deep learning framework for COVID-19 infection diagnosis using chest CT scans. Our framework incorporates an EfficientNetB3-based feature extractor. We employed three datasets; the CC-CCII set, the MasihDaneshvari Hospital (MDH) cohort, and the MosMedData cohort. Overall, these datasets constitute 7184 scans from 5693 subjects and include the COVID-19, non-COVID abnormal (NCA), common pneumonia (CP), non-pneumonia, and Normal classes. We evaluate ai-corona on test sets from the CC-CCII set, MDH cohort, and the entirety of the MosMedData cohort, for which it gained AUC scores of 0.997, 0.989, and 0.954, respectively. Our results indicates ai-corona outperforms all the alternative models. Lastly, our framework's diagnosis capabilities were evaluated as assistant to several experts. Accordingly, We observed an increase in both speed and accuracy of expert diagnosis when incorporating ai-corona's assistance.",2021,10.1371/journal.pone.0250952,diagnosis,True
AI-driven deep convolutional neural networks for chest X-ray pathology identification,"BACKGROUND: Chest X-ray images are widely used to detect many different lung diseases. However, reading chest X-ray images to accurately detect and classify different lung diseases by doctors is often difficult with large inter-reader variability. Thus, there is a huge demand for developing computer-aided automated schemes of chest X-ray images to help doctors more accurately and efficiently detect lung diseases depicting on chest X-ray images. OBJECTIVE: To develop convolution neural network (CNN) based deep learning models and compare their feasibility and performance to classify 14 chest diseases or pathology patterns based on chest X-rays. METHOD: Several CNN models pre-trained using ImageNet dataset are modified as transfer learning models and applied to classify between 14 different chest pathology and normal chest patterns depicting on chest X-ray images. In this process, a deep convolution generative adversarial network (DC-GAN) is also trained to mitigate the effects of small or imbalanced dataset and generate synthetic images to balance the dataset of different diseases. The classification models are trained and tested using a large dataset involving 91,324 frontal-view chest X-ray images. RESULTS: In this study, eight models are trained and compared. Among them, ResNet-152 model achieves an accuracy of 67% and 62% with and without data augmentation, respectively. Inception-V3, NasNetLarge, Xcaption, ResNet-50 and InceptionResNetV2 achieve accuracy of 68%, 62%, 66%, 66% and 54% respectively. Additionally, Resnet-152 with data augmentation achieves an accuracy of 83% but only for six classes. CONCLUSION: This study solves the problem of having fewer data by using GAN-based techniques to add synthetic images and demonstrates the feasibility of applying transfer learning CNN method to help classify 14 types of chest diseases depicting on chest X-ray images.",2022,10.3233/xst-211082,diagnosis,False
AI-Driven Model for Automatic Emphysema Detection in Low-Dose Computed Tomography Using Disease-Specific Augmentation,"The objective of this study is to evaluate the feasibility of a disease-specific deep learning (DL) model based on minimum intensity projection (minIP) for automated emphysema detection in low-dose computed tomography (LDCT) scans. LDCT scans of 240 individuals from a population-based cohort in the Netherlands (ImaLife study, mean age ± SD = 57 ± 6 years) were retrospectively chosen for training and internal validation of the DL model. For independent testing, LDCT scans of 125 individuals from a lung cancer screening cohort in the USA (NLST study, mean age ± SD = 64 ± 5 years) were used. Dichotomous emphysema diagnosis based on radiologists' annotation was used to develop the model. The automated model included minIP processing (slab thickness range: 1 mm to 11 mm), classification, and detection maps generation. The data-split for the pipeline evaluation involved class-balanced and imbalanced settings. The proposed DL pipeline showed the highest performance (area under receiver operating characteristics curve) for 11 mm slab thickness in both the balanced (ImaLife = 0.90 ± 0.05) and the imbalanced dataset (NLST = 0.77 ± 0.06). For ImaLife subcohort, the variation in minIP slab thickness from 1 to 11 mm increased the DL model's sensitivity from 75 to 88% and decreased the number of false-negative predictions from 10 to 5. The minIP-based DL model can automatically detect emphysema in LDCTs. The performance of thicker minIP slabs was better than that of thinner slabs. LDCT can be leveraged for emphysema detection by applying disease specific augmentation.",2022,10.1007/s10278-022-00599-7,diagnosis,True
"AI-driven quantification, staging and outcome prediction of COVID-19 pneumonia","Coronavirus disease 2019 (COVID-19) emerged in 2019 and disseminated around the world rapidly. Computed tomography (CT) imaging has been proven to be an important tool for screening, disease quantification and staging. The latter is of extreme importance for organizational anticipation (availability of intensive care unit beds, patient management planning) as well as to accelerate drug development through rapid, reproducible and quantified assessment of treatment response. Even if currently there are no specific guidelines for the staging of the patients, CT together with some clinical and biological biomarkers are used. In this study, we collected a multi-center cohort and we investigated the use of medical imaging and artificial intelligence for disease quantification, staging and outcome prediction. Our approach relies on automatic deep learning-based disease quantification using an ensemble of architectures, and a data-driven consensus for the staging and outcome prediction of the patients fusing imaging biomarkers with clinical and biological attributes. Highly promising results on multiple external/independent evaluation cohorts as well as comparisons with expert human readers demonstrate the potentials of our approach.",2021,10.1016/j.media.2020.101860,prognosis,True
AIforCOVID: Predicting the clinical outcomes in patients with COVID-19 applying AI to chest-X-rays. An Italian multicentre study,"Recent epidemiological data report that worldwide more than 53 million people have been infected by SARS-CoV-2, resulting in 1.3 million deaths. The disease has been spreading very rapidly and few months after the identification of the first infected, shortage of hospital resources quickly became a problem. In this work we investigate whether artificial intelligence working with chest X-ray (CXR) scans and clinical data can be used as a possible tool for the early identification of patients at risk of severe outcome, like intensive care or death. Indeed, further to induce lower radiation dose than computed tomography (CT), CXR is a simpler and faster radiological technique, being also more widespread. In this respect, we present three approaches that use features extracted from CXR images, either handcrafted or automatically learnt by convolutional neuronal networks, which are then integrated with the clinical data. As a further contribution, this work introduces a repository that collects data from 820 patients enrolled in six Italian hospitals in spring 2020 during the first COVID-19 emergency. The dataset includes CXR images, several clinical attributes and clinical outcomes. Exhaustive evaluation shows promising performance both in 10-fold and leave-one-centre-out cross-validation, suggesting that clinical data and images have the potential to provide useful information for the management of patients and hospital resources.",2021,10.1016/j.media.2021.102216,prognosis,False
AIR-Net: A novel multi-task learning method with auxiliary image reconstruction for predicting EGFR mutation status on CT images of NSCLC patients,"Automated and accurate EGFR mutation status prediction using computed tomography (CT) imagery is of great value for tailoring optimal treatments to non-small cell lung cancer (NSCLC) patients. However, existing deep learning based methods usually adopt a single task learning strategy to design and train EGFR mutation status prediction models with limited training data, which may be insufficient to learn distinguishable representations for promoting prediction performance. In this paper, a novel multi-task learning method named AIR-Net is proposed to precisely predict EGFR mutation status on CT images. First, an auxiliary image reconstruction task is effectively integrated with EGFR mutation status prediction, aiming at providing extra supervision at the training phase. Particularly, we adequately employ multi-level information in a shared encoder to generate more comprehensive representations of tumors. Second, a powerful feature consistency loss is further introduced to constrain semantic consistency of original and reconstructed images, which contributes to enhanced image reconstruction and offers more effective regularization to AIR-Net during training. Performance analysis of AIR-Net indicates that auxiliary image reconstruction plays an essential role in identifying EGFR mutation status. Furthermore, extensive experimental results demonstrate that our method achieves favorable performance against other competitive prediction methods. All the results executed in this study suggest that the effectiveness and superiority of AIR-Net in precisely predicting EGFR mutation status of NSCLC.",2022,10.1016/j.compbiomed.2021.105157,diagnosis,True
ALK molecular phenotype in non-small cell lung cancer: CT radiogenomic characterization,"PURPOSE: To present a radiogenomic computed tomographic (CT) characterization of anaplastic lymphoma kinase (ALK)-rearranged non-small cell lung cancer (NSCLC) (ALK+). MATERIALS AND METHODS: In this HIPAA-compliant institutional review board-approved retrospective study, CT studies, ALK status, and clinical-pathologic data in 172 patients with NSCLC from three institutions were analyzed. A screen of 24 CT image traits was performed in a training set of 59 patients, followed by random forest variable selection incorporating 24 CT traits plus six clinical-pathologic covariates to identify a radiogenomic predictor of ALK+ status. This predictor was then validated in an independent cohort (n = 113). Test-for-accuracy and subset analyses were performed. A similar analysis was performed to identify a biomarker associated with shorter progression-free survival (PFS) after therapy with the ALK inhibitor crizotinib. RESULTS: ALK+ status was associated with central tumor location, absence of pleural tail, and large pleural effusion. An ALK+ radiogenomic CT status biomarker consisting of these three imaging traits with patient age of younger than 60 years showed strong discriminatory power for ALK+ status, with a sensitivity of 83.3% (15 of 18), a specificity of 77.9% (74 of 95), and an accuracy of 78.8% (89 of 113) in independent testing. The discriminatory power was particularly strong in patients with operable disease (stage IIIA or lower), with a sensitivity of 100.0% (five of five), a specificity of 88.1% (37 of 42), and an accuracy of 89.4% (42 of 47). Tumors with a disorganized vessel pattern had a shorter PFS with crizotinib therapy than tumors without this trait (11.4 vs 20.2 months, P = .041). CONCLUSION: ALK+ NSCLC has distinct characteristics at CT imaging that, when combined with clinical covariates, discriminate ALK+ from non-ALK tumors and can potentially identify patients with a shorter durable response to crizotinib.",2014,10.1148/radiol.14140789,prognosis,True
An [18F]FDG-PET/CT deep learning method for fully automated detection of pathological mediastinal lymph nodes in lung cancer patients,"PURPOSE: The identification of pathological mediastinal lymph nodes is an important step in the staging of lung cancer, with the presence of metastases significantly affecting survival rates. Nodes are currently identified by a physician, but this process is time-consuming and prone to errors. In this paper, we investigate the use of artificial intelligence-based methods to increase the accuracy and consistency of this process. METHODS: Whole-body (18)F-labelled fluoro-2-deoxyglucose ([18F]FDG) positron emission tomography/computed tomography ([18F]FDG-PET/CT) scans (Philips Gemini TF) from 134 patients were retrospectively analysed. The thorax was automatically located, and then slices were fed into a U-Net to identify candidate regions. These regions were split into overlapping 3D cubes, which were individually predicted as positive or negative using a 3D CNN. From these predictions, pathological mediastinal nodes could be identified. A second cohort of 71 patients was then acquired from a different, newer scanner (GE Discovery MI), and the performance of the model on this dataset was tested with and without transfer learning. RESULTS: On the test set from the first scanner, our model achieved a sensitivity of 0.87 (95% confidence intervals [0.74, 0.94]) with 0.41 [0.22, 0.71] false positives/patient. This was comparable to the performance of an expert. Without transfer learning, on the test set from the second scanner, the corresponding results were 0.53 [0.35, 0.70] and 0.24 [0.10, 0.49], respectively. With transfer learning, these metrics were 0.88 [0.73, 0.97] and 0.69 [0.43, 1.04], respectively. CONCLUSION: Model performance was comparable to that of an expert on data from the same scanner. With transfer learning, the model can be applied to data from a different scanner. To our knowledge it is the first study of its kind to go directly from whole-body [18F]FDG-PET/CT scans to pathological mediastinal lymph node localisation.",2022,10.1007/s00259-021-05513-x,diagnosis,True
An adaptive pulmonary nodule detection algorithm,"Recently, lung cancer has been paid more and more attention. People have reached a consensus that early detection and early treatment can improve the survival rate of patients. Among them, pulmonary nodules are the important reference for doctors to determine the lung health. With the continuous improvement of CT image resolution, more suspected pulmonary nodule information appears from the impact of chest CT. How to relatively and accurately locate the suspected nodule location from a large number of CT images has brought challenges to the doctor's daily diagnosis. To solve the problem that the original DBSCAN clustering algorithm needs manual setting of the threshold, this paper proposes a region growing algorithm and an adaptive DBSCAN clustering algorithm to improve the accuracy of pulmonary nodule detection. The image is roughly processed and ROI (Regions of Interest) region is roughly extracted by CLAHE transform. The region growing algorithm is used to roughly process the adjacent region's expansibility and the suspected region in ROI, and mark the center point in the region and the boundary point of its point set. The mean value of region range is taken as the threshold value of DBSCAN clustering algorithm. The center of the point domain is used as the starting point of clustering, and the rough set of points is used as the MinPts threshold. Finally, the clustering results are labeled in the initial CT image. Experiments show that the pulmonary nodule detection method proposed in this paper effectively improves the accuracy of the detection results.",2020,10.3233/xst-200656,diagnosis,True
An aggregate method for thorax diseases classification,"A common problem found in real-word medical image classification is the inherent imbalance of the positive and negative patterns in the dataset where positive patterns are usually rare. Moreover, in the classification of multiple classes with neural network, a training pattern is treated as a positive pattern in one output node and negative in all the remaining output nodes. In this paper, the weights of a training pattern in the loss function are designed based not only on the number of the training patterns in the class but also on the different nodes where one of them treats this training pattern as positive and the others treat it as negative. We propose a combined approach of weights calculation algorithm for deep network training and the training optimization from the state-of-the-art deep network architecture for thorax diseases classification problem. Experimental results on the Chest X-Ray image dataset demonstrate that this new weighting scheme improves classification performances, also the training optimization from the EfficientNet improves the performance furthermore. We compare the aggregate method with several performances from the previous study of thorax diseases classifications to provide the fair comparisons against the proposed method.",2021,10.1038/s41598-021-81765-9,diagnosis,False
An approach to the classification of COVID-19 based on CT scans using convolutional features and genetic algorithms,"COVID-19 is a respiratory disease that, as of July 15th, 2021, has infected more than 187 million people worldwide and is responsible for more than 4 million deaths. An accurate diagnosis of COVID-19 is essential for the treatment and control of the disease. The use of computed tomography (CT) has shown to be promising for evaluating patients suspected of COVID-19 infection. The analysis of a CT examination is complex, and requires attention from a specialist. This paper presents a methodology for detecting COVID-19 from CT images. We first propose a convolutional neural network architecture to extract features from CT images, and then optimize the hyperparameters of the network using a tree Parzen estimator to choose the best parameters. Following this, we apply a selection of features using a genetic algorithm. Finally, classification is performed using four classifiers with different approaches. The proposed methodology achieved an accuracy of 0.997, a kappa index of 0.995, an AUROC of 0.997, and an AUPRC of 0.997 on the SARS-CoV-2 CT-Scan dataset, and an accuracy of 0.987, a kappa index of 0.975, an AUROC of 0.989, and an AUPRC of 0.987 on the COVID-CT dataset, using our CNN after optimization of the hyperparameters, the selection of features and the multi-layer perceptron classifier. Compared with pretrained CNNs and related state-of-the-art works, the results achieved by the proposed methodology were superior. Our results show that the proposed method can assist specialists in screening and can aid in diagnosing patients with suspected COVID-19.",2021,10.1016/j.compbiomed.2021.104744,diagnosis,True
An Artificial Intelligence-Based Chest X-ray Model on Human Nodule Detection Accuracy From a Multicenter Study,"IMPORTANCE: Most early lung cancers present as pulmonary nodules on imaging, but these can be easily missed on chest radiographs. OBJECTIVE: To assess if a novel artificial intelligence (AI) algorithm can help detect pulmonary nodules on radiographs at different levels of detection difficulty. DESIGN, SETTING, AND PARTICIPANTS: This diagnostic study included 100 posteroanterior chest radiograph images taken between 2000 and 2010 of adult patients from an ambulatory health care center in Germany and a lung image database in the US. Included images were selected to represent nodules with different levels of detection difficulties (from easy to difficult), and comprised both normal and nonnormal control. EXPOSURES: All images were processed with a novel AI algorithm, the AI Rad Companion Chest X-ray. Two thoracic radiologists established the ground truth and 9 test radiologists from Germany and the US independently reviewed all images in 2 sessions (unaided and AI-aided mode) with at least a 1-month washout period. MAIN OUTCOMES AND MEASURES: Each test radiologist recorded the presence of 5 findings (pulmonary nodules, atelectasis, consolidation, pneumothorax, and pleural effusion) and their level of confidence for detecting the individual finding on a scale of 1 to 10 (1 representing lowest confidence; 10, highest confidence). The analyzed metrics for nodules included sensitivity, specificity, accuracy, and receiver operating characteristics curve area under the curve (AUC). RESULTS: Images from 100 patients were included, with a mean (SD) age of 55 (20) years and including 64 men and 36 women. Mean detection accuracy across the 9 radiologists improved by 6.4% (95% CI, 2.3% to 10.6%) with AI-aided interpretation compared with unaided interpretation. Partial AUCs within the effective interval range of 0 to 0.2 false positive rate improved by 5.6% (95% CI, -1.4% to 12.0%) with AI-aided interpretation. Junior radiologists saw greater improvement in sensitivity for nodule detection with AI-aided interpretation as compared with their senior counterparts (12%; 95% CI, 4% to 19% vs 9%; 95% CI, 1% to 17%) while senior radiologists experienced similar improvement in specificity (4%; 95% CI, -2% to 9%) as compared with junior radiologists (4%; 95% CI, -3% to 5%). CONCLUSIONS AND RELEVANCE: In this diagnostic study, an AI algorithm was associated with improved detection of pulmonary nodules on chest radiographs compared with unaided interpretation for different levels of detection difficulty and for readers with different experience.",2021,10.1001/jamanetworkopen.2021.41096,diagnosis,False
An artificial neural network (ANN)-based lung-tumor motion predictor for intrafractional MR tumor tracking,"PURPOSE: To address practical issues of implementing artificial neural networks (ANN) for lung-tumor motion prediction in MRI-based intrafractional lung-tumor tracking. METHODS: A feedforward four-layered ANN structure is used to predict future tumor positions. A back-propagation algorithm is used for ANN learning. Adaptive learning is incorporated by continuously updating weights and learning rate during prediction. An ANN training scheme specific for MRI-based tracking is developed. A multiple-ANN structure is developed to reduce tracking failures caused by the lower imaging rates of MRI. We used particle swarm optimization to optimize the ANN structure and initial weights (IW) for each patient and treatment fraction. Prediction accuracy is evaluated using the 1D superior-inferior lung-tumor motions of 29 lung cancer patients for system delays of 120-520 ms, in increments of 80 ms. The result is compared with four different scenarios: (1), (2) ANN structure optimization + with∕without IW optimization, and (3), (4) no ANN structure optimization + with∕without IW optimization, respectively. An additional simulation is performed to assess the value of optimizing the ANN structure for each treatment fraction. RESULTS: For 120-520 ms system delays, mean RMSE values (ranges 0.0-2.8 mm from 29 patients) of 0.5-0.9 mm are observed, respectively. Using patient specific ANN structures, a 30%-60% decrease in mean RMSE values is observed as a result of IW optimization, alone. No significant advantages in prediction performance are observed, however, by optimizing for each fraction. CONCLUSIONS: A new ANN-based lung-tumor motion predictor is developed for MRI-based intrafractional tumor tracking. The prediction accuracy of our predictor is evaluated using a realistic simulated MR imaging rate and system delays. For 120-520 ms system delays, mean RMSE values of 0.5-0.9 mm (ranges 0.0-2.8 mm from 29 patients) are achieved. Further, the advantage of patient specific ANN structure and IW in lung-tumor motion prediction is demonstrated by a 30%-60% decrease in mean RMSE values.",2012,10.1118/1.4730294,treatment,False
An artificial-intelligence lung imaging analysis system (ALIAS) for population-based nodule computing in CT scans,"Computed tomography (CT) screening is essential for early lung cancer detection. With the development of artificial intelligence techniques, it is particularly desirable to explore the ability of current state-of-the-art methods and to analyze nodule features in terms of a large population. In this paper, we present an artificial-intelligence lung image analysis system (ALIAS) for nodule detection and segmentation. And after segmenting the nodules, the locations, sizes, as well as imaging features are computed at the population level for studying the differences between benign and malignant nodules. The results provide better understanding of the underlying imaging features and their ability for early lung cancer diagnosis.",2021,10.1016/j.compmedimag.2021.101899,diagnosis,True
An automated COVID-19 detection based on fused dynamic exemplar pyramid feature extraction and hybrid feature selection using deep learning,"The new coronavirus disease known as COVID-19 is currently a pandemic that is spread out the whole world. Several methods have been presented to detect COVID-19 disease. Computer vision methods have been widely utilized to detect COVID-19 by using chest X-ray and computed tomography (CT) images. This work introduces a model for the automatic detection of COVID-19 using CT images. A novel handcrafted feature generation technique and a hybrid feature selector are used together to achieve better performance. The primary goal of the proposed framework is to achieve a higher classification accuracy than convolutional neural networks (CNN) using handcrafted features of the CT images. In the proposed framework, there are four fundamental phases, which are preprocessing, fused dynamic sized exemplars based pyramid feature generation, ReliefF, and iterative neighborhood component analysis based feature selection and deep neural network classifier. In the preprocessing phase, CT images are converted into 2D matrices and resized to 256 × 256 sized images. The proposed feature generation network uses dynamic-sized exemplars and pyramid structures together. Two basic feature generation functions are used to extract statistical and textural features. The selected most informative features are forwarded to artificial neural networks (ANN) and deep neural network (DNN) for classification. ANN and DNN models achieved 94.10% and 95.84% classification accuracies respectively. The proposed fused feature generator and iterative hybrid feature selector achieved the best success rate, according to the results obtained by using CT images.",2021,10.1016/j.compbiomed.2021.104356,diagnosis,True
An automated diagnosis and classification of COVID-19 from chest CT images using a transfer learning-based convolutional neural network,"Researchers have developed more intelligent, highly responsive, and efficient detection methods owing to the COVID-19 demands for more widespread diagnosis. The work done deals with developing an AI-based framework that can help radiologists and other healthcare professionals diagnose COVID-19 cases at a high level of accuracy. However, in the absence of publicly available CT datasets, the development of such AI tools can prove challenging. Therefore, an algorithm for performing automatic and accurate COVID-19 classification using Convolutional Neural Network (CNN), pre-trained model, and Sparrow search algorithm (SSA) on CT lung images was proposed. The pre-trained CNN models used are SeresNext50, SeresNext101, SeNet154, MobileNet, MobileNetV2, MobileNetV3Small, and MobileNetV3Large. In addition, the SSA will be used to optimize the different CNN and transfer learning(TL) hyperparameters to find the best configuration for the pre-trained model used and enhance its performance. Two datasets are used in the experiments. There are two classes in the first dataset, while three in the second. The authors combined two publicly available COVID-19 datasets as the first dataset, namely the COVID-19 Lung CT Scans and COVID-19 CT Scan Dataset. In total, 14,486 images were included in this study. The authors analyzed the Large COVID-19 CT scan slice dataset in the second dataset, which utilized 17,104 images. Compared to other pre-trained models on both classes datasets, MobileNetV3Large pre-trained is the best model. As far as the three-classes dataset is concerned, a model trained on SeNet154 is the best available. Results show that, when compared to other CNN models like LeNet-5 CNN, COVID faster R-CNN, Light CNN, Fuzzy + CNN, Dynamic CNN, CNN and Optimized CNN, the proposed Framework achieves the best accuracy of 99.74% (two classes) and 98% (three classes).",2022,10.1016/j.compbiomed.2022.105383,diagnosis,True
An Automatic Detection System of Lung Nodule Based on Multigroup Patch-Based Deep Learning Network,"High-efficiency lung nodule detection dramatically contributes to the risk assessment of lung cancer. It is a significant and challenging task to quickly locate the exact positions of lung nodules. Extensive work has been done by researchers around this domain for approximately two decades. However, previous computer-aided detection (CADe) schemes are mostly intricate and time-consuming since they may require more image processing modules, such as the computed tomography image transformation, the lung nodule segmentation, and the feature extraction, to construct a whole CADe system. It is difficult for these schemes to process and analyze enormous data when the medical images continue to increase. Besides, some state of the art deep learning schemes may be strict in the standard of database. This study proposes an effective lung nodule detection scheme based on multigroup patches cut out from the lung images, which are enhanced by the Frangi filter. Through combining two groups of images, a four-channel convolution neural networks model is designed to learn the knowledge of radiologists for detecting nodules of four levels. This CADe scheme can acquire the sensitivity of 80.06% with 4.7 false positives per scan and the sensitivity of 94% with 15.1 false positives per scan. The results demonstrate that the multigroup patch-based learning system is efficient to improve the performance of lung nodule detection and greatly reduce the false positives under a huge amount of image data.",2018,10.1109/jbhi.2017.2725903,diagnosis,True
An effective approach for CT lung segmentation using mask region-based convolutional neural networks,"Computer vision systems have numerous tools to assist in various medical fields, notably in image diagnosis. Computed tomography (CT) is the principal imaging method used to assist in the diagnosis of diseases such as bone fractures, lung cancer, heart disease, and emphysema, among others. Lung cancer is one of the four main causes of death in the world. The lung regions in the CT images are marked manually by a specialist as this initial step is a significant challenge for computer vision techniques. Once defined, the lung regions are segmented for clinical diagnoses. This work proposes an automatic segmentation of the lungs in CT images, using the Convolutional Neural Network (CNN) Mask R-CNN, to specialize the model for lung region mapping, combined with supervised and unsupervised machine learning methods (Bayes, Support Vectors Machine (SVM), K-means and Gaussian Mixture Models (GMMs)). Our approach using Mask R-CNN with the K-means kernel produced the best results for lung segmentation reaching an accuracy of 97.68 ± 3.42% and an average runtime of 11.2 s. We compared our results against other works for validation purposes, and our approach had the highest accuracy and was faster than some state-of-the-art methods.",2020,10.1016/j.artmed.2020.101792,diagnosis,True
An Efficient Deep Learning Model to Detect COVID-19 Using Chest X-ray Images,"The tragic pandemic of COVID-19, due to the Severe Acute Respiratory Syndrome coronavirus-2 or SARS-CoV-2, has shaken the entire world, and has significantly disrupted healthcare systems in many countries. Because of the existing challenges and controversies to testing for COVID-19, improved and cost-effective methods are needed to detect the disease. For this purpose, machine learning (ML) has emerged as a strong forecasting method for detecting COVID-19 from chest X-ray images. In this paper, we used a Deep Learning Method (DLM) to detect COVID-19 using chest X-ray (CXR) images. Radiographic images are readily available and can be used effectively for COVID-19 detection compared to other expensive and time-consuming pathological tests. We used a dataset of 10,040 samples, of which 2143 had COVID-19, 3674 had pneumonia (but not COVID-19), and 4223 were normal (not COVID-19 or pneumonia). Our model had a detection accuracy of 96.43% and a sensitivity of 93.68%. The area under the ROC curve was 99% for COVID-19, 97% for pneumonia (but not COVID-19 positive), and 98% for normal cases. In conclusion, ML approaches may be used for rapid analysis of CXR images and thus enable radiologists to filter potential candidates in a time-effective manner to detect COVID-19.",2022,10.3390/ijerph19042013,diagnosis,False
An Efficient Method for Coronavirus Detection Through X-rays Using Deep Neural Network,"BACKGROUND: Coronavirus (COVID-19) is a group of infectious diseases caused by related viruses called coronaviruses. In humans, the seriousness of infection caused by a coronavirus in the respiratory tract can vary from mild to lethal. A serious illness can be developed in old people and those with underlying medical problems like diabetes, cardiovascular disease, cancer, and chronic respiratory disease. For the diagnosis of coronavirus disease, due to the growing number of cases, a limited number of test kits for COVID-19 are available in the hospitals. Hence, it is important to implement an automated system as an immediate alternative diagnostic option to pause the spread of COVID-19 in the population. OBJECTIVE: This paper proposes a deep learning model for the classification of coronavirus infected patient detection using chest X-ray radiographs. METHODS: A fully connected convolutional neural network model is developed to classify healthy and diseased X-ray radiographs. The proposed neural network model consists of seven convolutional layers with the rectified linear unit, softmax (last layer) activation functions, and max-pooling layers which were trained using the publicly available COVID-19 dataset. RESULTS AND CONCLUSION: For validation of the proposed model, the publicly available chest X-ray radiograph dataset consisting of COVID-19 and normal patient's images were used. Considering the performance of the results that are evaluated based on various evaluation metrics such as precision, recall, MSE, RMSE and accuracy, it is seen that the accuracy of the proposed CNN model is 98.07%.",2022,10.2174/1573405617999210112193220,diagnosis,False
An efficient mixture of deep and machine learning models for COVID-19 diagnosis in chest X-ray images,"A newly emerged coronavirus (COVID-19) seriously threatens human life and health worldwide. In coping and fighting against COVID-19, the most critical step is to effectively screen and diagnose infected patients. Among them, chest X-ray imaging technology is a valuable imaging diagnosis method. The use of computer-aided diagnosis to screen X-ray images of COVID-19 cases can provide experts with auxiliary diagnosis suggestions, which can reduce the burden of experts to a certain extent. In this study, we first used conventional transfer learning methods, using five pre-trained deep learning models, which the Xception model showed a relatively ideal effect, and the diagnostic accuracy reached 96.75%. In order to further improve the diagnostic accuracy, we propose an efficient diagnostic method that uses a combination of deep features and machine learning classification. It implements an end-to-end diagnostic model. The proposed method was tested on two datasets and performed exceptionally well on both of them. We first evaluated the model on 1102 chest X-ray images. The experimental results show that the diagnostic accuracy of Xception + SVM is as high as 99.33%. Compared with the baseline Xception model, the diagnostic accuracy is improved by 2.58%. The sensitivity, specificity and AUC of this model reached 99.27%, 99.38% and 99.32%, respectively. To further illustrate the robustness of our method, we also tested our proposed model on another dataset. Finally also achieved good results. Compared with related research, our proposed method has higher classification accuracy and efficient diagnostic performance. Overall, the proposed method substantially advances the current radiology based methodology, it can be very helpful tool for clinical practitioners and radiologists to aid them in diagnosis and follow-up of COVID-19 cases.",2020,10.1371/journal.pone.0242535,diagnosis,False
An Embedded Multi-branch 3D Convolution Neural Network for False Positive Reduction in Lung Nodule Detection,"Numerous lung nodule candidates can be produced through an automated lung nodule detection system. Classifying these candidates to reduce false positives is an important step in the detection process. The objective during this paper is to predict real nodules from a large number of pulmonary nodule candidates. Facing the challenge of the classification task, we propose a novel 3D convolution neural network (CNN) to reduce false positives in lung nodule detection. The novel 3D CNN includes embedded multiple branches in its structure. Each branch processes a feature map from a layer with different depths. All of these branches are cascaded at their ends; thus, features from different depth layers are combined to predict the categories of candidates. The proposed method obtains a competitive score in lung nodule candidate classification on LUNA16 dataset with an accuracy of 0.9783, a sensitivity of 0.8771, a precision of 0.9426, and a specificity of 0.9925. Moreover, a good performance on the competition performance metric (CPM) is also obtained with a score of 0.830. As a 3D CNN, the proposed model can learn complete and three-dimensional discriminative information about nodules and non-nodules to avoid some misidentification problems caused due to lack of spatial correlation information extracted from traditional methods or 2D networks. As an embedded multi-branch structure, the model is also more effective in recognizing the nodules of various shapes and sizes. As a result, the proposed method gains a competitive score on the false positive reduction in lung nodule detection and can be used as a reference for classifying nodule candidates.",2020,10.1007/s10278-020-00326-0,diagnosis,True
An ensemble learning method based on ordinal regression for COVID-19 diagnosis from chest CT,"Coronavirus disease 2019 (COVID-19) has brought huge losses to the world, and it remains a great threat to public health. X-ray computed tomography (CT) plays a central role in the management of COVID-19. Traditional diagnosis with pulmonary CT images is time-consuming and error-prone, which could not meet the need for precise and rapid COVID-19 screening. Nowadays, deep learning (DL) has been successfully applied to CT image analysis, which assists radiologists in workflow scheduling and treatment planning for patients with COVID-19. Traditional methods use cross-entropy as the loss function with a Softmax classifier following a fully-connected layer. Most DL-based classification methods target intraclass relationships in a certain class (early, progressive, severe, or dissipative phases), ignoring the natural order of different phases of the disease progression,i.e.,from an early stage and progress to a late stage. To learn both intraclass and interclass relationships among different stages and improve the accuracy of classification, this paper proposes an ensemble learning method based on ordinal regression, which leverages the ordinal information on COVID-19 phases. The proposed method uses multi-binary, neuron stick-breaking (NSB), and soft labels (SL) techniques, and ensembles the ordinal outputs through a median selection. To evaluate our method, we collected 172 confirmed cases. In a 2-fold cross-validation experiment, the accuracy is increased by 22% compared with traditional methods when we use modified ResNet-18 as the backbone. And precision, recall, andF1-score are also improved. The experimental results show that our proposed method achieves a better classification performance than the traditional methods, which helps establish guidelines for the classification of COVID-19 chest CT images.",2021,10.1088/1361-6560/ac34b2,diagnosis,True
An ensemble of neural networks provides expert-level prenatal detection of complex congenital heart disease,"Congenital heart disease (CHD) is the most common birth defect. Fetal screening ultrasound provides five views of the heart that together can detect 90% of complex CHD, but in practice, sensitivity is as low as 30%. Here, using 107,823 images from 1,326 retrospective echocardiograms and screening ultrasounds from 18- to 24-week fetuses, we trained an ensemble of neural networks to identify recommended cardiac views and distinguish between normal hearts and complex CHD. We also used segmentation models to calculate standard fetal cardiothoracic measurements. In an internal test set of 4,108 fetal surveys (0.9% CHD, >4.4 million images), the model achieved an area under the curve (AUC) of 0.99, 95% sensitivity (95% confidence interval (CI), 84-99%), 96% specificity (95% CI, 95-97%) and 100% negative predictive value in distinguishing normal from abnormal hearts. Model sensitivity was comparable to that of clinicians and remained robust on outside-hospital and lower-quality images. The model's decisions were based on clinically relevant features. Cardiac measurements correlated with reported measures for normal and abnormal hearts. Applied to guideline-recommended imaging, ensemble learning models could significantly improve detection of fetal CHD, a critical and global diagnostic challenge.",2021,10.1038/s41591-021-01342-5,diagnosis,False
An explainable AI system for automated COVID-19 assessment and lesion categorization from CT-scans,"COVID-19 infection caused by SARS-CoV-2 pathogen has been a catastrophic pandemic outbreak all over the world, with exponential increasing of confirmed cases and, unfortunately, deaths. In this work we propose an AI-powered pipeline, based on the deep-learning paradigm, for automated COVID-19 detection and lesion categorization from CT scans. We first propose a new segmentation module aimed at automatically identifying lung parenchyma and lobes. Next, we combine the segmentation network with classification networks for COVID-19 identification and lesion categorization. We compare the model's classification results with those obtained by three expert radiologists on a dataset of 166 CT scans. Results showed a sensitivity of 90.3% and a specificity of 93.5% for COVID-19 detection, at least on par with those yielded by the expert radiologists, and an average lesion categorization accuracy of about 84%. Moreover, a significant role is played by prior lung and lobe segmentation, that allowed us to enhance classification performance by over 6 percent points. The interpretation of the trained AI models reveals that the most significant areas for supporting the decision on COVID-19 identification are consistent with the lesions clinically associated to the virus, i.e., crazy paving, consolidation and ground glass. This means that the artificial models are able to discriminate a positive patient from a negative one (both controls and patients with interstitial pneumonia tested negative to COVID) by evaluating the presence of those lesions into CT scans. Finally, the AI models are integrated into a user-friendly GUI to support AI explainability for radiologists, which is publicly available at http://perceivelab.com/covid-ai. The whole AI system is unique since, to the best of our knowledge, it is the first AI-based software, publicly available, that attempts to explain to radiologists what information is used by AI methods for making decisions and that proactively involves them in the decision loop to further improve the COVID-19 understanding.",2021,10.1016/j.artmed.2021.102114,diagnosis,True
An image-based deep learning framework for individualizing radiotherapy dose,"BACKGROUND: Radiotherapy continues to be delivered uniformly without consideration of individual tumor characteristics. To advance toward more precise treatments in radiotherapy, we queried the lung computed tomography (CT)-derived feature space to identify radiation sensitivity parameters that can predict treatment failure and hence guide the individualization of radiotherapy dose. METHODS: We used a cohort-based registry of 849 patients with cancer in the lung treated with high dose radiotherapy using stereotactic body radiotherapy. We input pre-therapy lung CT images into a multi-task deep neural network, Deep Profiler, to generate an image fingerprint that primarily predicts time to event treatment outcomes and secondarily approximates classical radiomic features. We validated our findings in an independent study population (n = 95). Deep Profiler was combined with clinical variables to derive iGray, an individualized dose that estimates treatment failure probability to be <5%. FINDINGS: Radiation treatments in patients with high Deep Profiler scores fail at a significantly higher rate than in those with low scores. The 3-year cumulative incidences of local failure were 20.3% (95% CI: 16.0-24.9) and 5.7% (95% CI: 3.5-8.8), respectively. Deep Profiler independently predicted local failure (hazard ratio 1.65, 95% 1.02-2.66, p = 0.04). Models that included Deep Profiler and clinical variables predicted treatment failures with a concordance index of 0.72 (95% CI: 0.67-0.77), a significant improvement compared to classical radiomics or clinical variables alone (p = <0.001 and <0.001, respectively). Deep Profiler performed well in an external study population (n = 95), accurately predicting treatment failures across diverse clinical settings and CT scanner types (concordance index = 0.77 [95% CI: 0.69-0.92]). iGray had a wide dose range (21.1-277 Gy, BED), suggested dose reductions in 23.3% of patients and can be safely delivered in the majority of cases. INTERPRETATION: Our results indicate that there are image-distinct subpopulations that have differential sensitivity to radiotherapy. The image-based deep learning framework proposed herein is the first opportunity to use medical images to individualize radiotherapy dose.",2019,10.1016/s2589-7500(19)30058-5,treatment,True
An integrated autoencoder-based hybrid CNN-LSTM model for COVID-19 severity prediction from lung ultrasound,"The COVID-19 pandemic has become one of the biggest threats to the global healthcare system, creating an unprecedented condition worldwide. The necessity of rapid diagnosis calls for alternative methods to predict the condition of the patient, for which disease severity estimation on the basis of Lung Ultrasound (LUS) can be a safe, radiation-free, flexible, and favorable option. In this paper, a frame-based 4-score disease severity prediction architecture is proposed with the integration of deep convolutional and recurrent neural networks to consider both spatial and temporal features of the LUS frames. The proposed convolutional neural network (CNN) architecture implements an autoencoder network and separable convolutional branches fused with a modified DenseNet-201 network to build a vigorous, noise-free classification model. A five-fold cross-validation scheme is performed to affirm the efficacy of the proposed network. In-depth result analysis shows a promising improvement in the classification performance by introducing the Long Short-Term Memory (LSTM) layers after the proposed CNN architecture by an average of 7-12%, which is approximately 17% more than the traditional DenseNet architecture alone. From an extensive analysis, it is found that the proposed end-to-end scheme is very effective in detecting COVID-19 severity scores from LUS images.",2021,10.1016/j.compbiomed.2021.104296,prognosis,False
An original deep learning model using limited data for COVID-19 discrimination: A multicenter study,"OBJECTIVES: Artificial intelligence (AI) has been proved to be a highly efficient tool for COVID-19 diagnosis, but the large data size and heavy label force required for algorithm development and the poor generalizability of AI algorithms, to some extent, limit the application of AI technology in clinical practice. The aim of this study is to develop an AI algorithm with high robustness using limited chest CT data for COVID-19 discrimination. METHODS: A three dimensional algorithm that combined multi-instance learning with the LSTM architecture (3DMTM) was developed for differentiating COVID-19 from community acquired pneumonia (CAP) while logistic regression (LR), k-nearest neighbor (KNN), support vector machine (SVM), and a three dimensional convolutional neural network set for comparison. Totally, 515 patients with or without COVID-19 between December 2019 and March 2020 from five different hospitals were recruited and divided into relatively large (150 COVID-19 and 183 CAP cases) and relatively small datasets (17 COVID-19 and 35 CAP cases) for either training or validation and another independent dataset (37 COVID-19 and 93 CAP cases) for external test. Area under the receiver operating characteristic curve (AUC), sensitivity, specificity, precision, accuracy, F1 score, and G-mean were utilized for performance evaluation. RESULTS: In the external test cohort, the relatively large data-based 3DMTM-LD achieved an AUC of 0.956 (95% confidence interval, 95% CI, 0.929∼0.982) with 86.2% and 98.0% for its sensitivity and specificity. 3DMTM-SD got an AUC of 0.937 (95% CI, 0.909∼0.965), while the AUC of 3DCM-SD decreased dramatically to 0.714 (95% CI, 0.649∼0.780) with training data reduction. KNN-MMSD, LR-MMSD, SVM-MMSD, and 3DCM-MMSD benefited significantly from the inclusion of clinical information while models trained with relatively large dataset got slight performance improvement in COVID-19 discrimination. 3DMTM, trained with either CT or multi-modal data, presented comparably excellent performance in COVID-19 discrimination. CONCLUSIONS: The 3DMTM algorithm presented excellent robustness for COVID-19 discrimination with limited CT data. 3DMTM based on CT data performed comparably in COVID-19 discrimination with that trained with multi-modal information. Clinical information could improve the performance of KNN, LR, SVM, and 3DCM in COVID-19 discrimination, especially in the scenario with limited data for training.",2022,10.1002/mp.15549,diagnosis,True
An Uncertainty-Aware Transfer Learning-Based Framework for COVID-19 Diagnosis,"The early and reliable detection of COVID-19 infected patients is essential to prevent and limit its outbreak. The PCR tests for COVID-19 detection are not available in many countries, and also, there are genuine concerns about their reliability and performance. Motivated by these shortcomings, this article proposes a deep uncertainty-aware transfer learning framework for COVID-19 detection using medical images. Four popular convolutional neural networks (CNNs), including VGG16, ResNet50, DenseNet121, and InceptionResNetV2, are first applied to extract deep features from chest X-ray and computed tomography (CT) images. Extracted features are then processed by different machine learning and statistical modeling techniques to identify COVID-19 cases. We also calculate and report the epistemic uncertainty of classification results to identify regions where the trained models are not confident about their decisions (out of distribution problem). Comprehensive simulation results for X-ray and CT image data sets indicate that linear support vector machine and neural network models achieve the best results as measured by accuracy, sensitivity, specificity, and area under the receiver operating characteristic (ROC) curve (AUC). Also, it is found that predictive uncertainty estimates are much higher for CT images compared to X-ray images.",2021,10.1109/tnnls.2021.3054306,diagnosis,True
Analysis of clinical features and imaging signs of COVID-19 with the assistance of artificial intelligence,"OBJECTIVE: To explore the CT imaging features/signs of patients with different clinical types of Coronavirus Disease 2019 (COVID-19) via the application of artificial intelligence (AI), thus improving the understanding of COVID-19. PANTIENTS AND METHODS: Clinical data and chest CT imaging features of 58 patients confirmed with COVID-19 in the Fifth Medical Center of PLA General Hospital were retrospectively analyzed. According to the Guidelines on Novel Coronavirus-Infected Pneumonia Diagnosis and Treatment (Provisional 6th Edition), COVID-19 patients were divided into mild type (7), common type (34), severe type (7) and critical type (10 patients). The CT imaging features of the patients with different clinical types of COVID-19 types were analyzed, and the volume percentage of pneumonia lesions with respect to the lung lobes (where the lesion was located) and to the whole lung was calculated with the use of AI software. SPSS 21.0 software was used for statistical analysis. RESULTS: Common clinical manifestations of COVID-19 patients: fever was found in 47 patients (81.0%), cough in 31 (53.4%) and weakness in 10 (17.2%). Laboratory examinations: normal or decreased white blood cell (WBC) counts were observed in 52 patients (89.7%), decreased lymphocyte counts (LCs) in 14 (24.1%) and increased C-reactive protein (CRP) levels in 18 (31.0%). CT imaging features: there were 48 patients (94.1%) with lesions distributed in both lungs and 46 patients (90.2%) had lesions most visible in the lower lungs; the primary manifestations in patients with common type COVID-19 were ground-glass opacities (GGOs) (23/34, 67.6%) or mixed type (17/34, 50.0%), with lesions mainly distributed in the periphery of the lungs (28/34, 82.4%); the primary manifestations of patients with severe/critical type COVID-19 were consolidations (13/17, 76.5%) or mixed type (14/17, 82.4%), with lesions distributed in both the peripheral and central areas of lungs (14/17,82.4%); other common signs, including pleural parallel signs, halo signs, vascular thickening signs, crazy-paving signs and air bronchogram signs, were visible in patients with different clinical types, and pleural effusion was found in 5 patients with severe/critical COVID-19. AI software was used to calculate the volume percentages of pneumonia lesions with respect to the lung lobes (where the lesion was located) and to the whole lung. There were significant differences in the volume percentages of pneumonia lesions for the superior lobe of the left lung, the inferior lobe of the left lung, the superior lobe of the right lung, the inferior lobe of the right lung and the whole lung among patients with different clinical types (p<0.05). The area under the ROC curve (AUC) of the volume percentage of pneumonia lesions for the whole lung for the diagnosis of severe/critical type COVID-19 was 0.740, with sensitivity and specificity of 91.2% and 58.8%, respectively. CONCLUSIONS: The clinical and CT imaging features of COVID-19 patients were characteristic to a certain degree; thus, the clinical course and severity of COVID-19 could be evaluated with a combination of an analysis of clinical features and CT imaging features and assistant diagnosis by AI software.",2020,10.26355/eurrev_202008_22510,diagnosis,True
Analysis of COVID-19 Infections on a CT Image Using DeepSense Model,"In this paper, a data mining model on a hybrid deep learning framework is designed to diagnose the medical conditions of patients infected with the coronavirus disease 2019 (COVID-19) virus. The hybrid deep learning model is designed as a combination of convolutional neural network (CNN) and recurrent neural network (RNN) and named as DeepSense method. It is designed as a series of layers to extract and classify the related features of COVID-19 infections from the lungs. The computerized tomography image is used as an input data, and hence, the classifier is designed to ease the process of classification on learning the multidimensional input data using the Expert Hidden layers. The validation of the model is conducted against the medical image datasets to predict the infections using deep learning classifiers. The results show that the DeepSense classifier offers accuracy in an improved manner than the conventional deep and machine learning classifiers. The proposed method is validated against three different datasets, where the training data are compared with 70%, 80%, and 90% training data. It specifically provides the quality of the diagnostic method adopted for the prediction of COVID-19 infections in a patient.",2020,10.3389/fpubh.2020.599550,diagnosis,True
Analysis of high-resolution reconstruction of medical images based on deep convolutional neural networks in lung cancer diagnostics,"BACKGROUND AND OBJECTIVE: To study the diagnostic effect of 64-slice spiral CT and MRI high-resolution images based on deep convolutional neural networks(CNN) in lung cancer. METHODS: In this paper, we Select 74 patients with highly suspected lung cancer who were treated in our hospital from January 2017 to January 2021 as the research objects. The enhanced 64-slice spiral CT and MRI were used to detect and diagnose respectively, and the images and accuracy of CT diagnosis and MRI diagnosis were retrospectively analyzed. RESULTS: The accuracy of CT diagnosis is 94.6% (70/74), and the accuracy of MRI diagnosis is 89.2% (66/74). CT examination has the advantages of non-invasive, convenient operation and fast examination. MRI is showing there are advantages in the relationship between the chest wall and the mediastinum, and the relationship between the lesion and the large blood vessels. CONCLUSION: Enhanced CT and MRI examinations based on convolutional neural networks(CNN) to improve image clarity have high application value in the diagnosis of lung cancer patients, but the focus of performance is different.",2022,10.1016/j.cmpb.2021.106592,diagnosis,True
Analysis of segmentation of lung parenchyma based on deep learning methods,"Precise segmentation of lung parenchyma is essential for effective analysis of the lung. Due to the obvious contrast and large regional area compared to other tissues in the chest, lung tissue is less difficult to segment. Special attention to details of lung segmentation is also needed. To improve the quality and speed of segmentation of lung parenchyma based on computed tomography (CT) or computed tomography angiography (CTA) images, the 4th International Symposium on Image Computing and Digital Medicine (ISICDM 2020) provides interesting and valuable research ideas and approaches. For the work of lung parenchyma segmentation, 9 of the 12 participating teams used the U-Net network or its modified forms, and others used the methods to improve the segmentation accuracy include attention mechanism, multi-scale feature information fusion. Among them, U-Net achieves the best results including that the final dice coefficient of CT segmentation is 0.991 and the final dice coefficient of CTA segmentation is 0.984. In addition, attention U-Net and nnU-Net network also performs well. In this paper, the methods chosen by 12 teams from different research groups are evaluated and their segmentation results are analyzed for the study and references to those involved.",2021,10.3233/xst-210956,diagnosis,True
Analysis of the Diagnosis Model of Peripheral Non-Small-Cell Lung Cancer under Computed Tomography Images,"This study aimed to explore the effect of deep learning models on lung CT image lung parenchymal segmentation (LPS) and the application value of CT image texture features in the diagnosis of peripheral non-small-cell lung cancer (NSCLC). Data of peripheral lung cancer (PLC) patients was collected retrospectively and was divided into peripheral SCLC group and peripheral NSCLC group according to the pathological examination results, ResNet50 model and feature pyramid network (FPN) algorithm were undertaken to improve the Mask-RCNN model, and after the MaZda software extracted the texture features of the CT images of PLC patients, the Fisher coefficient was used to reduce the dimensionality, and the texture features of the CT images were analyzed and compared. The results showed that the average Dice coefficients of the 2D CH algorithm, Faster-RCNN, Mask-RCNN, and the algorithm proposed in the validation set were 0.882, 0.953, 0.961, and 0.986, respectively. The accuracy rates were 88.3%, 93.5%, 94.4%, and 97.2%. The average segmentation speeds in lung CT images were 0.289 s/sheet, 0.115 s/sheet, 0.108 s/sheet, and 0.089 s/sheet. The improved deep learning model showed higher accuracy, better robustness, and faster speed than other algorithms in the LPS of CT images. In summary, deep learning can achieve the LPS of CT images and show excellent segmentation efficiency. The texture parameters of GLCM in CT images have excellent differential diagnosis performance for NSCLC and SCLC and potential clinical application value.",2022,10.1155/2022/3107965,diagnosis,True
Analyzing inter-reader variability affecting deep ensemble learning for COVID-19 detection in chest radiographs,"Data-driven deep learning (DL) methods using convolutional neural networks (CNNs) demonstrate promising performance in natural image computer vision tasks. However, their use in medical computer vision tasks faces several limitations, viz., (i) adapting to visual characteristics that are unlike natural images; (ii) modeling random noise during training due to stochastic optimization and backpropagation-based learning strategy; (iii) challenges in explaining DL black-box behavior to support clinical decision-making; and (iv) inter-reader variability in the ground truth (GT) annotations affecting learning and evaluation. This study proposes a systematic approach to address these limitations through application to the pandemic-caused need for Coronavirus disease 2019 (COVID-19) detection using chest X-rays (CXRs). Specifically, our contribution highlights significant benefits obtained through (i) pretraining specific to CXRs in transferring and fine-tuning the learned knowledge toward improving COVID-19 detection performance; (ii) using ensembles of the fine-tuned models to further improve performance over individual constituent models; (iii) performing statistical analyses at various learning stages for validating results; (iv) interpreting learned individual and ensemble model behavior through class-selective relevance mapping (CRM)-based region of interest (ROI) localization; and, (v) analyzing inter-reader variability and ensemble localization performance using Simultaneous Truth and Performance Level Estimation (STAPLE) methods. We find that ensemble approaches markedly improved classification and localization performance, and that inter-reader variability and performance level assessment helps guide algorithm design and parameter optimization. To the best of our knowledge, this is the first study to construct ensembles, perform ensemble-based disease ROI localization, and analyze inter-reader variability and algorithm performance for COVID-19 detection in CXRs.",2020,10.1371/journal.pone.0242301,diagnosis,False
Ant colony optimization approaches to clustering of lung nodules from CT images,"Lung cancer is becoming a threat to mankind. Applying machine learning algorithms for detection and segmentation of irregular shaped lung nodules remains a remarkable milestone in CT scan image analysis research. In this paper, we apply ACO algorithm for lung nodule detection. We have compared the performance against three other algorithms, namely, Otsu algorithm, watershed algorithm, and global region based segmentation. In addition, we suggest a novel approach which involves variations of ACO, namely, refined ACO, logical ACO, and variant ACO. Variant ACO shows better reduction in false positives. In addition we propose black circular neighborhood approach to detect nodule centers from the edge detected image. Genetic algorithm based clustering is performed to cluster the nodules based on intensity, shape, and size. The performance of the overall approach is compared with hierarchical clustering to establish the improvisation in the proposed approach.",2014,10.1155/2014/572494,diagnosis,True
Any unique image biomarkers associated with COVID-19?,"OBJECTIVE: To define the uniqueness of chest CT infiltrative features associated with COVID-19 image characteristics as potential diagnostic biomarkers. METHODS: We retrospectively collected chest CT exams including n = 498 on 151 unique patients RT-PCR positive for COVID-19 and n = 497 unique patients with community-acquired pneumonia (CAP). Both COVID-19 and CAP image sets were partitioned into three groups for training, validation, and testing respectively. In an attempt to discriminate COVID-19 from CAP, we developed several classifiers based on three-dimensional (3D) convolutional neural networks (CNNs). We also asked two experienced radiologists to visually interpret the testing set and discriminate COVID-19 from CAP. The classification performance of the computer algorithms and the radiologists was assessed using the receiver operating characteristic (ROC) analysis, and the nonparametric approaches with multiplicity adjustments when necessary. RESULTS: One of the considered models showed non-trivial, but moderate diagnostic ability overall (AUC of 0.70 with 99% CI 0.56-0.85). This model allowed for the identification of 8-50% of CAP patients with only 2% of COVID-19 patients. CONCLUSIONS: Professional or automated interpretation of CT exams has a moderately low ability to distinguish between COVID-19 and CAP cases. However, the automated image analysis is promising for targeted decision-making due to being able to accurately identify a sizable subsect of non-COVID-19 cases. KEY POINTS: • Both human experts and artificial intelligent models were used to classify the CT scans. • ROC analysis and the nonparametric approaches were used to analyze the performance of the radiologists and computer algorithms. • Unique image features or patterns may not exist for reliably distinguishing all COVID-19 from CAP; however, there may be imaging markers that can identify a sizable subset of non-COVID-19 cases.",2020,10.1007/s00330-020-06956-w,diagnosis,True
Applicability of a prognostic CT-based radiomic signature model trained on stage I-III non-small cell lung cancer in stage IV non-small cell lung cancer,"OBJECTIVES: Recently it has been shown that radiomic features of computed tomography (CT) have prognostic information in stage I-III non-small cell lung cancer (NSCLC) patients. We aim to validate this prognostic radiomic signature in stage IV adenocarcinoma patients undergoing chemotherapy. MATERIALS AND METHODS: Two datasets of chemo-naive stage IV adenocarcinoma patients were investigated, dataset 1: 285 patients with CTs performed in a single center; dataset 2: 223 patients included in a multicenter clinical trial. The main exclusion criteria were EGFR mutation or unknown mutation status and non-delineated primary tumor. Radiomic features were calculated for the primary tumor. The c-index of cox regression was calculated and compared to the signature performance for overall survival (OS). RESULTS: In total CT scans from 195 patients were eligible for analysis. Patients having a prognostic index (PI) lower than the signature median (n = 92) had a significantly better OS than patients with a PI higher than the median (n = 103, HR 1.445, 95% CI 1.07-1.95, p = 0.02, c-index 0.576, 95% CI 0.527-0.624). CONCLUSION: The radiomic signature, derived from daily practice CT scans, has prognostic value for stage IV NSCLC, however the signature performs less than previously described for stage I-III NSCLC stages. In the future, machine learning techniques can potentially lead to a better prognostic imaging based model for stage IV NSCLC.",2018,10.1016/j.lungcan.2018.07.023,diagnosis,True
Application of a classifier combining bronchial transcriptomics and chest computed tomography features facilitates the diagnostic evaluation of lung cancer in smokers and nonsmokers,"Lung cancer screening by computed tomography (CT) reduces mortality but exhibited high false-positive rates. We established a diagnostic classifier combining chest CT features with bronchial transcriptomics. Patients with CT-detected suspected lung cancer were enrolled. The sample collected by bronchial brushing was used for RNA sequencing. The e1071 and pROC packages in R software was applied to build the model. Eventually, a total of 283 patients, including 183 with lung cancer and 100 with benign lesions, were included into final analysis. When incorporating transcriptomic data with radiological characteristics, the advanced model yielded 0.903 AUC with 81.1% NPV. Moreover, the classifier performed well regardless of lesion size, location, stage, histologic type or smoking status. Pathway analysis showed enhanced epithelial differentiation, tumor metastasis, and impaired immunity were predominant in smokers with cancer, whereas tumorigenesis played a central role in nonsmokers with cancer. Apoptosis and oxidative stress contributed critically in metastatic lung cancer; by contrast, immune dysfunction was pivotal in locally advanced lung cancer. Collectively, we devised a minimal-to-noninvasive, efficient diagnostic classifier for smokers and nonsmokers with lung cancer, which provides evidence for different mechanisms of cancer development and metastasis associated with smoking. A negative classifier result will help the physician make conservative diagnostic decisions.",2021,10.1002/ijc.33675,diagnosis,True
Application of artificial intelligence in the diagnosis of multiple primary lung cancer,"Artificial intelligence (AI) based on deep learning, convolutional neural networks and big data has been increasingly effective in the diagnosis and treatment of multiple primary pulmonary nodules. In comparison to previous imaging systems, AI measures more objective parameters such as three-dimensional (3D) volume, probability of malignant nodules, and possible pathological patterns, making the access to the properties of nodules more objective. In our retrospective study, a total of 53 patients with synchronous and metachronous multiple pulmonary nodules were enrolled of which 33 patients were confirmed by pathological tests to have primary binodules, and nine to have primary trinodules. A total of 15 patients had only one focus removed. The statistical results showed that the agreement in the AI diagnosis and postoperative pathological tests was 88.8% in identifying benign or malignant lesions. In addition, the probability of malignancy of benign lesions, preinvasive lesions (AAH, AIS) and invasive lesions (MIA, IA) was totally different (49.40±38.41% vs 80.22±13.55% vs 88.17±17.31%). The purpose of our study was to provide references for the future application of AI in the diagnosis and follow-up of multiple pulmonary nodules. AI may represent a relevant diagnostic aid that shows more accurate and objective results in the diagnosis of multiple pulmonary nodules, reducing the time required for interpretation of results by directly displaying visual information to doctors and patients and together with the clinical conditions of MPLC patients, offering plans for follow-up and treatment that may be more beneficial and reasonable for patients. Despite the great application potential in pneumosurgery, further research is needed to verify the accuracy and range of the application of AI.",2019,10.1111/1759-7714.13185,diagnosis,True
Application of deep learning (3-dimensional convolutional neural network) for the prediction of pathological invasiveness in lung adenocarcinoma: A preliminary study,"To compare results for radiological prediction of pathological invasiveness in lung adenocarcinoma between radiologists and a deep learning (DL) system.Ninety patients (50 men, 40 women; mean age, 66 years; range, 40-88 years) who underwent pre-operative chest computed tomography (CT) with 0.625-mm slice thickness were included in this retrospective study. Twenty-four cases of adenocarcinoma in situ (AIS), 20 cases of minimally invasive adenocarcinoma (MIA), and 46 cases of invasive adenocarcinoma (IVA) were pathologically diagnosed. Three radiologists of different levels of experience diagnosed each nodule by using previously documented CT findings to predict pathological invasiveness. DL was structured using a 3-dimensional (3D) convolutional neural network (3D-CNN) constructed with 2 successive pairs of convolution and max-pooling layers, and 2 fully connected layers. The output layer comprises 3 nodes to recognize the 3 conditions of adenocarcinoma (AIS, MIA, and IVA) or 2 nodes for 2 conditions (AIS and MIA/IVA). Results from DL and the 3 radiologists were statistically compared.No significant differences in pathological diagnostic accuracy rates were seen between DL and the 3 radiologists (P >.11). Receiver operating characteristic analysis demonstrated that area under the curve for DL (0.712) was almost the same as that for the radiologist with extensive experience (0.714; P = .98). Compared with the consensus results from radiologists, DL offered significantly inferior sensitivity (P = .0005), but significantly superior specificity (P = .02).Despite the small training data set, diagnostic performance of DL was almost the same as the radiologist with extensive experience. In particular, DL provided higher specificity than radiologists.",2019,10.1097/md.0000000000016119,prognosis,True
Application of Deep Learning in Lung Cancer Imaging Diagnosis,"Lung cancer is one of the malignant tumors with the highest fatality rate and nearest to our lives. It poses a great threat to human health and it mainly occurs in smokers. In our country, with the acceleration of industrialization, environmental pollution, and population aging, the cancer burden of lung cancer is increasing day by day. In the diagnosis of lung cancer, Computed Tomography (CT) images are a fairly common visualization tool. CT images visualize all tissues based on the absorption of X-rays. The diseased parts of the lung are collectively referred to as pulmonary nodules, the shape of nodules is different, and the risk of cancer will vary with the shape of nodules. Computer-aided diagnosis (CAD) is a very suitable method to solve this problem because the computer vision model can quickly scan every part of the CT image of the same quality for analysis and will not be affected by fatigue and emotion. The latest advances in deep learning enable computer vision models to help doctors diagnose various diseases, and in some cases, models have shown greater competitiveness than doctors. Based on the opportunity of technological development, the application of computer vision in medical imaging diagnosis of diseases has important research significance and value. In this paper, we have used a deep learning-based model on CT images of lung cancer and verified its effectiveness in the timely and accurate prediction of lungs disease. The proposed model has three parts: (i) detection of lung nodules, (ii) False Positive Reduction of the detected nodules to filter out ""false nodules,"" and (iii) classification of benign and malignant lung nodules. Furthermore, different network structures and loss functions were designed and realized at different stages. Additionally, to fine-tune the proposed deep learning-based mode and improve its accuracy in the detection Lung Nodule Detection, Noudule-Net, which is a detection network structure that combines U-Net and RPN, is proposed. Experimental observations have verified that the proposed scheme has exceptionally improved the expected accuracy and precision ratio of the underlined disease.",2022,10.1155/2022/6107940,diagnosis,True
Application of deep learning technique to manage COVID-19 in routine clinical practice using CT images: Results of 10 convolutional neural networks,"Fast diagnostic methods can control and prevent the spread of pandemic diseases like coronavirus disease 2019 (COVID-19) and assist physicians to better manage patients in high workload conditions. Although a laboratory test is the current routine diagnostic tool, it is time-consuming, imposing a high cost and requiring a well-equipped laboratory for analysis. Computed tomography (CT) has thus far become a fast method to diagnose patients with COVID-19. However, the performance of radiologists in diagnosis of COVID-19 was moderate. Accordingly, additional investigations are needed to improve the performance in diagnosing COVID-19. In this study is suggested a rapid and valid method for COVID-19 diagnosis using an artificial intelligence technique based. 1020 CT slices from 108 patients with laboratory proven COVID-19 (the COVID-19 group) and 86 patients with other atypical and viral pneumonia diseases (the non-COVID-19 group) were included. Ten well-known convolutional neural networks were used to distinguish infection of COVID-19 from non-COVID-19 groups: AlexNet, VGG-16, VGG-19, SqueezeNet, GoogleNet, MobileNet-V2, ResNet-18, ResNet-50, ResNet-101, and Xception. Among all networks, the best performance was achieved by ResNet-101 and Xception. ResNet-101 could distinguish COVID-19 from non-COVID-19 cases with an AUC of 0.994 (sensitivity, 100%; specificity, 99.02%; accuracy, 99.51%). Xception achieved an AUC of 0.994 (sensitivity, 98.04%; specificity, 100%; accuracy, 99.02%). However, the performance of the radiologist was moderate with an AUC of 0.873 (sensitivity, 89.21%; specificity, 83.33%; accuracy, 86.27%). ResNet-101 can be considered as a high sensitivity model to characterize and diagnose COVID-19 infections, and can be used as an adjuvant tool in radiology departments.",2020,10.1016/j.compbiomed.2020.103795,diagnosis,True
Application of deep learning to identify COVID-19 infection in posteroanterior chest X-rays,"INTRODUCTION: The objective of this study was to assess seven configurations of six convolutional deep neural network architectures for classification of chest X-rays (CXRs) as COVID-19 positive or negative. METHODS: The primary dataset consisted of 294 COVID-19 positive and 294 COVID-19 negative CXRs, the latter comprising roughly equally many pneumonia, emphysema, fibrosis, and healthy images. We used six common convolutional neural network architectures, VGG16, DenseNet121, DenseNet201, MobileNet, NasNetMobile and InceptionV3. We studied six models (one for each architecture) which were pre-trained on a vast repository of generic (non-CXR) images, as well as a seventh DenseNet121 model, which was pre-trained on a repository of CXR images. For each model, we replaced the output layers with custom fully connected layers for the task of binary classification of images as COVID-19 positive or negative. Performance metrics were calculated on a hold-out test set with CXRs from patients who were not included in the training/validation set. RESULTS: When pre-trained on generic images, the VGG16, DenseNet121, DenseNet201, MobileNet, NasNetMobile, and InceptionV3 architectures respectively produced hold-out test set areas under the receiver operating characteristic (AUROCs) of 0.98, 0.95, 0.97, 0.95, 0.99, and 0.96 for the COVID-19 classification of CXRs. The X-ray pre-trained DenseNet121 model, in comparison, had a test set AUROC of 0.87. DISCUSSION: Common convolutional neural network architectures with parameters pre-trained on generic images yield high-performance and well-calibrated COVID-19 CXR classification.",2021,10.1016/j.clinimag.2021.07.004,diagnosis,False
Application of logistic regression and convolutional neural network in prediction and diagnosis of high-risk populations of lung cancer,"OBJECTIVES: The early detection, early diagnosis, and early treatment of lung cancer are the best strategies to improve the 5-year survival rate. Logistic regression analysis can be a helpful tool in the early detection of high-risk groups of lung cancer. Convolutional neural network (CNN) could distinguish benign from malignant pulmonary nodules, which is critical for early precise diagnosis and treatment. Here, we developed a risk assessment model of lung cancer and a high-precision classification diagnostic model using these technologies so as to provide a basis for early screening of lung cancer and for intelligent differential diagnosis. METHODS: A total of 355 lung cancer patients, 444 patients with benign lung disease and 472 healthy people from The First Affiliated Hospital of Zhengzhou University were included in this study. Moreover, the dataset of 607 lung computed tomography images was collected from the above patients. The logistic regression method was employed to screen the high-risk groups of lung cancer, and the CNN model was designed to classify pulmonary nodules into benign or malignant nodules. RESULTS: The area under the curve of the lung cancer risk assessment model in the training set and the testing set were 0.823 and 0.710, respectively. After finely optimizing the settings of the CNN, the area under the curve could reach 0.984. CONCLUSIONS: This performance demonstrated that the lung cancer risk assessment model could be used to screen for high-risk individuals with lung cancer and the CNN framework was suitable for the differential diagnosis of pulmonary nodules.",2022,10.1097/cej.0000000000000684,prognosis,True
Artificial intelligence analysis of three-dimensional imaging data derives factors associated with postoperative recurrence in patients with radiologically solid-predominant small-sized lung cancers,"OBJECTIVES: Indications of limited resection, such as segmentectomy, have recently been reported for patients with solid-predominant lung cancers ≤2 cm. This study aims to identify unfavourable prognostic factors using three-dimensional imaging analysis with artificial intelligence (AI) technology. METHODS: A total of 157 patients who had clinical N0 non-small cell lung cancer with a radiological size ≤2 cm, and a consolidation tumour ratio > 0.5, who underwent anatomical lung resection between 2011 and 2017 were enrolled. To evaluate the three-dimensional structure, the ground-glass nodule/Solid Automatic Identification AI software Beta Version (AI software; Fujifilm Corporation, Japan) was used. RESULTS: Maximum standardized uptake value (SUVmax) and solid-part volume measured by AI software (AI-SV) showed significant differences between the 139 patients with adenocarcinoma and the 18 patients with non-adenocarcinoma. Among the adenocarcinoma patients, 42 patients (30.2%) were found to be pathological upstaging. Multivariable analysis demonstrated that high SUVmax, high carcinoembryonic antigen level and high AI-SV were significant prognostic factors for recurrence-free survival (RFS; P < 0.05). The 5-year RFS was compared between patients with tumours showing high SUVmax and those showing low SUVmax (67.7% vs 95.4%, respectively, P < 0.001). The 5-year RFS was 91.0% in patients with small AI-SV and 68.1% in those with high AI-SV (P = 0.001). CONCLUSIONS: High AI-SV, high SUVmax and abnormal carcinoembryonic antigen level were unfavourable prognostic factors of patients with solid-predominant lung adenocarcinoma with a radiological size ≤2 cm. Our results suggest that lobectomy should be preferred to segmentectomy for patients with these prognostic factors.",2022,10.1093/ejcts/ezab541,prognosis,True
Artificial Intelligence and Medical Internet of Things Framework for Diagnosis of Coronavirus Suspected Cases,"The world has been facing the COVID-19 pandemic since December 2019. Timely and efficient diagnosis of COVID-19 suspected patients plays a significant role in medical treatment. The deep transfer learning-based automated COVID-19 diagnosis on chest X-ray is required to counter the COVID-19 outbreak. This work proposes a real-time Internet of Things (IoT) framework for early diagnosis of suspected COVID-19 patients by using ensemble deep transfer learning. The proposed framework offers real-time communication and diagnosis of COVID-19 suspected cases. The proposed IoT framework ensembles four deep learning models such as InceptionResNetV2, ResNet152V2, VGG16, and DenseNet201. The medical sensors are utilized to obtain the chest X-ray modalities and diagnose the infection by using the deep ensemble model stored on the cloud server. The proposed deep ensemble model is compared with six well-known transfer learning models over the chest X-ray dataset. Comparative analysis revealed that the proposed model can help radiologists to efficiently and timely diagnose the COVID-19 suspected patients.",2021,10.1155/2021/3277988,diagnosis,False
Artificial intelligence and radiomics enhance the positive predictive value of digital chest tomosynthesis for lung cancer detection within SOS clinical trial,OBJECTIVE: To enhance the positive predictive value (PPV) of chest digital tomosynthesis (DTS) in the lung cancer detection with the analysis of radiomics features. METHOD: The investigation was carried out within the SOS clinical trial (NCT03645018) for lung cancer screening with DTS. Lung nodules were identified by visual analysis and then classified using the diameter and the radiological aspect of the nodule following lung-RADS. Haralick texture features were extracted from the segmented nodules. Both semantic variables and radiomics features were used to build a predictive model using logistic regression on a subset of variables selected with backward feature selection and using two machine learning: a Random Forest and a neural network with the whole subset of variables. The methods were applied to a train set and validated on a test set where diagnostic accuracy metrics were calculated. RESULTS: Binary visual analysis had a good sensitivity (0.95) but a low PPV (0.14). Lung-RADS classification increased the PPV (0.19) but with an unacceptable low sensitivity (0.65). Logistic regression showed a mildly increased PPV (0.29) but a lower sensitivity (0.20). Random Forest demonstrated a moderate PPV (0.40) but with a low sensitivity (0.30). Neural network demonstrated to be the best predictor with a high PPV (0.95) and a high sensitivity (0.90). CONCLUSIONS: The neural network demonstrated the best PPV. The use of visual analysis along with neural network could help radiologists to reduce the number of false positive in DTS. KEY POINTS: • We investigated several approaches to enhance the positive predictive value of chest digital tomosynthesis in the lung cancer detection. • Neural network demonstrated to be the best predictor with a nearly perfect PPV. • Neural network could help radiologists to reduce the number of false positive in DTS.,2020,10.1007/s00330-020-06783-z,diagnosis,True
Artificial Intelligence Augmentation of Radiologist Performance in Distinguishing COVID-19 from Pneumonia of Other Origin at Chest CT,"Background Coronavirus disease 2019 (COVID-19) and pneumonia of other diseases share similar CT characteristics, which contributes to the challenges in differentiating them with high accuracy. Purpose To establish and evaluate an artificial intelligence (AI) system for differentiating COVID-19 and other pneumonia at chest CT and assessing radiologist performance without and with AI assistance. Materials and Methods A total of 521 patients with positive reverse transcription polymerase chain reaction results for COVID-19 and abnormal chest CT findings were retrospectively identified from 10 hospitals from January 2020 to April 2020. A total of 665 patients with non-COVID-19 pneumonia and definite evidence of pneumonia at chest CT were retrospectively selected from three hospitals between 2017 and 2019. To classify COVID-19 versus other pneumonia for each patient, abnormal CT slices were input into the EfficientNet B4 deep neural network architecture after lung segmentation, followed by a two-layer fully connected neural network to pool slices together. The final cohort of 1186 patients (132 583 CT slices) was divided into training, validation, and test sets in a 7:2:1 and equal ratio. Independent testing was performed by evaluating model performance in separate hospitals. Studies were blindly reviewed by six radiologists without and then with AI assistance. Results The final model achieved a test accuracy of 96% (95% confidence interval [CI]: 90%, 98%), a sensitivity of 95% (95% CI: 83%, 100%), and a specificity of 96% (95% CI: 88%, 99%) with area under the receiver operating characteristic curve of 0.95 and area under the precision-recall curve of 0.90. On independent testing, this model achieved an accuracy of 87% (95% CI: 82%, 90%), a sensitivity of 89% (95% CI: 81%, 94%), and a specificity of 86% (95% CI: 80%, 90%) with area under the receiver operating characteristic curve of 0.90 and area under the precision-recall curve of 0.87. Assisted by the probabilities of the model, the radiologists achieved a higher average test accuracy (90% vs 85%, Δ = 5, P < .001), sensitivity (88% vs 79%, Δ = 9, P < .001), and specificity (91% vs 88%, Δ = 3, P = .001). Conclusion Artificial intelligence assistance improved radiologists' performance in distinguishing coronavirus disease 2019 pneumonia from non-coronavirus disease 2019 pneumonia at chest CT. © RSNA, 2020 Online supplemental material is available for this article.",2020,10.1148/radiol.2020201491,diagnosis,True
Artificial intelligence for detecting small FDG-positive lung nodules in digital PET/CT: impact of image reconstructions on diagnostic performance,"OBJECTIVES: To evaluate the diagnostic performance of a deep learning algorithm for automated detection of small (18)F-FDG-avid pulmonary nodules in PET scans, and to assess whether novel block sequential regularized expectation maximization (BSREM) reconstruction affects detection accuracy as compared to ordered subset expectation maximization (OSEM) reconstruction. METHODS: Fifty-seven patients with 92 (18)F-FDG-avid pulmonary nodules (all ≤ 2 cm) undergoing PET/CT for oncological (re-)staging were retrospectively included and a total of 8824 PET images of the lungs were extracted using OSEM and BSREM reconstruction. Per-slice and per-nodule sensitivity of a deep learning algorithm was assessed, with an expert readout by a radiologist/nuclear medicine physician serving as standard of reference. Receiver-operator characteristic (ROC) curve of OSEM and BSREM were assessed and the areas under the ROC curve (AUC) were compared. A maximum standardized uptake value (SUV(max))-based sensitivity analysis and a size-based sensitivity analysis with subgroups defined by nodule size was performed. RESULTS: The AUC of the deep learning algorithm for nodule detection using OSEM reconstruction was 0.796 (CI 95%; 0.772-0.869), and 0.848 (CI 95%; 0.828-0.869) using BSREM reconstruction. The AUC was significantly higher for BSREM compared to OSEM (p = 0.001). On a per-slice analysis, sensitivity and specificity were 66.7% and 79.0% for OSEM, and 69.2% and 84.5% for BSREM. On a per-nodule analysis, the overall sensitivity of OSEM was 81.5% compared to 87.0% for BSREM. CONCLUSIONS: Our results suggest that machine learning algorithms may aid detection of small (18)F-FDG-avid pulmonary nodules in clinical PET/CT. AI performed significantly better on images with BSREM than OSEM. KEY POINTS: • The diagnostic value of deep learning for detecting small lung nodules (≤ 2 cm) in PET images using BSREM and OSEM reconstruction was assessed. • BSREM yields higher SUV(max)of small pulmonary nodules as compared to OSEM reconstruction. • The use of BSREM translates into a higher detectability of small pulmonary nodules in PET images as assessed with artificial intelligence.",2020,10.1007/s00330-019-06498-w,diagnosis,True
Artificial intelligence for prediction of COVID-19 progression using CT imaging and clinical data,"OBJECTIVES: Early recognition of coronavirus disease 2019 (COVID-19) severity can guide patient management. However, it is challenging to predict when COVID-19 patients will progress to critical illness. This study aimed to develop an artificial intelligence system to predict future deterioration to critical illness in COVID-19 patients. METHODS: An artificial intelligence (AI) system in a time-to-event analysis framework was developed to integrate chest CT and clinical data for risk prediction of future deterioration to critical illness in patients with COVID-19. RESULTS: A multi-institutional international cohort of 1,051 patients with RT-PCR confirmed COVID-19 and chest CT was included in this study. Of them, 282 patients developed critical illness, which was defined as requiring ICU admission and/or mechanical ventilation and/or reaching death during their hospital stay. The AI system achieved a C-index of 0.80 for predicting individual COVID-19 patients' to critical illness. The AI system successfully stratified the patients into high-risk and low-risk groups with distinct progression risks (p < 0.0001). CONCLUSIONS: Using CT imaging and clinical data, the AI system successfully predicted time to critical illness for individual patients and identified patients with high risk. AI has the potential to accurately triage patients and facilitate personalized treatment. KEY POINT: • AI system can predict time to critical illness for patients with COVID-19 by using CT imaging and clinical data.",2022,10.1007/s00330-021-08049-8,prognosis,True
Artificial intelligence for the detection of COVID-19 pneumonia on chest CT using multinational datasets,"Chest CT is emerging as a valuable diagnostic tool for clinical management of COVID-19 associated lung disease. Artificial intelligence (AI) has the potential to aid in rapid evaluation of CT scans for differentiation of COVID-19 findings from other clinical entities. Here we show that a series of deep learning algorithms, trained in a diverse multinational cohort of 1280 patients to localize parietal pleura/lung parenchyma followed by classification of COVID-19 pneumonia, can achieve up to 90.8% accuracy, with 84% sensitivity and 93% specificity, as evaluated in an independent test set (not included in training and validation) of 1337 patients. Normal controls included chest CTs from oncology, emergency, and pneumonia-related indications. The false positive rate in 140 patients with laboratory confirmed other (non COVID-19) pneumonias was 10%. AI-based algorithms can readily identify CT scans with COVID-19 associated pneumonia, as well as distinguish non-COVID related pneumonias with high specificity in diverse patient populations.",2020,10.1038/s41467-020-17971-2,diagnosis,True
Artificial intelligence on COVID-19 pneumonia detection using chest xray images,"Recent studies show the potential of artificial intelligence (AI) as a screening tool to detect COVID-19 pneumonia based on chest x-ray (CXR) images. However, issues on the datasets and study designs from medical and technical perspectives, as well as questions on the vulnerability and robustness of AI algorithms have emerged. In this study, we address these issues with a more realistic development of AI-driven COVID-19 pneumonia detection models by generating our own data through a retrospective clinical study to augment the dataset aggregated from external sources. We optimized five deep learning architectures, implemented development strategies by manipulating data distribution to quantitatively compare study designs, and introduced several detection scenarios to evaluate the robustness and diagnostic performance of the models. At the current level of data availability, the performance of the detection model depends on the hyperparameter tuning and has less dependency on the quantity of data. InceptionV3 attained the highest performance in distinguishing pneumonia from normal CXR in two-class detection scenario with sensitivity (Sn), specificity (Sp), and positive predictive value (PPV) of 96%. The models attained higher general performance of 91-96% Sn, 94-98% Sp, and 90-96% PPV in three-class compared to four-class detection scenario. InceptionV3 has the highest general performance with accuracy, F1-score, and g-mean of 96% in the three-class detection scenario. For COVID-19 pneumonia detection, InceptionV3 attained the highest performance with 86% Sn, 99% Sp, and 91% PPV with an AUC of 0.99 in distinguishing pneumonia from normal CXR. Its capability of differentiating COVID-19 pneumonia from normal and non-COVID-19 pneumonia attained 0.98 AUC and a micro-average of 0.99 for other classes.",2021,10.1371/journal.pone.0257884,diagnosis,False
"Artificial Intelligence Predicts Severity of COVID-19 Based on Correlation of Exaggerated Monocyte Activation, Excessive Organ Damage and Hyperinflammatory Syndrome: A Prospective Clinical Study","BACKGROUND: Prediction of the severity of COVID-19 at its onset is important for providing adequate and timely management to reduce mortality. OBJECTIVE: To study the prognostic value of damage parameters and cytokines as predictors of severity of COVID-19 using an extensive immunologic profiling and unbiased artificial intelligence methods. METHODS: Sixty hospitalized COVID-19 patients (30 moderate and 30 severe) and 17 healthy controls were included in the study. The damage indicators high mobility group box 1 (HMGB1), lactate dehydrogenase (LDH), aspartate aminotransferase (AST), alanine aminotransferase (ALT), extensive biochemical analyses, a panel of 47 cytokines and chemokines were analyzed at weeks 1, 2 and 7 along with clinical complaints and CT scans of the lungs. Unbiased artificial intelligence (AI) methods (logistic regression and Support Vector Machine and Random Forest algorithms) were applied to investigate the contribution of each parameter to prediction of the severity of the disease. RESULTS: On admission, the severely ill patients had significantly higher levels of LDH, IL-6, monokine induced by gamma interferon (MIG), D-dimer, fibrinogen, glucose than the patients with moderate disease. The levels of macrophage derived cytokine (MDC) were lower in severely ill patients. Based on artificial intelligence analysis, eight parameters (creatinine, glucose, monocyte number, fibrinogen, MDC, MIG, C-reactive protein (CRP) and IL-6 have been identified that could predict with an accuracy of 83-87% whether the patient will develop severe disease. CONCLUSION: This study identifies the prognostic factors and provides a methodology for making prediction for COVID-19 patients based on widely accepted biomarkers that can be measured in most conventional clinical laboratories worldwide.",2021,10.3389/fimmu.2021.715072,prognosis,True
Artificial intelligence solution to classify pulmonary nodules on CT,"PURPOSE: The purpose of this study was to create an algorithm to detect and classify pulmonary nodules in two categories based on their volume greater than 100 mm(3) or not, using machine learning and deep learning techniques. MATERIALS AND METHOD: The dataset used to train the model was provided by the organization team of the SFR (French Radiological Society) Data Challenge 2019. An asynchronous and parallel 3-stages pipeline was developed to process all the data (a data ""pre-processing"" stage; a ""nodule detection"" stage; a ""classifier"" stage). Lung segmentation was achieved using 3D U-NET algorithm; nodule detection was done using 3D Retina-UNET and classifier stage with a support vector machine algorithm on selected features. Performances were assessed using area under receiver operating characteristics curve (AUROC). RESULTS: The pipeline showed good performance for pathological nodule detection and patient diagnosis. With the preparation dataset, an AUROC of 0.9058 (95% confidence interval [CI]: 0.8746-0.9362) was obtained, 87% yielding accuracy (95% CI: 84.83%-91.03%) for the ""nodule detection"" stage, corresponding to 86% specificity (95% CI: 82%-92%) and 89% sensitivity (95% CI: 84.83%-91.03%). CONCLUSION: A fully functional pipeline using 3D U-NET, 3D Retina-UNET and classifier stage with a support vector machine algorithm was developed, resulting in high capabilities for pulmonary nodule classification.",2020,10.1016/j.diii.2020.10.004,diagnosis,True
Artificial intelligence to codify lung CT in Covid-19 patients,"The spread of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has already assumed pandemic proportions, affecting over 100 countries in few weeks. A global response is needed to prepare health systems worldwide. Covid-19 can be diagnosed both on chest X-ray and on computed tomography (CT). Asymptomatic patients may also have lung lesions on imaging. CT investigation in patients with suspicion Covid-19 pneumonia involves the use of the high-resolution technique (HRCT). Artificial intelligence (AI) software has been employed to facilitate CT diagnosis. AI software must be useful categorizing the disease into different severities, integrating the structured report, prepared according to subjective considerations, with quantitative, objective assessments of the extent of the lesions. In this communication, we present an example of a good tool for the radiologist (Thoracic VCAR software, GE Healthcare, Italy) in Covid-19 diagnosis (Pan et al. in Radiology, 2020. https://doi.org/10.1148/radiol.2020200370). Thoracic VCAR offers quantitative measurements of the lung involvement. Thoracic VCAR can generate a clear, fast and concise report that communicates vital medical information to referring physicians. In the post-processing phase, software, thanks to the help of a colorimetric map, recognizes the ground glass and differentiates it from consolidation and quantifies them as a percentage with respect to the healthy parenchyma. AI software therefore allows to accurately calculate the volume of each of these areas. Therefore, keeping in mind that CT has high diagnostic sensitivity in identifying lesions, but not specific for Covid-19 and similar to other infectious viral diseases, it is mandatory to have an AI software that expresses objective evaluations of the percentage of ventilated lung parenchyma compared to the affected one.",2020,10.1007/s11547-020-01195-x,diagnosis,True
Artificial Intelligence-assisted chest X-ray assessment scheme for COVID-19,"OBJECTIVES: To study whether a trained convolutional neural network (CNN) can be of assistance to radiologists in differentiating Coronavirus disease (COVID)-positive from COVID-negative patients using chest X-ray (CXR) through an ambispective clinical study. To identify subgroups of patients where artificial intelligence (AI) can be of particular value and analyse what imaging features may have contributed to the performance of AI by means of visualisation techniques. METHODS: CXR of 487 patients were classified into [4] categories-normal, classical COVID, indeterminate, and non-COVID by consensus opinion of 2 radiologists. CXR which were classified as ""normal"" and ""indeterminate"" were then subjected to analysis by AI, and final categorisation provided as guided by prediction of the network. Precision and recall of the radiologist alone and radiologist assisted by AI were calculated in comparison to reverse transcriptase-polymerase chain reaction (RT-PCR) as the gold standard. Attention maps of the CNN were analysed to understand regions in the CXR important to the AI algorithm in making a prediction. RESULTS: The precision of radiologists improved from 65.9 to 81.9% and recall improved from 17.5 to 71.75 when assistance with AI was provided. AI showed 92% accuracy in classifying ""normal"" CXR into COVID or non-COVID. Analysis of attention maps revealed attention on the cardiac shadow in these ""normal"" radiographs. CONCLUSION: This study shows how deployment of an AI algorithm can complement a human expert in the determination of COVID status. Analysis of the detected features suggests possible subtle cardiac changes, laying ground for further investigative studies into possible cardiac changes. KEY POINTS: • Through an ambispective clinical study, we show how assistance with an AI algorithm can improve recall (sensitivity) and precision (positive predictive value) of radiologists in assessing CXR for possible COVID in comparison to RT-PCR. • We show that AI achieves the best results in images classified as ""normal"" by radiologists. We conjecture that possible subtle cardiac in the CXR, imperceptible to the human eye, may have contributed to this prediction. • The reported results may pave the way for a human computer collaboration whereby the expert with some help from the AI algorithm achieves higher accuracy in predicting COVID status on CXR than previously thought possible when considering either alone.",2021,10.1007/s00330-020-07628-5,diagnosis,False
Artificial Intelligence-based Fully Automated Per Lobe Segmentation and Emphysema-quantification Based on Chest Computed Tomography Compared With Global Initiative for Chronic Obstructive Lung Disease Severity of Smokers,"OBJECTIVES: The objective of this study was to evaluate an artificial intelligence (AI)-based prototype algorithm for the fully automated per lobe segmentation and emphysema quantification (EQ) on chest-computed tomography as it compares to the Global Initiative for Chronic Obstructive Lung Disease (GOLD) severity classification of chronic obstructive pulmonary disease (COPD) patients. METHODS: Patients (n=137) who underwent chest-computed tomography acquisition and spirometry within 6 months were retrospectively included in this Institutional Review Board-approved and Health Insurance Portability and Accountability Act-compliant study. Patient-specific spirometry data, which included forced expiratory volume in 1 second, forced vital capacity, and the forced expiratory volume in 1 second/forced vital capacity ratio (Tiffeneau-Index), were used to assign patients to their respective GOLD stage I to IV. Lung lobe segmentation was carried out using AI-RAD Companion software prototype (Siemens Healthineers), a deep convolution image-to-image network and emphysema was quantified in each lung lobe to detect the low attenuation volume. RESULTS: A strong correlation between the whole-lung-EQ and the GOLD stages was found (ρ=0.88, P<0.0001). The most significant correlation was noted in the left upper lobe (ρ=0.85, P<0.0001), and the weakest in the left lower lobe (ρ=0.72, P<0.0001) and right middle lobe (ρ=0.72, P<0.0001). CONCLUSIONS: AI-based per lobe segmentation and its EQ demonstrate a very strong correlation with the GOLD severity stages of COPD patients. Furthermore, the low attenuation volume of the left upper lobe not only showed the strongest correlation to GOLD severity but was also able to most clearly distinguish mild and moderate forms of COPD. This is particularly relevant due to the fact that early disease processes often elude conventional pulmonary function diagnostics. Earlier detection of COPD is a crucial element for positively altering the course of disease progression through various therapeutic options.",2020,10.1097/rti.0000000000000500,diagnosis,True
Artificial intelligence-enabled rapid diagnosis of patients with COVID-19,"For diagnosis of coronavirus disease 2019 (COVID-19), a SARS-CoV-2 virus-specific reverse transcriptase polymerase chain reaction (RT-PCR) test is routinely used. However, this test can take up to 2 d to complete, serial testing may be required to rule out the possibility of false negative results and there is currently a shortage of RT-PCR test kits, underscoring the urgent need for alternative methods for rapid and accurate diagnosis of patients with COVID-19. Chest computed tomography (CT) is a valuable component in the evaluation of patients with suspected SARS-CoV-2 infection. Nevertheless, CT alone may have limited negative predictive value for ruling out SARS-CoV-2 infection, as some patients may have normal radiological findings at early stages of the disease. In this study, we used artificial intelligence (AI) algorithms to integrate chest CT findings with clinical symptoms, exposure history and laboratory testing to rapidly diagnose patients who are positive for COVID-19. Among a total of 905 patients tested by real-time RT-PCR assay and next-generation sequencing RT-PCR, 419 (46.3%) tested positive for SARS-CoV-2. In a test set of 279 patients, the AI system achieved an area under the curve of 0.92 and had equal sensitivity as compared to a senior thoracic radiologist. The AI system also improved the detection of patients who were positive for COVID-19 via RT-PCR who presented with normal CT scans, correctly identifying 17 of 25 (68%) patients, whereas radiologists classified all of these patients as COVID-19 negative. When CT scans and associated clinical history are available, the proposed AI system can help to rapidly diagnose COVID-19 patients.",2020,10.1038/s41591-020-0931-3,diagnosis,True
Artificial intelligence-supported lung cancer detection by multi-institutional readers with multi-vendor chest radiographs: a retrospective clinical validation study,"BACKGROUND: We investigated the performance improvement of physicians with varying levels of chest radiology experience when using a commercially available artificial intelligence (AI)-based computer-assisted detection (CAD) software to detect lung cancer nodules on chest radiographs from multiple vendors. METHODS: Chest radiographs and their corresponding chest CT were retrospectively collected from one institution between July 2017 and June 2018. Two author radiologists annotated pathologically proven lung cancer nodules on the chest radiographs while referencing CT. Eighteen readers (nine general physicians and nine radiologists) from nine institutions interpreted the chest radiographs. The readers interpreted the radiographs alone and then reinterpreted them referencing the CAD output. Suspected nodules were enclosed with a bounding box. These bounding boxes were judged correct if there was significant overlap with the ground truth, specifically, if the intersection over union was 0.3 or higher. The sensitivity, specificity, accuracy, PPV, and NPV of the readers' assessments were calculated. RESULTS: In total, 312 chest radiographs were collected as a test dataset, including 59 malignant images (59 nodules of lung cancer) and 253 normal images. The model provided a modest boost to the reader's sensitivity, particularly helping general physicians. The performance of general physicians was improved from 0.47 to 0.60 for sensitivity, from 0.96 to 0.97 for specificity, from 0.87 to 0.90 for accuracy, from 0.75 to 0.82 for PPV, and from 0.89 to 0.91 for NPV while the performance of radiologists was improved from 0.51 to 0.60 for sensitivity, from 0.96 to 0.96 for specificity, from 0.87 to 0.90 for accuracy, from 0.76 to 0.80 for PPV, and from 0.89 to 0.91 for NPV. The overall increase in the ratios of sensitivity, specificity, accuracy, PPV, and NPV were 1.22 (1.14-1.30), 1.00 (1.00-1.01), 1.03 (1.02-1.04), 1.07 (1.03-1.11), and 1.02 (1.01-1.03) by using the CAD, respectively. CONCLUSION: The AI-based CAD was able to improve the ability of physicians to detect nodules of lung cancer in chest radiographs. The use of a CAD model can indicate regions physicians may have overlooked during their initial assessment.",2021,10.1186/s12885-021-08847-9,diagnosis,False
Artificial Neural Network-Based Deep Learning Model for COVID-19 Patient Detection Using X-Ray Chest Images,"The world is experiencing an unprecedented crisis due to the coronavirus disease (COVID-19) outbreak that has affected nearly 216 countries and territories across the globe. Since the pandemic outbreak, there is a growing interest in computational model-based diagnostic technologies to support the screening and diagnosis of COVID-19 cases using medical imaging such as chest X-ray (CXR) scans. It is discovered in initial studies that patients infected with COVID-19 show abnormalities in their CXR images that represent specific radiological patterns. Still, detection of these patterns is challenging and time-consuming even for skilled radiologists. In this study, we propose a novel convolutional neural network- (CNN-) based deep learning fusion framework using the transfer learning concept where parameters (weights) from different models are combined into a single model to extract features from images which are then fed to a custom classifier for prediction. We use gradient-weighted class activation mapping to visualize the infected areas of CXR images. Furthermore, we provide feature representation through visualization to gain a deeper understanding of the class separability of the studied models with respect to COVID-19 detection. Cross-validation studies are used to assess the performance of the proposed models using open-access datasets containing healthy and both COVID-19 and other pneumonia infected CXR images. Evaluation results show that the best performing fusion model can attain a classification accuracy of 95.49% with a high level of sensitivity and specificity.",2021,10.1155/2021/5513679,diagnosis,False
Artificial neural networks improve LDCT lung cancer screening: a comparative validation study,"BACKGROUND: This study proposes a prediction model for the automatic assessment of lung cancer risk based on an artificial neural network (ANN) with a data-driven approach to the low-dose computed tomography (LDCT) standardized structure report. METHODS: This comparative validation study analysed a prospective cohort from Chiayi Chang Gung Memorial Hospital, Taiwan. In total, 836 asymptomatic patients who had undergone LDCT scans between February 2017 and August 2018 were included, comprising 27 lung cancer cases and 809 controls. A derivation cohort of 602 participants (19 lung cancer cases and 583 controls) was collected to construct the ANN prediction model. A comparative validation of the ANN and Lung-RADS was conducted with a prospective cohort of 234 participants (8 lung cancer cases and 226 controls). The areas under the curves (AUCs) of the receiver operating characteristic (ROC) curves were used to compare the prediction models. RESULTS: At the cut-off of category 3, the Lung-RADS had a sensitivity of 12.5%, specificity of 96.0%, positive predictive value of 10.0%, and negative predictive value of 96.9%. At its optimal cut-off value, the ANN had a sensitivity of 75.0%, specificity of 85.0%, positive predictive value of 15.0%, and negative predictive value of 99.0%. The area under the ROC curve was 0.764 for the Lung-RADS and 0.873 for the ANN (P = 0.01). The two most important predictors used by the ANN for predicting lung cancer were the documented sizes of partially solid nodules and ground-glass nodules. CONCLUSIONS: Compared to the Lung-RADS, the ANN provided better sensitivity for the detection of lung cancer in an Asian population. In addition, the ANN provided a more refined discriminative ability than the Lung-RADS for lung cancer risk stratification with population-specific demographic characteristics. When lung nodules are detected and documented in a standardized structured report, ANNs may better provide important insights for lung cancer prediction than conventional rule-based criteria.",2020,10.1186/s12885-020-07465-1,diagnosis,True
Assessing PD-L1 expression in non-small cell lung cancer and predicting responses to immune checkpoint inhibitors using deep learning on computed tomography images,"Rationale: This study aimed to use computed tomography (CT) images to assess PD-L1 expression in non-small cell lung cancer (NSCLC) and predict response to immunotherapy. Methods: We retrospectively analyzed a PD-L1 expression dataset that consisted of 939 consecutive stage IIIB-IV NSCLC patients with pretreatment CT images. A deep convolutional neural network was trained and optimized with CT images from the training cohort (n = 750) and validation cohort (n = 93) to obtain a PD-L1 expression signature (PD-L1ES), which was evaluated using the test cohort (n = 96). Finally, a separate immunotherapy cohort (n = 94) was used to assess the prognostic value of PD-L1ES with respect to clinical outcome. Results: PD-L1ES was able to predict high PD-L1 expression (PD-L1 ≥ 50%) with areas under the receiver operating characteristic curve (AUC) of 0.78 (95% confidence interval (CI): 0.75~0.80), 0.71 (95% CI: 0.59~0.81), and 0.76 (95% CI: 0.66~0.85) in the training, validation, and test cohorts, respectively. In patients treated with anti-PD-1 antibody, low PD-L1ES was associated with improved progression-free survival (PFS) (median PFS 363 days in low score group vs 183 days in high score group; hazard ratio [HR]: 2.57, 95% CI: 1.22~5.44; P = 0.010). Additionally, when PD-L1ES was combined with a clinical model that was trained using age, sex, smoking history and family history of malignancy, the response to immunotherapy could be better predicted compared to either PD-L1ES or the clinical model alone. Conclusions: The deep learning model provides a noninvasive method to predict high PD-L1 expression of NSCLC and to infer clinical outcomes in response to immunotherapy. Additionally, this deep learning model combined with clinical models demonstrated improved stratification capabilities.",2021,10.7150/thno.48027,diagnosis,True
Assessing the Accuracy of a Deep Learning Method to Risk Stratify Indeterminate Pulmonary Nodules,"Rationale: The management of indeterminate pulmonary nodules (IPNs) remains challenging, resulting in invasive procedures and delays in diagnosis and treatment. Strategies to decrease the rate of unnecessary invasive procedures and optimize surveillance regimens are needed.Objectives: To develop and validate a deep learning method to improve the management of IPNs.Methods: A Lung Cancer Prediction Convolutional Neural Network model was trained using computed tomography images of IPNs from the National Lung Screening Trial, internally validated, and externally tested on cohorts from two academic institutions.Measurements and Main Results: The areas under the receiver operating characteristic curve in the external validation cohorts were 83.5% (95% confidence interval [CI], 75.4-90.7%) and 91.9% (95% CI, 88.7-94.7%), compared with 78.1% (95% CI, 68.7-86.4%) and 81.9 (95% CI, 76.1-87.1%), respectively, for a commonly used clinical risk model for incidental nodules. Using 5% and 65% malignancy thresholds defining low- and high-risk categories, the overall net reclassifications in the validation cohorts for cancers and benign nodules compared with the Mayo model were 0.34 (Vanderbilt) and 0.30 (Oxford) as a rule-in test, and 0.33 (Vanderbilt) and 0.58 (Oxford) as a rule-out test. Compared with traditional risk prediction models, the Lung Cancer Prediction Convolutional Neural Network was associated with improved accuracy in predicting the likelihood of disease at each threshold of management and in our external validation cohorts.Conclusions: This study demonstrates that this deep learning algorithm can correctly reclassify IPNs into low- or high-risk categories in more than a third of cancers and benign nodules when compared with conventional risk models, potentially reducing the number of unnecessary invasive procedures and delays in diagnosis.",2020,10.1164/rccm.201903-0505OC,diagnosis,True
Assessing the utility of autofluorescence-based pulmonary optical endomicroscopy to predict the malignant potential of solitary pulmonary nodules in humans,"Solitary pulmonary nodules are common, often incidental findings on chest CT scans. The investigation of pulmonary nodules is time-consuming and often leads to protracted follow-up with ongoing radiological surveillance, however, clinical calculators that assess the risk of the nodule being malignant exist to help in the stratification of patients. Furthermore recent advances in interventional pulmonology include the ability to both navigate to nodules and also to perform autofluorescence endomicroscopy. In this study we assessed the efficacy of incorporating additional information from label-free fibre-based optical endomicrosopy of the nodule on assessing risk of malignancy. Using image analysis and machine learning approaches, we find that this information does not yield any gain in predictive performance in a cohort of patients. Further advances with pulmonary endomicroscopy will require the addition of molecular tracers to improve information from this procedure.",2016,10.1038/srep31372,diagnosis,True
Assisting scalable diagnosis automatically via CT images in the combat against COVID-19,"The pandemic of Coronavirus Disease 2019 (COVID-19) is causing enormous loss of life globally. Prompt case identification is critical. The reference method is the real-time reverse transcription PCR (RT-PCR) assay, whose limitations may curb its prompt large-scale application. COVID-19 manifests with chest computed tomography (CT) abnormalities, some even before the onset of symptoms. We tested the hypothesis that the application of deep learning (DL) to 3D CT images could help identify COVID-19 infections. Using data from 920 COVID-19 and 1,073 non-COVID-19 pneumonia patients, we developed a modified DenseNet-264 model, COVIDNet, to classify CT images to either class. When tested on an independent set of 233 COVID-19 and 289 non-COVID-19 pneumonia patients, COVIDNet achieved an accuracy rate of 94.3% and an area under the curve of 0.98. As of March 23, 2020, the COVIDNet system had been used 11,966 times with a sensitivity of 91.12% and a specificity of 88.50% in six hospitals with PCR confirmation. Application of DL to CT images may improve both efficiency and capacity of case detection and long-term surveillance.",2021,10.1038/s41598-021-83424-5,diagnosis,True
Association of AI quantified COVID-19 chest CT and patient outcome,"PURPOSE: Severity scoring is a key step in managing patients with COVID-19 pneumonia. However, manual quantitative analysis by radiologists is a time-consuming task, while qualitative evaluation may be fast but highly subjective. This study aims to develop artificial intelligence (AI)-based methods to quantify disease severity and predict COVID-19 patient outcome. METHODS: We develop an AI-based framework that employs deep neural networks to efficiently segment lung lobes and pulmonary opacities. The volume ratio of pulmonary opacities inside each lung lobe gives the severity scores of the lobes, which are then used to predict ICU admission and mortality with three different machine learning methods. The developed methods were evaluated on datasets from two hospitals (site A: Firoozgar Hospital, Iran, 105 patients; site B: Massachusetts General Hospital, USA, 88 patients). RESULTS: AI-based severity scores are strongly associated with those evaluated by radiologists (Spearman's rank correlation 0.837, [Formula: see text]). Using AI-based scores produced significantly higher ([Formula: see text]) area under the ROC curve (AUC) values. The developed AI method achieved the best performance of AUC = 0.813 (95% CI [0.729, 0.886]) in predicting ICU admission and AUC = 0.741 (95% CI [0.640, 0.837]) in mortality estimation on the two datasets. CONCLUSIONS: Accurate severity scores can be obtained using the developed AI methods over chest CT images. The computed severity scores achieved better performance than radiologists in predicting COVID-19 patient outcome by consistently quantifying image features. Such developed techniques of severity assessment may be extended to other lung diseases beyond the current pandemic.",2021,10.1007/s11548-020-02299-5,diagnosis,True
Attention-embedded complementary-stream CNN for false positive reduction in pulmonary nodule detection,"False positive reduction plays a key role in computer-aided detection systems for pulmonary nodule detection in computed tomography (CT) scans. However, this remains a challenge owing to the heterogeneity and similarity of anisotropic pulmonary nodules. In this study, a novel attention-embedded complementary-stream convolutional neural network (AECS-CNN) is proposed to obtain more representative features of nodules for false positive reduction. The proposed network comprises three function blocks: 1) attention-guided multi-scale feature extraction, 2) complementary-stream block with an attention module for feature integration, and 3) classification block. The inputs of the network are multi-scale 3D CT volumes due to variations in nodule sizes. Subsequently, a gradual multi-scale feature extraction block with an attention module was applied to acquire more contextual information regarding the nodules. A subsequent complementary-stream integration block with an attention module was utilized to learn the significantly complementary features. Finally, the candidates were classified using a fully connected layer block. An exhaustive experiment on the LUNA16 challenge dataset was conducted to verify the effectiveness and performance of the proposed network. The AECS-CNN achieved a sensitivity of 0.92 with 4 false positives per scan. The results indicate that the attention mechanism can improve the network performance in false positive reduction, the proposed AECS-CNN can learn more representative features, and the attention module can guide the network to learn the discriminated feature channels and the crucial information embedded in the data, thereby effectively enhancing the performance of the detection system.",2021,10.1016/j.compbiomed.2021.104357,diagnosis,True
Attribute-guided image generation of three-dimensional computed tomography images of lung nodules using a generative adversarial network,"PURPOSE: To develop and evaluate a three-dimensional (3D) generative model of computed tomography (CT) images of lung nodules using a generative adversarial network (GAN). To guide the GAN, lung nodule size was used. MATERIALS AND METHODS: A public CT dataset of lung nodules was used, from where 1182 lung nodules were obtained. Our proposed GAN model used masked 3D CT images and nodule size information to generate images. To evaluate the generated CT images, two radiologists visually evaluated whether the CT images with lung nodule were true or generated, and the diagnostic ability was evaluated using receiver-operating characteristic analysis and area under the curves (AUC). Then, two models for classifying nodule size into five categories were trained, one using the true and the other using the generated CT images of lung nodules. Using true CT images, the classification accuracy of the sizes of the true lung nodules was calculated for the two classification models. RESULTS: The sensitivity, specificity, and AUC of the two radiologists were respectively as follows: radiologist 1: 81.3%, 37.7%, and 0.592; radiologist 2: 77.1%, 30.2%, and 0.597. For categorization of nodule size, the mean accuracy of the classification model constructed with true CT images was 85% (range 83.2-86.1%), and that with generated CT images was 85% (range 82.2-88.1%). CONCLUSIONS: Our results show that it was possible to generate 3D CT images of lung nodules that could be used to construct a classification model of lung nodule size without true CT images.",2020,10.1016/j.compbiomed.2020.104032,diagnosis,True
Augmenting existing deterioration indices with chest radiographs to predict clinical deterioration,"IMPORTANCE: When hospitals are at capacity, accurate deterioration indices could help identify low-risk patients as potential candidates for home care programs and alleviate hospital strain. To date, many existing deterioration indices are based entirely on structured data from the electronic health record (EHR) and ignore potentially useful information from other sources. OBJECTIVE: To improve the accuracy of existing deterioration indices by incorporating unstructured imaging data from chest radiographs. DESIGN, SETTING, AND PARTICIPANTS: Machine learning models were trained to predict deterioration of patients hospitalized with acute dyspnea using existing deterioration index scores and chest radiographs. Models were trained on hospitalized patients without coronavirus disease 2019 (COVID-19) and then subsequently tested on patients with COVID-19 between January 2020 and December 2020 at a single tertiary care center who had at least one radiograph taken within 48 hours of hospital admission. MAIN OUTCOMES AND MEASURES: Patient deterioration was defined as the need for invasive or non-invasive mechanical ventilation, heated high flow nasal cannula, IV vasopressor administration or in-hospital mortality at any time following admission. The EPIC deterioration index was augmented with unstructured data from chest radiographs to predict risk of deterioration. We compared discriminative performance of the models with and without incorporating chest radiographs using area under the receiver operating curve (AUROC), focusing on comparing the fraction and total patients identified as low risk at different negative predictive values (NPV). RESULTS: Data from 6278 hospitalizations were analyzed, including 5562 hospitalizations without COVID-19 (training cohort) and 716 with COVID-19 (216 in validation, 500 in held-out test cohort). At a NPV of 0.95, the best-performing image-augmented deterioration index identified 49 more (9.8%) individuals as low-risk compared to the deterioration index based on clinical data alone in the first 48 hours of admission. At a NPV of 0.9, the EPIC image-augmented deterioration index identified 26 more individuals (5.2%) as low-risk compared to the deterioration index based on clinical data alone in the first 48 hours of admission. CONCLUSION AND RELEVANCE: Augmenting existing deterioration indices with chest radiographs results in better identification of low-risk patients. The model augmentation strategy could be used in the future to incorporate other forms of unstructured data into existing disease models.",2022,10.1371/journal.pone.0263922,diagnosis,False
Augmenting lung cancer diagnosis on chest radiographs: positioning artificial intelligence to improve radiologist performance,"AIM: To evaluate the role that artificial intelligence (AI) could play in assisting radiologists as the first reader of chest radiographs (CXRs), to increase the accuracy and efficiency of lung cancer diagnosis by flagging positive cases before passing the remaining examinations to standard reporting. MATERIALS AND METHODS: A dataset of 400 CXRs including 200 difficult lung cancer cases was curated. Examinations were reviewed by three FRCR radiologists and an AI algorithm to establish performance in tumour identification. AI and radiologist labels were combined retrospectively to simulate the proposed AI triage workflow. RESULTS: When used as a standalone algorithm, AI classification was equivalent to the average radiologist performance. The best overall performances were achieved when AI was combined with radiologists, with an average reduction of missed cancers of 60%. Combination with AI also standardised the performance of radiologists. The greatest improvements were observed when common sources of errors were present, such as distracting findings. DISCUSSION: The proposed AI implementation pathway stands to reduce radiologist errors and improve clinician reporting performance. Furthermore, taking a radiologist-centric approach in the development of clinical AI holds promise for catching systematically missed lung cancers. This represents a tremendous opportunity to improve patient outcomes for lung cancer diagnosis.",2021,10.1016/j.crad.2021.03.021,diagnosis,False
Automated approach for segmenting gross tumor volumes for lung cancer stereotactic body radiation therapy using CT-based dense V-networks,"The aim of this study was to develop an automated segmentation approach for small gross tumor volumes (GTVs) in 3D planning computed tomography (CT) images using dense V-networks (DVNs) that offer more advantages in segmenting smaller structures than conventional V-networks. Regions of interest (ROI) with dimensions of 50 × 50 × 6-72 pixels in the planning CT images were cropped based on the GTV centroids when applying stereotactic body radiotherapy (SBRT) to patients. Segmentation accuracy of GTV contours for 192 lung cancer patients [with the following tumor types: 118 solid, 53 part-solid types and 21 pure ground-glass opacity (pure GGO)], who underwent SBRT, were evaluated based on a 10-fold cross-validation test using Dice's similarity coefficient (DSC) and Hausdorff distance (HD). For each case, 11 segmented GTVs consisting of three single outputs, four logical AND outputs, and four logical OR outputs from combinations of two or three outputs from DVNs were obtained by three runs with different initial weights. The AND output (combination of three outputs) achieved the highest values of average 3D-DSC (0.832 ± 0.074) and HD (4.57 ± 2.44 mm). The average 3D DSCs from the AND output for solid, part-solid and pure GGO types were 0.838 ± 0.074, 0.822 ± 0.078 and 0.819 ± 0.059, respectively. This study suggests that the proposed approach could be useful in segmenting GTVs for planning lung cancer SBRT.",2021,10.1093/jrr/rraa132,treatment,True
Automated Assessment of COVID-19 Reporting and Data System and Chest CT Severity Scores in Patients Suspected of Having COVID-19 Using Artificial Intelligence,"Background The coronavirus disease 2019 (COVID-19) pandemic has spread across the globe with alarming speed, morbidity, and mortality. Immediate triage of patients with chest infections suspected to be caused by COVID-19 using chest CT may be of assistance when results from definitive viral testing are delayed. Purpose To develop and validate an artificial intelligence (AI) system to score the likelihood and extent of pulmonary COVID-19 on chest CT scans using the COVID-19 Reporting and Data System (CO-RADS) and CT severity scoring systems. Materials and Methods The CO-RADS AI system consists of three deep-learning algorithms that automatically segment the five pulmonary lobes, assign a CO-RADS score for the suspicion of COVID-19, and assign a CT severity score for the degree of parenchymal involvement per lobe. This study retrospectively included patients who underwent a nonenhanced chest CT examination because of clinical suspicion of COVID-19 at two medical centers. The system was trained, validated, and tested with data from one of the centers. Data from the second center served as an external test set. Diagnostic performance and agreement with scores assigned by eight independent observers were measured using receiver operating characteristic analysis, linearly weighted κ values, and classification accuracy. Results A total of 105 patients (mean age, 62 years ± 16 [standard deviation]; 61 men) and 262 patients (mean age, 64 years ± 16; 154 men) were evaluated in the internal and external test sets, respectively. The system discriminated between patients with COVID-19 and those without COVID-19, with areas under the receiver operating characteristic curve of 0.95 (95% CI: 0.91, 0.98) and 0.88 (95% CI: 0.84, 0.93), for the internal and external test sets, respectively. Agreement with the eight human observers was moderate to substantial, with mean linearly weighted κ values of 0.60 ± 0.01 for CO-RADS scores and 0.54 ± 0.01 for CT severity scores. Conclusion With high diagnostic performance, the CO-RADS AI system correctly identified patients with COVID-19 using chest CT scans and assigned standardized CO-RADS and CT severity scores that demonstrated good agreement with findings from eight independent observers and generalized well to external data. © RSNA, 2020 Supplemental material is available for this article.",2021,10.1148/radiol.2020202439,diagnosis,True
Automated coronary artery calcification scoring in non-gated chest CT: agreement and reliability,"OBJECTIVE: To determine the agreement and reliability of fully automated coronary artery calcium (CAC) scoring in a lung cancer screening population. MATERIALS AND METHODS: 1793 low-dose chest CT scans were analyzed (non-contrast-enhanced, non-gated). To establish the reference standard for CAC, first automated calcium scoring was performed using a preliminary version of a method employing coronary calcium atlas and machine learning approach. Thereafter, each scan was inspected by one of four trained raters. When needed, the raters corrected initially automaticity-identified results. In addition, an independent observer subsequently inspected manually corrected results and discarded scans with gross segmentation errors. Subsequently, fully automatic coronary calcium scoring was performed. Agatston score, CAC volume and number of calcifications were computed. Agreement was determined by calculating proportion of agreement and examining Bland-Altman plots. Reliability was determined by calculating linearly weighted kappa (κ) for Agatston strata and intraclass correlation coefficient (ICC) for continuous values. RESULTS: 44 (2.5%) scans were excluded due to metal artifacts or gross segmentation errors. In the remaining 1749 scans, median Agatston score was 39.6 (P25-P75∶0-345.9), median volume score was 60.4 mm3 (P25-P75∶0-361.4) and median number of calcifications was 2 (P25-P75∶0-4) for the automated scores. The κ demonstrated very good reliability (0.85) for Agatston risk categories between the automated and reference scores. The Bland-Altman plots showed underestimation of calcium score values by automated quantification. Median difference was 2.5 (p25-p75∶0.0-53.2) for Agatston score, 7.6 (p25-p75∶0.0-94.4) for CAC volume and 1 (p25-p75∶0-5) for number of calcifications. The ICC was very good for Agatston score (0.90), very good for calcium volume (0.88) and good for number of calcifications (0.64). DISCUSSION: Fully automated coronary calcium scoring in a lung cancer screening setting is feasible with acceptable reliability and agreement despite an underestimation of the amount of calcium when compared to reference scores.",2014,10.1371/journal.pone.0091239,diagnosis,True
Automated detection and classification of tumor histotypes on dynamic PET imaging data through machine-learning driven voxel classification,"2-deoxy-2-fluorine-((18)F)fluoro-d-glucose Positron Emission Tomography/Computed Tomography ((18)F-FDG-PET/CT) is widely used in oncology mainly for diagnosis and staging of various cancer types, including lung cancer, which is the most common cancer worldwide. Since histopathologic subtypes of lung cancer show different degree of (18)F-FDG uptake, to date there are some diagnostic limits and uncertainties, hindering an (18)F-FDG-PET-driven classification of histologic subtypes of lung cancers. On the other hand, since activated macrophages, neutrophils, fibroblasts and granulation tissues also show an increased (18)F-FDG activity, infectious and/or inflammatory processes and post-surgical and post-radiation changes may cause false-positive results, especially for lymph-nodes assessment. Here we propose a model-free, machine-learning based algorithm for the automated classification of adenocarcinoma, the most common type of lung cancer, and other types of tumors. Input for the algorithm are dynamic acquisitions of PET data (dPET), providing for a spatially and temporally resolved characterization of the uptake kinetic. The algorithm consists in a trained Random Forest classifier which, relying contextually on several spatial and temporal features of (18)F-FDG uptake, generates as an outcome probability maps allowing to distinguish adenocarcinoma from other lung histotype and to identify metastatic lymph-nodes, ultimately increasing the specificity of the technique. Its performance, evaluated on a dPET dataset of 19 patients affected by primary lung cancer, provides a probability 0.943 ± 0.090 for the detection of adenocarcinoma. The use of this algorithm will guarantee an automatic and more accurate localization and discrimination of tumors, also providing a powerful tool for detecting at which extent tumor has spread beyond a primary tumor into lymphatic system.",2022,10.1016/j.compbiomed.2022.105423,diagnosis,False
Automated detection and quantification of COVID-19 pneumonia: CT imaging analysis by a deep learning-based software,"BACKGROUND: The novel coronavirus disease 2019 (COVID-19) is an emerging worldwide threat to public health. While chest computed tomography (CT) plays an indispensable role in its diagnosis, the quantification and localization of lesions cannot be accurately assessed manually. We employed deep learning-based software to aid in detection, localization and quantification of COVID-19 pneumonia. METHODS: A total of 2460 RT-PCR tested SARS-CoV-2-positive patients (1250 men and 1210 women; mean age, 57.7 ± 14.0 years (age range, 11-93 years) were retrospectively identified from Huoshenshan Hospital in Wuhan from February 11 to March 16, 2020. Basic clinical characteristics were reviewed. The uAI Intelligent Assistant Analysis System was used to assess the CT scans. RESULTS: CT scans of 2215 patients (90%) showed multiple lesions of which 36 (1%) and 50 patients (2%) had left and right lung infections, respectively (> 50% of each affected lung's volume), while 27 (1%) had total lung infection (> 50% of the total volume of both lungs). Overall, 298 (12%), 778 (32%) and 1300 (53%) patients exhibited pure ground glass opacities (GGOs), GGOs with sub-solid lesions and GGOs with both sub-solid and solid lesions, respectively. Moreover, 2305 (94%) and 71 (3%) patients presented primarily with GGOs and sub-solid lesions, respectively. Elderly patients (≥ 60 years) were more likely to exhibit sub-solid lesions. The generalized linear mixed model showed that the dorsal segment of the right lower lobe was the favoured site of COVID-19 pneumonia. CONCLUSION: Chest CT combined with analysis by the uAI Intelligent Assistant Analysis System can accurately evaluate pneumonia in COVID-19 patients.",2020,10.1007/s00259-020-04953-1,diagnosis,True
Automated detection and segmentation of thoracic lymph nodes from CT using 3D foveal fully convolutional neural networks,"BACKGROUND: In oncology, the correct determination of nodal metastatic disease is essential for patient management, as patient treatment and prognosis are closely linked to the stage of the disease. The aim of the study was to develop a tool for automatic 3D detection and segmentation of lymph nodes (LNs) in computed tomography (CT) scans of the thorax using a fully convolutional neural network based on 3D foveal patches. METHODS: The training dataset was collected from the Computed Tomography Lymph Nodes Collection of the Cancer Imaging Archive, containing 89 contrast-enhanced CT scans of the thorax. A total number of 4275 LNs was segmented semi-automatically by a radiologist, assessing the entire 3D volume of the LNs. Using this data, a fully convolutional neuronal network based on 3D foveal patches was trained with fourfold cross-validation. Testing was performed on an unseen dataset containing 15 contrast-enhanced CT scans of patients who were referred upon suspicion or for staging of bronchial carcinoma. RESULTS: The algorithm achieved a good overall performance with a total detection rate of 76.9% for enlarged LNs during fourfold cross-validation in the training dataset with 10.3 false-positives per volume and of 69.9% in the unseen testing dataset. In the training dataset a better detection rate was observed for enlarged LNs compared to smaller LNs, the detection rate for LNs with a short-axis diameter (SAD) ≥ 20 mm and SAD 5-10 mm being 91.6% and 62.2% (p < 0.001), respectively. Best detection rates were obtained for LNs located in Level 4R (83.6%) and Level 7 (80.4%). CONCLUSIONS: The proposed 3D deep learning approach achieves an overall good performance in the automatic detection and segmentation of thoracic LNs and shows reasonable generalizability, yielding the potential to facilitate detection during routine clinical work and to enable radiomics research without observer-bias.",2021,10.1186/s12880-021-00599-z,diagnosis,True
Automated Detection of COVID-19 Cases on Radiographs using Shape-Dependent Fibonacci-p Patterns,"The coronavirus (COVID-19) pandemic has been adversely affecting people's health globally. To diminish the effect of this widespread pandemic, it is essential to detect COVID-19 cases as quickly as possible. Chest radiographs are less expensive and are a widely available imaging modality for detecting chest pathology compared with CT images. They play a vital role in early prediction and developing treatment plans for suspected or confirmed COVID-19 chest infection patients. In this paper, a novel shape-dependent Fibonacci-p patterns-based feature descriptor using a machine learning approach is proposed. Computer simulations show that the presented system (1) increases the effectiveness of differentiating COVID-19, viral pneumonia, and normal conditions, (2) is effective on small datasets, and (3) has faster inference time compared to deep learning methods with comparable performance. Computer simulations are performed on two publicly available datasets; (a) the Kaggle dataset, and (b) the COVIDGR dataset. To assess the performance of the presented system, various evaluation parameters, such as accuracy, recall, specificity, precision, and f1-score are used. Nearly 100% differentiation between normal and COVID-19 radiographs is observed for the three-class classification scheme using the lung area-specific Kaggle radiographs. While Recall of 72.65 ± 6.83 and specificity of 77.72 ± 8.06 is observed for the COVIDGR dataset.",2021,10.1109/jbhi.2021.3069798,diagnosis,False
Automated detection of COVID-19 cases using deep neural networks with X-ray images,"The novel coronavirus 2019 (COVID-2019), which first appeared in Wuhan city of China in December 2019, spread rapidly around the world and became a pandemic. It has caused a devastating effect on both daily lives, public health, and the global economy. It is critical to detect the positive cases as early as possible so as to prevent the further spread of this epidemic and to quickly treat affected patients. The need for auxiliary diagnostic tools has increased as there are no accurate automated toolkits available. Recent findings obtained using radiology imaging techniques suggest that such images contain salient information about the COVID-19 virus. Application of advanced artificial intelligence (AI) techniques coupled with radiological imaging can be helpful for the accurate detection of this disease, and can also be assistive to overcome the problem of a lack of specialized physicians in remote villages. In this study, a new model for automatic COVID-19 detection using raw chest X-ray images is presented. The proposed model is developed to provide accurate diagnostics for binary classification (COVID vs. No-Findings) and multi-class classification (COVID vs. No-Findings vs. Pneumonia). Our model produced a classification accuracy of 98.08% for binary classes and 87.02% for multi-class cases. The DarkNet model was used in our study as a classifier for the you only look once (YOLO) real time object detection system. We implemented 17 convolutional layers and introduced different filtering on each layer. Our model (available at (https://github.com/muhammedtalo/COVID-19)) can be employed to assist radiologists in validating their initial screening, and can also be employed via cloud to immediately screen patients.",2020,10.1016/j.compbiomed.2020.103792,diagnosis,False
Automated detection of COVID-19 through convolutional neural network using chest x-ray images,"The COVID-19 epidemic has a catastrophic impact on global well-being and public health. More than 27 million confirmed cases have been reported worldwide until now. Due to the growing number of confirmed cases, and challenges to the variations of the COVID-19, timely and accurate classification of healthy and infected patients is essential to control and treat COVID-19. We aim to develop a deep learning-based system for the persuasive classification and reliable detection of COVID-19 using chest radiography. Firstly, we evaluate the performance of various state-of-the-art convolutional neural networks (CNNs) proposed over recent years for medical image classification. Secondly, we develop and train CNN from scratch. In both cases, we use a public X-Ray dataset for training and validation purposes. For transfer learning, we obtain 100% accuracy for binary classification (i.e., Normal/COVID-19) and 87.50% accuracy for tertiary classification (Normal/COVID-19/Pneumonia). With the CNN trained from scratch, we achieve 93.75% accuracy for tertiary classification. In the case of transfer learning, the classification accuracy drops with the increased number of classes. The results are demonstrated by comprehensive receiver operating characteristics (ROC) and confusion metric analysis with 10-fold cross-validation.",2022,10.1371/journal.pone.0262052,diagnosis,False
Automated detection of COVID-19 using ensemble of transfer learning with deep convolutional neural network based on CT scans,"PURPOSE: COVID-19 has infected millions of people worldwide. One of the most important hurdles in controlling the spread of this disease is the inefficiency and lack of medical tests. Computed tomography (CT) scans are promising in providing accurate and fast detection of COVID-19. However, determining COVID-19 requires highly trained radiologists and suffers from inter-observer variability. To remedy these limitations, this paper introduces an automatic methodology based on an ensemble of deep transfer learning for the detection of COVID-19. METHODS: A total of 15 pre-trained convolutional neural networks (CNNs) architectures: EfficientNets(B0-B5), NasNetLarge, NasNetMobile, InceptionV3, ResNet-50, SeResnet 50, Xception, DenseNet121, ResNext50 and Inception_resnet_v2 are used and then fine-tuned on the target task. After that, we built an ensemble method based on majority voting of the best combination of deep transfer learning outputs to further improve the recognition performance. We have used a publicly available dataset of CT scans, which consists of 349 CT scans labeled as being positive for COVID-19 and 397 negative COVID-19 CT scans that are normal or contain other types of lung diseases. RESULTS: The experimental results indicate that the majority voting of 5 deep transfer learning architecture with EfficientNetB0, EfficientNetB3, EfficientNetB5, Inception_resnet_v2, and Xception has the higher results than the individual transfer learning structure and among the other models based on precision (0.857), recall (0.854) and accuracy (0.85) metrics in diagnosing COVID-19 from CT scans. CONCLUSION: Our study based on an ensemble deep transfer learning system with different pre-trained CNNs architectures can work well on a publicly available dataset of CT images for the diagnosis of COVID-19 based on CT scans.",2021,10.1007/s11548-020-02286-w,diagnosis,True
Automated detection of lung cancer at ultralow dose PET/CT by deep neural networks - Initial results,"OBJECTIVES: We evaluated whether machine learning may be helpful for the detection of lung cancer in FDG-PET imaging in the setting of ultralow dose PET scans. MATERIALS AND METHODS: We studied the performance of an artificial neural network discriminating lung cancer patients (n = 50) from controls (n = 50) without pulmonary malignancies. A total of 3936 PET slices including images in which the lung tumor is visually present and image slices of patients with no lung cancer were exported. The diagnostic performance of the artificial neural network based on clinical standard dose PET images (PET(100%)) as well as with a tenfold (PET(10%)) and thirtyfold (PET(3.3%)) reduced radiation dose (∼0.11 mSv) was assessed. RESULTS: The area under the curve of the deep learning algorithm for lung cancer detection was 0.989, 0.983 and 0.970 for standard dose images (PET(100%)), and reduced dose PET(10%), and PET(3.3%) reconstruction, respectively. The artificial neural network achieved a sensitivity of 95.9% and 91.5% and a specificity of 98.1% and 94.2%, at standard dose and ultralow dose PET(3.3%), respectively. CONCLUSION: Our results suggest that machine learning algorithms may aid fully automated lung cancer detection even at very low effective radiation doses of 0.11 mSv. Further improvement of this technology might improve the specificity of lung cancer screening efforts and could lead to new applications of FDG-PET.",2018,10.1016/j.lungcan.2018.11.001,diagnosis,True
Automated detection of lung nodules and coronary artery calcium using artificial intelligence on low-dose CT scans for lung cancer screening: accuracy and prognostic value,"BACKGROUND: Artificial intelligence (AI) in diagnostic radiology is undergoing rapid development. Its potential utility to improve diagnostic performance for cardiopulmonary events is widely recognized, but the accuracy and precision have yet to be demonstrated in the context of current screening modalities. Here, we present findings on the performance of an AI convolutional neural network (CNN) prototype (AI-RAD Companion, Siemens Healthineers) that automatically detects pulmonary nodules and quantifies coronary artery calcium volume (CACV) on low-dose chest CT (LDCT), and compare results to expert radiologists. We also correlate AI findings with adverse cardiopulmonary outcomes in a retrospective cohort of 117 patients who underwent LDCT. METHODS: A total of 117 patients were enrolled in this study. Two CNNs were used to identify lung nodules and CACV on LDCT scans. All subjects were used for lung nodule analysis, and 96 subjects met the criteria for coronary artery calcium volume analysis. Interobserver concordance was measured using ICC and Cohen's kappa. Multivariate logistic regression and partial least squares regression were used for outcomes analysis. RESULTS: Agreement of the AI findings with experts was excellent (CACV ICC = 0.904, lung nodules Cohen's kappa = 0.846) with high sensitivity and specificity (CACV: sensitivity = .929, specificity = .960; lung nodules: sensitivity = 1, specificity = 0.708). The AI findings improved the prediction of major cardiopulmonary outcomes at 1-year follow-up including major adverse cardiac events and lung cancer (AUC(MACE) = 0.911, AUC(Lung Cancer) = 0.942). CONCLUSION: We conclude the AI prototype rapidly and accurately identifies significant risk factors for cardiopulmonary disease on standard screening low-dose chest CT. This information can be used to improve diagnostic ability, facilitate intervention, improve morbidity and mortality, and decrease healthcare costs. There is also potential application in countries with limited numbers of cardiothoracic radiologists.",2021,10.1186/s12916-021-01928-3,diagnosis,True
Automated detection of pulmonary nodules in PET/CT images: Ensemble false-positive reduction using a convolutional neural network technique,"PURPOSE: Automated detection of solitary pulmonary nodules using positron emission tomography (PET) and computed tomography (CT) images shows good sensitivity; however, it is difficult to detect nodules in contact with normal organs, and additional efforts are needed so that the number of false positives (FPs) can be further reduced. In this paper, the authors propose an improved FP-reduction method for the detection of pulmonary nodules in PET/CT images by means of convolutional neural networks (CNNs). METHODS: The overall scheme detects pulmonary nodules using both CT and PET images. In the CT images, a massive region is first detected using an active contour filter, which is a type of contrast enhancement filter that has a deformable kernel shape. Subsequently, high-uptake regions detected by the PET images are merged with the regions detected by the CT images. FP candidates are eliminated using an ensemble method; it consists of two feature extractions, one by shape/metabolic feature analysis and the other by a CNN, followed by a two-step classifier, one step being rule based and the other being based on support vector machines. RESULTS: The authors evaluated the detection performance using 104 PET/CT images collected by a cancer-screening program. The sensitivity in detecting candidates at an initial stage was 97.2%, with 72.8 FPs/case. After performing the proposed FP-reduction method, the sensitivity of detection was 90.1%, with 4.9 FPs/case; the proposed method eliminated approximately half the FPs existing in the previous study. CONCLUSIONS: An improved FP-reduction scheme using CNN technique has been developed for the detection of pulmonary nodules in PET/CT images. The authors' ensemble FP-reduction method eliminated 93% of the FPs; their proposed method using CNN technique eliminates approximately half the FPs existing in the previous study. These results indicate that their method may be useful in the computer-aided detection of pulmonary nodules using PET/CT images.",2016,10.1118/1.4948498,diagnosis,True
Automated Diagnosis of Chest X-Ray for Early Detection of COVID-19 Disease,"In March 2020, the World Health Organization announced the COVID-19 pandemic, its dangers, and its rapid spread throughout the world. In March 2021, the second wave of the pandemic began with a new strain of COVID-19, which was more dangerous for some countries, including India, recording 400,000 new cases daily and more than 4,000 deaths per day. This pandemic has overloaded the medical sector, especially radiology. Deep-learning techniques have been used to reduce the burden on hospitals and assist physicians for accurate diagnoses. In our study, two models of deep learning, ResNet-50 and AlexNet, were introduced to diagnose X-ray datasets collected from many sources. Each network diagnosed a multiclass (four classes) and a two-class dataset. The images were processed to remove noise, and a data augmentation technique was applied to the minority classes to create a balance between the classes. The features extracted by convolutional neural network (CNN) models were combined with traditional Gray-level Cooccurrence Matrix (GLCM) and Local Binary Pattern (LBP) algorithms in a 1-D vector of each image, which produced more representative features for each disease. Network parameters were tuned for optimum performance. The ResNet-50 network reached accuracy, sensitivity, specificity, and Area Under the Curve (AUC) of 95%, 94.5%, 98%, and 97.10%, respectively, with the multiclasses (COVID-19, viral pneumonia, lung opacity, and normal), while it reached accuracy, sensitivity, specificity, and AUC of 99%, 98%, 98%, and 97.51%, respectively, with the binary classes (COVID-19 and normal).",2021,10.1155/2021/6919483,diagnosis,False
Automated Diagnosis of COVID-19 Using Deep Features and Parameter Free BAT Optimization,"Background: Accurate and fast diagnosis of COVID-19 is very important to manage the medical conditions of affected persons. The task is challenging owing to shortage and ineffectiveness of clinical testing kits. However, the existing problems can be improved by employing computational intelligent techniques on radiological images like CT-Scans (Computed Tomography) of lungs. Extensive research has been reported using deep learning models to diagnose the severity of COVID-19 from CT images. This has undoubtedly minimized the manual involvement in abnormality identification but reported detection accuracy is limited. Methods: The present work proposes an expert model based on deep features and Parameter Free BAT (PF-BAT) optimized Fuzzy K-nearest neighbor (PF-FKNN) classifier to diagnose novel coronavirus. In this proposed model, features are extracted from the fully connected layer of transfer learned MobileNetv2 followed by FKNN training. The hyperparameters of FKNN are fine-tuned using PF-BAT. Results: The experimental results on the benchmark COVID CT scan data reveal that the proposed algorithm attains a validation accuracy of 99.38% which is better than the existing state-of-the-art methods proposed in past. Conclusion: The proposed model will help in timely and accurate identification of the coronavirus at the various phases. Such kind of rapid diagnosis will assist clinicians to manage the healthcare condition of patients well and will help in speedy recovery from the diseases. Clinical and Translational Impact Statement - The proposed automated system can provide accurate and fast detection of COVID-19 signature from lung radiographs. Also, the usage of lighter MobileNetv2 architecture makes it practical for deployment in real-time.",2021,10.1109/jtehm.2021.3077142,diagnosis,True
Automated identification of pulmonary arteries and veins depicted in non-contrast chest CT scans,"We present a novel integrative computerized solution to automatically identify and differentiate pulmonary arteries and veins depicted on chest computed tomography (CT) without iodinated contrast agents. We first identified the central extrapulmonary arteries and veins using a convolutional neural network (CNN) model. Then, a computational differential geometry method was used to automatically identify the tubular-like structures in the lungs with high densities, which we believe are the intrapulmonary vessels. Beginning with the extrapulmonary arteries and veins, we progressively traced the intrapulmonary vessels by following their skeletons and differentiated them into arteries and veins. Instead of manually labeling the numerous arteries and veins in the lungs for machine learning, this integrative strategy limits the manual effort only to the large extrapulmonary vessels. We used a dataset consisting of 120 chest CT scans acquired on different subjects using various protocols to develop, train, and test the algorithms. Our experiments on an independent test set (n = 15) showed promising performance. The computer algorithm achieved a sensitivity of ∼98% in labeling the pulmonary artery and vein branches when compared with a human expert's results, demonstrating the feasibility of our computerized solution in pulmonary artery/vein labeling.",2022,10.1016/j.media.2022.102367,diagnosis,True
Automated lung cancer diagnosis using three-dimensional convolutional neural networks,"Lung cancer is the deadliest cancer worldwide. It has been shown that early detection using low-dose computer tomography (LDCT) scans can reduce deaths caused by this disease. We present a general framework for the detection of lung cancer in chest LDCT images. Our method consists of a nodule detector trained on the LIDC-IDRI dataset followed by a cancer predictor trained on the Kaggle DSB 2017 dataset and evaluated on the IEEE International Symposium on Biomedical Imaging (ISBI) 2018 Lung Nodule Malignancy Prediction test set. Our candidate extraction approach is effective to produce accurate candidates with a recall of 99.6%. In addition, our false positive reduction stage classifies successfully the candidates and increases precision by a factor of 2000. Our cancer predictor obtained a ROC AUC of 0.913 and was ranked 1st place at the ISBI 2018 Lung Nodule Malignancy Prediction challenge. Graphical abstract.",2020,10.1007/s11517-020-02197-7,diagnosis,True
Automated Lung Nodule Detection and Classification Using Deep Learning Combined with Multiple Strategies,"Lung cancer is one of the major causes of cancer-related deaths due to its aggressive nature and delayed detections at advanced stages. Early detection of lung cancer is very important for the survival of an individual, and is a significant challenging problem. Generally, chest radiographs (X-ray) and computed tomography (CT) scans are used initially for the diagnosis of the malignant nodules; however, the possible existence of benign nodules leads to erroneous decisions. At early stages, the benign and the malignant nodules show very close resemblance to each other. In this paper, a novel deep learning-based model with multiple strategies is proposed for the precise diagnosis of the malignant nodules. Due to the recent achievements of deep convolutional neural networks (CNN) in image analysis, we have used two deep three-dimensional (3D) customized mixed link network (CMixNet) architectures for lung nodule detection and classification, respectively. Nodule detections were performed through faster R-CNN on efficiently-learned features from CMixNet and U-Net like encoder-decoder architecture. Classification of the nodules was performed through a gradient boosting machine (GBM) on the learned features from the designed 3D CMixNet structure. To reduce false positives and misdiagnosis results due to different types of errors, the final decision was performed in connection with physiological symptoms and clinical biomarkers. With the advent of the internet of things (IoT) and electro-medical technology, wireless body area networks (WBANs) provide continuous monitoring of patients, which helps in diagnosis of chronic diseases-especially metastatic cancers. The deep learning model for nodules' detection and classification, combined with clinical factors, helps in the reduction of misdiagnosis and false positive (FP) results in early-stage lung cancer diagnosis. The proposed system was evaluated on LIDC-IDRI datasets in the form of sensitivity (94%) and specificity (91%), and better results were obatined compared to the existing methods.",2019,10.3390/s19173722,diagnosis,True
Automated mapping and N-Staging of thoracic lymph nodes in contrast-enhanced CT scans of the chest using a fully convolutional neural network,"PURPOSE: To develop a deep-learning (DL)-based approach for thoracic lymph node (LN) mapping based on their anatomical location. METHOD: The training-and validation-dataset included 89 contrast-enhanced computed tomography (CT) scans of the chest. 4201 LNs were semi-automatically segmented and then assigned to LN levels according to their anatomical location. The LN level classification task was addressed by a multi-class segmentation procedure using a fully convolutional neural network. Mapping was performed by firstly determining potential level affiliation for each voxel and then performing majority voting over all voxels belonging to each LN. Mean classification accuracies on the validation data were calculated separately for each level and overall Top-1, Top-2 and Top-3 scores were determined, where a Top-X score describes how often the annotated class was within the top-X predictions. To demonstrate the clinical applicability of our model, we tested its N-staging capabilities in a simulated clinical use case scenario assuming a patient diseased with lung cancer. RESULTS: The artificial intelligence(AI)-based assignment revealed mean classification accuracies of 86.36 % (Top-1), 94.48 % (Top-2) and 96.10 % (Top-3). Best accuracies were achieved for LNs in the subcarinal level 7 (98.31 %) and axillary region (98.74 %). The highest misclassification rates were observed among LNs in adjacent levels. The proof-of-principle application in a simulated clinical use case scenario for automated tumor N-staging showed a mean classification accuracy of up to 96.14 % (Top-1). CONCLUSIONS: The proposed AI approach for automatic classification of LN levels in chest CT as well as the proof-of-principle-experiment for automatic N-staging, revealed promising results, warranting large-scale validation for clinical application.",2021,10.1016/j.ejrad.2021.109718,diagnosis,True
Automated Pulmonary Nodule Classification in Computed Tomography Images Using a Deep Convolutional Neural Network Trained by Generative Adversarial Networks,"Lung cancer is a leading cause of death worldwide. Although computed tomography (CT) examinations are frequently used for lung cancer diagnosis, it can be difficult to distinguish between benign and malignant pulmonary nodules on the basis of CT images alone. Therefore, a bronchoscopic biopsy may be conducted if malignancy is suspected following CT examinations. However, biopsies are highly invasive, and patients with benign nodules may undergo many unnecessary biopsies. To prevent this, an imaging diagnosis with high classification accuracy is essential. In this study, we investigate the automated classification of pulmonary nodules in CT images using a deep convolutional neural network (DCNN). We use generative adversarial networks (GANs) to generate additional images when only small amounts of data are available, which is a common problem in medical research, and evaluate whether the classification accuracy is improved by generating a large amount of new pulmonary nodule images using the GAN. Using the proposed method, CT images of 60 cases with confirmed pathological diagnosis by biopsy are analyzed. The benign nodules assessed in this study are difficult for radiologists to differentiate because they cannot be rejected as being malignant. A volume of interest centered on the pulmonary nodule is extracted from the CT images, and further images are created using axial sections and augmented data. The DCNN is trained using nodule images generated by the GAN and then fine-tuned using the actual nodule images to allow the DCNN to distinguish between benign and malignant nodules. This pretraining and fine-tuning process makes it possible to distinguish 66.7% of benign nodules and 93.9% of malignant nodules. These results indicate that the proposed method improves the classification accuracy by approximately 20% in comparison with training using only the original images.",2019,10.1155/2019/6051939,diagnosis,True
Automated pulmonary nodule detection based on three-dimensional shape-based feature descriptor,"Computer-aided detection (CAD) can help radiologists to detect pulmonary nodules at an early stage. In pulmonary nodule CAD systems, feature extraction is very important for describing the characteristics of nodule candidates. In this paper, we propose a novel three-dimensional shape-based feature descriptor to detect pulmonary nodules in CT scans. After lung volume segmentation, nodule candidates are detected using multi-scale dot enhancement filtering in the segmented lung volume. Next, we extract feature descriptors from the detected nodule candidates, and these are refined using an iterative wall elimination method. Finally, a support vector machine-based classifier is trained to classify nodules and non-nodules. The performance of the proposed system is evaluated on Lung Image Database Consortium data. The proposed method significantly reduces the number of false positives in nodule candidates. This method achieves 97.5% sensitivity, with only 6.76 false positives per scan.",2014,10.1016/j.cmpb.2013.08.015,diagnosis,True
Automated quantification of COVID-19 severity and progression using chest CT images,"OBJECTIVE: To develop and test computer software to detect, quantify, and monitor progression of pneumonia associated with COVID-19 using chest CT scans. METHODS: One hundred twenty chest CT scans from subjects with lung infiltrates were used for training deep learning algorithms to segment lung regions and vessels. Seventy-two serial scans from 24 COVID-19 subjects were used to develop and test algorithms to detect and quantify the presence and progression of infiltrates associated with COVID-19. The algorithm included (1) automated lung boundary and vessel segmentation, (2) registration of the lung boundary between serial scans, (3) computerized identification of the pneumonitis regions, and (4) assessment of disease progression. Agreement between radiologist manually delineated regions and computer-detected regions was assessed using the Dice coefficient. Serial scans were registered and used to generate a heatmap visualizing the change between scans. Two radiologists, using a five-point Likert scale, subjectively rated heatmap accuracy in representing progression. RESULTS: There was strong agreement between computer detection and the manual delineation of pneumonic regions with a Dice coefficient of 81% (CI 76-86%). In detecting large pneumonia regions (> 200 mm(3)), the algorithm had a sensitivity of 95% (CI 94-97%) and specificity of 84% (CI 81-86%). Radiologists rated 95% (CI 72 to 99) of heatmaps at least ""acceptable"" for representing disease progression. CONCLUSION: The preliminary results suggested the feasibility of using computer software to detect and quantify pneumonic regions associated with COVID-19 and to generate heatmaps that can be used to visualize and assess progression. KEY POINTS: • Both computer vision and deep learning technology were used to develop computer software to quantify the presence and progression of pneumonia associated with COVID-19 depicted on CT images. • The computer software was tested using both quantitative experiments and subjective assessment. • The computer software has the potential to assist in the detection of the pneumonic regions, monitor disease progression, and assess treatment efficacy related to COVID-19.",2021,10.1007/s00330-020-07156-2,prognosis,True
Automated system for lung nodules classification based on wavelet feature descriptor and support vector machine,"BACKGROUND: Lung cancer is a leading cause of death worldwide; it refers to the uncontrolled growth of abnormal cells in the lung. A computed tomography (CT) scan of the thorax is the most sensitive method for detecting cancerous lung nodules. A lung nodule is a round lesion which can be either non-cancerous or cancerous. In the CT, the lung cancer is observed as round white shadow nodules. The possibility to obtain a manually accurate interpretation from CT scans demands a big effort by the radiologist and might be a fatiguing process. Therefore, the design of a computer-aided diagnosis (CADx) system would be helpful as a second opinion tool. METHODS: The stages of the proposed CADx are: a supervised extraction of the region of interest to eliminate the shape differences among CT images. The Daubechies db1, db2, and db4 wavelet transforms are computed with one and two levels of decomposition. After that, 19 features are computed from each wavelet sub-band. Then, the sub-band and attribute selection is performed. As a result, 11 features are selected and combined in pairs as inputs to the support vector machine (SVM), which is used to distinguish CT images containing cancerous nodules from those not containing nodules. RESULTS: The clinical data set used for experiments consists of 45 CT scans from ELCAP and LIDC. For the training stage 61 CT images were used (36 with cancerous lung nodules and 25 without lung nodules). The system performance was tested with 45 CT scans (23 CT scans with lung nodules and 22 without nodules), different from that used for training. The results obtained show that the methodology successfully classifies cancerous nodules with a diameter from 2 mm to 30 mm. The total preciseness obtained was 82%; the sensitivity was 90.90%, whereas the specificity was 73.91%. CONCLUSIONS: The CADx system presented is competitive with other literature systems in terms of sensitivity. The system reduces the complexity of classification by not performing the typical segmentation stage of most CADx systems. Additionally, the novelty of the algorithm is the use of a wavelet feature descriptor.",2015,10.1186/s12938-015-0003-y,diagnosis,True
Automatic Calcium Scoring in Low-Dose Chest CT Using Deep Neural Networks With Dilated Convolutions,"Heavy smokers undergoing screening with low-dose chest CT are affected by cardiovascular disease as much as by lung cancer. Low-dose chest CT scans acquired in screening enable quantification of atherosclerotic calcifications and thus enable identification of subjects at increased cardiovascular risk. This paper presents a method for automatic detection of coronary artery, thoracic aorta, and cardiac valve calcifications in low-dose chest CT using two consecutive convolutional neural networks. The first network identifies and labels potential calcifications according to their anatomical location and the second network identifies true calcifications among the detected candidates. This method was trained and evaluated on a set of 1744 CT scans from the National Lung Screening Trial. To determine whether any reconstruction or only images reconstructed with soft tissue filters can be used for calcification detection, we evaluated the method on soft and medium/sharp filter reconstructions separately. On soft filter reconstructions, the method achieved F(1) scores of 0.89, 0.89, 0.67, and 0.55 for coronary artery, thoracic aorta, aortic valve, and mitral valve calcifications, respectively. On sharp filter reconstructions, the F(1) scores were 0.84, 0.81, 0.64, and 0.66, respectively. Linearly weighted kappa coefficients for risk category assignment based on per subject coronary artery calcium were 0.91 and 0.90 for soft and sharp filter reconstructions, respectively. These results demonstrate that the presented method enables reliable automatic cardiovascular risk assessment in all low-dose chest CT scans acquired for lung cancer screening.",2018,10.1109/tmi.2017.2769839,diagnosis,True
"Automatic Categorization and Scoring of Solid, Part-Solid and Non-Solid Pulmonary Nodules in CT Images with Convolutional Neural Network","We present a computer-aided diagnosis system (CADx) for the automatic categorization of solid, part-solid and non-solid nodules in pulmonary computerized tomography images using a Convolutional Neural Network (CNN). Provided with only a two-dimensional region of interest (ROI) surrounding each nodule, our CNN automatically reasons from image context to discover informative computational features. As a result, no image segmentation processing is needed for further analysis of nodule attenuation, allowing our system to avoid potential errors caused by inaccurate image processing. We implemented two computerized texture analysis schemes, classification and regression, to automatically categorize solid, part-solid and non-solid nodules in CT scans, with hierarchical features in each case learned directly by the CNN model. To show the effectiveness of our CNN-based CADx, an established method based on histogram analysis (HIST) was implemented for comparison. The experimental results show significant performance improvement by the CNN model over HIST in both classification and regression tasks, yielding nodule classification and rating performance concordant with those of practicing radiologists. Adoption of CNN-based CADx systems may reduce the inter-observer variation among screening radiologists and provide a quantitative reference for further nodule analysis.",2017,10.1038/s41598-017-08040-8,diagnosis,True
"Automatic classification between COVID-19 pneumonia, non-COVID-19 pneumonia, and the healthy on chest X-ray image: combination of data augmentation methods","This study aimed to develop and validate computer-aided diagnosis (CXDx) system for classification between COVID-19 pneumonia, non-COVID-19 pneumonia, and the healthy on chest X-ray (CXR) images. From two public datasets, 1248 CXR images were obtained, which included 215, 533, and 500 CXR images of COVID-19 pneumonia patients, non-COVID-19 pneumonia patients, and the healthy samples, respectively. The proposed CADx system utilized VGG16 as a pre-trained model and combination of conventional method and mixup as data augmentation methods. Other types of pre-trained models were compared with the VGG16-based model. Single type or no data augmentation methods were also evaluated. Splitting of training/validation/test sets was used when building and evaluating the CADx system. Three-category accuracy was evaluated for test set with 125 CXR images. The three-category accuracy of the CAD system was 83.6% between COVID-19 pneumonia, non-COVID-19 pneumonia, and the healthy. Sensitivity for COVID-19 pneumonia was more than 90%. The combination of conventional method and mixup was more useful than single type or no data augmentation method. In conclusion, this study was able to create an accurate CADx system for the 3-category classification. Source code of our CADx system is available as open source for COVID-19 research.",2020,10.1038/s41598-020-74539-2,diagnosis,False
Automatic classification of lung nodule candidates based on a novel 3D convolution network and knowledge transferred from a 2D network,"OBJECTIVE: In the automatic lung nodule detection system, the authenticity of a large number of nodule candidates needs to be judged, which is a classification task. However, the variable shapes and sizes of the lung nodules have posed a great challenge to the classification of candidates. To solve this problem, we propose a method for classifying nodule candidates through three-dimensional (3D) convolution neural network (ConvNet) model which is trained by transferring knowledge from a multiresolution two-dimensional (2D) ConvNet model. METHODS: In this scheme, a novel 3D ConvNet model is preweighted with the weights of the trained 2D ConvNet model, and then the 3D ConvNet model is trained with 3D image volumes. In this way, the knowledge transfer method can make 3D network easier to converge and make full use of the spatial information of nodules with different sizes and shapes to improve the classification accuracy. RESULTS: The experimental results on 551 065 pulmonary nodule candidates in the LUNA16 dataset show that our method gains a competitive average score in the false-positive reduction track in lung nodule detection, with the sensitivities of 0.619 and 0.642 at 0.125 and 0.25 FPs per scan, respectively. CONCLUSIONS: The proposed method can maintain satisfactory classification accuracy even when the false-positive rate is extremely small in the face of nodules of different sizes and shapes. Moreover, as a transfer learning idea, the method to transfer knowledge from 2D ConvNet to 3D ConvNet is the first attempt to carry out full migration of parameters of various layers including convolution layers, full connection layers, and classifier between different dimensional models, which is more conducive to utilizing the existing 2D ConvNet resources and generalizing transfer learning schemes.",2019,10.1002/mp.13867,diagnosis,True
Automatic classification of pulmonary peri-fissural nodules in computed tomography using an ensemble of 2D views and a convolutional neural network out-of-the-box,"In this paper, we tackle the problem of automatic classification of pulmonary peri-fissural nodules (PFNs). The classification problem is formulated as a machine learning approach, where detected nodule candidates are classified as PFNs or non-PFNs. Supervised learning is used, where a classifier is trained to label the detected nodule. The classification of the nodule in 3D is formulated as an ensemble of classifiers trained to recognize PFNs based on 2D views of the nodule. In order to describe nodule morphology in 2D views, we use the output of a pre-trained convolutional neural network known as OverFeat. We compare our approach with a recently presented descriptor of pulmonary nodule morphology, namely Bag of Frequencies, and illustrate the advantages offered by the two strategies, achieving performance of AUC = 0.868, which is close to the one of human experts.",2015,10.1016/j.media.2015.08.001,diagnosis,True
Automatic classification of solitary pulmonary nodules in PET/CT imaging employing transfer learning techniques,"Early and automatic diagnosis of Solitary Pulmonary Nodules (SPN) in Computed Tomography (CT) chest scans can provide early treatment for patients with lung cancer, as well as doctor liberation from time-consuming procedures. The purpose of this study is the automatic and reliable characterization of SPNs in CT scans extracted from Positron Emission Tomography and Computer Tomography (PET/CT) system. To achieve the aforementioned task, Deep Learning with Convolutional Neural Networks (CNN) is applied. The strategy of training specific CNN architectures from scratch and the strategy of transfer learning, by utilizing state-of-the-art pre-trained CNNs, are compared and evaluated. To enhance the training sets, data augmentation is performed. The publicly available database of CT scans, named as Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI), is also utilized to further expand the training set and is added to the PET/CT dataset. The results highlight the effectiveness of transfer learning and data augmentation for the classification task of small datasets. The best accuracy obtained on the PET/CT dataset reached 94%, utilizing a modification proposal of a state-of-the-art CNN, called VGG16, and enhancing the training set with LIDC-IDRI dataset. Besides, the proposed modification outperforms in terms of sensitivity several similar researches, which exploit the benefits of transfer learning. Overview of the experiment setup. The two datasets containing nodule representations are combined to evaluate the effectiveness of transfer learning over the traditional approach of training Convolutional Neural Networks from scratch.",2021,10.1007/s11517-021-02378-y,diagnosis,True
Automatic coronavirus disease 2019 diagnosis based on chest radiography and deep learning - Success story or dataset bias?,"PURPOSE: Over the last 2 years, the artificial intelligence (AI) community has presented several automatic screening tools for coronavirus disease 2019 (COVID-19) based on chest radiography (CXR), with reported accuracies often well over 90%. However, it has been noted that many of these studies have likely suffered from dataset bias, leading to overly optimistic results. The purpose of this study was to thoroughly investigate to what extent biases have influenced the performance of a range of previously proposed and promising convolutional neural networks (CNNs), and to determine what performance can be expected with current CNNs on a realistic and unbiased dataset. METHODS: Five CNNs for COVID-19 positive/negative classification were implemented for evaluation, namely VGG19, ResNet50, InceptionV3, DenseNet201, and COVID-Net. To perform both internal and cross-dataset evaluations, four datasets were created. The first dataset Valencian Region Medical Image Bank (BIMCV) followed strict reverse transcriptase-polymerase chain reaction (RT-PCR) test criteria and was created from a single reliable open access databank, while the second dataset (COVIDxB8) was created through a combination of six online CXR repositories. The third and fourth datasets were created by combining the opposing classes from the BIMCV and COVIDxB8 datasets. To decrease inter-dataset variability, a pre-processing workflow of resizing, normalization, and histogram equalization were applied to all datasets. Classification performance was evaluated on unseen test sets using precision and recall. A qualitative sanity check was performed by evaluating saliency maps displaying the top 5%, 10%, and 20% most salient segments in the input CXRs, to evaluate whether the CNNs were using relevant information for decision making. In an additional experiment and to further investigate the origin of potential dataset bias, all pixel values outside the lungs were set to zero through automatic lung segmentation before training and testing. RESULTS: When trained and evaluated on the single online source dataset (BIMCV), the performance of all CNNs is relatively low (precision: 0.65-0.72, recall: 0.59-0.71), but remains relatively consistent during external evaluation (precision: 0.58-0.82, recall: 0.57-0.72). On the contrary, when trained and internally evaluated on the combinatory datasets, all CNNs performed well across all metrics (precision: 0.94-1.00, recall: 0.77-1.00). However, when subsequently evaluated cross-dataset, results dropped substantially (precision: 0.10-0.61, recall: 0.04-0.80). For all datasets, saliency maps revealed the CNNs rarely focus on areas inside the lungs for their decision-making. However, even when setting all pixel values outside the lungs to zero, classification performance does not change and dataset bias remains. CONCLUSIONS: Results in this study confirm that when trained on a combinatory dataset, CNNs tend to learn the origin of the CXRs rather than the presence or absence of disease, a behavior known as short-cut learning. The bias is shown to originate from differences in overall pixel values rather than embedded text or symbols, despite consistent image pre-processing. When trained on a reliable, and realistic single-source dataset in which non-lung pixels have been masked, CNNs currently show limited sensitivity (<70%) for COVID-19 infection in CXR, questioning their use as a reliable automatic screening tool.",2022,10.1002/mp.15419,diagnosis,False
Automatic COVID-19 Detection Using Exemplar Hybrid Deep Features with X-ray Images,"COVID-19 and pneumonia detection using medical images is a topic of immense interest in medical and healthcare research. Various advanced medical imaging and machine learning techniques have been presented to detect these respiratory disorders accurately. In this work, we have proposed a novel COVID-19 detection system using an exemplar and hybrid fused deep feature generator with X-ray images. The proposed Exemplar COVID-19FclNet9 comprises three basic steps: exemplar deep feature generation, iterative feature selection and classification. The novelty of this work is the feature extraction using three pre-trained convolutional neural networks (CNNs) in the presented feature extraction phase. The common aspects of these pre-trained CNNs are that they have three fully connected layers, and these networks are AlexNet, VGG16 and VGG19. The fully connected layer of these networks is used to generate deep features using an exemplar structure, and a nine-feature generation method is obtained. The loss values of these feature extractors are computed, and the best three extractors are selected. The features of the top three fully connected features are merged. An iterative selector is used to select the most informative features. The chosen features are classified using a support vector machine (SVM) classifier. The proposed COVID-19FclNet9 applied nine deep feature extraction methods by using three deep networks together. The most appropriate deep feature generation model selection and iterative feature selection have been employed to utilise their advantages together. By using these techniques, the image classification ability of the used three deep networks has been improved. The presented model is developed using four X-ray image corpora (DB1, DB2, DB3 and DB4) with two, three and four classes. The proposed Exemplar COVID-19FclNet9 achieved a classification accuracy of 97.60%, 89.96%, 98.84% and 99.64% using the SVM classifier with 10-fold cross-validation for four datasets, respectively. Our developed Exemplar COVID-19FclNet9 model has achieved high classification accuracy for all four databases and may be deployed for clinical application.",2021,10.3390/ijerph18158052,diagnosis,False
Automatic creation of annotations for chest radiographs based on the positional information extracted from radiographic image reports,"BACKGROUND AND OBJECTIVE: In this study, we tried to create a machine-learning method that detects disease lesions from chest X-ray (CXR) images using a data set annotated with extracted CXR reports information. We set the nodule as the target disease lesion. Manually annotating nodules is costly in terms of time. Therefore, we used the report information to automatically produce training data for the object detection task. METHODS: First, we use semantic segmentation model PSP-Net to recognize lung fields described in the CXR reports. Next, a classification model ResNeSt-50 is used to discriminate the nodule in segmented right and left field. It also can provide attention map by Grad-Cam. If the attention region corresponds to the location of the nodule in the CXR reports, an attention bounding box is generated. Finally, object detection model Faster-RCNN was performed using generated attention bounding box. The bounding boxes predicted by Faster-RCNN were filtered to satisfy the location extracted from CXR reports. RESULTS: For lung field segmentation, a mean intersection of union of 0.889 was achieved in our best model. 15,156 chest radiographs are used for classification. The area under the receiver operating characteristics curve was 0.843 and 0.852 for the left and right lung, respectively. The detection precision of the generated attention bounding box was 0.341 to 0.531 depending on the binary setting for attention map. Through object detection process, the detection precisions of the bounding boxes were improved to 0.567 to 0.800. CONCLUSION: We successfully generated bounding boxes with nodule on CXR images based on the positional information of the diseases extracted from the CXR reports. Our method has the potential to provide bounding boxes for various lung lesions which can reduce the annotation burden for specialists. SHORT ABSTRACT: Machine learning for computer aided image diagnosis requires annotation of images, but manual annotation is time-consuming for medical doctor. In this study, we tried to create a machine-learning method that creates bounding boxes with disease lesions on chest X-ray (CXR) images using the positional information extracted from CXR reports. We set the nodule as the target lesion. First, we use PSP-Net to segment the lung field according to the CXR reports. Next, a classification model ResNeSt-50 was used to discriminate the nodule in segmented lung field. We also created an attention map using the Grad-Cam algorithm. If the area of attention matched the area annotated by the CXR report, the coordinate of the bounding box was considered as a possible nodule area. Finally, we used the attention information obtained from the nodule classification model and let the object detection model trained by all of the generated bounding boxes. Through object detection model, the precision of the bounding boxes to detect nodule is improved.",2021,10.1016/j.cmpb.2021.106331,diagnosis,False
Automatic deep learning-based pleural effusion classification in lung ultrasound images for respiratory pathology diagnosis,"Lung ultrasound (LUS) imaging as a point-of-care diagnostic tool for lung pathologies has been proven superior to X-ray and comparable to CT, enabling earlier and more accurate diagnosis in real-time at the patient's bedside. The main limitation to widespread use is its dependence on the operator training and experience. COVID-19 lung ultrasound findings predominantly reflect a pneumonitis pattern, with pleural effusion being infrequent. However, pleural effusion is easy to detect and to quantify, therefore it was selected as the subject of this study, which aims to develop an automated system for the interpretation of LUS of pleural effusion. A LUS dataset was collected at the Royal Melbourne Hospital which consisted of 623 videos containing 99,209 2D ultrasound images of 70 patients using a phased array transducer. A standardized protocol was followed that involved scanning six anatomical regions providing complete coverage of the lungs for diagnosis of respiratory pathology. This protocol combined with a deep learning algorithm using a Spatial Transformer Network provides a basis for automatic pathology classification on an image-based level. In this work, the deep learning model was trained using supervised and weakly supervised approaches which used frame- and video-based ground truth labels respectively. The reference was expert clinician image interpretation. Both approaches show comparable accuracy scores on the test set of 92.4% and 91.1%, respectively, not statistically significantly different. However, the video-based labelling approach requires significantly less effort from clinical experts for ground truth labelling.",2021,10.1016/j.ejmp.2021.02.023,diagnosis,False
Automatic detect lung node with deep learning in segmentation and imbalance data labeling,"In this study, a novel method with the U-Net-based network architecture, 2D U-Net, is employed to segment the position of lung nodules, which are an early symptom of lung cancer and have a high probability of becoming a carcinoma, especially when a lung nodule is bigger than 15 [Formula: see text]. A serious problem of considering deep learning for all medical images is imbalanced labeling between foreground and background. The lung nodule is the foreground which accounts for a lower percentage in a whole image. The evaluation function adopted in this study is dice coefficient loss, which is usually used in image segmentation tasks. The proposed pre-processing method in this study is to use complementary labeling as the input in U-Net. With this method, the labeling is swapped. The no-nodule position is labeled. And the position of the nodule becomes non-labeled. The result shows that the proposal in this study is efficient in a small quantity of data. This method, complementary labeling could be used in a small data quantity scenario. With the use of ROI segmentation model in the data pre-processing, the results of lung nodule detection can be improved a lot as shown in the experiments.",2021,10.1038/s41598-021-90599-4,diagnosis,True
Predictive Features of Thymic Carcinoma and High-Risk Thymomas Using Random Forest Analysis,"PURPOSE: To determine the predictive features of thymic carcinomas and high-risk thymomas using random forest algorithm. METHODS: A total of 137 patients with pathologically confirmed high-risk thymomas and thymic carcinomas were enrolled in this study. Three clinical features and 20 computed tomography features were reviewed. The association between computed tomography features and pathological patterns was analyzed by univariate analysis and random forest. The predictive efficiency of the random forest algorithm was evaluated by receiver operating characteristic curve analysis. RESULTS: There were 92 thymic carcinomas and 45 high-risk thymomas in this study. In univariate analysis, patient age, presence of myasthenia gravis, lesion shape, enhancement pattern, presence of necrosis or cystic change, mediastinal invasion, vessel invasion, lymphadenopathy, pericardial effusion, and distant organ metastasis were found to be statistically different between high-risk thymomas and thymic carcinomas (all P < 0.01). Random forest suggested that tumor shape, lymphadenopathy, and the presence of pericardial effusion were the key features in tumor differentiation. The predictive accuracy for the test data and whole data was 94.73% and 96.35%, respectively. Further receiver operating characteristic curve analysis showed the area under the curve was 0.957 (95% confidence interval, 0.986-0.929). CONCLUSIONS: The random forest model in the present study has high efficiency in predictive diagnosis of thymic carcinomas and high-risk thymomas. Tumor shape, lymphadenopathy, and pericardial effusion are the key features for tumor differentiation. Thymic tumors with irregular shape, the presence of lymphadenopathy, and pericardial effusion are highly indicative of thymic carcinomas.",2020,10.1097/rct.0000000000000953,diagnosis,True
Epicardial adipose tissue is associated with extent of pneumonia and adverse outcomes in patients with COVID-19,"AIM: We sought to examine the association of epicardial adipose tissue (EAT) quantified on chest computed tomography (CT) with the extent of pneumonia and adverse outcomes in patients with coronavirus disease 2019 (COVID-19). METHODS: We performed a post-hoc analysis of a prospective international registry comprising 109 consecutive patients (age 64 ± 16 years; 62% male) with laboratory-confirmed COVID-19 and noncontrast chest CT imaging. Using semi-automated software, we quantified the burden (%) of lung abnormalities associated with COVID-19 pneumonia. EAT volume (mL) and attenuation (Hounsfield units) were measured using deep learning software. The primary outcome was clinical deterioration (intensive care unit admission, invasive mechanical ventilation, or vasopressor therapy) or in-hospital death. RESULTS: In multivariable linear regression analysis adjusted for patient comorbidities, the total burden of COVID-19 pneumonia was associated with EAT volume (β = 10.6, p = 0.005) and EAT attenuation (β = 5.2, p = 0.004). EAT volume correlated with serum levels of lactate dehydrogenase (r = 0.361, p = 0.001) and C-reactive protein (r = 0.450, p < 0.001). Clinical deterioration or death occurred in 23 (21.1%) patients at a median of 3 days (IQR 1-13 days) following the chest CT. In multivariable logistic regression analysis, EAT volume (OR 5.1 [95% CI 1.8-14.1] per doubling p = 0.011) and EAT attenuation (OR 3.4 [95% CI 1.5-7.5] per 5 Hounsfield unit increase, p = 0.003) were independent predictors of clinical deterioration or death, as was total pneumonia burden (OR 2.5, 95% CI 1.4-4.6, p = 0.002), chronic lung disease (OR 1.3 [95% CI 1.1-1.7], p = 0.011), and history of heart failure (OR 3.5 [95% 1.1-8.2], p = 0.037). CONCLUSIONS: EAT measures quantified from chest CT are independently associated with extent of pneumonia and adverse outcomes in patients with COVID-19, lending support to their use in clinical risk stratification.",2021,10.1016/j.metabol.2020.154436,prognosis,True
Automatic quantification of myocardium and pericardial fat from coronary computed tomography angiography: a multicenter study,"OBJECTIVES: To develop a deep learning-based method for simultaneous myocardium and pericardial fat quantification from coronary computed tomography angiography (CCTA) for the diagnosis and treatment of cardiovascular disease (CVD). METHODS: We retrospectively identified CCTA data obtained between May 2008 and July 2018 in a multicenter (six centers) CVD study. The proposed method was evaluated on 422 patients' data by two studies. The first overall study involves training model on CVD patients and testing on non-CVD patients, as well as training on non-CVD patients and testing on CVD patients. The second study was performed using the leave-center-out approach. The method performance was evaluated using Dice similarity coefficient (DSC), Jaccard index (JAC), 95% Hausdorff distance (HD95), mean surface distance (MSD), residual mean square distance (RMSD), and the center of mass distance (CMD). The robustness of the proposed method was tested using the nonparametric Kruskal-Wallis test and post hoc test to assess the equality of distribution of DSC values among different tests. RESULTS: The automatic segmentation achieved a strong correlation with contour (ICC and R > 0.97, p value < 0.001 throughout all tests). The accuracy of the proposed method remained high through all the tests, with the median DSC higher than 0.88 for pericardial fat and 0.96 for myocardium. The proposed method also resulted in mean MSD, RMSD, HD95, and CMD of less than 1.36 mm for pericardial fat and 1.00 mm for myocardium. CONCLUSIONS: The proposed deep learning-based segmentation method enables accurate simultaneous quantification of myocardium and pericardial fat in a multicenter study. KEY POINTS: • Deep learning-based myocardium and pericardial fat segmentation method tested on 422 patients' coronary computed tomography angiography in a multicenter study. • The proposed method provides segmentations with high volumetric accuracy (ICC and R > 0.97, p value < 0.001) and similar shape as manual annotation by experienced radiologists (median Dice similarity coefficient ≥ 0.88 for pericardial fat and 0.96 for myocardium).",2021,10.1007/s00330-020-07482-5,diagnosis,True
Automatic Detection and Classification of Lung Nodules in CT Image Using Optimized Neuro Fuzzy Classifier with Cuckoo Search Algorithm,"The Lung nodules are very important to indicate the lung cancer, and its early detection enables timely treatment and increases the survival rate of patient. Even though lots of works are done in this area, still improvement in accuracy is required for improving the survival rate of the patient. The proposed method can classify the stages of lung cancer in addition to the detection of lung nodules. There are two parts in the proposed method, the first part is used for classifying normal/abnormal and second part is used for classifying stages of lung cancer. Totally 10 features from the lung region segmented image are considered for detection and classification. The first part of the proposed method classifies the input images with the aid of Naive Bayes classifier as normal or abnormal. The second part of the system classifies the four stages of lung cancer using Neuro Fuzzy classifier with Cuckoo Search algorithm. The results of proposed system show that the rate of accuracy of classification is improved and the results are compared with SVM, Neural Network and Neuro Fuzzy Classifiers.",2019,10.1007/s10916-019-1177-9,diagnosis,True
Automatic detection of COVID-19 from chest radiographs using deep learning,"INTRODUCTION: The breakdown of a deadly infectious disease caused by a newly discovered coronavirus (named SARS n-CoV2) back in December 2019 has shown no respite to slow or stop in general. This contagious disease has spread across different lengths and breadths of the globe, taking a death toll to nearly 700 k by the start of August 2020. The number is well expected to rise even more significantly. In the absence of a thoroughly tested and approved vaccine, the onus primarily lies on obliging to standard operating procedures and timely detection and isolation of the infected persons. The detection of SARS n-CoV2 has been one of the core concerns during the fight against this pandemic. To keep up with the scale of the outbreak, testing needs to be scaled at par with it. With the conventional PCR testing, most of the countries have struggled to minimize the gap between the scale of outbreak and scale of testing. METHOD: One way of expediting the scale of testing is to shift to a rigorous computational model driven by deep neural networks, as proposed here in this paper. The proposed model is a non-contact process of determining whether a subject is infected or not and is achieved by using chest radiographs; one of the most widely used imaging technique for clinical diagnosis due to fast imaging and low cost. The dataset used in this work contains 1428 chest radiographs with confirmed COVID-19 positive, common bacterial pneumonia, and healthy cases (no infection). We explored the pre-trained VGG-16 model for classification tasks in this. Transfer learning with fine-tuning was used in this study to train the network on relatively small chest radiographs effectively. RESULTS: Initial experiments showed that the model achieved promising results and can be significantly used to expedite COVID-19 detection. The experimentation showed an accuracy of 96% and 92.5% in two and three output class cases, respectively. CONCLUSION: We believe that this study could be used as an initial screening, which can help healthcare professionals to treat the COVID patients by timely detecting better and screening the presence of disease. IMPLICATION FOR PRACTICE: Its simplicity drives the proposed deep neural network model, the capability to work on small image dataset, the non-contact method with acceptable accuracy is a potential alternative for rapid COVID-19 testing that can be adapted by the medical fraternity considering the criticality of the time along with the magnitudes of the outbreak.",2021,10.1016/j.radi.2020.10.018,diagnosis,False
Automatic detection of COVID-19 in chest radiographs using serially concatenated deep and handcrafted features,"Since the infectious disease occurrence rate in the human community is gradually rising due to varied reasons, appropriate diagnosis and treatments are essential to control its spread. The recently discovered COVID-19 is one of the contagious diseases, which infected numerous people globally. This contagious disease is arrested by several diagnoses and handling actions. Medical image-supported diagnosis of COVID-19 infection is an approved clinical practice. This research aims to develop a new Deep Learning Method (DLM) to detect the COVID-19 infection using the chest X-ray. The proposed work implemented two methods namely, detection of COVID-19 infection using (i) a Firefly Algorithm (FA) optimized deep-features and (ii) the combined deep and machine features optimized with FA. In this work, a 5-fold cross-validation method is engaged to train and test detection methods. The performance of this system is analyzed individually resulting in the confirmation that the deep feature-based technique helps to achieve a detection accuracy of > 92% with SVM-RBF classifier and combining deep and machine features achieves > 96% accuracy with Fine KNN classifier. In the future, this technique may have potential to play a vital role in testing and validating the X-ray images collected from patients suffering from the infection diseases.",2022,10.3233/xst-211050,diagnosis,False
Automatic Detection of Covid-19 with Bidirectional LSTM Network Using Deep Features Extracted from Chest X-ray Images,"Coronavirus disease, which comes up in China at the end of 2019 and showed different symptoms in people infected, affected millions of people. Computer-aided expert systems are needed due to the inadequacy of the reverse transcription-polymerase chain reaction kit, which is widely used in the diagnosis of this disease. Undoubtedly, expert systems that provide effective solutions to many problems will be very useful in the detection of Covid-19 disease, especially when unskilled personnel and financial deficiencies in underdeveloped countries are taken into consideration. In the literature, there are numerous machine learning approaches built with different classifiers in the detection of this disease. This paper proposes an approach based on deep learning which detects Covid-19 and no-finding cases using chest X-ray images. Here, the classification performance of the Bi-LSTM network on the deep features was compared with the Deep Neural Network within the frame of the fivefold cross-validation technique. Accuracy, sensitivity, specificity and precision metrics were used to evaluate the classification performance of the trained models. Bi-LSTM network presented better performance compare to DNN with 97.6% value of high accuracy despite the few numbers of Covid-19 images in the dataset. In addition, it is understood that concatenated deep features more meaningful than deep features obtained with pre-trained networks by one by, as well. Consequently, it is thought that the proposed study based on the Bi-LSTM network and concatenated deep features will be noteworthy in the design of highly sensitive automated Covid-19 monitoring systems.",2022,10.1007/s12539-021-00463-2,diagnosis,False
Automatic detection of lung nodules in CT datasets based on stable 3D mass-spring models,"We propose a computer-aided detection (CAD) system which can detect small-sized (from 3mm) pulmonary nodules in spiral CT scans. A pulmonary nodule is a small lesion in the lungs, round-shaped (parenchymal nodule) or worm-shaped (juxtapleural nodule). Both kinds of lesions have a radio-density greater than lung parenchyma, thus appearing white on the images. Lung nodules might indicate a lung cancer and their early stage detection arguably improves the patient survival rate. CT is considered to be the most accurate imaging modality for nodule detection. However, the large amount of data per examination makes the full analysis difficult, leading to omission of nodules by the radiologist. We developed an advanced computerized method for the automatic detection of internal and juxtapleural nodules on low-dose and thin-slice lung CT scan. This method consists of an initial selection of nodule candidates list, the segmentation of each candidate nodule and the classification of the features computed for each segmented nodule candidate.The presented CAD system is aimed to reduce the number of omissions and to decrease the radiologist scan examination time. Our system locates with the same scheme both internal and juxtapleural nodules. For a correct volume segmentation of the lung parenchyma, the system uses a Region Growing (RG) algorithm and an opening process for including the juxtapleural nodules. The segmentation and the extraction of the suspected nodular lesions from CT images by a lung CAD system constitutes a hard task. In order to solve this key problem, we use a new Stable 3D Mass-Spring Model (MSM) combined with a spline curves reconstruction process. Our model represents concurrently the characteristic gray value range, the directed contour information as well as shape knowledge, which leads to a much more robust and efficient segmentation process. For distinguishing the real nodules among nodule candidates, an additional classification step is applied; furthermore, a neural network is applied to reduce the false positives (FPs) after a double-threshold cut. The system performance was tested on a set of 84 scans made available by the Lung Image Database Consortium (LIDC) annotated by four expert radiologists. The detection rate of the system is 97% with 6.1 FPs/CT. A reduction to 2.5 FPs/CT is achieved at 88% sensitivity. We presented a new 3D segmentation technique for lung nodules in CT datasets, using deformable MSMs. The result is a efficient segmentation process able to converge, identifying the shape of the generic ROI, after a few iterations. Our suitable results show that the use of the 3D AC model and the feature analysis based FPs reduction process constitutes an accurate approach to the segmentation and the classification of lung nodules.",2012,10.1016/j.compbiomed.2012.09.002,diagnosis,True
Automatic detection of multisize pulmonary nodules in CT images: Large-scale validation of the false-positive reduction step,"PURPOSE: Currently reported computer-aided detection (CAD) approaches face difficulties in identifying the diverse pulmonary nodules in thoracic computed tomography (CT) images, especially in heterogeneous datasets. We present a novel CAD system specifically designed to identify multisize nodule candidates in multiple heterogeneous datasets. METHODS: The proposed CAD scheme is divided into two phases: primary phase and final phase. The primary phase started with the lung segmentation algorithm and the segmented lungs were further refined using morphological closing process to include the pleural nodules. Next, we empirically formulated three subalgorithms modules to detect different sizes of nodule candidates (≥3 and <6 mm; ≥6 and <10 mm; and ≥10 mm). Each subalgorithm module included a multistage flow of rule-based thresholding and morphological processes. In the final phase, the nodule candidates were augmented to boost the performance of the classifier. The CAD system was trained using a total number of nodule candidates = 201,654 (after augmentation) and nonnodule candidates = 731,486. A rich set of 515 features based on cluster, texture, and voxel-based intensity features were utilized to train a neural network classifier. The proposed method was trained on 899 scans from the Lung Image Database Consortium/Image Database Resource Initiative (LIDC-IDRI). The CAD system was also independently tested on 153 CT scans taken from the AAPM-SPIE-LungX Dataset and two subsets from the Early Lung Cancer Action Project (ELCAP and PCF). RESULTS: For the LIDC-IDRI training set, the proposed CAD scheme yielded an overall sensitivity of 85.6% (1189/1390) and 83.5% (1161/1390) at 8 FP/scan and 1 FP/scan, respectively. For the three independent test sets, the CAD system achieved an average sensitivity of 68.4% at 8 FP/scan. CONCLUSION: The authors conclude that the proposed CAD system can identify dissimilar nodule candidates in the multiple heterogeneous datasets. It could be considered as a useful tool to support radiologists during screening trials.",2018,10.1002/mp.12746,diagnosis,True
Automatic detection of pneumonia in chest X-ray images using textural features,"Fast and accurate diagnosis is critical for the triage and management of pneumonia, particularly in the current scenario of a COVID-19 pandemic, where this pathology is a major symptom of the infection. With the objective of providing tools for that purpose, this study assesses the potential of three textural image characterisation methods: radiomics, fractal dimension and the recently developed superpixel-based histon, as biomarkers to be used for training Artificial Intelligence (AI) models in order to detect pneumonia in chest X-ray images. Models generated from three different AI algorithms have been studied: K-Nearest Neighbors, Support Vector Machine and Random Forest. Two open-access image datasets were used in this study. In the first one, a dataset composed of paediatric chest X-ray, the best performing generated models achieved an 83.3% accuracy with 89% sensitivity for radiomics, 89.9% accuracy with 93.6% sensitivity for fractal dimension and 91.3% accuracy with 90.5% sensitivity for superpixels based histon. Second, a dataset derived from an image repository developed primarily as a tool for studying COVID-19 was used. For this dataset, the best performing generated models resulted in a 95.3% accuracy with 99.2% sensitivity for radiomics, 99% accuracy with 100% sensitivity for fractal dimension and 99% accuracy with 98.6% sensitivity for superpixel-based histons. The results confirm the validity of the tested methods as reliable and easy-to-implement automatic diagnostic tools for pneumonia.",2022,10.1016/j.compbiomed.2022.105466,diagnosis,False
Automatic detection of pulmonary nodules in CT images by incorporating 3D tensor filtering with local image feature analysis,"Computer-aided detection (CAD) technology has been developed and demonstrated its potential to assist radiologists in detecting pulmonary nodules especially at an early stage. In this paper, we present a novel scheme for automatic detection of pulmonary nodules in CT images based on a 3D tensor filtering algorithm and local image feature analysis. We first apply a series of preprocessing steps to segment the lung volume and generate the isotropic volumetric CT data. Next, a unique 3D tensor filtering approach and local image feature analysis are used to detect nodule candidates. A 3D level set segmentation method is used to correct and refine the boundaries of nodule candidates subsequently. Then, we extract the features of the detected candidates and select the optimal features by using a CFS (Correlation Feature Selection) subset evaluator attribute selection method. Finally, a random forest classifier is trained to classify the detected candidates. The performance of this CAD scheme is validated using two datasets namely, the LUNA16 (Lung Nodule Analysis 2016) database and the ANODE09 (Automatic Nodule Detection 2009) database. By applying a 10-fold cross-validation method, the CAD scheme yielded a sensitivity of 79.3% at an average of 4 false positive detections per scan (FP/Scan) for the former dataset, and a sensitivity of 84.62% and 2.8 FP/Scan for the latter dataset, respectively. Our detection results show that the use of 3D tensor filtering algorithm combined with local image feature analysis constitutes an effective approach to detect pulmonary nodules.",2018,10.1016/j.ejmp.2018.01.019,diagnosis,True
Automatic feature learning using multichannel ROI based on deep structured algorithms for computerized lung cancer diagnosis,"This study aimed to analyze the ability of extracting automatically generated features using deep structured algorithms in lung nodule CT image diagnosis, and compare its performance with traditional computer aided diagnosis (CADx) systems using hand-crafted features. All of the 1018 cases were acquired from Lung Image Database Consortium (LIDC) public lung cancer database. The nodules were segmented according to four radiologists' markings, and 13,668 samples were generated by rotating every slice of nodule images. Three multichannel ROI based deep structured algorithms were designed and implemented in this study: convolutional neural network (CNN), deep belief network (DBN), and stacked denoising autoencoder (SDAE). For the comparison purpose, we also implemented a CADx system using hand-crafted features including density features, texture features and morphological features. The performance of every scheme was evaluated by using a 10-fold cross-validation method and an assessment index of the area under the receiver operating characteristic curve (AUC). The observed highest area under the curve (AUC) was 0.899±0.018 achieved by CNN, which was significantly higher than traditional CADx with the AUC=0.848±0.026. The results from DBN was also slightly higher than CADx, while SDAE was slightly lower. By visualizing the automatic generated features, we found some meaningful detectors like curvy stroke detectors from deep structured schemes. The study results showed the deep structured algorithms with automatically generated features can achieve desirable performance in lung nodule diagnosis. With well-tuned parameters and large enough dataset, the deep learning algorithms can have better performance than current popular CADx. We believe the deep learning algorithms with similar data preprocessing procedure can be used in other medical image analysis areas as well.",2017,10.1016/j.compbiomed.2017.04.006,diagnosis,True
Automatic learning-based beam angle selection for thoracic IMRT,"PURPOSE: The treatment of thoracic cancer using external beam radiation requires an optimal selection of the radiation beam directions to ensure effective coverage of the target volume and to avoid unnecessary treatment of normal healthy tissues. Intensity modulated radiation therapy (IMRT) planning is a lengthy process, which requires the planner to iterate between choosing beam angles, specifying dose-volume objectives and executing IMRT optimization. In thorax treatment planning, where there are no class solutions for beam placement, beam angle selection is performed manually, based on the planner's clinical experience. The purpose of this work is to propose and study a computationally efficient framework that utilizes machine learning to automatically select treatment beam angles. Such a framework may be helpful for reducing the overall planning workload. METHODS: The authors introduce an automated beam selection method, based on learning the relationships between beam angles and anatomical features. Using a large set of clinically approved IMRT plans, a random forest regression algorithm is trained to map a multitude of anatomical features into an individual beam score. An optimization scheme is then built to select and adjust the beam angles, considering the learned interbeam dependencies. The validity and quality of the automatically selected beams evaluated using the manually selected beams from the corresponding clinical plans as the ground truth. RESULTS: The analysis included 149 clinically approved thoracic IMRT plans. For a randomly selected test subset of 27 plans, IMRT plans were generated using automatically selected beams and compared to the clinical plans. The comparison of the predicted and the clinical beam angles demonstrated a good average correspondence between the two (angular distance 16.8° ± 10°, correlation 0.75 ± 0.2). The dose distributions of the semiautomatic and clinical plans were equivalent in terms of primary target volume coverage and organ at risk sparing and were superior over plans produced with fixed sets of common beam angles. The great majority of the automatic plans (93%) were approved as clinically acceptable by three radiation therapy specialists. CONCLUSIONS: The results demonstrated the feasibility of utilizing a learning-based approach for automatic selection of beam angles in thoracic IMRT planning. The proposed method may assist in reducing the manual planning workload, while sustaining plan quality.",2015,10.1118/1.4908000,treatment,False
Automatic Lung Nodule Detection Combined With Gaze Information Improves Radiologists' Screening Performance,"Early diagnosis of lung cancer via computed tomography can significantly reduce the morbidity and mortality rates associated with the pathology. However, searching lung nodules is a high complexity task, which affects the success of screening programs. Whilst computer-aided detection systems can be used as second observers, they may bias radiologists and introduce significant time overheads. With this in mind, this study assesses the potential of using gaze information for integrating automatic detection systems in the clinical practice. For that purpose, 4 radiologists were asked to annotate 20 scans from a public dataset while being monitored by an eye tracker device, and an automatic lung nodule detection system was developed. Our results show that radiologists follow a similar search routine and tend to have lower fixation periods in regions where finding errors occur. The overall detection sensitivity of the specialists was 0.67±0.07, whereas the system achieved 0.69. Combining the annotations of one radiologist with the automatic system significantly improves the detection performance to similar levels of two annotators. Filtering automatic detection candidates only for low fixation regions still significantly improves the detection sensitivity without increasing the number of false-positives.",2020,10.1109/jbhi.2020.2976150,diagnosis,True
Automatic lung nodule detection in thoracic CT scans using dilated slice-wise convolutions,"PURPOSE: Most state-of-the-art automated medical image analysis methods for volumetric data rely on adaptations of two-dimensional (2D) and three-dimensional (3D) convolutional neural networks (CNNs). In this paper, we develop a novel unified CNN-based model that combines the benefits of 2D and 3D networks for analyzing volumetric medical images. METHODS: In our proposed framework, multiscale contextual information is first extracted from 2D slices inside a volume of interest (VOI). This is followed by dilated 1D convolutions across slices to aggregate in-plane features in a slice-wise manner and encode the information in the entire volume. Moreover, we formalize a curriculum learning strategy for a two-stage system (i.e., a system that consists of screening and false positive reduction), where the training samples are presented to the network in a meaningful order to further improve the performance. RESULTS: We evaluated the proposed approach by developing a computer-aided detection (CADe) system for lung nodules. Our results on 888 CT exams demonstrate that the proposed approach can effectively analyze volumetric data by achieving a sensitivity of > 0.99 in the screening stage and a sensitivity of > 0.96 at eight false positives per case in the false positive reduction stage. CONCLUSION: Our experimental results show that the proposed method provides competitive results compared to state-of-the-art 3D frameworks. In addition, we illustrate the benefits of curriculum learning strategies in two-stage systems that are of common use in medical imaging applications.",2021,10.1002/mp.14915,diagnosis,True
Automatic lung nodule detection using a 3D deep convolutional neural network combined with a multi-scale prediction strategy in chest CTs,"OBJECTIVE: A novel computer-aided detection (CAD) scheme for lung nodule detection using a 3D deep convolutional neural network combined with a multi-scale prediction strategy is proposed to assist radiologists by providing a second opinion on accurate lung nodule detection, which is a crucial step in early diagnosis of lung cancer. METHOD: A 3D deep convolutional neural network (CNN) with multi-scale prediction was used to detect lung nodules after the lungs were segmented from chest CT scans, with a comprehensive method utilized. Compared with a 2D CNN, a 3D CNN can utilize richer spatial 3D contextual information and generate more discriminative features after being trained with 3D samples to fully represent lung nodules. Furthermore, a multi-scale lung nodule prediction strategy, including multi-scale cube prediction and cube clustering, is also proposed to detect extremely small nodules. RESULT: The proposed method was evaluated on 888 thin-slice scans with 1186 nodules in the LUNA16 database. All results were obtained via 10-fold cross-validation. Three options of the proposed scheme are provided for selection according to the actual needs. The sensitivity of the proposed scheme with the primary option reached 87.94% and 92.93% at one and four false positives per scan, respectively. Meanwhile, the competition performance metric (CPM) score is very satisfying (0.7967). CONCLUSION: The experimental results demonstrate the outstanding detection performance of the proposed nodule detection scheme. In addition, the proposed scheme can be extended to other medical image recognition fields.",2018,10.1016/j.compbiomed.2018.10.011,diagnosis,True
Automatic lung nodule detection using multi-scale dot nodule-enhancement filter and weighted support vector machines in chest computed tomography,"A novel CAD scheme for automated lung nodule detection is proposed to assist radiologists with the detection of lung cancer on CT scans. The proposed scheme is composed of four major steps: (1) lung volume segmentation, (2) nodule candidate extraction and grouping, (3) false positives reduction for the non-vessel tree group, and (4) classification for the vessel tree group. Lung segmentation is performed first. Then, 3D labeling technology is used to divide nodule candidates into two groups. For the non-vessel tree group, nodule candidates are classified as true nodules at the false positive reduction stage if the candidates survive the rule-based classifier and are not screened out by the dot filter. For the vessel tree group, nodule candidates are extracted using dot filter. Next, RSFS feature selection is used to select the most discriminating features for classification. Finally, WSVM with an undersampling approach is adopted to discriminate true nodules from vessel bifurcations in vessel tree group. The proposed method was evaluated on 154 thin-slice scans with 204 nodules in the LIDC database. The performance of the proposed CAD scheme yielded a high sensitivity (87.81%) while maintaining a low false rate (1.057 FPs/scan). The experimental results indicate the performance of our method may be better than the existing methods.",2019,10.1371/journal.pone.0210551,diagnosis,True
Automatic opportunistic osteoporosis screening using low-dose chest computed tomography scans obtained for lung cancer screening,"OBJECTIVE: Osteoporosis is a prevalent and treatable condition, but it remains underdiagnosed. In this study, a deep learning-based system was developed to automatically measure bone mineral density (BMD) for opportunistic osteoporosis screening using low-dose chest computed tomography (LDCT) scans obtained for lung cancer screening. METHODS: First, a deep learning model was trained and tested with 200 annotated LDCT scans to segment and label all vertebral bodies (VBs). Then, the mean CT numbers of the trabecular area of target VBs were obtained based on the segmentation mask through geometric operations. Finally, a linear function was built to map the trabecular CT numbers of target VBs to their BMDs collected from approved software used for osteoporosis diagnosis. The diagnostic performance of the developed system was evaluated using an independent dataset of 374 LDCT scans with standard BMDs and osteoporosis diagnosis. RESULTS: Our deep learning model achieved a mean Dice coefficient of 86.6% for VB segmentation and 97.5% accuracy for VB labeling. Line regression and Bland-Altman analyses showed good agreement between the predicted BMD and the ground truth, with correlation coefficients of 0.964-0.968 and mean errors of 2.2-4.0 mg/cm(3). The area under the curve (AUC) was 0.927 for detecting osteoporosis and 0.942 for distinguishing low BMD. CONCLUSION: The proposed deep learning-based system demonstrated the potential to automatically perform opportunistic osteoporosis screening using LDCT scans obtained for lung cancer screening. KEY POINTS: • Osteoporosis is a prevalent but underdiagnosed condition that can increase the risk of fracture. • A deep learning-based system was developed to fully automate bone mineral density measurement in low-dose chest computed tomography scans. • The developed system achieved high accuracy for automatic opportunistic osteoporosis screening using low-dose chest computed tomography scans obtained for lung cancer screening.",2020,10.1007/s00330-020-06679-y,diagnosis,True
Automatic pulmonary ground-glass opacity nodules detection and classification based on 3D neural network,"PURPOSE: Pulmonary ground-glass opacity (GGO) nodules are more likely to be malignant compared with solid solitary nodules. Due to indistinct boundaries of GGO nodules, the detection and diagnosis are challenging for doctors. Therefore, designing an automatic GGO nodule detection and classification scheme is significantly essential. METHODS: In this paper, we proposed a two-stage 3D GGO nodule detection and classification framework. First, we used a pretrained 3D U-Net to extract lung parenchyma. Second, we adapted the architecture of Mask region-based convolutional neural networks (RCNN) to handle 3D medical images. The 3D model was then applied to detect the locations of GGO nodules and classify lesions (benign or malignant). The class-balanced loss function was also used to balance the number of benign and malignant lesions. Finally, we employed a novel false positive elimination scheme called the feature-based weighted clustering (FWC) to promote the detection accuracy further. RESULTS: The experiments were conducted based on fivefold cross-validation with the imbalanced data set. Experimental results showed that the mean average precision could keep a high level (0.5182) in the phase of detection. Meanwhile, the false positive rate was effectively controlled, and the competition performance metric (CPM) reached 0.817 benefited from the FWC algorithm. The comparative statistical analyses with other deep learning methods also proved the effectiveness of our proposed method. CONCLUSIONS: We put forward an automatic pulmonary GGO nodules detection and classification framework based on deep learning. The proposed method locate and classify nodules accurately, which could be an effective tool to help doctors in clinical diagnoses.",2022,10.1002/mp.15501,diagnosis,True
Automatic Pulmonary Nodule Detection in CT Scans Using Convolutional Neural Networks Based on Maximum Intensity Projection,"Accurate pulmonary nodule detection is a crucial step in lung cancer screening. Computer-aided detection (CAD) systems are not routinely used by radiologists for pulmonary nodule detection in clinical practice despite their potential benefits. Maximum intensity projection (MIP) images improve the detection of pulmonary nodules in radiological evaluation with computed tomography (CT) scans. Inspired by the clinical methodology of radiologists, we aim to explore the feasibility of applying MIP images to improve the effectiveness of automatic lung nodule detection using convolutional neural networks (CNNs). We propose a CNN-based approach that takes MIP images of different slab thicknesses (5 mm, 10 mm, 15 mm) and 1 mm axial section slices as input. Such an approach augments the two-dimensional (2-D) CT slice images with more representative spatial information that helps discriminate nodules from vessels through their morphologies. Our proposed method achieves sensitivity of 92.7% with 1 false positive per scan and sensitivity of 94.2% with 2 false positives per scan for lung nodule detection on 888 scans in the LIDC-IDRI dataset. The use of thick MIP images helps the detection of small pulmonary nodules (3 mm-10 mm) and results in fewer false positives. Experimental results show that utilizing MIP images can increase the sensitivity and lower the number of false positives, which demonstrates the effectiveness and significance of the proposed MIP-based CNNs framework for automatic pulmonary nodule detection in CT scans. The proposed method also shows the potential that CNNs could gain benefits for nodule detection by combining the clinical procedure.",2020,10.1109/tmi.2019.2935553,diagnosis,True
Automatic pulmonary vessel segmentation on noncontrast chest CT: deep learning algorithm developed using spatiotemporally matched virtual noncontrast images and low-keV contrast-enhanced vessel maps,"OBJECTIVES: To develop a deep learning-based pulmonary vessel segmentation algorithm (DLVS) from noncontrast chest CT and to investigate its clinical implications in assessing vascular remodeling of chronic obstructive lung disease (COPD) patients. METHODS: For development, 104 pulmonary CT angiography scans (49,054 slices) using a dual-source CT were collected, and spatiotemporally matched virtual noncontrast and 50-keV images were generated. Vessel maps were extracted from the 50-keV images. The 3-dimensional U-Net-based DLVS was trained to segment pulmonary vessels (with a vessel map as the output) from virtual noncontrast images (as the input). For external validation, vendor-independent noncontrast CT images (n = 14) and the VESSEL 12 challenge open dataset (n = 3) were used. For each case, 200 points were selected including 20 intra-lesional points, and the probability value for each point was extracted. For clinical validation, we included 281 COPD patients with low-dose noncontrast CTs. The DLVS-calculated volume of vessels with a cross-sectional area < 5 mm(2) (PVV5) and the PVV5 divided by total vessel volume (%PVV5) were measured. RESULTS: DLVS correctly segmented 99.1% of the intravascular points (1,387/1,400) and 93.1% of the extravascular points (1,309/1,400). The areas-under-the receiver-operating characteristic curve (AUROCs) were 0.977 and 0.969 for the two external validation datasets. For the COPD patients, both PPV5 and %PPV5 successfully differentiated severe patients whose FEV1 < 50 (AUROCs; 0.715 and 0.804) and were significantly correlated with the emphysema index (Ps < .05). CONCLUSIONS: DLVS successfully segmented pulmonary vessels on noncontrast chest CT by utilizing spatiotemporally matched 50-keV images from a dual-source CT scanner and showed promising clinical applicability in COPD. KEY POINTS: • We developed a deep learning pulmonary vessel segmentation algorithm using virtual noncontrast images and 50-keV enhanced images produced by a dual-source CT scanner. • Our algorithm successfully segmented vessels on diseased lungs. • Our algorithm showed promising results in assessing the loss of small vessel density in COPD patients.",2021,10.1007/s00330-021-08036-z,diagnosis,True
Automatic recognition of 3D GGO CT imaging signs through the fusion of hybrid resampling and layer-wise fine-tuning CNNs,"Ground-glass opacity (GGO) is a common CT imaging sign on high-resolution CT, which means the lesion is more likely to be malignant compared to common solid lung nodules. The automatic recognition of GGO CT imaging signs is of great importance for early diagnosis and possible cure of lung cancers. The present GGO recognition methods employ traditional low-level features and system performance improves slowly. Considering the high-performance of CNN model in computer vision field, we proposed an automatic recognition method of 3D GGO CT imaging signs through the fusion of hybrid resampling and layer-wise fine-tuning CNN models in this paper. Our hybrid resampling is performed on multi-views and multi-receptive fields, which reduces the risk of missing small or large GGOs by adopting representative sampling panels and processing GGOs with multiple scales simultaneously. The layer-wise fine-tuning strategy has the ability to obtain the optimal fine-tuning model. Multi-CNN models fusion strategy obtains better performance than any single trained model. We evaluated our method on the GGO nodule samples in publicly available LIDC-IDRI dataset of chest CT scans. The experimental results show that our method yields excellent results with 96.64% sensitivity, 71.43% specificity, and 0.83 F1 score. Our method is a promising approach to apply deep learning method to computer-aided analysis of specific CT imaging signs with insufficient labeled images. Graphical abstract We proposed an automatic recognition method of 3D GGO CT imaging signs through the fusion of hybrid resampling and layer-wise fine-tuning CNN models in this paper. Our hybrid resampling reduces the risk of missing small or large GGOs by adopting representative sampling panels and processing GGOs with multiple scales simultaneously. The layer-wise fine-tuning strategy has ability to obtain the optimal fine-tuning model. Our method is a promising approach to apply deep learning method to computer-aided analysis of specific CT imaging signs with insufficient labeled images.",2018,10.1007/s11517-018-1850-z,diagnosis,True
Automatic Scoring of Multiple Semantic Attributes With Multi-Task Feature Leverage: A Study on Pulmonary Nodules in CT Images,"The gap between the computational and semantic features is the one of major factors that bottlenecks the computer-aided diagnosis (CAD) performance from clinical usage. To bridge this gap, we exploit three multi-task learning (MTL) schemes to leverage heterogeneous computational features derived from deep learning models of stacked denoising autoencoder (SDAE) and convolutional neural network (CNN), as well as hand-crafted Haar-like and HoG features, for the description of 9 semantic features for lung nodules in CT images. We regard that there may exist relations among the semantic features of ""spiculation"", ""texture"", ""margin"", etc., that can be explored with the MTL. The Lung Image Database Consortium (LIDC) data is adopted in this study for the rich annotation resources. The LIDC nodules were quantitatively scored w.r.t. 9 semantic features from 12 radiologists of several institutes in U.S.A. By treating each semantic feature as an individual task, the MTL schemes select and map the heterogeneous computational features toward the radiologists' ratings with cross validation evaluation schemes on the randomly selected 2400 nodules from the LIDC dataset. The experimental results suggest that the predicted semantic scores from the three MTL schemes are closer to the radiologists' ratings than the scores from single-task LASSO and elastic net regression methods. The proposed semantic attribute scoring scheme may provide richer quantitative assessments of nodules for better support of diagnostic decision and management. Meanwhile, the capability of the automatic association of medical image contents with the clinical semantic terms by our method may also assist the development of medical search engine.",2017,10.1109/tmi.2016.2629462,diagnosis,True
Automatic segmentation of airway tree based on local intensity filter and machine learning technique in 3D chest CT volume,"PURPOSE: Airway segmentation plays an important role in analyzing chest computed tomography (CT) volumes for computerized lung cancer detection, emphysema diagnosis and pre- and intra-operative bronchoscope navigation. However, obtaining a complete 3D airway tree structure from a CT volume is quite a challenging task. Several researchers have proposed automated airway segmentation algorithms basically based on region growing and machine learning techniques. However, these methods fail to detect the peripheral bronchial branches, which results in a large amount of leakage. This paper presents a novel approach for more accurate extraction of the complex airway tree. METHODS: This proposed segmentation method is composed of three steps. First, Hessian analysis is utilized to enhance the tube-like structure in CT volumes; then, an adaptive multiscale cavity enhancement filter is employed to detect the cavity-like structure with different radii. In the second step, support vector machine learning will be utilized to remove the false positive (FP) regions from the result obtained in the previous step. Finally, the graph-cut algorithm is used to refine the candidate voxels to form an integrated airway tree. RESULTS: A test dataset including 50 standard-dose chest CT volumes was used for evaluating our proposed method. The average extraction rate was about 79.1 % with the significantly decreased FP rate. CONCLUSION: A new method of airway segmentation based on local intensity structure and machine learning technique was developed. The method was shown to be feasible for airway segmentation in a computer-aided diagnosis system for a lung and bronchoscope guidance system.",2017,10.1007/s11548-016-1492-2,diagnosis,True
Automatic segmentation of lung nodules with growing neural gas and support vector machine,"Lung cancer is distinguished by presenting one of the highest incidences and one of the highest rates of mortality among all other types of cancer. Unfortunately, this disease is often diagnosed late, affecting the treatment outcome. In order to help specialists in the search and identification of lung nodules in tomographic images, many research centers have developed computer-aided detection systems (CAD systems) to automate procedures. This work seeks to develop a methodology for automatic detection of lung nodules. The proposed method consists of the acquisition of computerized tomography images of the lung, the reduction of the volume of interest through techniques for the extraction of the thorax, extraction of the lung, and reconstruction of the original shape of the parenchyma. After that, growing neural gas (GNG) is applied to constrain even more the structures that are denser than the pulmonary parenchyma (nodules, blood vessels, bronchi, etc.). The next stage is the separation of the structures resembling lung nodules from other structures, such as vessels and bronchi. Finally, the structures are classified as either nodule or non-nodule, through shape and texture measurements together with support vector machine. The methodology ensures that nodules of reasonable size be found with 86% sensitivity and 91% specificity. This results in a mean accuracy of 91% for 10 experiments of training and testing in a sample of 48 nodules occurring in 29 exams. The rate of false positives per exam was of 0.138, for the 29 exams analyzed.",2012,10.1016/j.compbiomed.2012.09.003,diagnosis,True
Automatic segmentation of lung tumors on CT images based on a 2D & 3D hybrid convolutional neural network,"OBJECTIVE: A stable and accurate automatic tumor delineation method has been developed to facilitate the intelligent design of lung cancer radiotherapy process. The purpose of this paper is to introduce an automatic tumor segmentation network for lung cancer on CT images based on deep learning. METHODS: In this paper, a hybrid convolution neural network (CNN) combining 2D CNN and 3D CNN was implemented for the automatic lung tumor delineation using CT images. 3D CNN used V-Net model for the extraction of tumor context information from CT sequence images. 2D CNN used an encoder-decoder structure based on dense connection scheme, which could expand information flow and promote feature propagation. Next, 2D features and 3D features were fused through a hybrid module. Meanwhile, the hybrid CNN was compared with the individual 3D CNN and 2D CNN, and three evaluation metrics, Dice, Jaccard and Hausdorff distance (HD), were used for quantitative evaluation. The relationship between the segmentation performance of hybrid network and the GTV volume size was also explored. RESULTS: The newly introduced hybrid CNN was trained and tested on a dataset of 260 cases, and could achieve a median value of 0.73, with mean and stand deviation of 0.72 ± 0.10 for the Dice metric, 0.58 ± 0.13 and 21.73 ± 13.30 mm for the Jaccard and HD metrics, respectively. The hybrid network significantly outperformed the individual 3D CNN and 2D CNN in the three examined evaluation metrics (p < 0.001). A larger GTV present a higher value for the Dice metric, but its delineation at the tumor boundary is unstable. CONCLUSIONS: The implemented hybrid CNN was able to achieve good lung tumor segmentation performance on CT images. ADVANCES IN KNOWLEDGE: The hybrid CNN has valuable prospect with the ability to segment lung tumor.",2021,10.1259/bjr.20210038,diagnosis,True
Automatic segmentation of organs at risk and tumors in CT images of lung cancer from partially labelled datasets with a semi-supervised conditional nnU-Net,"BACKGROUND AND OBJECTIVE: Accurately and reliably defining organs at risk (OARs) and tumors are the cornerstone of radiation therapy (RT) treatment planning for lung cancer. Almost all segmentation networks based on deep learning techniques rely on fully annotated data with strong supervision. However, existing public imaging datasets encountered in the RT domain frequently include singly labelled tumors or partially labelled organs because annotating full OARs and tumors in CT images is both rigorous and tedious. To utilize labelled data from different sources, we proposed a dual-path semi-supervised conditional nnU-Net for OARs and tumor segmentation that is trained on a union of partially labelled datasets. METHODS: The framework employs the nnU-Net as the base model and introduces a conditioning strategy by incorporating auxiliary information as an additional input layer into the decoder. The conditional nnU-Net efficiently leverages prior conditional information to classify the target class at the pixelwise level. Specifically, we employ the uncertainty-aware mean teacher (UA-MT) framework to assist in OARs segmentation, which can effectively leverage unlabelled data (images from a tumor labelled dataset) by encouraging consistent predictions of the same input under different perturbations. Furthermore, we individually design different combinations of loss functions to optimize the segmentation of OARs (Dice loss and cross-entropy loss) and tumors (Dice loss and focal loss) in a dual path. RESULTS: The proposed method is evaluated on two publicly available datasets of the spinal cord, left and right lung, heart, esophagus, and lung tumor, in which satisfactory segmentation performance has been achieved in term of both the region-based Dice similarity coefficient (DSC) and the boundary-based Hausdorff distance (HD). CONCLUSIONS: The proposed semi-supervised conditional nnU-Net breaks down the barriers between nonoverlapping labelled datasets and further alleviates the problem of ""data hunger"" and ""data waste"" in multi-class segmentation. The method has the potential to help radiologists with RT treatment planning in clinical practice.",2021,10.1016/j.cmpb.2021.106419,treatment,True
Automatic segmentation of organs-at-risks of nasopharynx cancer and lung cancer by cross-layer attention fusion network with TELD-Loss,"PURPOSE: Radiotherapy is one of the main treatments of nasopharyngeal cancer (NPC) and lung cancer. Accurate segmentation of organs at risks (OARs) in CT images is a key step in radiotherapy planning for NPC and lung cancer. However, the segmentation of OARs is influenced by the highly imbalanced size of organs, which often results in very poor segmentation results for small and difficult-to-segment organs. In addition, the complex morphological changes and fuzzy boundaries of OARs also pose great challenges to the segmentation task. In this paper, we propose a cross-layer attention fusion network (CLAF-CNN) to solve the problem of accurately segmenting OARs. METHODS: In CLAF-CNN, we integrate the spatial attention maps of the adjacent spatial attention modules to make the segmentation targets more accurately focused, so that the network can capture more target-related features. In this way, the spatial attention modules in the network can be learned and optimized together. In addition, we introduce a new Top-K exponential logarithmic Dice loss (TELD-Loss) to solve the imbalance problem in OAR segmentation. The TELD-Loss further introduces a Top-K optimization mechanism based on Dice loss and exponential logarithmic loss, which makes the network pay more attention to small organs and difficult-to-segment organs, so as to enhance the overall performance of the segmentation model. RESULTS: We validated our framework on the OAR segmentation datasets of the head and neck and lung CT images in the StructSeg 2019 challenge. Experiments show that the CLAF-CNN outperforms the state-of-the-art attention-based segmentation methods in the OAR segmentation task with average Dice coefficient of 79.65% for head and neck OARs and 88.39% for lung OARs. CONCLUSIONS: This work provides a new network named CLAF-CNN which contains cross-layer spatial attention map fusion architecture and TELD-Loss for OAR segmentation. Results demonstrated that the proposed method could obtain accurate segmentation results for OARs, which has a potential of improving the efficiency of radiotherapy planning for nasopharynx cancer and lung cancer.",2021,10.1002/mp.15260,treatment,True
Automatic weighing attribute to retrieve similar lung cancer nodules,"BACKGROUND: Cancer is a disease characterized as an uncontrolled growth of abnormal cells that invades neighboring tissues and destroys them. Lung cancer is the primary cause of cancer-related deaths in the world, and it diagnosis is a complex task for specialists and it presents some big challenges as medical image interpretation process, pulmonary nodule detection and classification. In order to aid specialists in the early diagnosis of lung cancer, computer assistance must be integrated in the imaging interpretation and pulmonary nodule classification processes. Methods of Content-Based Image Retrieval (CBIR) have been described as one promising technique to computer-aided diagnosis and is expected to aid radiologists on image interpretation with a second opinion. However, CBIR presents some limitations: image feature extraction process and appropriate similarity measure. The efficiency of CBIR systems depends on calculating image features that may be relevant to the case similarity analysis. When specialists classify a nodule, they are supported by information from exams, images, etc. But each information has more or less weight over decision making about nodule malignancy. Thus, finding a way to measure the weight allows improvement of image retrieval process through the assignment of higher weights to that attributes that best characterize the nodules. METHODS: In this context, the aim of this work is to present a method to automatically calculate attribute weights based on local learning to reflect the interpretation on image retrieval process. The process consists of two stages that are performed sequentially and cyclically: Evaluation Stage and Training Stage. At each iteration the weights are adjusted according to retrieved nodules. After some iterations, it is possible reach a set of attribute weights that optimize the recovery of similar nodes. RESULTS: The results achieved by updated weights were promising because was possible increase precision by 10% to 6% on average to retrieve of benign and malignant nodules, respectively, with recall of 25% compared with tests without weights associated to attributes in similarity metric. The best result, we reaching values over 100% of precision average until thirtieth lung cancer nodule retrieved. CONCLUSIONS: Based on the results, WED applied to the three vectors used attributes (3D TA, 3D MSA and InV), with weights adjusted by the process, always achieved better results than those found with ED. With the weights, the Precision was increased on average by 17.3% compared with using ED.",2016,10.1186/s12911-016-0313-4,diagnosis,True
Auxiliary Diagnosis for COVID-19 with Deep Transfer Learning,"To assist physicians identify COVID-19 and its manifestations through the automatic COVID-19 recognition and classification in chest CT images with deep transfer learning. In this retrospective study, the used chest CT image dataset covered 422 subjects, including 72 confirmed COVID-19 subjects (260 studies, 30,171 images), 252 other pneumonia subjects (252 studies, 26,534 images) that contained 158 viral pneumonia subjects and 94 pulmonary tuberculosis subjects, and 98 normal subjects (98 studies, 29,838 images). In the experiment, subjects were split into training (70%), validation (15%) and testing (15%) sets. We utilized the convolutional blocks of ResNets pretrained on the public social image collections and modified the top fully connected layer to suit our task (the COVID-19 recognition). In addition, we tested the proposed method on a finegrained classification task; that is, the images of COVID-19 were further split into 3 main manifestations (ground-glass opacity with 12,924 images, consolidation with 7418 images and fibrotic streaks with 7338 images). Similarly, the data partitioning strategy of 70%-15%-15% was adopted. The best performance obtained by the pretrained ResNet50 model is 94.87% sensitivity, 88.46% specificity, 91.21% accuracy for COVID-19 versus all other groups, and an overall accuracy of 89.01% for the three-category classification in the testing set. Consistent performance was observed from the COVID-19 manifestation classification task on images basis, where the best overall accuracy of 94.08% and AUC of 0.993 were obtained by the pretrained ResNet18 (P < 0.05). All the proposed models have achieved much satisfying performance and were thus very promising in both the practical application and statistics. Transfer learning is worth for exploring to be applied in recognition and classification of COVID-19 on CT images with limited training data. It not only achieved higher sensitivity (COVID-19 vs the rest) but also took far less time than radiologists, which is expected to give the auxiliary diagnosis and reduce the workload for the radiologists.",2021,10.1007/s10278-021-00431-8,diagnosis,True
Auxiliary Diagnosis of Lung Cancer with Magnetic Resonance Imaging Data under Deep Learning,"This study was aimed at two image segmentation methods of three-dimensional (3D) U-shaped network (U-Net) and multilevel boundary sensing residual U-shaped network (RUNet) and their application values on the auxiliary diagnosis of lung cancer. In this study, on the basis of the 3D U-Net segmentation method, the multilevel boundary sensing RUNet was worked out after optimization. 92 patients with lung cancer were selected, and their clinical data were counted; meanwhile, the lung nodule detection was performed to obtain the segmentation effect under 3D U-Net. The accuracy of 3D U-Net and multilevel boundary sensing RUNet was compared on lung magnetic resonance imaging (MRI) after lung nodule segmentation. Patients with benign lung tumors were taken as controls; the blood immune biochemical indicators progastrin-releasing peptide (pro-CRP), carcinoembryonic antigen (CEA), and neuron-specific enolase (NSE) in patients with malignant lung tumors were analyzed. It was found that the accuracy, sensitivity, and specificity were all greater than 90% under the algorithm-based MRI of benign and malignant tumor patients. Based on the imaging signs for the MRI image of lung nodules, the segmentation effect of the RUNet was clearer than that of the 3D U-Net. In addition, serum levels of pro-CRP, NSE, and CAE in patients with benign lung tumors were 28.9 pg/mL, 12.5 ng/mL, and 10.8 ng/mL, respectively, which were lower than 175.6 pg/mL, 33.6 ng/mL, and 31.9 ng/mL in patients with malignant lung tumors significantly (P < 0.05). Thus, the RUNet image segmentation method was better than the 3D U-Net. The pro-CRP, CEA, and NSE could be used as diagnostic indicators for malignant lung tumors.",2022,10.1155/2022/1994082,diagnosis,False
Based on improved deep convolutional neural network model pneumonia image classification,"Pneumonia remains the leading infectious cause of death in children under the age of five, killing about 700,000 children each year and affecting 7% of the world's population. X-ray images of lung become the key to the diagnosis of this disease, skilled doctors in the diagnosis of a certain degree of subjectivity, if the use of computer-aided medical diagnosis to automatically detect lung abnormalities, will improve the accuracy of diagnosis. This research aims to introduce a deep learning technology based on the combination of Xception neural network and long-term short-term memory (LSTM), which can realize automatic diagnosis of patients with pneumonia in X-ray images. First, the model uses the Xception network to extract the deep features of the data, passes the extracted features to the LSTM, and then the LSTM detects the extracted features, and finally selects the most needed features. Secondly, in the training set samples, the traditional cross-entropy loss cannot more balance the mismatch between categories. Therefore, this research combines Pearson's feature selection ideas, fusion of the correlation between the two loss functions, and optimizes the problem. The experimental results show that the accuracy rate of this paper is 96%, the receiver operator characteristic curve accuracy rate is 99%, the precision rate is 98%, the recall rate is 91%, and the F1 score accuracy rate is 94%. Compared with the existing technical methods, the research has achieved expected results on the currently available datasets. And assist doctors to provide higher reliability in the classification task of childhood pneumonia.",2021,10.1371/journal.pone.0258804,diagnosis,False
Bayesian-based optimized deep learning model to detect COVID-19 patients using chest X-ray image data,"Coronavirus Disease 2019 (COVID-19) is extremely infectious and rapidly spreading around the globe. As a result, rapid and precise identification of COVID-19 patients is critical. Deep Learning has shown promising performance in a variety of domains and emerged as a key technology in Artificial Intelligence. Recent advances in visual recognition are based on image classification and artefacts detection within these images. The purpose of this study is to classify chest X-ray images of COVID-19 artefacts in changed real-world situations. A novel Bayesian optimization-based convolutional neural network (CNN) model is proposed for the recognition of chest X-ray images. The proposed model has two main components. The first one utilizes CNN to extract and learn deep features. The second component is a Bayesian-based optimizer that is used to tune the CNN hyperparameters according to an objective function. The used large-scale and balanced dataset comprises 10,848 images (i.e., 3616 COVID-19, 3616 normal cases, and 3616 Pneumonia). In the first ablation investigation, we compared Bayesian optimization to three distinct ablation scenarios. We used convergence charts and accuracy to compare the three scenarios. We noticed that the Bayesian search-derived optimal architecture achieved 96% accuracy. To assist qualitative researchers, address their research questions in a methodologically sound manner, a comparison of research method and theme analysis methods was provided. The suggested model is shown to be more trustworthy and accurate in real world.",2022,10.1016/j.compbiomed.2022.105213,diagnosis,False
BEMD-3DCNN-based method for COVID-19 detection,"The coronavirus outbreak continues to spread around the world and no one knows when it will stop. Therefore, from the first day of the identification of the virus in Wuhan, China, scientists have launched numerous research projects to understand the nature of the virus, how to detect it, and search for the most effective medicine to help and protect patients. Importantly, a rapid diagnostic and detection system is a priority and should be developed to stop COVID-19 from spreading. Medical imaging techniques have been used for this purpose. Current research is focused on exploiting different backbones like VGG, ResNet, DenseNet, or combining them to detect COVID-19. By using these backbones many aspects cannot be analyzed like the spatial and contextual information in the images, although this information can be useful for more robust detection performance. In this paper, we used 3D representation of the data as input for the proposed 3DCNN-based deep learning model. The process includes using the Bi-dimensional Empirical Mode Decomposition (BEMD) technique to decompose the original image into IMFs, and then building a video of these IMF images. The formed video is used as input for the 3DCNN model to classify and detect the COVID-19 virus. The 3DCNN model consists of a 3D VGG-16 backbone followed by a Context-aware attention (CAA) module, and then fully connected layers for classification. Each CAA module takes the feature maps of different blocks of the backbone, which allows learning from different feature maps. In our experiments, we used 6484 X-ray images, of which 1802 were COVID-19 positive cases, 1910 normal cases, and 2772 pneumonia cases. The experiment results showed that our proposed technique achieved the desired results on the selected dataset. Additionally, the use of the 3DCNN model with contextual information processing exploited CAA networks to achieve better performance.",2022,10.1016/j.compbiomed.2021.105188,diagnosis,False
Benign-malignant pulmonary nodule classification in low-dose CT with convolutional features,"PURPOSE: Low-Dose Computed Tomography (LDCT) is the most common imaging modality for lung cancer diagnosis. The presence of nodules in the scans does not necessarily portend lung cancer, as there is an intricate relationship between nodule characteristics and lung cancer. Therefore, benign-malignant pulmonary nodule classification at early detection is a crucial step to improve diagnosis and prolong patient survival. The aim of this study is to propose a method for predicting nodule malignancy based on deep abstract features. METHODS: To efficiently capture both intra-nodule heterogeneities and contextual information of the pulmonary nodules, a dual pathway model was developed to integrate the intra-nodule characteristics with contextual attributes. The proposed approach was implemented with both supervised and unsupervised learning schemes. A random forest model was added as a second component on top of the networks to generate the classification results. The discrimination power of the model was evaluated by calculating the Area Under the Receiver Operating Characteristic Curve (AUROC) metric. RESULTS: Experiments on 1297 manually segmented nodules show that the integration of context and target supervised deep features have a great potential for accurate prediction, resulting in a discrimination power of 0.936 in terms of AUROC, which outperformed the classification performance of the Kaggle 2017 challenge winner. CONCLUSION: Empirical results demonstrate that integrating nodule target and context images into a unified network improves the discrimination power, outperforming the conventional single pathway convolutional neural networks.",2021,10.1016/j.ejmp.2021.03.013,diagnosis,True
Bone Suppression on Chest Radiographs for Pulmonary Nodule Detection: Comparison between a Generative Adversarial Network and Dual-Energy Subtraction,"OBJECTIVE: To compare the effects of bone suppression imaging using deep learning (BSp-DL) based on a generative adversarial network (GAN) and bone subtraction imaging using a dual energy technique (BSt-DE) on radiologists' performance for pulmonary nodule detection on chest radiographs (CXRs). MATERIALS AND METHODS: A total of 111 adults, including 49 patients with 83 pulmonary nodules, who underwent both CXR using the dual energy technique and chest CT, were enrolled. Using CT as a reference, two independent radiologists evaluated CXR images for the presence or absence of pulmonary nodules in three reading sessions (standard CXR, BSt-DE CXR, and BSp-DL CXR). Person-wise and nodule-wise performances were assessed using receiver-operating characteristic (ROC) and alternative free-response ROC (AFROC) curve analyses, respectively. Subgroup analyses based on nodule size, location, and the presence of overlapping bones were performed. RESULTS: BSt-DE with an area under the AFROC curve (AUAFROC) of 0.996 and 0.976 for readers 1 and 2, respectively, and BSp-DL with AUAFROC of 0.981 and 0.958, respectively, showed better nodule-wise performance than standard CXR (AUAFROC of 0.907 and 0.808, respectively; p ≤ 0.005). In the person-wise analysis, BSp-DL with an area under the ROC curve (AUROC) of 0.984 and 0.931 for readers 1 and 2, respectively, showed better performance than standard CXR (AUROC of 0.915 and 0.798, respectively; p ≤ 0.011) and comparable performance to BSt-DE (AUROC of 0.988 and 0.974; p ≥ 0.064). BSt-DE and BSp-DL were superior to standard CXR for detecting nodules overlapping with bones (p < 0.017) or in the upper/middle lung zone (p < 0.017). BSt-DE was superior (p < 0.017) to BSp-DL in detecting peripheral and sub-centimeter nodules. CONCLUSION: BSp-DL (GAN-based bone suppression) showed comparable performance to BSt-DE and can improve radiologists' performance in detecting pulmonary nodules on CXRs. Nevertheless, for better delineation of small and peripheral nodules, further technical improvements are required.",2022,10.3348/kjr.2021.0146,diagnosis,True
Boundary Restored Network for Subpleural Pulmonary Lesion Segmentation on Ultrasound Images at Local and Global Scales,"To evaluate the application of machine learning for the detection of subpleural pulmonary lesions (SPLs) in ultrasound (US) scans, we propose a novel boundary-restored network (BRN) for automated SPL segmentation to avoid issues associated with manual SPL segmentation (subjectivity, manual segmentation errors, and high time consumption). In total, 1612 ultrasound slices from 255 patients in which SPLs were visually present were exported. The segmentation performance of the neural network based on the Dice similarity coefficient (DSC), Matthews correlation coefficient (MCC), Jaccard similarity metric (Jaccard), Average Symmetric Surface Distance (ASSD), and Maximum symmetric surface distance (MSSD) was assessed. Our dual-stage boundary-restored network (BRN) outperformed existing segmentation methods (U-Net and a fully convolutional network (FCN)) for the segmentation accuracy parameters including DSC (83.45 ± 16.60%), MCC (0.8330 ± 0.1626), Jaccard (0.7391 ± 0.1770), ASSD (5.68 ± 2.70 mm), and MSSD (15.61 ± 6.07 mm). It also outperformed the original BRN in terms of the DSC by almost 5%. Our results suggest that deep learning algorithms aid fully automated SPL segmentation in patients with SPLs. Further improvement of this technology might improve the specificity of lung cancer screening efforts and could lead to new applications of lung US imaging.",2020,10.1007/s10278-020-00356-8,diagnosis,False
BS-Net: Learning COVID-19 pneumonia severity on a large chest X-ray dataset,"In this work we design an end-to-end deep learning architecture for predicting, on Chest X-rays images (CXR), a multi-regional score conveying the degree of lung compromise in COVID-19 patients. Such semi-quantitative scoring system, namely Brixia score, is applied in serial monitoring of such patients, showing significant prognostic value, in one of the hospitals that experienced one of the highest pandemic peaks in Italy. To solve such a challenging visual task, we adopt a weakly supervised learning strategy structured to handle different tasks (segmentation, spatial alignment, and score estimation) trained with a ""from-the-part-to-the-whole"" procedure involving different datasets. In particular, we exploit a clinical dataset of almost 5,000 CXR annotated images collected in the same hospital. Our BS-Net demonstrates self-attentive behavior and a high degree of accuracy in all processing stages. Through inter-rater agreement tests and a gold standard comparison, we show that our solution outperforms single human annotators in rating accuracy and consistency, thus supporting the possibility of using this tool in contexts of computer-assisted monitoring. Highly resolved (super-pixel level) explainability maps are also generated, with an original technique, to visually help the understanding of the network activity on the lung areas. We also consider other scores proposed in literature and provide a comparison with a recently proposed non-specific approach. We eventually test the performance robustness of our model on an assorted public COVID-19 dataset, for which we also provide Brixia score annotations, observing good direct generalization and fine-tuning capabilities that highlight the portability of BS-Net in other clinical settings. The CXR dataset along with the source code and the trained model are publicly released for research purposes.",2021,10.1016/j.media.2021.102046,diagnosis,False
CAD system for lung nodule detection using deep learning with CNN,"The early detection of pulmonary nodules using computer-aided diagnosis (CAD) systems is very essential in reducing mortality rates of lung cancer. In this paper, we propose a new deep learning approach to improve the classification accuracy of pulmonary nodules in computed tomography (CT) images. Our proposed CNN-5CL (convolutional neural network with 5 convolutional layers) approach uses an 11-layer convolutional neural network (with 5 convolutional layers) for automatic feature extraction and classification. The proposed method is evaluated using LIDC/IDRI images. The proposed method is implemented in the Python platform, and the performance is evaluated with metrics such as accuracy, sensitivity, specificity, and receiver operating characteristics (ROC). The results show that the proposed method achieves accuracy, sensitivity, specificity, and area under the roc curve (AUC) of 98.88%, 99.62%, 93.73%, and 0.928, respectively. The proposed approach outperforms various other methods such as Naïve Bayes, K-nearest neighbor, support vector machine, adaptive neuro fuzzy inference system methods, and also other deep learning-based approaches.",2022,10.1007/s11517-021-02462-3,diagnosis,True
CADxReport: Chest x-ray report generation using co-attention mechanism and reinforcement learning,"BACKGROUND: Automated generation of radiological reports for different imaging modalities is essentially required to smoothen the clinical workflow and alleviate radiologists' workload. It involves the careful amalgamation of image processing techniques for medical image interpretation and language generation techniques for report generation. This paper presents CADxReport, a coattention and reinforcement learning based technique for generating clinically accurate reports from chest x-ray (CXR) images. METHOD: CADxReport, uses VGG19 network pre-trained over ImageNet dataset and a multi-label classifier for extracting visual and semantic features from CXR images, respectively. The co-attention mechanism with both the features is used to generate a context vector, which is then passed to HLSTM for radiological report generation. The model is trained using reinforcement learning to maximize CIDEr rewards. OpenI dataset, having 7, 470 CXRs along with 3, 955 associated structured radiological reports, is used for training and testing. RESULTS: Our proposed model is able to generate clinically accurate reports from CXR images. The quantitative evaluations confirm satisfactory results by achieving the following performance scores: BLEU-1 = 0.577, BLEU-2 = 0.478, BLEU-3 = 0.403, BLEU-4 = 0.346, ROUGE = 0.618 and CIDEr = 0.380. CONCLUSIONS: The evaluation using BLEU, ROUGE, and CIDEr score metrics indicates that the proposed model generates sufficiently accurate CXR reports and outperforms most of the state-of-the-art methods for the given task.",2022,10.1016/j.compbiomed.2022.105498,diagnosis,False
Can a Novel Deep Neural Network Improve the Computer-Aided Detection of Solid Pulmonary Nodules and the Rate of False-Positive Findings in Comparison to an Established Machine Learning Computer-Aided Detection?,"OBJECTIVE: The aim of this study was to compare the performance of 2 approved computer-aided detection (CAD) systems for detection of pulmonary solid nodules (PSNs) in an oncologic cohort. The first CAD system is based on a conventional machine learning approach (VD10F), and the other is based on a deep 3D convolutional neural network (CNN) CAD software (VD20A). METHODS AND MATERIALS: Nine hundred sixty-seven patients with a total of 2451 PSNs were retrospectively evaluated using the 2 different CAD systems. All patients had thin-slice chest computed tomography (0.6 mm) using 100 kV and 100 mAs and a high-resolution kernel (I50f). The CAD images generated by VD10F were transferred to the PACS for evaluation. The images generated by VD20A were evaluated using a Web browser-based viewer. Finally, a senior radiologist who was blinded for the CAD results examined the thin-slice images of every patient (ground truth). RESULTS: A total of 2451 PSNs were detected by the senior radiologist. CAD-VD10F detected 1401 true-positive, 143 false-negative, 565 false-positive (FP), and 342 true-negative PSNs, resulting in sensitivity of 90.7%, specificity of 37.7%, positive predictive value of 0.71, and negative predictive value of 0.70. CAD-VD20A detected 1381 true-positive, 163 false-negative, 337 FP, and 570 true-negative PSNs, resulting in sensitivity of 89.4%, specificity of 62.8%, positive predictive value of 0.80, and negative predictive value 0.77, respectively. The rate of FP per scan was 0.6 for CAD-VD10F and 0.3 for CAD-VD20A. CONCLUSIONS: The new deep learning-based CAD software (VD20A) shows similar sensitivity with the conventional CAD software (VD10F), but a significantly higher specificity.",2021,10.1097/rli.0000000000000713,diagnosis,True
Can artificial intelligence distinguish between malignant and benign mediastinal lymph nodes using sonographic features on EBUS images?,"AIMS: This study aimed to develop a new intelligent diagnostic approach using an artificial neural network (ANN). Moreover, we investigated whether the learning-method-guided quantitative analysis approach adequately described mediastinal lymphadenopathies on endobronchial ultrasound (EBUS) images. METHODS: In total, 345 lymph nodes (LNs) from 345 EBUS images were used as source input datasets for the application group. The group consisted of 300 and 45 textural patterns as input and output variables, respectively. The input and output datasets were processed using MATLAB. All these datasets were utilized for the training and testing of the ANN. RESULTS: The best diagnostic accuracy was 82% of that obtained from the textural patterns of the LNs pattern (89% sensitivity, 72% specificity, and 78.2% area under the curve). The negative predictive values were 81% compared to the corresponding positive predictive values of 83%. Due to the application group's pattern-based evaluation, the LN pattern was statistically significant (p = .002). CONCLUSIONS: The proposed intelligent approach could be useful in making diagnoses. Further development is required to improve the diagnostic accuracy of the visual interpretation.",2020,10.1080/03007995.2020.1837763,diagnosis,False
Can Deep Learning-Based Volumetric Analysis Predict Oxygen Demand Increase in Patients with COVID-19 Pneumonia?,"Background and Objectives: This study aimed to investigate whether predictive indicators for the deterioration of respiratory status can be derived from the deep learning data analysis of initial chest computed tomography (CT) scans of patients with coronavirus disease 2019 (COVID-19). Materials and Methods: Out of 117 CT scans of 75 patients with COVID-19 admitted to our hospital between April and June 2020, we retrospectively analyzed 79 CT scans that had a definite time of onset and were performed prior to any medication intervention. Patients were grouped according to the presence or absence of increased oxygen demand after CT scan. Quantitative volume data of lung opacity were measured automatically using a deep learning-based image analysis system. The sensitivity, specificity, and area under the receiver operating characteristic curve (AUC) of the opacity volume data were calculated to evaluate the accuracy of the system in predicting the deterioration of respiratory status. Results: All 79 CT scans were included (median age, 62 years (interquartile range, 46-77 years); 56 (70.9%) were male. The volume of opacity was significantly higher for the increased oxygen demand group than for the nonincreased oxygen demand group (585.3 vs. 132.8 mL, p < 0.001). The sensitivity, specificity, and AUC were 76.5%, 68.2%, and 0.737, respectively, in the prediction of increased oxygen demand. Conclusion: Deep learning-based quantitative analysis of the affected lung volume in the initial CT scans of patients with COVID-19 can predict the deterioration of respiratory status to improve treatment and resource management.",2021,10.3390/medicina57111148,prognosis,True
Can Peritumoral Radiomics Improve the Prediction of Malignancy of Solid Pulmonary Nodule Smaller Than 2 cm?,"RATIONALE AND OBJECTIVES: To compare the ability of radiomics models including the perinodular parenchyma and standard nodular radiomics model in lung cancer diagnosis of solid pulmonary nodules smaller than 2 cm. MATERIALS AND METHODS: In this retrospective study, the computed tomography (CT) scans of 206 patients with a lung nodule from a single institution in 2012-2019 were collected. For each nodule, four volumes of interest were defined using the gross tumor volume (GTV) and peritumoral volumes (PTVs) of 5, 10, and 15 mm around the tumor. RESULTS: Radiomics models created from GTV, GTV plus 5 mm of PTV, GTV plus 10 mm of PTV, and GTV plus 15 mm of PTV achieved AUCs of 0.89, 0.81, 0.81, and 0.73, respectively, in the validation cohort for the diagnostic classification of benign and malignant pulmonary nodules. The performance of the models gradually decreased as the PTV increased. Wavelet features were the primary features identified in optimal radiomics signatures (2/3 in R, 4/5 in GTV plus 5 mm PTV, 3/4 in GTV plus 10 mm PTV, 2/3 in GTV plus 15 mm PTV). CONCLUSION: Our study indicated that the radiomics signatures of GTV had a good prediction ability in distinguishing benign and malignant solid pulmonary nodules smaller than 2 cm on CT. However, the radiomics feature of the surrounding parenchyma of the nodule did not enhance the effectiveness of the diagnostic model.",2022,10.1016/j.acra.2020.10.029,diagnosis,True
Can texture features improve the differentiation of infiltrative lung adenocarcinoma appearing as ground glass nodules in contrast-enhanced CT?,"OBJECTIVES: To investigate the validity and efficacy of comparing texture features from contrast-enhanced images with non-enhanced images in identifying infiltrative lung adenocarcinoma represented as ground glass nodules (GGN). MATERIALS AND METHODS: A retrospective cohort study was conducted with patients presenting with lung adenocarcinoma and treated at a single centre between January 2015 to December 2017. All patients underwent standard and contrast-enhanced thoracic CT scans with 0.5 mm collimation and 1 mm slice reconstruction thickness before surgery. A total of 34 lung adenocarcinoma patients (representing 34 lesions) were analysed; including 21 instances of invasive adenocarcinoma (IAC) lesions, 4 instances of adenocarcinoma in situ (AIS) lesions, and 9 minimally invasive adenocarcinoma (MIA) lesions. After radiologists manually segmented the lesions, texture features were quantitatively extracted using Artificial Intelligence Kit (AK) software. Then, multivariate logistic regression analysis based on standard and contrast-enhanced CT texture features was employed to analyse the invasiveness of lung adenocarcinoma lesions appearing as GGNs. A receiver operating characteristic (ROC) curve analysis was used to evaluate the performance of those models. RESULTS: A total of 21 quantitative texture features were extracted using the AK software. After dimensionality reduction, 5 and 3 features extracted from thin-section unenhanced and contrast-enhanced CT, respectively, were used to establish the model. The area under the ROC curve (AUC) values for unenhanced CT and enhanced CT features were 0.890 and 0.868, respectively. There was no significant difference (P = 0.190) in the AUC between models based on non-enhanced and contrast-enhanced CT texture features. CONCLUSION: Compared with unenhanced CT, texture features extracted from contrast-enhanced CT provided no benefit in improving the differential diagnosis of infiltrative lung adenocarcinoma from non-infiltrative malignancies appearing as GGNs.",2019,10.1016/j.ejrad.2019.06.010,prognosis,True
Capnovolumetry in combination with clinical history for the diagnosis of asthma and COPD,"Capnovolumetry performed during resting ventilation is an easily applicable diagnostic tool sensitive to airway obstruction. In the present analysis, we investigated in which way capnovolumetric parameters can be combined with basic anamnestic information to support the diagnosis of asthma and COPD. Among 1400 patients of a previous diagnostic study, we selected 1057 patients with a diagnosis of asthma (n = 433), COPD (n = 260), or without respiratory disease (n = 364). Besides performing capnovolumetry, patients answered questions on symptoms and smoking status. Logistic regression analysis, single decision trees (CHAID), and ensembles of trees (random forest) were used to identify diagnostic patterns of asthma and COPD. In the random forest approach, area/volume of phase 3, dyspnea upon strong exertion, s3/s2, and current smoking were identified as relevant parameters for COPD vs control. For asthma vs control, they were wheezing, volume of phase 2, current smoking, and dyspnea at strong exertion. For COPD vs asthma, s3/s2 was the primary criterion, followed by current smoking and smoking history. These parameters were also identified as relevant in single decision trees. Regarding the diagnosis of asthma vs control, COPD vs control, and COPD vs asthma, the area under the curve was 0.623, 0.875, and 0.880, respectively, in the random forest approach. Our results indicate that for the diagnosis of asthma and COPD capnovolumetry can be combined with basic anamnestic information in a simple, intuitive, and efficient manner. As capnovolumetry requires less cooperation from the patient than spirometry, this approach might be helpful for clinical practice.",2020,10.1038/s41533-020-00190-z,diagnosis,False
Cardiac Rhythm Device Identification Using Neural Networks,"OBJECTIVES: This paper reports the development, validation, and public availability of a new neural network-based system which attempts to identify the manufacturer and even the model group of a pacemaker or defibrillator from a chest radiograph. BACKGROUND: Medical staff often need to determine the model of a pacemaker or defibrillator (cardiac rhythm device) quickly and accurately. Current approaches involve comparing a device's radiographic appearance with a manual flow chart. METHODS: In this study, radiographic images of 1,676 devices, comprising 45 models from 5 manufacturers were extracted. A convolutional neural network was developed to classify the images, using a training set of 1,451 images. The testing set contained an additional 225 images consisting of 5 examples of each model. The network's ability to identify the manufacturer of a device was compared with that of cardiologists, using a published flowchart. RESULTS: The neural network was 99.6% (95% confidence interval [CI]: 97.5% to 100.0%) accurate in identifying the manufacturer of a device from a radiograph and 96.4% (95% CI: 93.1% to 98.5%) accurate in identifying the model group. Among 5 cardiologists who used the flowchart, median identification of manufacturer accuracy was 72.0% (range 62.2% to 88.9%), and model group identification was not possible. The network's ability to identify the manufacturer of the devices was significantly superior to that of all the cardiologists (p < 0.0001 compared with the median human identification; p < 0.0001 compared with the best human identification). CONCLUSIONS: A neural network can accurately identify the manufacturer and even model group of a cardiac rhythm device from a radiograph and exceeds human performance. This system may speed up the diagnosis and treatment of patients with cardiac rhythm devices, and it is publicly accessible online.",2019,10.1016/j.jacep.2019.02.003,diagnosis,False
Cardiothoracic ratio measurement using artificial intelligence: observer and method validation studies,"BACKGROUND: Artificial Intelligence (AI) is a promising tool for cardiothoracic ratio (CTR) measurement that has been technically validated but not clinically evaluated on a large dataset. We observed and validated AI and manual methods for CTR measurement using a large dataset and investigated the clinical utility of the AI method. METHODS: Five thousand normal chest x-rays and 2,517 images with cardiomegaly and CTR values, were analyzed using manual, AI-assisted, and AI-only methods. AI-only methods obtained CTR values from a VGG-16 U-Net model. An in-house software was used to aid the manual and AI-assisted measurements and to record operating time. Intra and inter-observer experiments were performed on manual and AI-assisted methods and the averages were used in a method variation study. AI outcomes were graded in the AI-assisted method as excellent (accepted by both users independently), good (required adjustment), and poor (failed outcome). Bland-Altman plot with coefficient of variation (CV), and coefficient of determination (R-squared) were used to evaluate agreement and correlation between measurements. Finally, the performance of a cardiomegaly classification test was evaluated using a CTR cutoff at the standard (0.5), optimum, and maximum sensitivity. RESULTS: Manual CTR measurements on cardiomegaly data were comparable to previous radiologist reports (CV of 2.13% vs 2.04%). The observer and method variations from the AI-only method were about three times higher than from the manual method (CV of 5.78% vs 2.13%). AI assistance resulted in 40% excellent, 56% good, and 4% poor grading. AI assistance significantly improved agreement on inter-observer measurement compared to manual methods (CV; bias: 1.72%; - 0.61% vs 2.13%; - 1.62%) and was faster to perform (2.2 ± 2.4 secs vs 10.6 ± 1.5 secs). The R-squared and classification-test were not reliable indicators to verify that the AI-only method could replace manual operation. CONCLUSIONS: AI alone is not yet suitable to replace manual operations due to its high variation, but it is useful to assist the radiologist because it can reduce observer variation and operation time. Agreement of measurement should be used to compare AI and manual methods, rather than R-square or classification performance tests.",2021,10.1186/s12880-021-00625-0,diagnosis,False
Cardiovascular signatures of COVID-19 predict mortality and identify barrier stabilizing therapies,"BACKGROUND: Endothelial cell (EC) activation, endotheliitis, vascular permeability, and thrombosis have been observed in patients with severe coronavirus disease 2019 (COVID-19), indicating that the vasculature is affected during the acute stages of SARS-CoV-2 infection. It remains unknown whether circulating vascular markers are sufficient to predict clinical outcomes, are unique to COVID-19, and if vascular permeability can be therapeutically targeted. METHODS: Prospectively evaluating the prevalence of circulating inflammatory, cardiac, and EC activation markers as well as developing a microRNA atlas in 241 unvaccinated patients with suspected SARS-CoV-2 infection allowed for prognostic value assessment using a Random Forest model machine learning approach. Subsequent ex vivo experiments assessed EC permeability responses to patient plasma and were used to uncover modulated gene regulatory networks from which rational therapeutic design was inferred. FINDINGS: Multiple inflammatory and EC activation biomarkers were associated with mortality in COVID-19 patients and in severity-matched SARS-CoV-2-negative patients, while dysregulation of specific microRNAs at presentation was specific for poor COVID-19-related outcomes and revealed disease-relevant pathways. Integrating the datasets using a machine learning approach further enhanced clinical risk prediction for in-hospital mortality. Exposure of ECs to COVID-19 patient plasma resulted in severity-specific gene expression responses and EC barrier dysfunction, which was ameliorated using angiopoietin-1 mimetic or recombinant Slit2-N. INTERPRETATION: Integration of multi-omics data identified microRNA and vascular biomarkers prognostic of in-hospital mortality in COVID-19 patients and revealed that vascular stabilizing therapies should be explored as a treatment for endothelial dysfunction in COVID-19, and other severe diseases where endothelial dysfunction has a central role in pathogenesis. FUNDING: This work was directly supported by grant funding from the Ted Rogers Center for Heart Research, Toronto, Ontario, Canada and the Peter Munk Cardiac Center, Toronto, Ontario, Canada.",2022,10.1016/j.ebiom.2022.103982,prognosis,False
CARes-UNet: Content-aware residual UNet for lesion segmentation of COVID-19 from chest CT images,"PURPOSE: Coronavirus disease 2019 (COVID-19) has caused a serious global health crisis. It has been proven that the deep learning method has great potential to assist doctors in diagnosing COVID-19 by automatically segmenting the lesions in computed tomography (CT) slices. However, there are still several challenges restricting the application of these methods, including high variation in lesion characteristics and low contrast between lesion areas and healthy tissues. Moreover, the lack of high-quality labeled samples and large number of patients lead to the urgency to develop a high accuracy model, which performs well not only under supervision but also with semi-supervised methods. METHODS: We propose a content-aware lung infection segmentation deep residual network (content-aware residual UNet (CARes-UNet)) to segment the lesion areas of COVID-19 from the chest CT slices. In our CARes-UNet, the residual connection was used in the convolutional block, which alleviated the degradation problem during the training. Then, the content-aware upsampling modules were introduced to improve the performance of the model while reducing the computation cost. Moreover, to achieve faster convergence, an advanced optimizer named Ranger was utilized to update the model's parameters during training. Finally, we employed a semi-supervised segmentation framework to deal with the problem of lacking pixel-level labeled data. RESULTS: We evaluated our approach using three public datasets with multiple metrics and compared its performance to several models. Our method outperforms other models in multiple indicators, for instance in terms of Dice coefficient on COVID-SemiSeg Dataset, CARes-UNet got the score 0.731, and semi-CARes-UNet further boosted it to 0.776. More ablation studies were done and validated the effectiveness of each key component of our proposed model. CONCLUSIONS: Compared with the existing neural network methods applied to the COVID-19 lesion segmentation tasks, our CARes-UNet can gain more accurate segmentation results, and semi-CARes-UNet can further improve it using semi-supervised learning methods while presenting a possible way to solve the problem of lack of high-quality annotated samples. Our CARes-UNet and semi-CARes-UNet can be used in artificial intelligence-empowered computer-aided diagnosis system to improve diagnostic accuracy in this ongoing COVID-19 pandemic.",2021,10.1002/mp.15231,diagnosis,True
Cell Painting predicts impact of lung cancer variants,"Most variants in most genes across most organisms have an unknown impact on the function of the corresponding gene. This gap in knowledge is especially acute in cancer, where clinical sequencing of tumors now routinely reveals patient-specific variants whose functional impact on the corresponding genes is unknown, impeding clinical utility. Transcriptional profiling was able to systematically distinguish these variants of unknown significance as impactful vs. neutral in an approach called expression-based variant-impact phenotyping. We profiled a set of lung adenocarcinoma-associated somatic variants using Cell Painting, a morphological profiling assay that captures features of cells based on microscopy using six stains of cell and organelle components. Using deep-learning-extracted features from each cell's image, we found that cell morphological profiling (cmVIP) can predict variants' functional impact and, particularly at the single-cell level, reveals biological insights into variants that can be explored at our public online portal. Given its low cost, convenient implementation, and single-cell resolution, cmVIP profiling therefore seems promising as an avenue for using non-gene specific assays to systematically assess the impact of variants, including disease-associated alleles, on gene function.",2022,10.1091/mbc.E21-11-0538,prognosis,False
Central focused convolutional neural networks: Developing a data-driven model for lung nodule segmentation,"Accurate lung nodule segmentation from computed tomography (CT) images is of great importance for image-driven lung cancer analysis. However, the heterogeneity of lung nodules and the presence of similar visual characteristics between nodules and their surroundings make it difficult for robust nodule segmentation. In this study, we propose a data-driven model, termed the Central Focused Convolutional Neural Networks (CF-CNN), to segment lung nodules from heterogeneous CT images. Our approach combines two key insights: 1) the proposed model captures a diverse set of nodule-sensitive features from both 3-D and 2-D CT images simultaneously; 2) when classifying an image voxel, the effects of its neighbor voxels can vary according to their spatial locations. We describe this phenomenon by proposing a novel central pooling layer retaining much information on voxel patch center, followed by a multi-scale patch learning strategy. Moreover, we design a weighted sampling to facilitate the model training, where training samples are selected according to their degree of segmentation difficulty. The proposed method has been extensively evaluated on the public LIDC dataset including 893 nodules and an independent dataset with 74 nodules from Guangdong General Hospital (GDGH). We showed that CF-CNN achieved superior segmentation performance with average dice scores of 82.15% and 80.02% for the two datasets respectively. Moreover, we compared our results with the inter-radiologists consistency on LIDC dataset, showing a difference in average dice score of only 1.98%.",2017,10.1016/j.media.2017.06.014,diagnosis,True
Changes in CT Radiomic Features Associated with Lymphocyte Distribution Predict Overall Survival and Response to Immunotherapy in Non-Small Cell Lung Cancer,"No predictive biomarkers can robustly identify patients with non-small cell lung cancer (NSCLC) who will benefit from immune checkpoint inhibitor (ICI) therapies. Here, in a machine learning setting, we compared changes (""delta"") in the radiomic texture (DelRADx) of CT patterns both within and outside tumor nodules before and after two to three cycles of ICI therapy. We found that DelRADx patterns could predict response to ICI therapy and overall survival (OS) for patients with NSCLC. We retrospectively analyzed data acquired from 139 patients with NSCLC at two institutions, who were divided into a discovery set (D(1) = 50) and two independent validation sets (D(2) = 62, D(3) = 27). Intranodular and perinodular texture descriptors were extracted, and the relative differences were computed. A linear discriminant analysis (LDA) classifier was trained with 8 DelRADx features to predict RECIST-derived response. Association of delta-radiomic risk score (DRS) with OS was determined. The association of DelRADx features with tumor-infiltrating lymphocyte (TIL) density on the diagnostic biopsies (n = 36) was also evaluated. The LDA classifier yielded an AUC of 0.88 ± 0.08 in distinguishing responders from nonresponders in D(1), and 0.85 and 0.81 in D(2) and D(3) DRS was associated with OS [HR: 1.64; 95% confidence interval (CI), 1.22-2.21; P = 0.0011; C-index = 0.72). Peritumoral Gabor features were associated with the density of TILs on diagnostic biopsy samples. Our results show that DelRADx could be used to identify early functional responses in patients with NSCLC.",2020,10.1158/2326-6066.Cir-19-0476,prognosis,True
Chest computed tomography in the diagnosis of COVID-19 in patients with false negative RT-PCR,"OBJECTIVE: To evaluate the role of chest computed tomography in patients with COVID-19 who presented initial negative result in reverse transcriptase-polymerase chain reaction (RT-PCR). METHODS: A single-center, retrospective study that evaluated 39 patients with negative RT-PCR for COVID-19, who underwent chest computed tomography and had a final clinical or serological diagnosis of COVID-19. The visual tomographic classification was evaluated according to the Consensus of the Radiological Society of North America and software developed with artificial intelligence for automatic detection of findings and chance estimation of COVID-19. RESULTS: In the visual tomographic analysis, only one of them (3%) presented computed tomography classified as negative, 69% were classified as typical and 28% as indeterminate. In the evaluation using the software, only four (about 10%) had a probability of COVID-19 <25%. CONCLUSION: Computed tomography can play an important role in management of suspected cases of COVID-19 with initial negative results in RT-PCR, especially considering those patients outside the ideal window for sample collection for RT-PCR.",2021,10.31744/einstein_journal/2021AO6363,diagnosis,True
Chest CT Evaluation of 11 Persistent Asymptomatic Patients with SARS-CoV-2 Infection,"In total, 11 asymptomatic carriers who underwent nasal or oropharyngeal swab tests for SARS-CoV-2 after being in close contact with patients who developed symptomatic 2019 coronavirus disease (COVID-19) were enrolled in this study. The chest multidetector computed tomography (CT) images of the enrolled patients were qualitatively and quantitatively analyzed. The findings of the first chest CT were normal in 3 (27.3%) patients, 2 of whom were aged below 15 years. The lesions of 2 (18.2%) patients involved 1 lobe with unifocal presence. Subpleural lesions were observed in 7 (63.6%) patients. Ground glass opacity (GGO) was the most common sign observed in 7 (63.6%) patients. Crazy-paving pattern and consolidation were detected in 2 (18.2%) and 4 (36.4%) patients, respectively. Based on deep learning and quantitative analysis, the mean volume of intrapulmonary lesions in the first CT image was 85.73 ± 84.46 cm(3). In patients with positive findings on CT images, the average interval between positive real-time reverse transcriptase polymerase chain reaction assay and peak volume on CT images was 5.1 ± 3.1 days. In conclusion, typical CT findings can be detected in over 70% of asymptomatic SARS-CoV-2 carriers. The initial presentation is typically GGO along the subpleural regions and bronchi, which absorbs in approximately 5 days.",2021,10.7883/yoken.JJID.2020.264,diagnosis,True
Chest CT for triage during COVID-19 on the emergency department: myth or truth?,"PURPOSE: We aimed to investigate the diagnostic performance of chest CT compared with first RT-PCR results in adult patients suspected of COVID-19 infection in an ED setting. We also constructed a predictive machine learning model based on chest CT and additional data to improve the diagnostic accuracy of chest CT. METHODS: This study's cohort consisted of 319 patients who underwent chest CT and RT-PCR testing at the ED. Patient characteristics, demographics, symptoms, vital signs, laboratory tests, and chest CT results (CO-RADS) were collected. With first RT-PCR as reference standard, the diagnostic performance of chest CT using the CO-RADS score was assessed. Additionally, a predictive machine learning model was constructed using logistic regression. RESULTS: Chest CT, with first RT-PCR as a reference, had a sensitivity, specificity, PPV, and NPV of 90.2%, 88.2%, 84.5%, and 92.7%, respectively. The prediction model with CO-RADS, ferritin, leucocyte count, CK, days of complaints, and diarrhea as predictors had a sensitivity, specificity, PPV, and NPV of 89.3%, 93.4%, 90.8%, and 92.3%, respectively. CONCLUSION: Chest CT, using the CO-RADS scoring system, is a sensitive and specific method that can aid in the diagnosis of COVID-19, especially if RT-PCR tests are scarce during an outbreak. Combining a predictive machine learning model could further improve the accuracy of diagnostic chest CT for COVID-19. Further candidate predictors should be analyzed to improve our model. However, RT-PCR should remain the primary standard of testing as up to 9% of RT-PCR positive patients are not diagnosed by chest CT or our machine learning model.",2020,10.1007/s10140-020-01821-1,diagnosis,True
Chest Radiographs in Congestive Heart Failure: Visualizing Neural Network Learning,"Purpose To examine Generative Visual Rationales (GVRs) as a tool for visualizing neural network learning of chest radiograph features in congestive heart failure (CHF). Materials and Methods A total of 103 489 frontal chest radiographs in 46 712 patients acquired from January 1, 2007, to December 31, 2016, were divided into a labeled data set (with B-type natriuretic peptide [BNP] result as a marker of CHF) and unlabeled data set (without BNP result). A generative model was trained on the unlabeled data set, and a neural network was trained on the encoded representations of the labeled data set to estimate BNP. The model was used to visualize how a radiograph with high estimated BNP would look without disease (a ""healthy"" radiograph). An overfitted model was developed for comparison, and 100 GVRs were blindly assessed by two experts for features of CHF. Area under the receiver operating characteristic curve (AUC), κ coefficient, and mixed-effects logistic regression were used for statistical analyses. Results At a cutoff BNP of 100 ng/L as a marker of CHF, the correctly trained model achieved an AUC of 0.82. Assessment of GVRs revealed that the correctly trained model highlighted conventional radiographic features of CHF as reasons for an elevated BNP prediction more frequently than the overfitted model, including cardiomegaly (153 [76.5%] of 200 vs 64 [32%] of 200, respectively; P < .001) and pleural effusions (47 [23.5%] of 200 vs 16 [8%] of 200, respectively; P = .003). Conclusion Features of congestive heart failure on chest radiographs learned by neural networks can be identified using Generative Visual Rationales, enabling detection of bias and overfitted models. © RSNA, 2018 See also the editorial by Ngo in this issue.",2019,10.1148/radiol.2018180887,combined,False
Chest X-ray Classification for the Detection of COVID-19 Using Deep Learning Techniques,"Recent technological developments pave the path for deep learning-based techniques to be used in almost every domain of life. The precision of deep learning techniques make it possible for these to be used in the medical field for the classification and detection of various diseases. Recently, the coronavirus (COVID-19) pandemic has put a lot of pressure on the health system all around the world. The diagnosis of COVID-19 is possible by PCR testing and medical imagining. Since COVID-19 is highly contagious, diagnosis using chest X-ray is considered safe in various situations. In this study, a deep learning-based technique is proposed to classify COVID-19 infection from other non-COVID-19 infections. To classify COVID-19, three different pre-trained models named EfficientNetB1, NasNetMobile and MobileNetV2 are used. The augmented dataset is used for training deep learning models while two different training strategies have been used for classification. In this study, not only are the deep learning model fine-tuned but also the hyperparameters are fine-tuned, which significantly improves the performance of the fine-tuned deep learning models. Moreover, the classification head is regularized to improve the performance. For the evaluation of the proposed techniques, several performance parameters are used to gauge the performance. EfficientNetB1 with regularized classification head outperforms the other models. The proposed technique successfully classifies four classes that include COVID-19, viral pneumonia, lung opacity, and normal, with an accuracy of 96.13%. The proposed technique shows superiority in terms of accuracy when compared with recent techniques present in the literature.",2022,10.3390/s22031211,diagnosis,False
Chest X-ray image phase features for improved diagnosis of COVID-19 using convolutional neural network,"PURPOSE: Recently, the outbreak of the novel coronavirus disease 2019 (COVID-19) pandemic has seriously endangered human health and life. In fighting against COVID-19, effective diagnosis of infected patient is critical for preventing the spread of diseases. Due to limited availability of test kits, the need for auxiliary diagnostic approach has increased. Recent research has shown radiography of COVID-19 patient, such as CT and X-ray, contains salient information about the COVID-19 virus and could be used as an alternative diagnosis method. Chest X-ray (CXR) due to its faster imaging time, wide availability, low cost, and portability gains much attention and becomes very promising. In order to reduce intra- and inter-observer variability, during radiological assessment, computer-aided diagnostic tools have been used in order to supplement medical decision making and subsequent management. Computational methods with high accuracy and robustness are required for rapid triaging of patients and aiding radiologist in the interpretation of the collected data. METHOD: In this study, we design a novel multi-feature convolutional neural network (CNN) architecture for multi-class improved classification of COVID-19 from CXR images. CXR images are enhanced using a local phase-based image enhancement method. The enhanced images, together with the original CXR data, are used as an input to our proposed CNN architecture. Using ablation studies, we show the effectiveness of the enhanced images in improving the diagnostic accuracy. We provide quantitative evaluation on two datasets and qualitative results for visual inspection. Quantitative evaluation is performed on data consisting of 8851 normal (healthy), 6045 pneumonia, and 3323 COVID-19 CXR scans. RESULTS: In Dataset-1, our model achieves 95.57% average accuracy for a three classes classification, 99% precision, recall, and F1-scores for COVID-19 cases. For Dataset-2, we have obtained 94.44% average accuracy, and 95% precision, recall, and F1-scores for detection of COVID-19. CONCLUSIONS: Our proposed multi-feature-guided CNN achieves improved results compared to single-feature CNN proving the importance of the local phase-based CXR image enhancement. Future work will involve further evaluation of the proposed method on a larger-size COVID-19 dataset as they become available.",2021,10.1007/s11548-020-02305-w,diagnosis,False
Choquet Integral and Coalition Game-Based Ensemble of Deep Learning Models for COVID-19 Screening From Chest X-Ray Images,"Under the present circumstances, when we are still under the threat of different strains of coronavirus, and since the most widely used method for COVID-19 detection, RT-PCR is a tedious and time-consuming manual procedure with poor precision, the application of Artificial Intelligence (AI) and Computer-Aided Diagnosis (CAD) is inevitable. Though, some vaccines have now been authorized worldwide, it will take huge time to reach everyone, especially in developing countries. In this work, we have analyzed Chest X-ray (CXR) images for the detection of the coronavirus. The primary agenda of this proposed research study is to leverage the classification performance of the deep learning models using ensemble learning. Many papers have proposed different ensemble learning techniques in this field, some methods using aggregation functions like Weighted Arithmetic Mean (WAM) among others. However, none of these methods take into consideration the decisions that subsets of the classifiers take. In this paper, we have applied Choquet integral for ensemble and propose a novel method for the evaluation of fuzzy measures using coalition game theory, information theory, and Lambda fuzzy approximation. Three different sets of fuzzy measures are calculated using three different weighting schemes along with information theory and coalition game theory. Using these three sets of fuzzy measures, three Choquet integrals are calculated and their decisions are finally combined. Besides, we have created a database by combining several image repositories developed recently. Impressive results on the newly developed dataset and the challenging COVIDx dataset support the efficacy and robustness of the proposed method. Our experimental results outperform many recently proposed methods.",2021,10.1109/jbhi.2021.3111415,diagnosis,False
Chronic Obstructive Pulmonary Disease Quantification Using CT Texture Analysis and Densitometry: Results From the Danish Lung Cancer Screening Trial,"OBJECTIVE. The purpose of this study is to establish whether texture analysis and densitometry are complementary quantitative measures of chronic obstructive pulmonary disease (COPD) in a lung cancer screening setting. MATERIALS AND METHODS. This was a retrospective study of data collected prospectively (in 2004-2010) in the Danish Lung Cancer Screening Trial. The texture score, relative area of emphysema, and percentile density were computed for 1915 baseline low-dose lung CT scans and were evaluated, both individually and in combination, for associations with lung function (i.e., forced expiratory volume in 1 second as a percentage of predicted normal [FEV(1)% predicted]), diagnosis of mild to severe COPD, and prediction of a rapid decline in lung function. Multivariate linear regression models with lung function as the outcome were compared using the likelihood ratio test or the Vuong test, and AUC values for diagnostic and prognostic capabilities were compared using the DeLong test. RESULTS. Texture showed a significantly stronger association with lung function (p < 0.001 vs densitometric measures), a significantly higher diagnostic AUC value (for COPD, 0.696; for Global Initiative for Chronic Obstructive Lung Disease (GOLD) grade 1, 0.648; for GOLD grade 2, 0.768; and for GOLD grade 3, 0.944; p < 0.001 vs densitometric measures), and a higher but not significantly different association with lung function decline. In addition, only texture could predict a rapid decline in lung function (AUC value, 0.538; p < 0.05 vs random guessing). The combination of texture and both densitometric measures strengthened the association with lung function and decline in lung function (p < 0.001 and p < 0.05, respectively, vs texture) but did not improve diagnostic or prognostic performance. CONCLUSION. The present study highlights texture as a promising quantitative CT measure of COPD to use alongside, or even instead of, densitometric measures. Moreover, texture may allow early detection of COPD in subjects who undergo lung cancer screening.",2020,10.2214/ajr.19.22300,diagnosis,True
Chronic Obstructive Pulmonary Disease: Thoracic CT Texture Analysis and Machine Learning to Predict Pulmonary Ventilation,"Background Fixed airflow limitation and ventilation heterogeneity are common in chronic obstructive pulmonary disease (COPD). Conventional noncontrast CT provides airway and parenchymal measurements but cannot be used to directly determine lung function. Purpose To develop, train, and test a CT texture analysis and machine-learning algorithm to predict lung ventilation heterogeneity in participants with COPD. Materials and Methods In this prospective study (ClinicalTrials.gov: NCT02723474; conducted from January 2010 to February 2017), participants were randomized to optimization (n = 1), training (n = 67), and testing (n = 27) data sets. Hyperpolarized (HP) helium 3 ((3)He) MRI ventilation maps were co-registered with thoracic CT to provide ground truth labels, and 87 quantitative imaging features were extracted and normalized to lung averages to generate 174 features. The volume-of-interest dimension and the training data sampling method were optimized to maximize the area under the receiver operating characteristic curve (AUC). Forward feature selection was performed to reduce the number of features; logistic regression, linear support vector machine, and quadratic support vector machine classifiers were trained through fivefold cross validation. The highest-performing classification model was applied to the test data set. Pearson coefficients were used to determine the relationships between the model, MRI, and pulmonary function measurements. Results The quadratic support vector machine performed best in training and was applied to the test data set. Model-predicted ventilation maps had an accuracy of 88% (95% confidence interval [CI]: 88%, 88%) and an AUC of 0.82 (95% CI: 0.82, 0.83) when the HP (3)He MRI ventilation maps were used as the reference standard. Model-predicted ventilation defect percentage (VDP) was correlated with VDP at HP (3)He MRI (r = 0.90, P < .001). Both model-predicted and HP (3)He MRI VDP were correlated with forced expiratory volume in 1 second (FEV(1)) (model: r = -0.65, P < .001; MRI: r = -0.70, P < .001), ratio of FEV(1) to forced vital capacity (model: r = -0.73, P < .001; MRI: r = -0.75, P < .001), diffusing capacity (model: r = -0.69, P < .001; MRI: r = -0.65, P < .001), and quality-of-life score (model: r = 0.59, P = .001; MRI: r = 0.65, P < .001). Conclusion Model-predicted ventilation maps generated by using CT textures and machine learning were correlated with MRI ventilation maps (r = 0.90, P < .001). © RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Fain in this issue.",2019,10.1148/radiol.2019190450,combined,True
Classification and Quantification of Emphysema Using a Multi-Scale Residual Network,"Automated tissue classification is an essential step for quantitative analysis and treatment of emphysema. Although many studies have been conducted in this area, there still remain two major challenges. First, different emphysematous tissue appears in different scales, which we call ""inter-class variations."" Second, the intensities of CT images acquired from different patients, scanners or scanning protocols may vary, which we call ""intra-class variations"". In this paper, we present a novel multi-scale residual network with two channels of raw CT image and its differential excitation component. We incorporate multi-scale information into our networks to address the challenge of inter-class variations. In addition to the conventional raw CT image, we use its differential excitation component as a pair of inputs to handle intra-class variations. Experimental results show that our approach has superior performance over the state-of-the- art methods, achieving a classification accuracy of 93.74% on our original emphysema database. Based on the classification results, we also perform the quantitative analysis of emphysema in 50 subjects by correlating the quantitative results (the area percentage of each class) with pulmonary functions. We show that centrilobular emphysema (CLE) and panlobular emphysema (PLE) have strong correlation with the pulmonary functions and the sum of CLE and PLE can be used as a new and accurate measure of emphysema severity instead of the conventional measure (sum of all subtypes of emphysema). The correlations between the new measure and various pulmonary functions are up to |r| = 0.922 (r is correlation coefficient).",2019,10.1109/jbhi.2018.2890045,diagnosis,True
Classification and Segmentation Algorithm in Benign and Malignant Pulmonary Nodules under Different CT Reconstruction,"METHODS: The imaging data of 55 patients with chest CT plain scan in the Xuancheng People's Hospital were collected retrospectively. The data of each patient included lung window reconstruction, mediastinum reconstruction, and bone window reconstruction. The depth neural network and 3D convolution neural network were used to construct the model and train the classification and segmentation algorithm. The pathological results were the gold standard for benign and malignant pulmonary nodules. The classification and segmentation algorithms under three CT reconstruction algorithms were compared and analyzed by analysis of variance. RESULTS: Under the three CT reconstruction algorithms, the classification accuracy of pulmonary nodule density types was 98.2%, 96.4%, and 94.5%, respectively. The Dice coefficients of all nodule segmentation were 80.32% ± 5.91%, 79.83% ± 6.12%, and 80.17% ± 5.89%, respectively. The diagnostic accuracy between benign and malignant pulmonary nodules under different reconstruction algorithms was 98.2%, 96.4%, and 94.5%, respectively. There was no significant difference in the classification accuracy, Dice coefficients, and diagnostic accuracy of pulmonary nodules under three different reconstruction algorithms (all P > 0.05). CONCLUSION: The depth neural network algorithm combined with 3D convolution neural network has a good efficiency in identifying benign and malignant pulmonary nodules under different CT reconstruction classification and segmentation algorithms.",2022,10.1155/2022/3490463,diagnosis,True
Classification by a stacking model using CNN features for COVID-19 infection diagnosis,"Affecting millions of people all over the world, the COVID-19 pandemic has caused the death of hundreds of thousands of people since its beginning. Examinations also found that even if the COVID-19 patients initially survived the coronavirus, pneumonia left behind by the virus may still cause severe diseases resulting in organ failure and therefore death in the future. The aim of this study is to classify COVID-19, normal and viral pneumonia using the chest X-ray images with machine learning methods. A total of 3486 chest X-ray images from three classes were first classified by three single machine learning models including the support vector machine (SVM), logistics regression (LR), artificial neural network (ANN) models, and then by a stacking model that was created by combining these 3 single models. Several performance evaluation indices including recall, precision, F-1 score, and accuracy were computed to evaluate and compare classification performance of 3 single four models and the final stacking model used in the study. As a result of the evaluations, the models namely, SVM, ANN, LR, and stacking, achieved 90.2%, 96.2%, 96.7%, and 96.9%classification accuracy, respectively. The study results indicate that the proposed stacking model is a fast and inexpensive method for assisting COVID-19 diagnosis, which can have potential to assist physicians and nurses to better and more efficiently diagnose COVID-19 infection cases in the busy clinical environment.",2022,10.3233/xst-211031,diagnosis,False
Classification of Benign and Malignant Lung Nodules Based on Deep Convolutional Network Feature Extraction,"With the rapid development of detection technology, CT imaging technology has been widely used in the early clinical diagnosis of lung nodules. However, accurate assessment of the nature of the nodule remains a challenging task due to the subjective nature of the radiologist. With the increasing amount of publicly available lung image data, it has become possible to use convolutional neural networks for benign and malignant classification of lung nodules. However, as the network depth increases, network training methods based on gradient descent usually lead to gradient dispersion. Therefore, we propose a novel deep convolutional network approach to classify the benignity and malignancy of lung nodules. Firstly, we segmented, extracted, and performed zero-phase component analysis whitening on images of lung nodules. Then, a multilayer perceptron was introduced into the structure to construct a deep convolutional network. Finally, the minibatch stochastic gradient descent method with a momentum coefficient is used to fine-tune the deep convolutional network to avoid the gradient dispersion. The 750 lung nodules in the lung image database are used for experimental verification. Classification accuracy of the proposed method can reach 96.0%. The experimental results show that the proposed method can provide an objective and efficient aid to solve the problem of classifying benign and malignant lung nodules in medical images.",2021,10.1155/2021/8769652,diagnosis,True
Classification of benign and malignant lung nodules from CT images based on hybrid features,"The classification of benign and malignant lung nodules has great significance for the early detection of lung cancer, since early diagnosis of nodules can greatly increase patient survival. In this paper, we propose a novel classification method for lung nodules based on hybrid features from computed tomography (CT) images. The method fused 3D deep dual path network (DPN) features, local binary pattern (LBP)-based texture features and histogram of oriented gradients (HOG)-based shape features to characterize lung nodules. DPN is a convolutional neural network which integrates the advantages of aggregated residual transformations (ResNeXt) for feature reuse and a densely convolutional network (DenseNet) for exploring new features. LBP is a prominent feature descriptor for texture classification, when combining with the HOG descriptor, it can improve the classification performance considerably. To differentiate malignant nodules from benign ones, a gradient boosting machine (GBM) algorithm is employed. We evaluated the proposed method on the publicly available LUng Nodule Analysis 2016 (LUNA16) dataset with 1004 nodules, achieving an area under the receiver operating characteristic curve (AUC) of 0.9687 and accuracy of 93.78%. The promising results demonstrate that our method has strong robustness on the classification of nodule patterns by virtue of the joint use of texture features, shape features and 3D deep DPN features. The method has the potential to help radiologists to interpret diagnostic data and make decisions in clinical practice.",2019,10.1088/1361-6560/ab2544,diagnosis,True
Classification of COVID-19 and Influenza Patients Using Deep Learning,"Coronavirus (COVID-19) is a deadly virus that initially starts with flu-like symptoms. COVID-19 emerged in China and quickly spread around the globe, resulting in the coronavirus epidemic of 2019-22. As this virus is very similar to influenza in its early stages, its accurate detection is challenging. Several techniques for detecting the virus in its early stages are being developed. Deep learning techniques are a handy tool for detecting various diseases. For the classification of COVID-19 and influenza, we proposed tailored deep learning models. A publicly available dataset of X-ray images was used to develop proposed models. According to test results, deep learning models can accurately diagnose normal, influenza, and COVID-19 cases. Our proposed long short-term memory (LSTM) technique outperformed the CNN model in the evaluation phase on chest X-ray images, achieving 98% accuracy.",2022,10.1155/2022/8549707,diagnosis,True
Classification of COVID-19 by Compressed Chest CT Image through Deep Learning on a Large Patients Cohort,"Corona Virus Disease (COVID-19) has spread globally quickly, and has resulted in a large number of causalities and medical resources insufficiency in many countries. Reverse-transcriptase polymerase chain reaction (RT-PCR) testing is adopted as biopsy tool for confirmation of virus infection. However, its accuracy is as low as 60-70%, which is inefficient to uncover the infected. In comparison, the chest CT has been considered as the prior choice in diagnosis and monitoring progress of COVID-19 infection. Although the COVID-19 diagnostic systems based on artificial intelligence have been developed for assisting doctors in diagnosis, the small sample size and the excessive time consumption limit their applications. To this end, this paper proposed a diagnosis prototype system for COVID-19 infection testing. The proposed deep learning model is trained and is tested on 2267 CT sequences from 1357 patients clinically confirmed with COVID-19 and 1235 CT sequences from non-infected people. The main highlights of the prototype system are: (1) no data augmentation is needed to accurately discriminate the COVID-19 from normal controls with the specificity of 0.92 and sensitivity of 0.93; (2) the raw DICOM image is not necessary in testing. Highly compressed image like Jpeg can be used to allow a quick diagnosis; and (3) it discriminates the virus infection within 6 seconds and thus allows an online test with light cost. We also applied our model on 48 asymptomatic patients diagnosed with COVID-19. We found that: (1) the positive rate of RT-PCR assay is 63.5% (687/1082). (2) 45.8% (22/48) of the RT-PCR assay is negative for asymptomatic patients, yet the accuracy of CT scans is 95.8%. The online detection system is available: http://212.64.70.65/covid .",2021,10.1007/s12539-020-00408-1,diagnosis,True
Classification of COVID-19 Chest CT Images Based on Ensemble Deep Learning,"Novel coronavirus pneumonia (NCP) has become a global pandemic disease, and computed tomography-based (CT) image analysis and recognition are one of the important tools for clinical diagnosis. In order to assist medical personnel to achieve an efficient and fast diagnosis of patients with new coronavirus pneumonia, this paper proposes an assisted diagnosis algorithm based on ensemble deep learning. The method combines the Stacked Generalization ensemble learning with the VGG16 deep learning to form a cascade classifier, and the information constituting the cascade classifier comes from multiple subsets of the training set, each of which is used to collect deviant information about the generalization behavior of the data set, such that this deviant information fills the cascade classifier. The algorithm was experimentally validated for classifying patients with novel coronavirus pneumonia, patients with common pneumonia (CP), and normal controls, and the algorithm achieved a prediction accuracy of 93.57%, sensitivity of 94.21%, specificity of 93.93%, precision of 89.40%, and F1-score of 91.74% for the three categories. The results show that the method proposed in this paper has good classification performance and can significantly improve the performance of deep neural networks for multicategory prediction tasks.",2021,10.1155/2021/5528441,diagnosis,True
Classification of COVID-19 chest X-Ray and CT images using a type of dynamic CNN modification method,"Understanding and classifying Chest X-Ray (CXR) and computerised tomography (CT) images are of great significance for COVID-19 diagnosis. The existing research on the classification for COVID-19 cases faces the challenges of data imbalance, insufficient generalisability, the lack of comparative study, etc. To address these problems, this paper proposes a type of modified MobileNet to classify COVID-19 CXR images and a modified ResNet architecture for CT image classification. In particular, a modification method of convolutional neural networks (CNN) is designed to solve the gradient vanishing problem and improve the classification performance through dynamically combining features in different layers of a CNN. The modified MobileNet is applied to the classification of COVID-19, Tuberculosis, viral pneumonia (with the exception of COVID-19), bacterial pneumonia and normal controls using CXR images. Also, the proposed modified ResNet is used for the classification of COVID-19, non-COVID-19 infections and normal controls using CT images. The results show that the proposed methods achieve 99.6% test accuracy on the five-category CXR image dataset and 99.3% test accuracy on the CT image dataset. Six advanced CNN architectures and two specific COVID-19 detection models, i.e., COVID-Net and COVIDNet-CT are used in comparative studies. Two benchmark datasets and a CXR image dataset which combines eight different CXR image sources are employed to evaluate the performance of the above models. The results show that the proposed methods outperform the comparative models in classification accuracy, sensitivity, and precision, which demonstrate their potential in computer-aided diagnosis for healthcare applications.",2021,10.1016/j.compbiomed.2021.104425,diagnosis,True
Classification of COVID-19 patients from chest CT images using multi-objective differential evolution-based convolutional neural networks,"Early classification of 2019 novel coronavirus disease (COVID-19) is essential for disease cure and control. Compared with reverse-transcription polymerase chain reaction (RT-PCR), chest computed tomography (CT) imaging may be a significantly more trustworthy, useful, and rapid technique to classify and evaluate COVID-19, specifically in the epidemic region. Almost all hospitals have CT imaging machines; therefore, the chest CT images can be utilized for early classification of COVID-19 patients. However, the chest CT-based COVID-19 classification involves a radiology expert and considerable time, which is valuable when COVID-19 infection is growing at rapid rate. Therefore, an automated analysis of chest CT images is desirable to save the medical professionals' precious time. In this paper, a convolutional neural networks (CNN) is used to classify the COVID-19-infected patients as infected (+ve) or not (-ve). Additionally, the initial parameters of CNN are tuned using multi-objective differential evolution (MODE). Extensive experiments are performed by considering the proposed and the competitive machine learning techniques on the chest CT images. Extensive analysis shows that the proposed model can classify the chest CT images at a good accuracy rate.",2020,10.1007/s10096-020-03901-z,diagnosis,True
Classification of early stage non-small cell lung cancers on computed tomographic images into histological types using radiomic features: interobserver delineation variability analysis,"Radiomics, which involves the extraction of large numbers of quantitative features from medical images, has attracted attention in cancer research. In radiomics analysis, tumor segmentation is a crucial step. In this study, we evaluated the potential application of radiomics for predicting the histology of early stage non-small cell lung cancer (NSCLC) by analyzing interobserver variability in tumor delineation. Forty patient datasets were included in this study, 21 involving adenocarcinomas and 19 involving squamous cell carcinomas. All patients underwent stereotactic body radiotherapy treatment. In total, 476 features were extracted from each dataset, representing treatment planning, computed tomography images, and gross tumor volume (GTV). The definition of GTV can significantly affect the histology prediction. Therefore, in the present study, the effect of interobserver tumor delineation variability on radiomic features was evaluated by preparing 4 volumes of interest (VOIs) for each patient, as follows: the original GTV (which was delineated at treatment planning); two GTVs delineated retrospectively by radiation oncologists; and a semi-automatic GTV contoured by a medical physicist. Radiomic features extracted from each VOI were then analyzed using a naïve Bayesian model. Area-under-the-curve (AUC) analysis showed that interobserver variability in delineation is a significant factor in radiomics performance. Nevertheless, with 8 selected features, AUC values averaged over the VOIs were high (0.725 ± 0.070). The present study indicated that radiomics has potential for predicting early stage NSCLC histology despite variability in delineation. The high prediction accuracy implies that noninvasive histology evaluation by radiomics is a promising clinical application.",2018,10.1007/s12194-017-0433-2,diagnosis,True
Classification of lung nodules based on CT images using squeeze-and-excitation network and aggregated residual transformations,"Lung cancer is pointed as a leading cause of cancer death worldwide. Early lung nodule diagnosis has great significance for treating lung cancer and increasing patient survival. In this paper, we present a novel method to classify the malignant from benign lung nodules based on CT images using squeeze-and-excitation network and aggregated residual transformations (SE-ResNeXt). The state-of-the-art SE-ResNeXt module, which integrates the advantages of SENet for feature recalibration and ResNeXt for feature reuse, has great ability in boosting feature discriminability on imaging pattern recognition. The method is evaluated on the public available LUng Nodule Analysis 2016 (LUNA16) database with 1004 (450 malignant and 554 benign) nodules, achieving an area under the receiver operating characteristic curve (AUC) of 0. 9563 and accuracy of 91.67%. The promising results demonstrate that our method has strong robustness in the classification of nodules. The method has the potential to help radiologists better interpret diagnostic data and differentiate the benign from malignant lung nodules on CT images in clinical practice. To our best knowledge, the effectiveness of SE-ResNeXt on lung nodule classification has not been extensively explored.",2020,10.1007/s11547-019-01130-9,diagnosis,True
Classification of Lung Nodules Based on Deep Residual Networks and Migration Learning,"The classification process of lung nodule detection in a traditional computer-aided detection (CAD) system is complex, and the classification result is heavily dependent on the performance of each step in lung nodule detection, causing low classification accuracy and high false positive rate. In order to alleviate these issues, a lung nodule classification method based on a deep residual network is proposed. Abandoning traditional image processing methods and taking the 50-layer ResNet network structure as the initial model, the deep residual network is constructed by combining residual learning and migration learning. The proposed approach is verified by conducting experiments on the lung computed tomography (CT) images from the publicly available LIDC-IDRI database. An average accuracy of 98.23% and a false positive rate of 1.65% are obtained based on the ten-fold cross-validation method. Compared with the conventional support vector machine (SVM)-based CAD system, the accuracy of our method improved by 9.96% and the false positive rate decreased by 6.95%, while the accuracy improved by 1.75% and 2.42%, respectively, and the false positive rate decreased by 2.07% and 2.22%, respectively, in contrast to the VGG19 model and InceptionV3 convolutional neural networks. The experimental results demonstrate the effectiveness of our proposed method in lung nodule classification for CT images.",2020,10.1155/2020/8975078,diagnosis,True
Classification of lung nodules in CT scans using three-dimensional deep convolutional neural networks with a checkpoint ensemble method,"BACKGROUND: Accurately detecting and examining lung nodules early is key in diagnosing lung cancers and thus one of the best ways to prevent lung cancer deaths. Radiologists spend countless hours detecting small spherical-shaped nodules in computed tomography (CT) images. In addition, even after detecting nodule candidates, a considerable amount of effort and time is required for them to determine whether they are real nodules. The aim of this paper is to introduce a high performance nodule classification method that uses three dimensional deep convolutional neural networks (DCNNs) and an ensemble method to distinguish nodules between non-nodules. METHODS: In this paper, we use a three dimensional deep convolutional neural network (3D DCNN) with shortcut connections and a 3D DCNN with dense connections for lung nodule classification. The shortcut connections and dense connections successfully alleviate the gradient vanishing problem by allowing the gradient to pass quickly and directly. Connections help deep structured networks to obtain general as well as distinctive features of lung nodules. Moreover, we increased the dimension of DCNNs from two to three to capture 3D features. Compared with shallow 3D CNNs used in previous studies, deep 3D CNNs more effectively capture the features of spherical-shaped nodules. In addition, we use an alternative ensemble method called the checkpoint ensemble method to boost performance. RESULTS: The performance of our nodule classification method is compared with that of the state-of-the-art methods which were used in the LUng Nodule Analysis 2016 Challenge. Our method achieves higher competition performance metric (CPM) scores than the state-of-the-art methods using deep learning. In the experimental setup ESB-ALL, the 3D DCNN with shortcut connections and the 3D DCNN with dense connections using the checkpoint ensemble method achieved the highest CPM score of 0.910. CONCLUSION: The result demonstrates that our method of using a 3D DCNN with shortcut connections, a 3D DCNN with dense connections, and the checkpoint ensemble method is effective for capturing 3D features of nodules and distinguishing nodules between non-nodules.",2018,10.1186/s12880-018-0286-0,diagnosis,True
Classification of malignant and benign lung nodules using taxonomic diversity index and phylogenetic distance,"Lung cancer presents the highest cause of death among patients around the world, in addition of being one of the smallest survival rates after diagnosis. Therefore, this study proposes a methodology for diagnosis of lung nodules in benign and malignant tumors based on image processing and pattern recognition techniques. Mean phylogenetic distance (MPD) and taxonomic diversity index (Δ) were used as texture descriptors. Finally, the genetic algorithm in conjunction with the support vector machine were applied to select the best training model. The proposed methodology was tested on computed tomography (CT) images from the Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI), with the best sensitivity of 93.42%, specificity of 91.21%, accuracy of 91.81%, and area under the ROC curve of 0.94. The results demonstrate the promising performance of texture extraction techniques using mean phylogenetic distance and taxonomic diversity index combined with phylogenetic trees. Graphical Abstract Stages of the proposed methodology.",2018,10.1007/s11517-018-1841-0,diagnosis,True
Classification of malignant lung cancer using deep learning,"In the automatic detection of suspicious shaded regions on CT images derived from the LIDC-IDRI dataset, the diagnostic system plays a significant role. This paper introduces an automatic recognition method for lung nodules of the regions of concern (ROI). The lung regions are segmented from DICOM image size 512 × 512 by adding a median filter, Gaussian filter, Gabor filter and watershed algorithm. AlexNet uses 227 × 227 × 3 with ""fc7"" (fully connected) layers and GoogLeNet uses 224 × 224 × 3 with ""pool5-drop 7 × 7 s1"" layers. Here, the authors explain what is better about AlexNet and GoogLeNet through its performance analysis, feature extraction, classification, sensitivity, specificity, detection and false alarm rate with time complexity. A multi-class SVM classifier with 100% precision and specificity provided the best performance in deep learning neural networks.",2021,10.1080/03091902.2020.1853837,diagnosis,True
Classification of pulmonary nodules by using hybrid features,"Early detection of pulmonary nodules is extremely important for the diagnosis and treatment of lung cancer. In this study, a new classification approach for pulmonary nodules from CT imagery is presented by using hybrid features. Four different methods are introduced for the proposed system. The overall detection performance is evaluated using various classifiers. The results are compared to similar techniques in the literature by using standard measures. The proposed approach with the hybrid features results in 90.7% classification accuracy (89.6% sensitivity and 87.5% specificity).",2013,10.1155/2013/148363,diagnosis,True
Classification of Severe and Critical Covid-19 Using Deep Learning and Radiomics,"OBJECTIVE: The coronavirus disease 2019 (COVID-19) is rapidly spreading inside China and internationally. We aimed to construct a model integrating information from radiomics and deep learning (DL) features to discriminate critical cases from severe cases of COVID-19 using computed tomography (CT) images. METHODS: We retrospectively enrolled 217 patients from three centers in China, including 82 patients with severe disease and 135 with critical disease. Patients were randomly divided into a training cohort (n = 174) and a test cohort (n = 43). We extracted 102 3-dimensional radiomic features from automatically segmented lung volume and selected the significant features. We also developed a 3-dimensional DL network based on center-cropped slices. Using multivariable logistic regression, we then created a merged model based on significant radiomic features and DL scores. We employed the area under the receiver operating characteristic curve (AUC) to evaluate the model's performance. We then conducted cross validation, stratified analysis, survival analysis, and decision curve analysis to evaluate the robustness of our method. RESULTS: The merged model can distinguish critical patients with AUCs of 0.909 (95% confidence interval [CI]: 0.859-0.952) and 0.861 (95% CI: 0.753-0.968) in the training and test cohorts, respectively. Stratified analysis indicated that our model was not affected by sex, age, or chronic disease. Moreover, the results of the merged model showed a strong correlation with patient outcomes. SIGNIFICANCE: A model combining radiomic and DL features of the lung could help distinguish critical cases from severe cases of COVID-19.",2020,10.1109/jbhi.2020.3036722,diagnosis,True
Classification of the COVID-19 infected patients using DenseNet201 based deep transfer learning,"Deep learning models are widely used in the automatic analysis of radiological images. These techniques can train the weights of networks on large datasets as well as fine tuning the weights of pre-trained networks on small datasets. Due to the small COVID-19 dataset available, the pre-trained neural networks can be used for diagnosis of coronavirus. However, these techniques applied on chest CT image is very limited till now. Hence, the main aim of this paper to use the pre-trained deep learning architectures as an automated tool to detection and diagnosis of COVID-19 in chest CT. A DenseNet201 based deep transfer learning (DTL) is proposed to classify the patients as COVID infected or not i.e. COVID-19 (+) or COVID (-). The proposed model is utilized to extract features by using its own learned weights on the ImageNet dataset along with a convolutional neural structure. Extensive experiments are performed to evaluate the performance of the propose DTL model on COVID-19 chest CT scan images. Comparative analyses reveal that the proposed DTL based COVID-19 classification model outperforms the competitive approaches.Communicated by Ramaswamy H. Sarma.",2021,10.1080/07391102.2020.1788642,diagnosis,True
Classification of Volumetric Images Using Multi-Instance Learning and Extreme Value Theorem,"Volumetric imaging is an essential diagnostic tool for medical practitioners. The use of popular techniques such as convolutional neural networks (CNN) for analysis of volumetric images is constrained by the availability of detailed (with local annotations) training data and GPU memory. In this paper, the volumetric image classification problem is posed as a multi-instance classification problem and a novel method is proposed to adaptively select positive instances from positive bags during the training phase. This method uses the extreme value theory to model the feature distribution of the images without a pathology and use it to identify positive instances of an imaged pathology. The experimental results, on three separate image classification tasks (i.e. classify retinal OCT images according to the presence or absence of fluid build-ups, emphysema detection in pulmonary 3D-CT images and detection of cancerous regions in 2D histopathology images) show that the proposed method produces classifiers that have similar performance to fully supervised methods and achieves the state of the art performance in all examined test cases.",2020,10.1109/tmi.2019.2936244,diagnosis,True
Classifying chest CT images as COVID-19 positive/negative using a convolutional neural network ensemble model and uniform experimental design method,"BACKGROUND: To classify chest computed tomography (CT) images as positive or negative for coronavirus disease 2019 (COVID-19) quickly and accurately, researchers attempted to develop effective models by using medical images. RESULTS: A convolutional neural network (CNN) ensemble model was developed for classifying chest CT images as positive or negative for COVID-19. To classify chest CT images acquired from COVID-19 patients, the proposed COVID19-CNN ensemble model combines the use of multiple trained CNN models with a majority voting strategy. The CNN models were trained to classify chest CT images by transfer learning from well-known pre-trained CNN models and by applying their algorithm hyperparameters as appropriate. The combination of algorithm hyperparameters for a pre-trained CNN model was determined by uniform experimental design. The chest CT images (405 from COVID-19 patients and 397 from healthy patients) used for training and performance testing of the COVID19-CNN ensemble model were obtained from an earlier study by Hu in 2020. Experiments showed that, the COVID19-CNN ensemble model achieved 96.7% accuracy in classifying CT images as COVID-19 positive or negative, which was superior to the accuracies obtained by the individual trained CNN models. Other performance measures (i.e., precision, recall, specificity, and F(1)-score) obtained bythe COVID19-CNN ensemble model were higher than those obtained by individual trained CNN models. CONCLUSIONS: The COVID19-CNN ensemble model had superior accuracy and excellent capability in classifying chest CT images as COVID-19 positive or negative.",2021,10.1186/s12859-021-04083-x,diagnosis,True
"Clinical and laboratory data, radiological structured report findings and quantitative evaluation of lung involvement on baseline chest CT in COVID-19 patients to predict prognosis","OBJECTIVE: To evaluate by means of regression models the relationships between baseline clinical and laboratory data and lung involvement on baseline chest CT and to quantify the thoracic disease using an artificial intelligence tool and a visual scoring system to predict prognosis in patients with COVID-19 pneumonia. MATERIALS AND METHODS: This study included 103 (41 women and 62 men; 68.8 years of mean age-range, 29-93 years) with suspicious COVID-19 viral infection evaluated by reverse transcription real-time fluorescence polymerase chain reaction (RT-PCR) test. All patients underwent CT examinations at the time of admission in addition to clinical and laboratory findings recording. All chest CT examinations were reviewed using a structured report. Moreover, using an artificial intelligence tool we performed an automatic segmentation on CT images based on Hounsfield unit to calculate residual healthy lung parenchyma, ground-glass opacities (GGO), consolidations and emphysema volumes for both right and left lungs. Two expert radiologists, in consensus, attributed at the CT pulmonary disease involvement a severity score using a scale of 5 levels; the score was attributed for GGO and consolidation for each lung, and then, an overall radiological severity visual score was obtained summing the single score. Univariate and multivariate regression analysis was performed. RESULTS: Symptoms and comorbidities did not show differences statistically significant in terms of patient outcome. Instead, SpO2 was significantly lower in patients hospitalized in critical conditions or died while age, HS CRP, leukocyte count, neutrophils, LDH, d-dimer, troponin, creatinine and azotemia, ALT, AST and bilirubin values were significantly higher. GGO and consolidations were the main CT patterns (a variable combination of GGO and consolidations was found in 87.8% of patients). CT COVID-19 disease was prevalently bilateral (77.6%) with peripheral distribution (74.5%) and multiple lobes localizations (52.0%). Consolidation, emphysema and residual healthy lung parenchyma volumes showed statistically significant differences in the three groups of patients based on outcome (patients discharged at home, patients hospitalized in stable conditions and patient hospitalized in critical conditions or died) while GGO volume did not affect the patient's outcome. Moreover, the overall radiological severity visual score (cutoff ≥ 8) was a predictor of patient outcome. The highest value of R-squared (R(2) = 0.93) was obtained by the model that combines clinical/laboratory findings at CT volumes. The highest accuracy was obtained by clinical/laboratory and CT findings model with a sensitivity, specificity and accuracy, respectively, of 88%, 78% and 81% to predict discharged/stable patients versus critical/died patients. CONCLUSION: In conclusion, both CT visual score and computerized software-based quantification of the consolidation, emphysema and residual healthy lung parenchyma on chest CT images were independent predictors of outcome in patients with COVID-19 pneumonia.",2021,10.1007/s11547-020-01293-w,prognosis,True
Clinical evaluation of a deep-learning-based computer-aided detection system for the detection of pulmonary nodules in a large teaching hospital,"AIM: To evaluate a deep-learning-based computer-aided detection (DL-CAD) software system for pulmonary nodule detection on computed tomography (CT) images and assess its added value in the clinical practice of a large teaching hospital. MATERIALS AND METHODS: A retrospective analysis was performed of 145 chest CT examinations by comparing the output of the DL-CAD software with a reference standard based on the consensus reading of three radiologists. For every nodule in each scan, the location, composition, and maximum diameter in the axial plane were recorded. The subgroup of chest CT examinations (n = 97) without any nodules was used to determine the negative predictive value at the given clinical sensitivity threshold setting. RESULTS: The radiologists found 91 nodules and the CAD system 130 nodules of which 80 were true positive. The measured sensitivity was 88% and the mean false-positive rate was 1.04 false positives/scan. The negative predictive value was 95%. For 23 nodules, there was a size discrepancy of which 19 (83%) were measured smaller by the radiologist. The agreement of nodule composition between the CAD results and the reference standard was 95%. CONCLUSIONS: The present study found a sensitivity of 88% and a false-positive rate of 1.04 false positives/scan, which match the vendor specification. Together with the measured negative predictive value of 95% the system performs very well; however, these rates are still not good enough to replace the radiologist, even for the specific task of nodule detection. Furthermore, a surprisingly high rate of overestimation of nodule size was observed, which can lead to too many follow-up examinations.",2021,10.1016/j.crad.2021.07.012,diagnosis,True
Clinical evaluation of atlas and deep learning based automatic contouring for lung cancer,"BACKGROUND AND PURPOSE: Contouring of organs at risk (OARs) is an important but time consuming part of radiotherapy treatment planning. The aim of this study was to investigate whether using institutional created software-generated contouring will save time if used as a starting point for manual OAR contouring for lung cancer patients. MATERIAL AND METHODS: Twenty CT scans of stage I-III NSCLC patients were used to compare user adjusted contours after an atlas-based and deep learning contour, against manual delineation. The lungs, esophagus, spinal cord, heart and mediastinum were contoured for this study. The time to perform the manual tasks was recorded. RESULTS: With a median time of 20 min for manual contouring, the total median time saved was 7.8 min when using atlas-based contouring and 10 min for deep learning contouring. Both atlas based and deep learning adjustment times were significantly lower than manual contouring time for all OARs except for the left lung and esophagus of the atlas based contouring. CONCLUSIONS: User adjustment of software generated contours is a viable strategy to reduce contouring time of OARs for lung radiotherapy while conforming to local clinical standards. In addition, deep learning contouring shows promising results compared to existing solutions.",2018,10.1016/j.radonc.2017.11.012,treatment,True
Clinical Factors and Quantitative CT Parameters Associated With ICU Admission in Patients of COVID-19 Pneumonia: A Multicenter Study,"The clinical spectrum of COVID-19 pneumonia is varied. Thus, it is important to identify risk factors at an early stage for predicting deterioration that require transferring the patients to ICU. A retrospective multicenter study was conducted on COVID-19 patients admitted to designated hospitals in China from Jan 17, 2020, to Feb 17, 2020. Clinical presentation, laboratory data, and quantitative CT parameters were also collected. The result showed that increasing risks of ICU admission were associated with age > 60 years (odds ratio [OR], 12.72; 95% confidence interval [CI], 2.42-24.61; P = 0.032), coexisting conditions (OR, 5.55; 95% CI, 1.59-19.38; P = 0.007) and CT derived total opacity percentage (TOP) (OR, 8.0; 95% CI, 1.45-39.29; P = 0.016). In conclusion, older age, coexisting conditions, larger TOP at the time of hospital admission are associated with ICU admission in patients with COVID-19 pneumonia. Early monitoring the progression of the disease and implementing appropriate therapies are warranted.",2021,10.3389/fpubh.2021.648360,prognosis,True
Clinical impact of a deep learning system for automated detection of missed pulmonary nodules on routine body computed tomography including the chest region,"OBJECTIVES: To evaluate the clinical impact of a deep learning system (DLS) for automated detection of pulmonary nodules on computed tomography (CT) images as a second reader. METHODS: This single-centre retrospective study screened 21,150 consecutive body CT studies from September 2018 to February 2019. Pulmonary nodules detected by the DLS on axial CT images but not mentioned in initial radiology reports were flagged. Flagged images were scored by four board-certificated radiologists each with at least 5 years of experience. Nodules with scores of 2 (understandable miss) or 3 (should not be missed) were then categorised as unlikely to be clinically significant (2a or 3a) or likely to be clinically significant (2b or 3b) according to the 2017 Fleischner guidelines for pulmonary nodules. The miss rate was defined as the total number of studies receiving scores of 2 or 3 divided by total screened studies. RESULTS: Among 172 nodules flagged by the DLS, 60 (35%) missed nodules were confirmed by the radiologists. The nodules were further categorised as 2a, 2b, 3a, and 3b in 24, 14, 10, and 12 studies, respectively, with an overall positive predictive value of 35%. Missed pulmonary nodules were identified in 0.3% of all CT images, and one-third of these lesions were considered clinically significant. CONCLUSIONS: Use of DLS-assisted automated detection as a second reader can identify missed pulmonary nodules, some of which may be clinically significant. CLINICAL RELEVANCE/APPLICATION: Use of DLS to help radiologists detect pulmonary lesions may improve patient care. KEY POINTS: • DLS-assisted automated detection as a second reader is feasible in a large consecutive cohort. • Performance of combined radiologists and DLS was better than DLS or radiologists alone. • Pulmonary nodules were missed more frequently in abdomino-pelvis CT than the thoracic CT.",2022,10.1007/s00330-021-08412-9,diagnosis,True
Clinical suitability of deep learning based synthetic CTs for adaptive proton therapy of lung cancer,"PURPOSE: Adaptive proton therapy (APT) of lung cancer patients requires frequent volumetric imaging of diagnostic quality. Cone-beam CT (CBCT) can provide these daily images, but x-ray scattering limits CBCT-image quality and hampers dose calculation accuracy. The purpose of this study was to generate CBCT-based synthetic CTs using a deep convolutional neural network (DCNN) and investigate image quality and clinical suitability for proton dose calculations in lung cancer patients. METHODS: A dataset of 33 thoracic cancer patients, containing CBCTs, same-day repeat CTs (rCT), planning-CTs (pCTs), and clinical proton treatment plans, was used to train and evaluate a DCNN with and without a pCT-based correction method. Mean absolute error (MAE), mean error (ME), peak signal-to-noise ratio, and structural similarity were used to quantify image quality. The evaluation of clinical suitability was based on recalculation of clinical proton treatment plans. Gamma pass ratios, mean dose to target volumes and organs at risk, and normal tissue complication probabilities (NTCP) were calculated. Furthermore, proton radiography simulations were performed to assess the HU-accuracy of sCTs in terms of range errors. RESULTS: On average, sCTs without correction resulted in a MAE of 34 ± 6 HU and ME of 4 ± 8 HU. The correction reduced the MAE to 31 ± 4HU (ME to 2 ± 4HU). Average 3%/3 mm gamma pass ratios increased from 93.7% to 96.8%, when the correction was applied. The patient specific correction reduced mean proton range errors from 1.5 to 1.1 mm. Relative mean target dose differences between sCTs and rCT were below ± 0.5% for all patients and both synthetic CTs (with/without correction). NTCP values showed high agreement between sCTs and rCT (<2%). CONCLUSION: CBCT-based sCTs can enable accurate proton dose calculations for APT of lung cancer patients. The patient specific correction method increased the image quality and dosimetric accuracy but had only a limited influence on clinically relevant parameters.",2021,10.1002/mp.15333,combined,True
Cloud-Based Lung Tumor Detection and Stage Classification Using Deep Learning Techniques,"Artificial intelligence (AI), Internet of Things (IoT), and the cloud computing have recently become widely used in the healthcare sector, which aid in better decision-making for a radiologist. PET imaging or positron emission tomography is one of the most reliable approaches for a radiologist to diagnosing many cancers, including lung tumor. In this work, we proposed stage classification of lung tumor which is a more challenging task in computer-aided diagnosis. As a result, a modified computer-aided diagnosis is being considered as a way to reduce the heavy workloads and second opinion to radiologists. In this paper, we present a strategy for classifying and validating different stages of lung tumor progression, as well as a deep neural model and data collection using cloud system for categorizing phases of pulmonary illness. The proposed system presents a Cloud-based Lung Tumor Detector and Stage Classifier (Cloud-LTDSC) as a hybrid technique for PET/CT images. The proposed Cloud-LTDSC initially developed the active contour model as lung tumor segmentation, and multilayer convolutional neural network (M-CNN) for classifying different stages of lung cancer has been modelled and validated with standard benchmark images. The performance of the presented technique is evaluated using a benchmark image LIDC-IDRI dataset of 50 low doses and also utilized the lung CT DICOM images. Compared with existing techniques in the literature, our proposed method achieved good result for the performance metrics accuracy, recall, and precision evaluated. Under numerous aspects, our proposed approach produces superior outcomes on all of the applied dataset images. Furthermore, the experimental result achieves an average lung tumor stage classification accuracy of 97%-99.1% and an average of 98.6% which is significantly higher than the other existing techniques.",2022,10.1155/2022/4185835,prognosis,True
CNN models discriminating between pulmonary micro-nodules and non-nodules from CT images,"BACKGROUND: Early and automatic detection of pulmonary nodules from CT lung screening is the prerequisite for precise management of lung cancer. However, a large number of false positives appear in order to increase the sensitivity, especially for detecting micro-nodules (diameter < 3 mm), which increases the radiologists' workload and causes unnecessary anxiety for the patients. To decrease the false positive rate, we propose to use CNN models to discriminate between pulmonary micro-nodules and non-nodules from CT image patches. METHODS: A total of 13,179 micro-nodules and 21,315 non-nodules marked by radiologists are extracted with three different patch sizes (16 × 16, 32 × 32 and 64 × 64) from LIDC/IDRI database and used in the experiments. Three CNN models with different depths (1, 2 or 4 convolutional layers) are designed; their performances are evaluated by the fivefold cross-validation in term of the accuracy, area under the curve (AUC), F-score and sensitivity. The network parameters are also optimized. RESULTS: It is found that the performance of the CNN models is greatly dependent on the patches size and the number of convolutional layers. The CNN model with two convolutional layers presented the best performance in case of 32 × 32 patches size, achieving an accuracy of 88.28%, an AUC of 0.87, a F-score of 83.45% and a sensitivity of 83.82%. CONCLUSIONS: The CNN models with appropriate depth and size of image patches can effectively discriminate between pulmonary micro-nodules and non-nodules, and reduce the false positives and help manage lung cancer precisely.",2018,10.1186/s12938-018-0529-x,combined,True
CO-IRv2: Optimized InceptionResNetV2 for COVID-19 detection from chest CT images,"This paper focuses on the application of deep learning (DL) in the diagnosis of coronavirus disease (COVID-19). The novelty of this work is in the introduction of optimized InceptionResNetV2 for COVID-19 (CO-IRv2) method. A part of the CO-IRv2 scheme is derived from the concepts of InceptionNet and ResNet with hyperparameter tuning, while the remaining part is a new architecture consisting of a global average pooling layer, batch normalization, dense layers, and dropout layers. The proposed CO-IRv2 is applied to a new dataset of 2481 computed tomography (CT) images formed by collecting two independent datasets. Data resizing and normalization are performed, and the evaluation is run up to 25 epochs. Various performance metrics, including precision, recall, accuracy, F1-score, area under the receiver operating characteristics (AUC) curve are used as performance metrics. The effectiveness of three optimizers known as Adam, Nadam and RMSProp are evaluated in classifying suspected COVID-19 patients and normal people. Results show that for CO-IRv2 and for CT images, the obtained accuracies of Adam, Nadam and RMSProp optimizers are 94.97%, 96.18% and 96.18%, respectively. Furthermore, it is shown here that for the case of CT images, CO-IRv2 with Nadam optimizer has better performance than existing DL algorithms in the diagnosis of COVID-19 patients. Finally, CO-IRv2 is applied to an X-ray dataset of 1662 images resulting in a classification accuracy of 99.40%.",2021,10.1371/journal.pone.0259179,diagnosis,True
Co-occurrence of Local Anisotropic Gradient Orientations (CoLlAGe): A new radiomics descriptor,"In this paper, we introduce a new radiomic descriptor, Co-occurrence of Local Anisotropic Gradient Orientations (CoLlAGe) for capturing subtle differences between benign and pathologic phenotypes which may be visually indistinguishable on routine anatomic imaging. CoLlAGe seeks to capture and exploit local anisotropic differences in voxel-level gradient orientations to distinguish similar appearing phenotypes. CoLlAGe involves assigning every image voxel an entropy value associated with the co-occurrence matrix of gradient orientations computed around every voxel. The hypothesis behind CoLlAGe is that benign and pathologic phenotypes even though they may appear similar on anatomic imaging, will differ in their local entropy patterns, in turn reflecting subtle local differences in tissue microarchitecture. We demonstrate CoLlAGe's utility in three clinically challenging classification problems: distinguishing (1) radiation necrosis, a benign yet confounding effect of radiation treatment, from recurrent tumors on T1-w MRI in 42 brain tumor patients, (2) different molecular sub-types of breast cancer on DCE-MRI in 65 studies and (3) non-small cell lung cancer (adenocarcinomas) from benign fungal infection (granulomas) on 120 non-contrast CT studies. For each of these classification problems, CoLlAGE in conjunction with a random forest classifier outperformed state of the art radiomic descriptors (Haralick, Gabor, Histogram of Gradient Orientations).",2016,10.1038/srep37241,diagnosis,True
CoLe-CNN+: Context learning - Convolutional neural network for COVID-19-Ground-Glass-Opacities detection and segmentation,"BACKGROUND AND OBJECTIVE: The most common tool for population-wide COVID-19 identification is the Reverse Transcription-Polymerase Chain Reaction test that detects the presence of the virus in the throat (or sputum) in swab samples. This test has a sensitivity between 59% and 71%. However, this test does not provide precise information regarding the extension of the pulmonary infection. Moreover, it has been proven that through the reading of a computed tomography (CT) scan, a clinician can provide a more complete perspective of the severity of the disease. Therefore, we propose a comprehensive system for fully-automated COVID-19 detection and lesion segmentation from CT scans, powered by deep learning strategies to support decision-making process for the diagnosis of COVID-19. METHODS: In the workflow proposed, the input CT image initially goes through lung delineation, then COVID-19 detection and finally lesion segmentation. The chosen neural network has a U-shaped architecture using a newly introduced Multiple Convolutional Layers structure, that produces a lung segmentation mask within a novel pipeline for direct COVID-19 detection and segmentation. In addition, we propose a customized loss function that guarantees an optimal balance on average between sensitivity and precision. RESULTS: Lungs' segmentation results show a sensitivity near 99% and Dice-score of 97%. No false positives were observed in the detection network after 10 different runs with an average accuracy of 97.1%. The average accuracy for lesion segmentation was approximately 99%. Using UNet as a benchmark, we compared our results with several other techniques proposed in the literature, obtaining the largest improvement over the UNet outcomes. CONCLUSIONS: The method proposed in this paper outperformed the state-of-the-art methods for COVID-19 lesion segmentation from CT images, and improved by 38.2% the results for F1-score of UNet. The high accuracy observed in this work opens up a wide range of possible applications of our algorithm in other fields related to medical image segmentation.",2021,10.1016/j.compbiomed.2021.104689,diagnosis,True
Combination of computed tomography imaging-based radiomics and clinicopathological characteristics for predicting the clinical benefits of immune checkpoint inhibitors in lung cancer,"BACKGROUND: In this study, we tested whether a combination of radiomic features extracted from baseline pre-immunotherapy computed tomography (CT) images and clinicopathological characteristics could be used as novel noninvasive biomarkers for predicting the clinical benefits of non-small cell lung cancer (NSCLC) patients treated with immune checkpoint inhibitors (ICIs). METHODS: The data from 92 consecutive patients with lung cancer who had been treated with ICIs were retrospectively analyzed. In total, 88 radiomic features were selected from the pretreatment CT images for the construction of a random forest model. Radiomics model 1 was constructed based on the Rad-score. Using multivariate logistic regression analysis, the Rad-score and significant predictors were integrated into a single predictive model (radiomics nomogram model 1) to predict the durable clinical benefit (DCB) of ICIs. Radiomics model 2 was developed based on the same Rad-score as radiomics model 1.Using multivariate Cox proportional hazards regression analysis, the Rad-score, and independent risk factors, radiomics nomogram model 2 was constructed to predict the progression-free survival (PFS). RESULTS: The models successfully predicted the patients who would benefit from ICIs. For radiomics model 1, the area under the receiver operating characteristic curve values for the training and validation cohorts were 0.848 and 0.795, respectively, whereas for radiomics nomogram model 1, the values were 0.902 and 0.877, respectively. For the PFS prediction, the Harrell's concordance indexes for the training and validation cohorts were 0.717 and 0.760, respectively, using radiomics model 2, whereas they were 0.749 and 0.791, respectively, using radiomics nomogram model 2. CONCLUSIONS: CT-based radiomic features and clinicopathological factors can be used prior to the initiation of immunotherapy for identifying NSCLC patients who are the most likely to benefit from the therapy. This could guide the individualized treatment strategy for advanced NSCLC.",2021,10.1186/s12931-021-01780-2,treatment,True
Combination of computer-aided detection algorithms for automatic lung nodule identification,"PURPOSE: The aim of this work is to evaluate the potential of combining different computer-aided detection (CADe) methods to increase the actual support for radiologists of automated systems in the identification of pulmonary nodules in CT scans. METHODS: The outputs of three different CADe systems developed by researchers of the Italian MAGIC-5 collaboration were combined. The systems are: the CAMCADe (based on a Channeler-Ant-Model which segments vessel tree and nodule candidates and a neural classifier), the RGVPCADe (a Region-Growing- Volume-Plateau algorithm detects nodule candidates and a neural network reduces false positives); the VBNACADe (two dedicated procedures, based respectively on a 3D dot-enhancement algorithm and on intersections of pleura surface normals, identifies internal and juxtapleural nodules, and a Voxel-Based-Neural-Approach reduces false positives. A dedicated OsiriX plugin implemented with the Cocoa environments of MacOSX allows annotating nodules and visualizing singles and combined CADe findings. RESULTS: The combined CADe has been tested on thin slice (lower than 2 mm) CTs of the LIDC public research database and the results have been compared with those obtained by the single systems. The FROC (Free Receiver Operating Characteristic) curves show better results than the best of the single approaches. CONCLUSIONS: Has been demonstrated that the combination of different approaches offers better results than each single CADe system. A clinical validation of the combined CADe as second reader is being addressed by means of the dedicated OsiriX plugin.",2012,10.1007/s11548-011-0637-6,diagnosis,True
Combination of Deep Learning-Based Denoising and Iterative Reconstruction for Ultra-Low-Dose CT of the Chest: Image Quality and Lung-RADS Evaluation,"OBJECTIVE. The objective of our study was to assess the effect of the combination of deep learning-based denoising (DLD) and iterative reconstruction (IR) on image quality and Lung Imaging Reporting and Data System (Lung-RADS) evaluation on chest ultra-low-dose CT (ULDCT). MATERIALS AND METHODS. Forty-one patients with 252 nodules were evaluated retrospectively. All patients underwent ULDCT (mean ± SD, 0.19 ± 0.01 mSv) and standard-dose CT (SDCT) (6.46 ± 2.28 mSv). ULDCT images were reconstructed using hybrid iterative reconstruction (HIR) and model-based iterative reconstruction (MBIR), and they were postprocessed using DLD (i.e., HIR-DLD and MBIR-DLD). SDCT images were reconstructed using filtered back projection. Three independent radiologists subjectively evaluated HIR, HIR-DLD, MBIR, and MBIR-DLD images on a 5-point scale in terms of noise, streak artifact, nodule edge, clarity of small vessels, homogeneity of the normal lung parenchyma, and overall image quality. Two radiologists independently evaluated the nodules according to Lung-RADS using HIR, MBIR, HIR-DLD, and MBIR-DLD ULDCT images and SDCT images. The median scores for subjective analysis were analyzed using Wilcoxon signed rank test with Bonferroni correction. Intraobserver agreement for Lung-RADS category between ULDCT and SDCT was evaluated using the weighted kappa coefficient. RESULTS. In the subjective analysis, ULDCT with DLD showed significantly better scores than did ULDCT without DLD (p < 0.001), and MBIR-DLD showed the best scores among the ULDCT images (p < 0.001) for all items. In the Lung-RADS evaluation, HIR showed fair or moderate agreement (reader 1 and reader 2: κw = 0.46 and 0.32, respectively); MBIR, moderate or good agreement (κw = 0.68 and 0.57); HIR-DLD, moderate agreement (κw = 0.53 and 0.48); and MBIR-DLD, good agreement (κw = 0.70 and 0.72). CONCLUSION. DLD improved the image quality of both HIR and MBIR on ULDCT. MBIR-DLD was superior to HIR_DLD for image quality and for Lung-RADS evaluation.",2020,10.2214/ajr.19.22680,combined,True
Combination of radiological and gray level co-occurrence matrix textural features used to distinguish solitary pulmonary nodules by computed tomography,"The objective of this study was to investigate the method of the combination of radiological and textural features for the differentiation of malignant from benign solitary pulmonary nodules by computed tomography. Features including 13 gray level co-occurrence matrix textural features and 12 radiological features were extracted from 2,117 CT slices, which came from 202 (116 malignant and 86 benign) patients. Lasso-type regularization to a nonlinear regression model was applied to select predictive features and a BP artificial neural network was used to build the diagnostic model. Eight radiological and two textural features were obtained after the Lasso-type regularization procedure. Twelve radiological features alone could reach an area under the ROC curve (AUC) of 0.84 in differentiating between malignant and benign lesions. The 10 selected characters improved the AUC to 0.91. The evaluation results showed that the method of selecting radiological and textural features appears to yield more effective in the distinction of malignant from benign solitary pulmonary nodules by computed tomography.",2013,10.1007/s10278-012-9547-6,diagnosis,True
Combining chest X-rays and electronic health record (EHR) data using machine learning to diagnose acute respiratory failure,"OBJECTIVE: When patients develop acute respiratory failure (ARF), accurately identifying the underlying etiology is essential for determining the best treatment. However, differentiating between common medical diagnoses can be challenging in clinical practice. Machine learning models could improve medical diagnosis by aiding in the diagnostic evaluation of these patients. MATERIALS AND METHODS: Machine learning models were trained to predict the common causes of ARF (pneumonia, heart failure, and/or chronic obstructive pulmonary disease [COPD]). Models were trained using chest radiographs and clinical data from the electronic health record (EHR) and applied to an internal and external cohort. RESULTS: The internal cohort of 1618 patients included 508 (31%) with pneumonia, 363 (22%) with heart failure, and 137 (8%) with COPD based on physician chart review. A model combining chest radiographs and EHR data outperformed models based on each modality alone. Models had similar or better performance compared to a randomly selected physician reviewer. For pneumonia, the combined model area under the receiver operating characteristic curve (AUROC) was 0.79 (0.77-0.79), image model AUROC was 0.74 (0.72-0.75), and EHR model AUROC was 0.74 (0.70-0.76). For heart failure, combined: 0.83 (0.77-0.84), image: 0.80 (0.71-0.81), and EHR: 0.79 (0.75-0.82). For COPD, combined: AUROC = 0.88 (0.83-0.91), image: 0.83 (0.77-0.89), and EHR: 0.80 (0.76-0.84). In the external cohort, performance was consistent for heart failure and increased for COPD, but declined slightly for pneumonia. CONCLUSIONS: Machine learning models combining chest radiographs and EHR data can accurately differentiate between common causes of ARF. Further work is needed to determine how these models could act as a diagnostic aid to clinicians in clinical settings.",2022,10.1093/jamia/ocac030,diagnosis,False
Combining computed tomography and biologically effective dose in radiomics and deep learning improves prediction of tumor response to robotic lung stereotactic body radiation therapy,"PURPOSE: The aim of this study is to improve the performance of machine learning (ML) models in predicting response of non-small cell lung cancer (NSCLC) to stereotactic body radiation therapy (SBRT) by integrating image features from pre-treatment computed tomography (CT) with features from the biologically effective dose (BED) distribution. MATERIALS AND METHODS: Image features, consisting of crafted radiomic features or machine-learned features extracted using a convolutional neural network, were calculated from pre-treatment CT data and from dose distributions converted into BED for 80 NSCLC lesions over 76 patients treated with robotic guided SBRT. ML models using different combinations of features were trained to predict complete or partial response according to response criteria in solid tumors, including radiomics CT (Rad(CT) ), radiomics CT and BED (Rad(CT,BED) ), deep learning (DL) CT (DL(CT) ), and DL CT and BED (DL(CT,BED) ). Training of ML included feature selection by neighborhood component analysis followed by ensemble ML using robust boosting. A model was considered as acceptable when the sum of average sensitivity and specificity on test data in repeated cross validations was at least 1.5. RESULTS: Complete or partial response occurred in 58 out of 80 lesions. The best models to predict the tumor response were those using BED variables, achieving significantly better area under curve (AUC) and accuracy than those using only features from CT, including a Rad(CT,BED) model using three radiomic features from BED, which scored an accuracy of 0.799 (95% confidence intervals (0.75-0.85)) and AUC of 0.773 (0.688-0.846), and a DL(CT,BED) model also using three variables with an accuracy of 0.798 (0.649-0.829) and AUC of 0.812 (0.755-0.867). CONCLUSION: According to our results, the inclusion of BED features improves the response prediction of ML models for lung cancer patients undergoing SBRT, regardless of the use of radiomic or DL features.",2021,10.1002/mp.15178,prognosis,True
Combining Deep Learning and Knowledge-driven Reasoning for Chest X-Ray Findings Detection,"The application of deep learning algorithms in medical imaging analysis is a steadily growing research area. While deep learning methods are thriving in the medical domain, they seldom utilize the rich knowledge associated with connected radiology reports. The knowledge derived from these reports can be utilized to enhance the performance of deep learning models. In this work, we used a comprehensive chest X-ray findings vocabulary to automatically annotate an extensive collection of chest X-rays using associated radiology reports and a vocabulary-driven concept annotation algorithm. The annotated X-rays are used to train a deep neural network classifier for finding detection. Finally, we developed a knowledge-driven reasoning algorithm that leverages knowledge learned from X-ray reports to improve upon the deep learning module's performance on finding detection. Our results suggest that combining deep learning and knowledge from radiology reports in a hybrid framework can significantly enhance overall performance in the CXR finding detection.",2020,,diagnosis,False
Combining machine learning and texture analysis to differentiate mediastinal lymph nodes in lung cancer patients,"Evaluate whether texture analysis associated with machine learning approaches could differentiate between malignant and benign lymph nodes. A total 18 patients with lung cancer were selected, with 39 lymph nodes, being 15 malignant and 24 benign. Retrospective computed tomography scans were utilized both with and without contrast medium. The great differential of this work was the use of 15 textures from mediastinal lymph nodes, with five different physicians as operators. First and second order statistical textures such as gray level run length and co-occurrence matrix were extracted and applied to three different machine learning classifiers. The best machine learning classifier demonstrated a variability of less than 5% among operators. The support vector machine (SVM) classifier presented 95% of the area under the ROC curve (AUC) and 89% of sensitivity for sequences without contrast medium. SVM classifier presented 93% of AUC and 86% of sensitivity for sequences with contrast medium. Texture analysis and machine learning may be helpful in the differentiation between malign and benign lymph nodes. This study can aid the physician in diagnosis and staging of lymph nodes and potentially reduce the number of invasive analysis to histopathological confirmation.",2021,10.1007/s13246-021-00988-2,diagnosis,True
"Combining multi-scale feature fusion with multi-attribute grading, a CNN model for benign and malignant classification of pulmonary nodules","Lung cancer has the highest mortality rate of all cancers, and early detection can improve survival rates. In the recent years, low-dose CT has been widely used to detect lung cancer. However, the diagnosis is limited by the subjective experience of doctors. Therefore, the main purpose of this study is to use convolutional neural network to realize the benign and malignant classification of pulmonary nodules in CT images. We collected 1004 cases of pulmonary nodules from LIDC-IDRI dataset, among which 554 cases were benign and 450 cases were malignant. According to the doctors' annotates on the center coordinates of the nodules, two 3D CT image patches of pulmonary nodules with different scales were extracted. In this study, our work focuses on two aspects. Firstly, we constructed a multi-stream multi-task network (MSMT), which combined multi-scale feature with multi-attribute classification for the first time, and applied it to the classification of benign and malignant pulmonary nodules. Secondly, we proposed a new loss function to balance the relationship between different attributes. The final experimental results showed that our model was effective compared with the same type of study. The area under ROC curve, accuracy, sensitivity, and specificity were 0.979, 93.92%, 92.60%, and 96.25%, respectively.",2020,10.1007/s10278-020-00333-1,diagnosis,True
Commercial AI solutions in detecting COVID-19 pneumonia in chest CT: not yet ready for clinical implementation?,"OBJECTIVES: In response to the COVID-19 pandemic, many researchers have developed artificial intelligence (AI) tools to differentiate COVID-19 pneumonia from other conditions in chest CT. However, in many cases, performance has not been clinically validated. The aim of this study was to evaluate the performance of commercial AI solutions in differentiating COVID-19 pneumonia from other lung conditions. METHODS: Four commercial AI solutions were evaluated on a dual-center clinical dataset consisting of 500 CT studies; COVID-19 pneumonia was microbiologically proven in 50 of these. Sensitivity, specificity, positive and negative predictive values, and AUC were calculated. In a subgroup analysis, the performance of the AI solutions in differentiating COVID-19 pneumonia from other conditions was evaluated in CT studies with ground-glass opacities (GGOs). RESULTS: Sensitivity and specificity ranges were 62-96% and 31-80%, respectively. Negative and positive predictive values ranged between 82-99% and 19-25%, respectively. AUC was in the range 0.54-0.79. In CT studies with GGO, sensitivity remained unchanged. However, specificity was lower, and ranged between 15 and 53%. AUC for studies with GGO was in the range 0.54-0.69. CONCLUSIONS: This study highlights the variable specificity and low positive predictive value of AI solutions in diagnosing COVID-19 pneumonia in chest CT. However, one solution yielded acceptable values for sensitivity. Thus, with further improvement, commercial AI solutions currently under development have the potential to be integrated as alert tools in clinical routine workflow. Randomized trials are needed to assess the true benefits and also potential harms of the use of AI in image analysis. KEY POINTS: • Commercial AI solutions achieved a sensitivity and specificity ranging from 62 to 96% and from 31 to 80%, respectively, in identifying patients suspicious for COVID-19 in a clinical dataset. • Sensitivity remained within the same range, while specificity was even lower in subgroup analysis of CT studies with ground-glass opacities, and interrater agreement between the commercial AI solutions was minimal to nonexistent. • Thus, commercial AI solutions have the potential to be integrated as alert tools for the detection of patients with lung changes suspicious for COVID-19 pneumonia in a clinical routine workflow, if further improvement is made.",2022,10.1007/s00330-021-08409-4,diagnosis,True
Comparative evaluation of support vector machines for computer aided diagnosis of lung cancer in CT based on a multi-dimensional data set,"Lung cancer is one of the most common forms of cancer resulting in over a million deaths per year worldwide. In this paper, the usage of support vector machine (SVM) classification for lung cancer is investigated, presenting a systematic quantitative evaluation against Boosting, Decision trees, k-nearest neighbor, LASSO regressions, neural networks and random forests. A large database of 5984 regions of interest (ROIs) and 488 input features (including textural features, patient characteristics, and morphological features) were used to train the classifiers and evaluate for their performance. The evaluation for classifiers' performance was based on a tenfold cross validation framework, receiver operating characteristic curve (ROC), and Matthews correlation coefficient. Area under curve (AUC) of SVM, Boosting, Decision trees, k-nearest neighbor, LASSO, neural networks, random forests were 0.94, 0.86, 0.73, 0.72, 0.91, 0.92, and 0.85, respectively. It was proved that SVM classification offered significantly increased classification performance compared to the reference methods. This scheme may be used as an auxiliary tool to differentiate between benign and malignant SPNs of CT images in future.",2013,10.1016/j.cmpb.2013.04.016,diagnosis,True
Comparing different machine learning techniques for predicting COVID-19 severity,"BACKGROUND: Coronavirus disease 2019 (COVID-19) is still ongoing spreading globally, machine learning techniques were used in disease diagnosis and to predict treatment outcomes, which showed favorable performance. The present study aims to predict COVID-19 severity at admission by different machine learning techniques including random forest (RF), support vector machine (SVM), and logistic regression (LR). Feature importance to COVID-19 severity were further identified. METHODS: A retrospective design was adopted in the JinYinTan Hospital from January 26 to March 28, 2020, eighty-six demographic, clinical, and laboratory features were selected with LassoCV method, Spearman's rank correlation, experts' opinions, and literature evaluation. RF, SVM, and LR were performed to predict severe COVID-19, the performance of the models was compared by the area under curve (AUC). Additionally, feature importance to COVID-19 severity were analyzed by the best performance model. RESULTS: A total of 287 patients were enrolled with 36.6% severe cases and 63.4% non-severe cases. The median age was 60.0 years (interquartile range: 49.0-68.0 years). Three models were established using 23 features including 1 clinical, 1 chest computed tomography (CT) and 21 laboratory features. Among three models, RF yielded better overall performance with the highest AUC of 0.970 than SVM of 0.948 and LR of 0.928, RF also achieved a favorable sensitivity of 96.7%, specificity of 69.5%, and accuracy of 84.5%. SVM had sensitivity of 93.9%, specificity of 79.0%, and accuracy of 88.5%. LR also achieved a favorable sensitivity of 92.3%, specificity of 72.3%, and accuracy of 85.2%. Additionally, chest-CT had highest importance to illness severity, and the following features were neutrophil to lymphocyte ratio, lactate dehydrogenase, and D-dimer, respectively. CONCLUSIONS: Our results indicated that RF could be a useful predictive tool to identify patients with severe COVID-19, which may facilitate effective care and further optimize resources.",2022,10.1186/s40249-022-00946-4,diagnosis,True
Comparison of (18)F-FDG avidity at PET of benign and malignant pure ground-glass opacities: a paradox? Part II: artificial neural network integration of the PET/CT characteristics of ground-glass opacities to predict their likelihood of malignancy,"AIM: To assess the ability of artificial neural networks (ANNs) to predict the likelihood of malignancy of pure ground-glass opacities (GGOs), using observations from computed tomography (CT) and 2-[(18)F]-fluoro-2-deoxy-d-glucose (FDG) positron-emission tomography (PET) images and relevant clinical information. MATERIALS AND METHODS: One hundred and twenty-five cases of pure GGOs described in a previous article were used to train and evaluate the performance of an ANN to predict the likelihood of malignancy in each of the GGOs. Eighty-five cases selected randomly were used for training the network and the remaining 40 cases for testing. The ANN was constructed from the image data and basic clinical information. The predictions of the ANN were compared with blinded expert estimates of the likelihood of malignancy. RESULTS: The ANN showed excellent predictive value in estimating the likelihood of malignancy (AUC = 0.98±0.02). Employing the optimal cut-off point from the receiver operating characteristic (ROC) curve, the ANN correctly identified 11/11 malignant lesions (sensitivity 100%) and 27/29 benign lesions (specificity 93.1%). The expert readers found 23 lesions indeterminate and correctly identified 17 lesions as benign. CONCLUSION: ANNs have potential to improve diagnostic certainty in the classification of pure GGOs, based upon their CT appearance, intensity of FDG uptake, and relevant clinical information, and may therefore, be useful to help direct clinical and imaging follow-up.",2019,10.1016/j.crad.2019.04.024,diagnosis,True
Comparison of a radiomic biomarker with volumetric analysis for decoding tumour phenotypes of lung adenocarcinoma with different disease-specific survival,"OBJECTIVES: To compare a multi-feature-based radiomic biomarker with volumetric analysis in discriminating lung adenocarcinomas with different disease-specific survival on computed tomography (CT) scans. METHODS: This retrospective study obtained institutional review board approval and was Health Insurance Portability and Accountability Act (HIPAA) compliant. Pathologically confirmed lung adenocarcinoma (n = 431) manifested as subsolid nodules on CT were identified. Volume and percentage solid volume were measured by using a computer-assisted segmentation method. Radiomic features quantifying intensity, texture and wavelet were extracted from the segmented volume of interest (VOI). Twenty best features were chosen by using the Relief method and subsequently fed to a support vector machine (SVM) for discriminating adenocarcinoma in situ (AIS)/minimally invasive adenocarcinoma (MIA) from invasive adenocarcinoma (IAC). Performance of the radiomic signatures was compared with volumetric analysis via receiver-operating curve (ROC) analysis and logistic regression analysis. RESULTS: The accuracy of proposed radiomic signatures for predicting AIS/MIA from IAC achieved 80.5% with ROC analysis (Az value, 0.829; sensitivity, 72.1%; specificity, 80.9%), which showed significantly higher accuracy than volumetric analysis (69.5%, P = 0.049). Regression analysis showed that radiomic signatures had superior prognostic performance to volumetric analysis, with AIC values of 81.2% versus 70.8%, respectively. CONCLUSIONS: The radiomic tumour-phenotypes biomarker exhibited better diagnostic accuracy than traditional volumetric analysis in discriminating lung adenocarcinoma with different disease-specific survival. KEY POINTS: • Radiomic biomarker on CT was designed to identify phenotypes of lung adenocarcinoma • Built up radiomic signature for lung adenocarcinoma manifested as subsolid nodules • Retrospective study showed radiomic signature had greater diagnostic accuracy than volumetric analysis • Radiomics help to evaluate intratumour heterogeneity within lung adenocarcinoma • Medical decision can be given with more confidence.",2017,10.1007/s00330-017-4855-3,combined,True
Comparison of Artificial Intelligence-Based Fully Automatic Chest CT Emphysema Quantification to Pulmonary Function Testing,"OBJECTIVE. The purpose of this study was to evaluate an artificial intelligence (AI)-based prototype algorithm for fully automated quantification of emphysema on chest CT compared with pulmonary function testing (spirometry). MATERIALS AND METHODS. A total of 141 patients (72 women, mean age ± SD of 66.46 ± 9.7 years [range, 23-86 years]; 69 men, mean age of 66.72 ± 11.4 years [range, 27-91 years]) who underwent both chest CT acquisition and spirometry within 6 months were retrospectively included. The spirometry-based Tiffeneau index (TI; calculated as the ratio of forced expiratory volume in the first second to forced vital capacity) was used to measure emphysema severity; a value less than 0.7 was considered to indicate airway obstruction. Segmentation of the lung based on two different reconstruction methods was carried out by using a deep convolution image-to-image network. This multilayer convolutional neural network was combined with multilevel feature chaining and depth monitoring. To discriminate the output of the network from ground truth, an adversarial network was used during training. Emphysema was quantified using spatial filtering and attenuation-based thresholds. Emphysema quantification and TI were compared using the Spearman correlation coefficient. RESULTS. The mean TI for all patients was 0.57 ± 0.13. The mean percentages of emphysema using reconstruction methods 1 and 2 were 9.96% ± 11.87% and 8.04% ± 10.32%, respectively. AI-based emphysema quantification showed very strong correlation with TI (reconstruction method 1, ρ = -0.86; reconstruction method 2, ρ = -0.85; both p < 0.0001), indicating that AI-based emphysema quantification meaningfully reflects clinical pulmonary physiology. CONCLUSION. AI-based, fully automated emphysema quantification shows good correlation with TI, potentially contributing to an image-based diagnosis and quantification of emphysema severity.",2020,10.2214/ajr.19.21572,diagnosis,True
Comparison of Chest Radiograph Interpretations by Artificial Intelligence Algorithm vs Radiology Residents,"IMPORTANCE: Chest radiography is the most common diagnostic imaging examination performed in emergency departments (EDs). Augmenting clinicians with automated preliminary read assistants could help expedite their workflows, improve accuracy, and reduce the cost of care. OBJECTIVE: To assess the performance of artificial intelligence (AI) algorithms in realistic radiology workflows by performing an objective comparative evaluation of the preliminary reads of anteroposterior (AP) frontal chest radiographs performed by an AI algorithm and radiology residents. DESIGN, SETTING, AND PARTICIPANTS: This diagnostic study included a set of 72 findings assembled by clinical experts to constitute a full-fledged preliminary read of AP frontal chest radiographs. A novel deep learning architecture was designed for an AI algorithm to estimate the findings per image. The AI algorithm was trained using a multihospital training data set of 342 126 frontal chest radiographs captured in ED and urgent care settings. The training data were labeled from their associated reports. Image-based F1 score was chosen to optimize the operating point on the receiver operating characteristics (ROC) curve so as to minimize the number of missed findings and overcalls per image read. The performance of the model was compared with that of 5 radiology residents recruited from multiple institutions in the US in an objective study in which a separate data set of 1998 AP frontal chest radiographs was drawn from a hospital source representative of realistic preliminary reads in inpatient and ED settings. A triple consensus with adjudication process was used to derive the ground truth labels for the study data set. The performance of AI algorithm and radiology residents was assessed by comparing their reads with ground truth findings. All studies were conducted through a web-based clinical study application system. The triple consensus data set was collected between February and October 2018. The comparison study was preformed between January and October 2019. Data were analyzed from October to February 2020. After the first round of reviews, further analysis of the data was performed from March to July 2020. MAIN OUTCOMES AND MEASURES: The learning performance of the AI algorithm was judged using the conventional ROC curve and the area under the curve (AUC) during training and field testing on the study data set. For the AI algorithm and radiology residents, the individual finding label performance was measured using the conventional measures of label-based sensitivity, specificity, and positive predictive value (PPV). In addition, the agreement with the ground truth on the assignment of findings to images was measured using the pooled κ statistic. The preliminary read performance was recorded for AI algorithm and radiology residents using new measures of mean image-based sensitivity, specificity, and PPV designed for recording the fraction of misses and overcalls on a per image basis. The 1-sided analysis of variance test was used to compare the means of each group (AI algorithm vs radiology residents) using the F distribution, and the null hypothesis was that the groups would have similar means. RESULTS: The trained AI algorithm achieved a mean AUC across labels of 0.807 (weighted mean AUC, 0.841) after training. On the study data set, which had a different prevalence distribution, the mean AUC achieved was 0.772 (weighted mean AUC, 0.865). The interrater agreement with ground truth finding labels for AI algorithm predictions had pooled κ value of 0.544, and the pooled κ for radiology residents was 0.585. For the preliminary read performance, the analysis of variance test was used to compare the distributions of AI algorithm and radiology residents' mean image-based sensitivity, PPV, and specificity. The mean image-based sensitivity for AI algorithm was 0.716 (95% CI, 0.704-0.729) and for radiology residents was 0.720 (95% CI, 0.709-0.732) (P = .66), while the PPV was 0.730 (95% CI, 0.718-0.742) for the AI algorithm and 0.682 (95% CI, 0.670-0.694) for the radiology residents (P < .001), and specificity was 0.980 (95% CI, 0.980-0.981) for the AI algorithm and 0.973 (95% CI, 0.971-0.974) for the radiology residents (P < .001). CONCLUSIONS AND RELEVANCE: These findings suggest that it is possible to build AI algorithms that reach and exceed the mean level of performance of third-year radiology residents for full-fledged preliminary read of AP frontal chest radiographs. This diagnostic study also found that while the more complex findings would still benefit from expert overreads, the performance of AI algorithms was associated with the amount of data available for training rather than the level of difficulty of interpretation of the finding. Integrating such AI systems in radiology workflows for preliminary interpretations has the potential to expedite existing radiology workflows and address resource scarcity while improving overall accuracy and reducing the cost of care.",2020,10.1001/jamanetworkopen.2020.22779,diagnosis,False
Comparison of CT and MRI images for the prediction of soft-tissue sarcoma grading and lung metastasis via a convolutional neural networks model,"AIM: To realise the automated prediction of soft-tissue sarcoma (STS) grading and lung metastasis based on computed tomography (CT), T1-weighted (T1W) magnetic resonance imaging (MRI), and fat-suppressed T2-weighted MRI (FST2W) via the convolutional neural networks (CNN) model. MATERIALS AND METHODS: MRI and CT images of 51 patients diagnosed with STS were analysed retrospectively. The patients could be divided into three groups based on disease grading: high-grade group (n=28), intermediate-grade group (n=15), low-grade group (n=8). Among these patients, 32 had lung metastasis, while the remaining 19 had no lung metastasis. The data were divided into the training, validation, and testing groups according to the ratio of 5:2:3. The receiver operating characteristic (ROC) curves and accuracy values were acquired using the testing dataset to evaluate the performance of the CNN model. RESULTS: For STS grading, the accuracy of the T1W, FST2W, CT, and the fusion of T1W and FST2W testing data were 0.86, 0.89, 0.86, and 0.85, respectively. In addition, Area Under Curve (AUC) were 0.96, 0.97, 0.97, and 0.94 respectively. For the prediction of lung metastasis, the accuracy of the T1W, FST2W, CT, and the fusion of T1W and FST2W test data were 0.92, 0.93, 0.88, and 0.91, respectively. The corresponding AUC values were 0.97, 0.96, 0.95, and 0.95, respectively. FST2W MRI performed best for predicting STS grading and lung metastasis. CONCLUSION: MRI and CT images combined with the CNN model can be useful for making predictions regarding STS grading and lung metastasis, thus providing help for patient diagnosis and treatment.",2020,10.1016/j.crad.2019.08.008,diagnosis,True
Comparison of Deep Learning Approaches for Multi-Label Chest X-Ray Classification,"The increased availability of labeled X-ray image archives (e.g. ChestX-ray14 dataset) has triggered a growing interest in deep learning techniques. To provide better insight into the different approaches, and their applications to chest X-ray classification, we investigate a powerful network architecture in detail: the ResNet-50. Building on prior work in this domain, we consider transfer learning with and without fine-tuning as well as the training of a dedicated X-ray network from scratch. To leverage the high spatial resolution of X-ray data, we also include an extended ResNet-50 architecture, and a network integrating non-image data (patient age, gender and acquisition type) in the classification process. In a concluding experiment, we also investigate multiple ResNet depths (i.e. ResNet-38 and ResNet-101). In a systematic evaluation, using 5-fold re-sampling and a multi-label loss function, we compare the performance of the different approaches for pathology classification by ROC statistics and analyze differences between the classifiers using rank correlation. Overall, we observe a considerable spread in the achieved performance and conclude that the X-ray-specific ResNet-38, integrating non-image data yields the best overall results. Furthermore, class activation maps are used to understand the classification process, and a detailed analysis of the impact of non-image features is provided.",2019,10.1038/s41598-019-42294-8,diagnosis,False
"Comparison of deep learning, radiomics and subjective assessment of chest CT findings in SARS-CoV-2 pneumonia","PURPOSE: Comparison of deep learning algorithm, radiomics and subjective assessment of chest CT for predicting outcome (death or recovery) and intensive care unit (ICU) admission in patients with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection. METHODS: The multicenter, ethical committee-approved, retrospective study included non-contrast-enhanced chest CT of 221 SARS-CoV-2 positive patients from Italy (n = 196 patients; mean age 64 ± 16 years) and Denmark (n = 25; mean age 69 ± 13 years). A thoracic radiologist graded presence, type and extent of pulmonary opacities and severity of motion artifacts in each lung lobe on all chest CTs. Thin-section CT images were processed with CT Pneumonia Analysis Prototype (Siemens Healthineers) which yielded segmentation masks from a deep learning (DL) algorithm to derive features of lung abnormalities such as opacity scores, mean HU, as well as volume and percentage of all-attenuation and high-attenuation (opacities >-200 HU) opacities. Separately, whole lung radiomics were obtained for all CT exams. Analysis of variance and multiple logistic regression were performed for data analysis. RESULTS: Moderate to severe respiratory motion artifacts affected nearly one-quarter of chest CTs in patients. Subjective severity assessment, DL-based features and radiomics predicted patient outcome (AUC 0.76 vs AUC 0.88 vs AUC 0.83) and need for ICU admission (AUC 0.77 vs AUC 0.0.80 vs 0.82). Excluding chest CT with motion artifacts, the performance of DL-based and radiomics features improve for predicting ICU admission. CONCLUSION: DL-based and radiomics features of pulmonary opacities from chest CT were superior to subjective assessment for differentiating patients with favorable and adverse outcomes.",2021,10.1016/j.clinimag.2021.06.036,diagnosis,True
Comparison of pirfenidone and corticosteroid treatments at the COVID-19 pneumonia with the guide of artificial intelligence supported thoracic computed tomography,"AIM: We aimed to investigate the effect of short-term pirfenidone treatment on prolonged COVID-19 pneumonia. METHOD: Hospital files of patients hospitalised with a diagnosis of critical COVID-19 pneumonia from November 2020 to March 2021 were retrospectively reviewed. Chest computed tomography images taken both before treatment and 2 months after treatment, demographic characteristics and laboratory parameters of patients receiving pirfenidone + methylprednisolone (n = 13) and only methylprednisolones (n = 9) were recorded. Pulmonary function tests were performed after the second month of the treatment. CT involvement rates were determined by machine learning. RESULTS: A total of 22 patients, 13 of whom (59.1%) were using methylprednisolone + pirfenidone and 9 of whom (40.9%) were using only methylprednisolone were included. When the blood gas parameters and pulmonary function tests of the patients were compared at the end of the second month, it was found that the FEV1, FEV1%, FVC and FVC% values were statistically significantly higher in the methylprednisolone + pirfenidone group compared with the methylprednisolone group (P = .025, P = .012, P = .026 and P = .017, respectively). When the rates of change in CT scans at diagnosis and second month of treatment were examined, it was found that the involvement rates in the methylprednisolone + pirfenidone group were statistically significantly decreased (P < .001). CONCLUSION: Antifibrotic agents can reduce fibrosis that may develop in the future. These can also help dose reduction and/or non-use strategy for methylprednisolone therapy, which has many side effects. Further large series and randomised controlled studies are needed on this subject.",2021,10.1111/ijcp.14961,treatment,True
Computed Tomography Image Processing Analysis in COVID-19 Patient Follow-Up Assessment,"The rapid worldwide spread of the COVID-19 pandemic has infected patients around the world in a short space of time. Chest computed tomography (CT) images of patients who are infected with COVID-19 can offer early diagnosis and efficient forecast monitoring at a low cost. The diagnosis of COVID-19 on CT in an automated way can speed up many tasks and the application of medical treatments. This can help complement reverse transcription-polymerase chain reaction (RT-PCR) diagnosis. The aim of this work is to develop a system that automatically identifies ground-glass opacity (GGO) and pulmonary infiltrates (PIs) on CT images from patients with COVID-19. The purpose is to assess the disease progression during the patient's follow-up assessment and evaluation. We propose an efficient methodology that incorporates oversegmentation mean shift followed by superpixel-SLIC (simple linear iterative clustering) algorithm on CT images with COVID-19 for pulmonary parenchyma segmentation. To identify the pulmonary parenchyma, we described each superpixel cluster according to its position, grey intensity, second-order texture, and spatial-context-saliency features to classify by a tree random forest (TRF). Second, by applying the watershed segmentation to the mean-shift clusters, only pulmonary parenchyma segmentation-identified zones showed GGO and PI based on the description of each watershed cluster of its position, grey intensity, gradient entropy, second-order texture, Euclidean position to the border region of the PI zone, and global saliency features, after using TRF. Our classification results for pulmonary parenchyma identification on CT images with COVID-19 had a precision of over 92% and recall of over 92% on twofold cross validation. For GGO, the PI identification showed 96% precision and 96% recall on twofold cross validation.",2021,10.1155/2021/8869372,combined,True
Computed Tomography-Based Radiomic Features for Diagnosis of Indeterminate Small Pulmonary Nodules,"OBJECTIVE: This study aimed to determine the potential of radiomic features extracted from preoperative computed tomography to discriminate malignant from benign indeterminate small (≤10 mm) pulmonary nodules. METHODS: A total of 197 patients with 210 nodules who underwent surgical resections between January 2011 and March 2017 were analyzed. Three hundred eighty-five radiomic features were extracted from the computed tomographic images. Feature selection and data dimension reduction were performed using the Kruskal-Wallis test, Spearman correlation analysis, and principal component analysis. The random forest was used for radiomic signature building. The receiver operating characteristic curve analysis was used to evaluate the model performance. RESULTS: Fifteen principal component features were selected for modeling. The area under the curve, sensitivity, specificity, and accuracy of the prediction model were 0.877 (95% confidence interval [CI], 0.795-0.959), 81.8% (95% CI, 72.0%-90.9%), 77.4% (95% CI, 63.9%-89.3%), and 80.0% (95% CI, 72.0%-86.7%) in the validation cohort, respectively. CONCLUSIONS: Computed tomography-based radiomic features showed good discriminative power for benign and malignant indeterminate small pulmonary nodules.",2020,10.1097/rct.0000000000000976,diagnosis,True
Computed Tomography-Based Radiomics Signature: A Potential Indicator of Epidermal Growth Factor Receptor Mutation in Pulmonary Adenocarcinoma Appearing as a Subsolid Nodule,"BACKGROUND: Lung adenocarcinoma (LADC) with epidermal growth factor receptor (EGFR) mutation is considered a subgroup of lung cancer sensitive to EGFR-targeted tyrosine kinase inhibitors. We aimed to develop and validate a computed tomography (CT)-based radiomics signature for prediction of EGFR mutation status in LADC appearing as a subsolid nodule. MATERIALS AND METHODS: A total of 467 eligible patients were divided into training and validation cohorts (n = 306 and 161, respectively). Radiomics features were extracted from unenhanced CT images by using Pyradiomics. A CT-based radiomics signature for distinguishing EGFR mutation status was constructed using the random forest (RF) method in the training cohort and then tested in the validation cohort. A combination of the radiomics signature with a clinical factors model was also constructed using the RF method. The performance of the model was evaluated using the area under the curve (AUC) of a receiver operating characteristic curve. RESULTS: In this study, 64.2% (300/467) of the patients showed EGFR mutations. L858R mutation of exon 21 was the most common mutation type (185/301). We identified a CT-based radiomics signature that successfully discriminated between EGFR positive and EGFR negative in the training cohort (AUC = 0.831) and the validation cohort (AUC = 0.789). The radiomics signature combined with the clinical factors model was not superior to the simple radiomics signature in the two cohorts (p > .05). CONCLUSION: As a noninvasive method, the CT-based radiomics signature can be used to predict the EGFR mutation status of LADC appearing as a subsolid nodule. IMPLICATIONS FOR PRACTICE: Lung adenocarcinoma (LADC) with epidermal growth factor receptor (EGFR) mutation is considered a subgroup of lung cancer that is sensitive to EGFR-targeted tyrosine kinase inhibitors. However, some patients with inoperable subsolid LADC are unable to undergo tissue sampling by biopsy for molecular analysis in clinical practice. A computed tomography-based radiomics signature may serve as a noninvasive biomarker to predict the EGFR mutation status of subsolid LADCs when mutational profiling is not available or possible.",2019,10.1634/theoncologist.2018-0706,combined,True
Computer aid screening of COVID-19 using X-ray and CT scan images: An inner comparison,"The objective of this study is to conduct a critical analysis to investigate and compare a group of computer aid screening methods of COVID-19 using chest X-ray images and computed tomography (CT) images. The computer aid screening method includes deep feature extraction, transfer learning, and machine learning image classification approach. The deep feature extraction and transfer learning method considered 13 pre-trained CNN models. The machine learning approach includes three sets of handcrafted features and three classifiers. The pre-trained CNN models include AlexNet, GoogleNet, VGG16, VGG19, Densenet201, Resnet18, Resnet50, Resnet101, Inceptionv3, Inceptionresnetv2, Xception, MobileNetv2 and ShuffleNet. The handcrafted features are GLCM, LBP & HOG, and machine learning based classifiers are KNN, SVM & Naive Bayes. In addition, the different paradigms of classifiers are also analyzed. Overall, the comparative analysis is carried out in 65 classification models, i.e., 13 in deep feature extraction, 13 in transfer learning, and 39 in the machine learning approaches. Finally, all classification models perform better when applying to the chest X-ray image set as comparing to the use of CT scan image set. Among 65 classification models, the VGG19 with SVM achieved the highest accuracy of 99.81%when applying to the chest X-ray images. In conclusion, the findings of this analysis study are beneficial for the researchers who are working towards designing computer aid tools for screening COVID-19 infection diseases.",2021,10.3233/xst-200784,diagnosis,True
Computer-aided COVID-19 diagnosis and a comparison of deep learners using augmented CXRs,"BACKGROUND: Coronavirus Disease 2019 (COVID-19) is contagious, producing respiratory tract infection, caused by a newly discovered coronavirus. Its death toll is too high, and early diagnosis is the main problem nowadays. Infected people show a variety of symptoms such as fatigue, fever, tastelessness, dry cough, etc. Some other symptoms may also be manifested by radiographic visual identification. Therefore, Chest X-Rays (CXR) play a key role in the diagnosis of COVID-19. METHODS: In this study, we use Chest X-Rays images to develop a computer-aided diagnosis (CAD) of the disease. These images are used to train two deep networks, the Convolution Neural Network (CNN), and the Long Short-Term Memory Network (LSTM) which is an artificial Recurrent Neural Network (RNN). The proposed study involves three phases. First, the CNN model is trained on raw CXR images. Next, it is trained on pre-processed CXR images and finally enhanced CXR images are used for deep network CNN training. Geometric transformations, color transformations, image enhancement, and noise injection techniques are used for augmentation. From augmentation, we get 3,220 augmented CXRs as training datasets. In the final phase, CNN is used to extract the features of CXR imagery that are fed to the LSTM model. The performance of the four trained models is evaluated by the evaluation techniques of different models, including accuracy, specificity, sensitivity, false-positive rate, and receiver operating characteristic (ROC) curve. RESULTS: We compare our results with other benchmark CNN models. Our proposed CNN-LSTM model gives superior accuracy (99.02%) than the other state-of-the-art models. Our method to get improved input, helped the CNN model to produce a very high true positive rate (TPR 1) and no false-negative result whereas false negative was a major problem while using Raw CXR images. CONCLUSIONS: We conclude after performing different experiments that some image pre-processing and augmentation, remarkably improves the results of CNN-based models. It will help a better early detection of the disease that will eventually reduce the mortality rate of COVID.",2022,10.3233/xst-211047,diagnosis,False
Computer-aided detection (CADe) and diagnosis (CADx) system for lung cancer with likelihood of malignancy,"BACKGROUND: CADe and CADx systems for the detection and diagnosis of lung cancer have been important areas of research in recent decades. However, these areas are being worked on separately. CADe systems do not present the radiological characteristics of tumors, and CADx systems do not detect nodules and do not have good levels of automation. As a result, these systems are not yet widely used in clinical settings. METHODS: The purpose of this article is to develop a new system for detection and diagnosis of pulmonary nodules on CT images, grouping them into a single system for the identification and characterization of the nodules to improve the level of automation. The article also presents as contributions: the use of Watershed and Histogram of oriented Gradients (HOG) techniques for distinguishing the possible nodules from other structures and feature extraction for pulmonary nodules, respectively. For the diagnosis, it is based on the likelihood of malignancy allowing more aid in the decision making by the radiologists. A rule-based classifier and Support Vector Machine (SVM) have been used to eliminate false positives. RESULTS: The database used in this research consisted of 420 cases obtained randomly from LIDC-IDRI. The segmentation method achieved an accuracy of 97 % and the detection system showed a sensitivity of 94.4 % with 7.04 false positives per case. Different types of nodules (isolated, juxtapleural, juxtavascular and ground-glass) with diameters between 3 mm and 30 mm have been detected. For the diagnosis of malignancy our system presented ROC curves with areas of: 0.91 for nodules highly unlikely of being malignant, 0.80 for nodules moderately unlikely of being malignant, 0.72 for nodules with indeterminate malignancy, 0.67 for nodules moderately suspicious of being malignant and 0.83 for nodules highly suspicious of being malignant. CONCLUSIONS: From our preliminary results, we believe that our system is promising for clinical applications assisting radiologists in the detection and diagnosis of lung cancer.",2016,10.1186/s12938-015-0120-7,diagnosis,True
Computer-aided Detection of Subsolid Nodules at Chest CT: Improved Performance with Deep Learning-based CT Section Thickness Reduction,"Background Studies on the optimal CT section thickness for detecting subsolid nodules (SSNs) with computer-aided detection (CAD) are lacking. Purpose To assess the effect of CT section thickness on CAD performance in the detection of SSNs and to investigate whether deep learning-based super-resolution algorithms for reducing CT section thickness can improve performance. Materials and Methods CT images obtained with 1-, 3-, and 5-mm-thick sections were obtained in patients who underwent surgery between March 2018 and December 2018. Patients with resected synchronous SSNs and those without SSNs (negative controls) were retrospectively evaluated. The SSNs, which ranged from 6 to 30 mm, were labeled ground-truth lesions. A deep learning-based CAD system was applied to SSN detection on CT images of each section thickness and those converted from 3- and 5-mm section thickness into 1-mm section thickness by using the super-resolution algorithm. The CAD performance on each section thickness was evaluated and compared by using the jackknife alternative free response receiver operating characteristic figure of merit. Results A total of 308 patients (mean age ± standard deviation, 62 years ± 10; 183 women) with 424 SSNs (310 part-solid and 114 nonsolid nodules) and 182 patients without SSNs (mean age, 65 years ± 10; 97 men) were evaluated. The figures of merit differed across the three section thicknesses (0.92, 0.90, and 0.89 for 1, 3, and 5 mm, respectively; P = .04) and between 1- and 5-mm sections (P = .04). The figures of merit varied for nonsolid nodules (0.78, 0.72, and 0.66 for 1, 3, and 5 mm, respectively; P < .001) but not for part-solid nodules (range, 0.93-0.94; P = .76). The super-resolution algorithm improved CAD sensitivity on 3- and 5-mm-thick sections (P = .02 for 3 mm, P < .001 for 5 mm). Conclusion Computer-aided detection (CAD) of subsolid nodules performed better at 1-mm section thickness CT than at 3- and 5-mm section thickness CT, particularly with nonsolid nodules. Application of a super-resolution algorithm improved the sensitivity of CAD at 3- and 5-mm section thickness CT. © RSNA, 2021 Online supplemental material is available for this article. See also the editorial by Goo in this issue.",2021,10.1148/radiol.2021203387,diagnosis,True
Computer-Aided Detection System for the Classification of Non-Small Cell Lung Lesions using SVM,"INTRODUCTION: Lung carcinoma is the most commonly cancer causing deaths throughout the world that mainly occurs due to smoking. Small cell lung cancer and Non-small cell lung cancer (NSCLC) are the two different types of Lung cancer. For the detection and classification of lung cancer, there are different techniques in the literature. METHODS: This paper emphasis on the three class classification of the Adenocarcinomas, Squamous cell carcinomas, and large cell carcinomas of NSCLC. For precise and superior results, Computer Aided Detection (CADe) system has been designed so that the radiologist can diagnose carcinoma in the ultrasonic images conveniently. CADe analyses the quality of the images, selects the region of interest, preprocesses the data, extracts the features and classifies the cancer. RESULTS: After exhaustive literature survey, Laws' mask features and SVM classifier with Gaussian RBF kernels have been used in this paper. The experimentation was performed on 92 images using 50% - 50% training and testing criteria. CONCLUSION: Comparative study reveals that our system for separating three class lung cancer provides 95.65% average accuracy for Laws' mask 3 dimensions using the SVM classifier that is maximum among the existing methods reported in the literature using the same dataset.",2020,10.2174/1573409916666200102122021,diagnosis,True
Computer-Aided Diagnosis (CAD) of Pulmonary Nodule of Thoracic CT Image Using Transfer Learning,"Computer-aided diagnosis (CAD) has already been widely used in medical image processing. We recently make another trial to implement convolutional neural network (CNN) on the classification of pulmonary nodules of thoracic CT images. The biggest challenge in medical image classification with the help of CNN is the difficulty of acquiring enough samples, and overfitting is a common problem when there are not enough images for training. Transfer learning has been verified as reasonable in dealing with such problems with an acceptable loss value. We use the classic LeNet-5 model to classify pulmonary nodules of thoracic CT images, including benign and malignant pulmonary nodules, and different malignancies of the malignant nodules. The CT images are obtained from Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI) where both pulmonary nodule scanning and nodule annotations are available. These images are labeled and stored in a medical images knowledge base (KB), which is designed and implemented in our previous work. We implement the 10-folder cross validation (CV) to testify the robustness of the classification model we trained. The result demonstrates that the transfer learning of the LeNet-5 is good for classifying pulmonary nodules of thoracic CT images, and the average values of Top-1 accuracy are 97.041% and 96.685% respectively. We believe that our work is beneficial and has potential for practical diagnosis of lung nodules.",2019,10.1007/s10278-019-00204-4,diagnosis,True
Computer-Aided Diagnosis of COVID-19 CT Scans Based on Spatiotemporal Information Fusion,"Coronavirus disease (COVID-19) is highly contagious and pathogenic. Currently, the diagnosis of COVID-19 is based on nucleic acid testing, but it has false negatives and hysteresis. The use of lung CT scans can help screen and effectively monitor diagnosed cases. The application of computer-aided diagnosis technology can reduce the burden on doctors, which is conducive to rapid and large-scale diagnostic screening. In this paper, we proposed an automatic detection method for COVID-19 based on spatiotemporal information fusion. Using the segmentation network in the deep learning method to segment the lung area and the lesion area, the spatiotemporal information features of multiple CT scans are extracted to perform auxiliary diagnosis analysis. The performance of this method was verified on the collected dataset. We achieved the classification of COVID-19 CT scans and non-COVID-19 CT scans and analyzed the development of the patients' condition through the CT scans. The average accuracy rate is 96.7%, sensitivity is 95.2%, and F1 score is 95.9%. Each scan takes about 30 seconds for detection.",2021,10.1155/2021/6649591,diagnosis,True
Computer-aided diagnosis of endobronchial ultrasound images using convolutional neural network,"BACKGROUND AND OBJECTIVE: In the United States, lung cancer is the leading cause of cancer death. The survival rate could increase by early detection. In recent years, the endobronchial ultrasonography (EBUS) images have been utilized to differentiate between benign and malignant lesions and guide transbronchial needle aspiration because it is real-time, radiation-free and has better performance. However, the diagnosis depends on the subjective judgment from doctors. In some previous studies, which using the grayscale image textures of the EBUS images to classify the lung lesions but it belonged to semi-automated system which still need the experts to select a part of the lesion first. Therefore, the main purpose of this study was to achieve full automation assistance by using convolution neural network. METHODS: First of all, the EBUS images resized to the input size of convolution neural network (CNN). And then, the training data were rotated and flipped. The parameters of the model trained with ImageNet previously were transferred to the CaffeNet used to classify the lung lesions. And then, the parameter of the CaffeNet was optimized by the EBUS training data. The features with 4096 dimension were extracted from the 7th fully connected layer and the support vector machine (SVM) was utilized to differentiate benign and malignant. This study was validated with 164 cases including 56 benign and 108 malignant. RESULTS: According to the experiment results, applying the classification by the features from the CNN with transfer learning had better performance than the conventional method with gray level co-occurrence matrix (GLCM) features. The accuracy, sensitivity, specificity, and the area under ROC achieved 85.4% (140/164), 87.0% (94/108), 82.1% (46/56), and 0.8705, respectively. CONCLUSIONS: From the experiment results, it has potential ability to diagnose EBUS images with CNN.",2019,10.1016/j.cmpb.2019.05.020,diagnosis,False
Computer-aided diagnosis of ground glass pulmonary nodule by fusing deep learning and radiomics features,"OBJECTIVES: This study aims to develop a computer-aided diagnosis (CADx) scheme to classify between benign and malignant ground glass nodules (GGNs), and fuse deep leaning and radiomics imaging features to improve the classification performance. METHODS: We first retrospectively collected 513 surgery histopathology confirmed GGNs from two centers. Among these GGNs, 100 were benign and 413 were malignant. All malignant tumors were stage I lung adenocarcinoma. To segment GGNs, we applied a deep convolutional neural network and residual architecture to train and build a 3D U-Net. Then, based on the pre-trained U-Net, we used a transfer learning approach to build a deep neural network (DNN) to classify between benign and malignant GGNs. With the GGN segmentation results generated by 3D U-Net, we also developed a CT radiomics model by adopting a series of image processing techniques, i.e. radiomics feature extraction, feature selection, synthetic minority over-sampling technique, and support vector machine classifier training/testing, etc. Finally, we applied an information fusion method to fuse the prediction scores generated by DNN based CADx model and CT-radiomics based model. To evaluate the proposed model performance, we conducted a comparison experiment by testing on an independent testing dataset. RESULTS: Comparing with DNN model and radiomics model, our fusion model yielded a significant higher area under a receiver operating characteristic curve (AUC) value of 0.73 ± 0.06 (P < 0.01). The fusion model generated an accuracy of 75.6%, F1 score of 84.6%, weighted average F1 score of 70.3%, and Matthews correlation coefficient of 43.6%, which were higher than the DNN model and radiomics model individually. CONCLUSIONS: Our experimental results demonstrated that (1) applying a CADx scheme was feasible to diagnosis of early-stage lung adenocarcinoma, (2) deep image features and radiomics features provided complementary information in classifying benign and malignant GGNs, and (3) it was an effective way to build DNN model with limited dataset by using transfer learning. Thus, to build a robust image analysis based CADx model, one can combine different types of image features to decode the imaging phenotypes of GGN.",2021,10.1088/1361-6560/abe735,diagnosis,True
Computer-aided diagnosis of ground-glass opacity pulmonary nodules using radiomic features analysis,"This study aims to develop a CT-based radiomic features analysis approach for diagnosis of ground-glass opacity (GGO) pulmonary nodules, and also assess whether computer-aided diagnosis (CADx) performance changes in classifying between benign and malignant nodules associated with histopathological subtypes namely, adenocarcinoma in situ (AIS), minimally invasive adenocarcinoma (MIA), and invasive adenocarcinoma (IAC), respectively. The study involves 182 histopathology-confirmed GGO nodules collected from two cancer centers. Among them, 59 are benign, 50 are AIS, 32 are MIA, and 41 are IAC nodules. Four training/testing data sets-(1) all nodules, (2) benign and AIS nodules, (3) benign and MIA nodules, (4) benign and IAC nodules-are assembled based on their histopathological subtypes. We first segment pulmonary nodules depicted in CT images by using a 3D region growing and geodesic active contour level set algorithm. Then, we computed and extracted 1117 quantitative imaging features based on the 3D segmented nodules. After conducting radiomic features normalization process, we apply a leave-one-out cross-validation (LOOCV) method to build models by embedding with a Relief feature selection, synthetic minority oversampling technique (SMOTE) and three machine-learning classifiers namely, support vector machine classifier, logistic regression classifier and Gaussian Naïve Bayes classifier. When separately using four data sets to train and test three classifiers, the average areas under receiver operating characteristic curves (AUC) are 0.75, 0.55, 0.77 and 0.93, respectively. When testing on an independent data set, our scheme yields higher accuracy than two radiologists (61.3% versus radiologist 1: 53.1% and radiologist 2: 56.3%). This study demonstrates that: (1) the feasibility of using CT-based radiomic features analysis approach to distinguish between benign and malignant GGO nodules, (2) higher performance of CADx scheme in diagnosing GGO nodules comparing with radiologist, and (3) a consistently positive trend between classification performance and invasive grade of GGO nodules. Thus, to improve the CADx performance in diagnosing of GGO nodules, one should assemble an optimal training data set dominated with more nodules associated with non-invasive lung adenocarcinoma (i.e. AIS and MIA).",2019,10.1088/1361-6560/ab2757,diagnosis,True
Computer-aided diagnosis of lung cancer: the effect of training data sets on classification accuracy of lung nodules,"This study aims to develop a computer-aided diagnosis (CADx) scheme for classification between malignant and benign lung nodules, and also assess whether CADx performance changes in detecting nodules associated with early and advanced stage lung cancer. The study involves 243 biopsy-confirmed pulmonary nodules. Among them, 76 are benign, 81 are stage I and 86 are stage III malignant nodules. The cases are separated into three data sets involving: (1) all nodules, (2) benign and stage I malignant nodules, and (3) benign and stage III malignant nodules. A CADx scheme is applied to segment lung nodules depicted on computed tomography images and we initially computed 66 3D image features. Then, three machine learning models namely, a support vector machine, naïve Bayes classifier and linear discriminant analysis, are separately trained and tested by using three data sets and a leave-one-case-out cross-validation method embedded with a Relief-F feature selection algorithm. When separately using three data sets to train and test three classifiers, the average areas under receiver operating characteristic curves (AUC) are 0.94, 0.90 and 0.99, respectively. When using the classifiers trained using data sets with all nodules, average AUC values are 0.88 and 0.99 for detecting early and advanced stage nodules, respectively. AUC values computed from three classifiers trained using the same data set are consistent without statistically significant difference (p > 0.05). This study demonstrates (1) the feasibility of applying a CADx scheme to accurately distinguish between benign and malignant lung nodules, and (2) a positive trend between CADx performance and cancer progression stage. Thus, in order to increase CADx performance in detecting subtle and early cancer, training data sets should include more diverse early stage cancer cases.",2018,10.1088/1361-6560/aaa610,diagnosis,True
"Computer-aided diagnosis of lung nodule classification between benign nodule, primary lung cancer, and metastatic lung cancer at different image size using deep convolutional neural network with transfer learning","We developed a computer-aided diagnosis (CADx) method for classification between benign nodule, primary lung cancer, and metastatic lung cancer and evaluated the following: (i) the usefulness of the deep convolutional neural network (DCNN) for CADx of the ternary classification, compared with a conventional method (hand-crafted imaging feature plus machine learning), (ii) the effectiveness of transfer learning, and (iii) the effect of image size as the DCNN input. Among 1240 patients of previously-built database, computed tomography images and clinical information of 1236 patients were included. For the conventional method, CADx was performed by using rotation-invariant uniform-pattern local binary pattern on three orthogonal planes with a support vector machine. For the DCNN method, CADx was evaluated using the VGG-16 convolutional neural network with and without transfer learning, and hyperparameter optimization of the DCNN method was performed by random search. The best averaged validation accuracies of CADx were 55.9%, 68.0%, and 62.4% for the conventional method, the DCNN method with transfer learning, and the DCNN method without transfer learning, respectively. For image size of 56, 112, and 224, the best averaged validation accuracy for the DCNN with transfer learning were 60.7%, 64.7%, and 68.0%, respectively. DCNN was better than the conventional method for CADx, and the accuracy of DCNN improved when using transfer learning. Also, we found that larger image sizes as inputs to DCNN improved the accuracy of lung nodule classification.",2018,10.1371/journal.pone.0200721,diagnosis,True
Computer-aided diagnosis of lung nodule using gradient tree boosting and Bayesian optimization,"We aimed to evaluate a computer-aided diagnosis (CADx) system for lung nodule classification focussing on (i) usefulness of the conventional CADx system (hand-crafted imaging feature + machine learning algorithm), (ii) comparison between support vector machine (SVM) and gradient tree boosting (XGBoost) as machine learning algorithms, and (iii) effectiveness of parameter optimization using Bayesian optimization and random search. Data on 99 lung nodules (62 lung cancers and 37 benign lung nodules) were included from public databases of CT images. A variant of the local binary pattern was used for calculating a feature vector. SVM or XGBoost was trained using the feature vector and its corresponding label. Tree Parzen Estimator (TPE) was used as Bayesian optimization for parameters of SVM and XGBoost. Random search was done for comparison with TPE. Leave-one-out cross-validation was used for optimizing and evaluating the performance of our CADx system. Performance was evaluated using area under the curve (AUC) of receiver operating characteristic analysis. AUC was calculated 10 times, and its average was obtained. The best averaged AUC of SVM and XGBoost was 0.850 and 0.896, respectively; both were obtained using TPE. XGBoost was generally superior to SVM. Optimal parameters for achieving high AUC were obtained with fewer numbers of trials when using TPE, compared with random search. Bayesian optimization of SVM and XGBoost parameters was more efficient than random search. Based on observer study, AUC values of two board-certified radiologists were 0.898 and 0.822. The results show that diagnostic accuracy of our CADx system was comparable to that of radiologists with respect to classifying lung nodules.",2018,10.1371/journal.pone.0195875,diagnosis,True
"Computer-Aided Diagnosis of Lung Nodules in Computed Tomography by Using Phylogenetic Diversity, Genetic Algorithm, and SVM","Lung cancer is pointed as the major cause of death among patients with cancer throughout the world. This work is intended to develop a methodology for diagnosis of lung nodules using images from the Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI). The proposed methodology uses image processing and pattern recognition techniques. In order to differentiate between the patterns of malignant and benign nodules, we used phylogenetic diversity by means of particular indexes, that are: intensive quadratic entropy, extensive quadratic entropy, average taxonomic distinctness, total taxonomic distinctness, and pure diversity indexes. After that, we applied the genetic algorithm for selection of the best model. In the tests' stage, we applied the proposed methodology to 1405 (394 malignant and 1011 benign) nodules. The proposed work presents promising results at the classification into malignant and benign, achieving accuracy of 92.52%, sensitivity of 93.1% and specificity of 92.26%. The results demonstrated a good rate of correct detections using texture features. Since a precocious detection allows a faster therapeutic intervention, thus a more favorable prognostic to the patient, we propose herein a methodology that contributes to the area in this aspect.",2017,10.1007/s10278-017-9973-6,diagnosis,True
Computer-aided diagnosis of pectus excavatum using CT images and deep learning methods,"Pectus excavatum (PE) is one of the most common chest wall defects. Accurate assessment of PE deformities is critical for effective surgical intervention. Index-based evaluations have become the standard for objectively estimating PE, however, these indexes cannot represent the whole information of chest CT images and may associated with significant error due to the individual differences. To overcome these limitations, this paper developed a computer-aided diagnosis (CAD) system based on the convolutional neural network (CNN) to automatically learn discriminative features and classify PE images. We also adopted block-wise fine-tuning methods based on the transfer learning strategy to reduce the potential risk of overfitting caused by limited data and experimentally explored the best fine-tuning degree. Our method achieved a high level of classification accuracy with 94.76% for PE diagnosis. Furthermore, we proposed a majority rule-based voting method to provide a comprehensively diagnostic results for each patient, which integrated the classification results of the whole thorax. The promising results support the feasibility of our proposed CNN-based CAD system for automatic PE diagnosis, which paves a way for comprehensive assessments of PE in clinics.",2020,10.1038/s41598-020-77361-y,diagnosis,True
Computer-Aided Diagnosis Research of a Lung Tumor Based on a Deep Convolutional Neural Network and Global Features,"Based on the better generalization ability and the feature learning ability of the deep convolutional neural network, it is very significant to use the DCNN on the computer-aided diagnosis of a lung tumor. Firstly, a deep convolutional neural network was constructed according to the fuzzy characteristics and the complexity of lung CT images. Secondly, the relation between model parameters (iterations, different resolution) and recognition rate is discussed. Thirdly, the effects of different model structures for the identification of a lung tumor were analyzed by changing convolution kernel size, feature dimension, and depth of the network. Fourthly, the different optimization methods on how to influence the DCNN performance were discussed from three aspects containing pooling methods (maximum pooling and mean pooling), activation function (sigmoid and ReLU), and training algorithm (batch gradient descent and gradient descent with momentum). Finally, the experimental results verified the feasibility of DCNN used on computer-aided diagnosis of lung tumors, and it can achieve a good recognition rate when selecting the appropriate model parameters and model structure and using the method of gradient descent with momentum.",2021,10.1155/2021/5513746,diagnosis,True
Computer-Aided Diagnosis system for diagnosis of pulmonary emphysema using bio-inspired algorithms,"Pulmonary emphysema is a condition characterized by the destruction and permanent enlargement of the alveoli of the lungs. The destruction of gas-exchanging alveoli causes shortness of breath followed by a chronic cough and sputum production. A Computer-Aided Diagnosis (CAD) framework for diagnosing pulmonary emphysema from chest Computed Tomography (CT) slices has been designed and implemented in this study. The process of implementing the CAD framework includes segmenting the lung tissues and extracting the regions of interest (ROIs) using the Spatial Intuitionistic Fuzzy C-Means clustering algorithm. The ROIs that were considered in this work were emphysematous lesions - namely, centrilobular, paraseptal, and bullae that were labelled by an expert radiologist. The shape, texture, and run-length features were extracted from each ROI. A wrapper approach that employed four bio-inspired algorithms - namely, Moth-Flame Optimization (MFO), Firefly Optimization (FFO), Artificial Bee Colony Optimization, and Ant Colony Optimization - with the accuracy of the support vector machine classifier as the fitness function was used to select the optimal feature subset. The selected features of each bio-inspired algorithm were trained independently using the Extreme Learning Machine classifier based on the tenfold cross-validation technique. The framework was tested on real-time and public emphysema datasets to perform binary classification of lung CT slices of patients with and without the presence of emphysema. The framework that used MFO and FFO for feature selection produced superior results regarding accuracy, precision, recall, and specificity for the real-time dataset and the public dataset, respectively, when compared to the other bio-inspired algorithms.",2020,10.1016/j.compbiomed.2020.103940,diagnosis,True
Computer-aided lung nodule recognition by SVM classifier based on combination of random undersampling and SMOTE,"In lung cancer computer-aided detection/diagnosis (CAD) systems, classification of regions of interest (ROI) is often used to detect/diagnose lung nodule accurately. However, problems of unbalanced datasets often have detrimental effects on the performance of classification. In this paper, both minority and majority classes are resampled to increase the generalization ability. We propose a novel SVM classifier combined with random undersampling (RU) and SMOTE for lung nodule recognition. The combinations of the two resampling methods not only achieve a balanced training samples but also remove noise and duplicate information in the training sample and retain useful information to improve the effective data utilization, hence improving performance of SVM algorithm for pulmonary nodules classification under the unbalanced data. Eight features including 2D and 3D features are extracted for training and classification. Experimental results show that for different sizes of training datasets our RU-SMOTE-SVM classifier gets the highest classification accuracy among the four kinds of classifiers, and the average classification accuracy is more than 92.94%.",2015,10.1155/2015/368674,diagnosis,True
Computer-Aided System for the Detection of Multicategory Pulmonary Tuberculosis in Radiographs,"The early screening and diagnosis of tuberculosis plays an important role in the control and treatment of tuberculosis infections. In this paper, an integrated computer-aided system based on deep learning is proposed for the detection of multiple categories of tuberculosis lesions in chest radiographs. In this system, the fully convolutional neural network method is used to segment the lung area from the entire chest radiograph for pulmonary tuberculosis detection. Different from the previous analysis of the whole chest radiograph, we focus on the specific tuberculosis lesion areas for the analysis and propose the first multicategory tuberculosis lesion detection method. In it, a learning scalable pyramid structure is introduced into the Faster Region-based Convolutional Network (Faster RCNN), which effectively improves the detection of small-area lesions, mines indistinguishable samples during the training process, and uses reinforcement learning to reduce the detection of false-positive lesions. To compare our method with the current tuberculosis detection system, we propose a classification rule for whole chest X-rays using a multicategory tuberculosis lesion detection model and achieve good performance on two public datasets (Montgomery: AUC = 0.977 and accuracy = 0.926; Shenzhen: AUC = 0.941 and accuracy = 0.902). Our proposed computer-aided system is superior to current systems that can be used to assist radiologists in diagnoses and public health providers in screening for tuberculosis in areas where tuberculosis is endemic.",2020,10.1155/2020/9205082,diagnosis,False
Computer-Assisted Decision Support System in Pulmonary Cancer detection and stage classification on CT images,"Pulmonary cancer is considered as one of the major causes of death worldwide. For the detection of lung cancer, computer-assisted diagnosis (CADx) systems have been designed. Internet-of-Things (IoT) has enabled ubiquitous internet access to biomedical datasets and techniques; in result, the progress in CADx is significant. Unlike the conventional CADx, deep learning techniques have the basic advantage of an automatic exploitation feature as they have the ability to learn mid and high level image representations. We proposed a Computer-Assisted Decision Support System in Pulmonary Cancer by using the novel deep learning based model and metastasis information obtained from MBAN (Medical Body Area Network). The proposed model, DFCNet, is based on the deep fully convolutional neural network (FCNN) which is used for classification of each detected pulmonary nodule into four lung cancer stages. The performance of proposed work is evaluated on different datasets with varying scan conditions. Comparison of proposed classifier is done with the existing CNN techniques. Overall accuracy of CNN and DFCNet was 77.6% and 84.58%, respectively. Experimental results illustrate the effectiveness of proposed method for the detection and classification of lung cancer nodules. These results demonstrate the potential for the proposed technique in helping the radiologists in improving nodule detection accuracy with efficiency.",2018,10.1016/j.jbi.2018.01.005,diagnosis,True
Computer-assisted subtyping and prognosis for non-small cell lung cancer patients with unresectable tumor,"BACKGROUND: The histological classification or subtyping of non-small cell lung cancer is essential for systematic therapy decisions. Differentiating between the two main subtypes of pulmonary adenocarcinoma and squamous cell carcinoma highlights the considerable differences that exist in the prognosis of patient outcomes. Physicians rely on a pathological analysis to reveal these phenotypic variations that requires invasive methods, such as biopsy and resection sample, but almost 70% of tumors are unresectable at the point of diagnosis. METHOD: A computational method that fuses two frameworks of computerized subtyping and prognosis was proposed, and it was validated against publicly available dataset in The Cancer Imaging Archive that consisted of 82 curated patients with CT scans. The accuracy of the proposed method was compared with the gold standard of pathological analysis, as defined by theInternational Classification of Disease for Oncology (ICD-O). A series of survival outcome test cases were evaluated using the Kaplan-Meier estimator and log-rank test (p-value) between the computational method and ICD-O. RESULTS: The computational method demonstrated high accuracy in subtyping (96.2%) and good consistency in the statistical significance of overall survival prediction for adenocarcinoma and squamous cell carcinoma patients (p < 0.03) with respect to its counterpart pathological subtyping (p < 0.02). The degree of reproducibility between prognosis taken on computational and pathological subtyping was substantial with an averaged concordance correlation coefficient (CCC) of 0.9910. CONCLUSION: The findings in this study support the idea that quantitative analysis is capable of representing tissue characteristics, as offered by a qualitative analysis.",2018,10.1016/j.compmedimag.2018.04.003,prognosis,True
Computerized characterization of lung nodule subtlety using thoracic CT images,"The goal of this work is to design computerized image analysis techniques for automatically characterizing lung nodule subtlety in CT images. Automated subtlety estimation methods may help in computer-aided detection (CAD) assessment by quantifying dataset difficulty and facilitating comparisons among different CAD algorithms. A dataset containing 813 nodules from 499 patients was obtained from the Lung Image Database Consortium. Each nodule was evaluated by four radiologists regarding nodule subtlety using a 5-point rating scale (1: most subtle). We developed a 3D technique for segmenting lung nodules using a prespecified initial ROI. Texture and morphological features were automatically extracted from the segmented nodules and their margins. The dataset was partitioned into trainers and testers using a 1:1 ratio. An artificial neural network (ANN) was trained with average reader subtlety scores as the reference. Effective features for characterizing nodule subtlety were selected based on the training set using the ANN and a stepwise feature selection method. The performance of the classifier was evaluated using prediction probability (PK) as an agreement measure, which is considered a generalization of the area under the receiver operating characteristic curve when the reference standard is multi-level. Using an ANN classifier trained with a set of 2 features (selected from a total of 30 features), including compactness and average gray value, the test concordance between computer scores and the average reader scores was 0.789 ± 0.014. Our results show that the proposed method had strong agreement with the average of subtlety scores provided by radiologists.",2014,10.1088/0031-9155/59/4/897,diagnosis,True
"Computerized detection of lung nodules by means of ""virtual dual-energy"" radiography","Major challenges in current computer-aided detection (CADe) schemes for nodule detection in chest radiographs (CXRs) are to detect nodules that overlap with ribs and/or clavicles and to reduce the frequent false positives (FPs) caused by ribs. Detection of such nodules by a CADe scheme is very important, because radiologists are likely to miss such subtle nodules. Our purpose in this study was to develop a CADe scheme with improved sensitivity and specificity by use of ""virtual dual-energy"" (VDE) CXRs where ribs and clavicles are suppressed with massive-training artificial neural networks (MTANNs). To reduce rib-induced FPs and detect nodules overlapping with ribs, we incorporated the VDE technology in our CADe scheme. The VDE technology suppressed rib and clavicle opacities in CXRs while maintaining soft-tissue opacity by use of the MTANN technique that had been trained with real dual-energy imaging. Our scheme detected nodule candidates on VDE images by use of a morphologic filtering technique. Sixty morphologic and gray-level-based features were extracted from each candidate from both original and VDE CXRs. A nonlinear support vector classifier was employed for classification of the nodule candidates. A publicly available database containing 140 nodules in 140 CXRs and 93 normal CXRs was used for testing our CADe scheme. All nodules were confirmed by computed tomography examinations, and the average size of the nodules was 17.8 mm. Thirty percent (42/140) of the nodules were rated ""extremely subtle"" or ""very subtle"" by a radiologist. The original scheme without VDE technology achieved a sensitivity of 78.6% (110/140) with 5 (1165/233) FPs per image. By use of the VDE technology, more nodules overlapping with ribs or clavicles were detected and the sensitivity was improved substantially to 85.0% (119/140) at the same FP rate in a leave-one-out cross-validation test, whereas the FP rate was reduced to 2.5 (583/233) per image at the same sensitivity level as the original CADe scheme obtained (Difference between the specificities of the original and the VDE-based CADe schemes was statistically significant). In particular, the sensitivity of our VDE-based CADe scheme for subtle nodules (66.7% = 28/42) was statistically significantly higher than that of the original CADe scheme (57.1% = 24/42). Therefore, by use of VDE technology, the sensitivity and specificity of our CADe scheme for detection of nodules, especially subtle nodules, in CXRs were improved substantially.",2013,10.1109/tbme.2012.2226583,diagnosis,False
Computerized detection of lung nodules through radiomics,"PURPOSE: Lung cancer is a major cause of cancer deaths, and the 5-year survival rate of stage IV lung cancer patients is only 2%. However, the 5-year survival rate of stage I lung cancer patients significantly increases to 50%. As such, spiral computed tomography (CT) scans are necessary to diagnose high-risk lung cancer patients in early stages. In this study, a computer-aided detection (CAD) system with radiomics was proposed. This system could automatically detect pulmonary nodules and reduce radiologists' workloads and human errors. METHODS: In the proposed scheme, a nodular enhancement filter was used to segment nodule candidates and extract radiomic features. A synthetic minority over-sampling technique was also applied to balance the samples, and a random forest method was utilized to distinguish between real nodules and false positive detections. The radiomics approach quantified intratumor heterogeneity and multifrequency information, which are highly correlated with lung nodules. RESULTS: The proposed method was used to evaluate 1004 CT cases from the well-known Lung Image Database Consortium, and 88.9% sensitivity with four false positive detections per CT scan was obtained by randomly selecting 502 cases for training and 502 other cases for testing. CONCLUSIONS: The proposed scheme yielded a high performance on the LIDC database. Therefore, the proposed scheme is possibly effective for various CT configurations used in routine diagnosis and lung cancer screening.",2017,10.1002/mp.12331,diagnosis,True
Computerized texture analysis of persistent part-solid ground-glass nodules: differentiation of preinvasive lesions from invasive pulmonary adenocarcinomas,"PURPOSE: To retrospectively investigate the value of computerized three-dimensional texture analysis for differentiation of preinvasive lesions from invasive pulmonary adenocarcinomas (IPAs) that manifest as part-solid ground-glass nodules (GGNs). MATERIALS AND METHODS: The institutional review board approved this retrospective study with a waiver of patients' informed consent. The study consisted of 86 patients with 86 pathologic analysis-confirmed part-solid GGNs (mean size, 16 mm ± 5.4 [standard deviation]) who had undergone computed tomographic (CT) imaging between January 2005 and October 2011. Each part-solid GGN was manually segmented and its computerized texture features were quantitatively extracted by using an in-house software program. Multivariate logistic regression analysis was performed to investigate the differentiating factors of preinvasive lesions from IPAs. Three-layered artificial neural networks (ANNs) with a back-propagation algorithm and receiver operating characteristic curve analysis were used to build a discriminating model with texture features and to evaluate its discriminating performance. RESULTS: Pathologic analysis confirmed 58 IPAs (seven minimally invasive adenocarcinomas and 51 invasive adenocarcinomas) and 28 preinvasive lesions (four atypical adenomatous hyperplasias and 24 adenocarcinomas in situ). IPAs and preinvasive lesions exhibited significant differences in various histograms and volumetric parameters (P < .05). Multivariate analysis revealed that smaller mass (adjusted odds ratio, 0.092) and higher kurtosis (adjusted odds ratio, 3.319) are significant differentiators of preinvasive lesions from IPAs (P < .05). With mean attenuation, standard deviation of attenuation, mass, kurtosis, and entropy, the ANNs model showed excellent accuracy in differentiation of preinvasive lesions from IPAs (area under the curve, 0.981). CONCLUSION: In part-solid GGNs, higher kurtosis and smaller mass are significant differentiators of preinvasive lesions from IPAs, and preinvasive lesions can be accurately differentiated from IPAs by using computerized texture analysis. Online supplemental material is available for this article.",2014,10.1148/radiol.14132187,diagnosis,True
Computerized Tomography Image Feature under Convolutional Neural Network Algorithm Evaluated for Therapeutic Effect of Clarithromycin Combined with Salmeterol/Fluticasone on Chronic Obstructive Pulmonary Disease,"This study was to explore the use of convolutional neural network (CNN) for the classification and recognition of computerized tomography (CT) images of chronic obstructive pulmonary disease (COPD) and the therapeutic effect of clarithromycin combined with salmeterol/fluticasone. First, the clinical data of COPD patients treated in hospital from September 2018 to December 2020 were collected, and CT and X-ray images were also collected. CT-CNN and X ray-CNN single modal models were constructed based on the LeNet-5 model. The randomized fusion algorithm was introduced to construct a fused CNN model for the diagnosis of COPD patients, and the recognition effect of the model was verified. Subsequently, the three-dimensional reconstruction of the patient's bronchus was performed using the classified CT images, and the changes of CT quantitative parameters in COPD patients were compared and analyzed. Finally, COPD patients were treated with salmeterol/fluticasone (COPD-C) and combined with clarithromycin (COPD-T). In addition, the differences between patients' lung function indexes, blood gas indexes, St. George respiratory questionnaire (SGRQ) scores, and the number of acute exacerbations (AECOPD) before and after treatment were evaluated. The results showed that the randomized fusion model under different iteration times and batch sizes always had the highest recognition rate, sensitivity, and specificity compared to the two single modal CNN models, but it also had longer training time. After CT images were used to quantitatively evaluate the changes of the patient's bronchus, it was found that the area of the upper and lower lung lobes of the affected side of COPD patients and the ratio of the area of the tube wall to the bronchus were significantly changed. The lung function, blood gas index, and SGRQ score of COPD-T patients were significantly improved compared with the COPD-C group (P < 0.05), but there was no considerable difference in AECOPD (P > 0.05). In summary, the randomized fusion-based CNN model can improve the recognition rate of COPD, and salmeterol/fluticasone combined with clarithromycin therapy can significantly improve the clinical treatment effect of COPD patients.",2021,10.1155/2021/8563181,combined,True
Content-Based Image Retrieval of Chest CT with Convolutional Neural Network for Diffuse Interstitial Lung Disease: Performance Assessment in Three Major Idiopathic Interstitial Pneumonias,"OBJECTIVE: To assess the performance of content-based image retrieval (CBIR) of chest CT for diffuse interstitial lung disease (DILD). MATERIALS AND METHODS: The database was comprised by 246 pairs of chest CTs (initial and follow-up CTs within two years) from 246 patients with usual interstitial pneumonia (UIP, n = 100), nonspecific interstitial pneumonia (NSIP, n = 101), and cryptogenic organic pneumonia (COP, n = 45). Sixty cases (30-UIP, 20-NSIP, and 10-COP) were selected as the queries. The CBIR retrieved five similar CTs as a query from the database by comparing six image patterns (honeycombing, reticular opacity, emphysema, ground-glass opacity, consolidation and normal lung) of DILD, which were automatically quantified and classified by a convolutional neural network. We assessed the rates of retrieving the same pairs of query CTs, and the number of CTs with the same disease class as query CTs in top 1-5 retrievals. Chest radiologists evaluated the similarity between retrieved CTs and queries using a 5-scale grading system (5-almost identical; 4-same disease; 3-likelihood of same disease is half; 2-likely different; and 1-different disease). RESULTS: The rate of retrieving the same pairs of query CTs in top 1 retrieval was 61.7% (37/60) and in top 1-5 retrievals was 81.7% (49/60). The CBIR retrieved the same pairs of query CTs more in UIP compared to NSIP and COP (p = 0.008 and 0.002). On average, it retrieved 4.17 of five similar CTs from the same disease class. Radiologists rated 71.3% to 73.0% of the retrieved CTs with a similarity score of 4 or 5. CONCLUSION: The proposed CBIR system showed good performance for retrieving chest CTs showing similar patterns for DILD.",2021,10.3348/kjr.2020.0603,diagnosis,True
Contralaterally Enhanced Networks for Thoracic Disease Detection,"Identifying and locating diseases in chest X-rays are very challenging, due to the low visual contrast between normal and abnormal regions, and distortions caused by other overlapping tissues. An interesting phenomenon is that there exist many similar structures in the left and right parts of the chest, such as ribs, lung fields and bronchial tubes. This kind of similarities can be used to identify diseases in chest X-rays, according to the experience of broad-certificated radiologists. Aimed at improving the performance of existing detection methods, we propose a deep end-to-end module to exploit the contralateral context information for enhancing feature representations of disease proposals. First of all, under the guidance of the spine line, the spatial transformer network is employed to extract local contralateral patches, which can provide valuable context information for disease proposals. Then, we build up a specific module, based on both additive and subtractive operations, to fuse the features of the disease proposal and the contralateral patch. Our method can be integrated into both fully and weakly supervised disease detection frameworks. It achieves 33.17 AP50 on a carefully annotated private chest X-ray dataset which contains 31,000 images. Experiments on the NIH chest X-ray dataset indicate that our method achieves state-of-the-art performance in weakly-supervised disease localization.",2021,10.1109/tmi.2021.3077913,diagnosis,False
Contrastive Cross-Site Learning With Redesigned Net for COVID-19 CT Classification,"The pandemic of coronavirus disease 2019 (COVID-19) has lead to a global public health crisis spreading hundreds of countries. With the continuous growth of new infections, developing automated tools for COVID-19 identification with CT image is highly desired to assist the clinical diagnosis and reduce the tedious workload of image interpretation. To enlarge the datasets for developing machine learning methods, it is essentially helpful to aggregate the cases from different medical systems for learning robust and generalizable models. This paper proposes a novel joint learning framework to perform accurate COVID-19 identification by effectively learning with heterogeneous datasets with distribution discrepancy. We build a powerful backbone by redesigning the recently proposed COVID-Net in aspects of network architecture and learning strategy to improve the prediction accuracy and learning efficiency. On top of our improved backbone, we further explicitly tackle the cross-site domain shift by conducting separate feature normalization in latent space. Moreover, we propose to use a contrastive training objective to enhance the domain invariance of semantic embeddings for boosting the classification performance on each dataset. We develop and evaluate our method with two public large-scale COVID-19 diagnosis datasets made up of CT images. Extensive experiments show that our approach consistently improves the performanceson both datasets, outperforming the original COVID-Net trained on each dataset by 12.16% and 14.23% in AUC respectively, also exceeding existing state-of-the-art multi-site learning methods.",2020,10.1109/jbhi.2020.3023246,diagnosis,True
Contribution of artificial intelligence applications developed with the deep learning method to the diagnosis of COVID-19 pneumonia on computed tomography,"INTRODUCTION: Computed tomography (CT) is an auxiliary modality in the diagnosis of the novel Coronavirus (COVID-19) disease and can guide physicians in the presence of lung involvement. In this study, we aimed to investigate the contribution of deep learning to diagnosis in patients with typical COVID-19 pneumonia findings on CT. MATERIALS AND METHODS: This study retrospectively evaluated 690 lesions obtained from 35 patients diagnosed with COVID-19 pneumonia based on typical findings on non-contrast high-resolution CT (HRCT) in our hospital. The diagnoses of the patients were also confirmed by other necessary tests. HRCT images were assessed in the parenchymal window. In the images obtained, COVID-19 lesions were detected. For the deep Convolutional Neural Network (CNN) algorithm, the Confusion matrix was used based on a Tensorflow Framework in Python. RESULT: A total of 596 labeled lesions obtained from 224 sections of the images were used for the training of the algorithm, 89 labeled lesions from 27 sections were used in validation, and 67 labeled lesions from 25 images in testing. Fifty-six of the 67 lesions used in the testing stage were accurately detected by the algorithm while the remaining 11 were not recognized. There was no false positive. The Recall, Precision and F1 score values in the test group were 83.58, 1, and 91.06, respectively. CONCLUSIONS: We successfully detected the COVID-19 pneumonia lesions on CT images using the algorithms created with artificial intelligence. The integration of deep learning into the diagnostic stage in medicine is an important step for the diagnosis of diseases that can cause lung involvement in possible future pandemics.",2021,10.5578/tt.20219606,diagnosis,True
Conventional Filtering Versus U-Net Based Models for Pulmonary Nodule Segmentation in CT Images,"Lung cancer is considered one of the deadliest diseases in the world. An early and accurate diagnosis aims to promote the detection and characterization of pulmonary nodules, which is of vital importance to increase the patients' survival rates. The mentioned characterization is done through a segmentation process, facing several challenges due to the diversity in nodular shape, size, and texture, as well as the presence of adjacent structures. This paper tackles pulmonary nodule segmentation in computed tomography scans proposing three distinct methodologies. First, a conventional approach which applies the Sliding Band Filter (SBF) to estimate the filter's support points, matching the border coordinates. The remaining approaches are Deep Learning based, using the U-Net and a novel network called SegU-Net to achieve the same goal. Their performance is compared, as this work aims to identify the most promising tool to improve nodule characterization. All methodologies used 2653 nodules from the LIDC database, achieving a Dice score of 0.663, 0.830, and 0.823 for the SBF, U-Net and SegU-Net respectively. This way, the U-Net based models yield more identical results to the ground truth reference annotated by specialists, thus being a more reliable approach for the proposed exercise. The novel network revealed similar scores to the U-Net, while at the same time reducing computational cost and improving memory efficiency. Consequently, such study may contribute to the possible implementation of this model in a decision support system, assisting the physicians in establishing a reliable diagnosis of lung pathologies based on this segmentation task.",2020,10.1007/s10916-020-1541-9,diagnosis,True
Convolution kernel and iterative reconstruction affect the diagnostic performance of radiomics and deep learning in lung adenocarcinoma pathological subtypes,"BACKGROUND: The aim of this study was to investigate the influence of convolution kernel and iterative reconstruction on the diagnostic performance of radiomics and deep learning (DL) in lung adenocarcinomas. METHODS: A total of 183 patients with 215 lung adenocarcinomas were included in this study. All CT imaging data was reconstructed with three reconstruction algorithms (ASiR at 0%, 30%, 60% strength), each with two convolution kernels (bone and standard). A total of 171 nodules were selected as the training-validation set, whereas 44 nodules were selected as the testing set. Logistic regression and a DL framework-DenseNets were selected to tackle the task. Three logical experiments were implemented to fully explore the influence of the studied parameters on the diagnostic performance. The receiver operating characteristic curve (ROC) was used to evaluate the performance of constructed models. RESULTS: In Experiments A and B, no statistically significant results were found in the radiomic method, whereas two and six pairs were statistically significant (P < 0.05) in the DL method. In Experiment_C, significant differences in one and four models were found in the radiomics and DL methods, respectively. Moreover, models constructed with standard convolution kernel data outperformed that constructed with bone convolution kernel data in all studied ASiR levels in the DL method. In the DL method, B0 and S60 performed best in bone and standard convolution kernel, respectively. CONCLUSION: The results demonstrated that DL was more susceptible to CT parameter variability than radiomics. Standard convolution kernel images seem to be more appropriate for imaging analysis. Further investigation with a larger sample size is needed.",2019,10.1111/1759-7714.13161,combined,True
Convolutional Neural Network Addresses the Confounding Impact of CT Reconstruction Kernels on Radiomics Studies,"Achieving high feature reproducibility while preserving biological information is one of the main challenges for the generalizability of current radiomics studies. Non-clinical imaging variables, such as reconstruction kernels, have shown to significantly impact radiomics features. In this study, we retrain an open-source convolutional neural network (CNN) to harmonize computerized tomography (CT) images with various reconstruction kernels to improve feature reproducibility and radiomic model performance using epidermal growth factor receptor (EGFR) mutation prediction in lung cancer as a paradigm. In the training phase, the CNN was retrained and tested on 32 lung cancer patients' CT images between two different groups of reconstruction kernels (smooth and sharp). In the validation phase, the retrained CNN was validated on an external cohort of 223 lung cancer patients' CT images acquired using different CT scanners and kernels. The results showed that the retrained CNN could be successfully applied to external datasets with different CT scanner parameters, and harmonization of reconstruction kernels from sharp to smooth could significantly improve the performance of radiomics model in predicting EGFR mutation status in lung cancer. In conclusion, the CNN based method showed great potential in improving feature reproducibility and generalizability by harmonizing medical images with heterogeneous reconstruction kernels.",2021,10.3390/tomography7040074,combined,True
Convolutional Neural Network ensembles for accurate lung nodule malignancy prediction 2 years in the future,"Convolutional Neural Networks (CNNs) have been utilized for to distinguish between benign lung nodules and those that will become malignant. The objective of this study was to use an ensemble of CNNs to predict which baseline nodules would be diagnosed as lung cancer in a second follow up screening after more than one year. Low-dose helical computed tomography images and data were utilized from the National Lung Screening Trial (NLST). The malignant nodules and nodule positive controls were divided into training and test cohorts. T0 nodules were used to predict lung cancer incidence at T1 or T2. To increase the sample size, image augmentation was performed using rotations, flipping, and elastic deformation. Three CNN architectures were designed for malignancy prediction, and each architecture was trained using seven different seeds to create the initial weights. This enabled variability in the CNN models which were combined to generate a robust, more accurate ensemble model. Augmenting images using only rotation and flipping and training with images from T0 yielded the best accuracy to predict lung cancer incidence at T2 from a separate test cohort (Accuracy = 90.29%; AUC = 0.96) based on an ensemble 21 models. Images augmented by rotation and flipping enabled effective learning by increasing the relatively small sample size. Ensemble learning with deep neural networks is a compelling approach that accurately predicted lung cancer incidence at the second screening after the baseline screen mostly 2 years later.",2020,10.1016/j.compbiomed.2020.103882,prognosis,True
Convolutional Neural Networks in Predicting Nodal and Distant Metastatic Potential of Newly Diagnosed Non-Small Cell Lung Cancer on FDG PET Images,"OBJECTIVE. The purpose of this study was to assess, by analyzing features of the primary tumor with (18)F-FDG PET, the utility of deep machine learning with a convolutional neural network (CNN) in predicting the potential of newly diagnosed non-small cell lung cancer (NSCLC) to metastasize to lymph nodes or distant sites. MATERIALS AND METHODS. Consecutively registered patients with newly diagnosed, untreated NSCLC were retrospectively included in a single-center study. PET images were segmented with local image features extraction software, and data were used for CNN training and validation after data augmentation strategies were used. The standard of reference for designation of N category was invasive lymph node sampling or 6-month follow-up imaging. Distant metastases developing during the study follow-up period were assessed by imaging (CT or PET/CT), in tissue obtained from new suspected sites of disease, and according to the treating oncologist's designation. RESULTS. A total of 264 patients with NSCLC participated in follow-up for a median of 25.2 months (range, 6-43 months). N category designations were available for 223 of 264 (84.5%) patients, and M category for all 264. The sensitivity, specificity, and accuracy of CNN for predicting node positivity were 0.74 ± 0.32, 0.84 ± 0.16, and 0.80 ± 0.17. The corresponding values for predicting distant metastases were 0.45 ± 0.08, 0.79 ± 0.06, and 0.63 ± 0.05. CONCLUSION. This study showed that using a CNN to analyze segmented PET images of patients with previously untreated NSCLC can yield moderately high accuracy for designation of N category, although this may be insufficient to preclude invasive lymph node sampling. The sensitivity of the CNN in predicting distant metastases is fairly poor, although specificity is moderately high.",2020,10.2214/ajr.19.22346,prognosis,False
Convolutional Neural Networks Promising in Lung Cancer T-Parameter Assessment on Baseline FDG-PET/CT,"AIM: To develop an algorithm, based on convolutional neural network (CNN), for the classification of lung cancer lesions as T1-T2 or T3-T4 on staging fluorodeoxyglucose positron emission tomography (FDG-PET)/CT images. METHODS: We retrospectively selected a cohort of 472 patients (divided in the training, validation, and test sets) submitted to staging FDG-PET/CT within 60 days before biopsy or surgery. TNM system seventh edition was used as reference. Postprocessing was performed to generate an adequate dataset. The input of CNNs was a bounding box on both PET and CT images, cropped around the lesion centre. The results were classified as Correct (concordance between reference and prediction) and Incorrect (discordance between reference and prediction). Accuracy (Correct/[Correct + Incorrect]), recall (Correctly predicted T3-T4/[all T3-T4]), and specificity (Correctly predicted T1-T2/[all T1-T2]), as commonly defined in deep learning models, were used to evaluate CNN performance. The area under the curve (AUC) was calculated for the final model. RESULTS: The algorithm, composed of two networks (a ""feature extractor"" and a ""classifier""), developed and tested achieved an accuracy, recall, specificity, and AUC of 87%, 69%, 69%, and 0.83; 86%, 77%, 70%, and 0.73; and 90%, 47%, 67%, and 0.68 in the training, validation, and test sets, respectively. CONCLUSION: We obtained proof of concept that CNNs can be used as a tool to assist in the staging of patients affected by lung cancer.",2018,10.1155/2018/1382309,diagnosis,True
COPD identification and grading based on deep learning of lung parenchyma and bronchial wall in chest CT images,"OBJECTIVE: Chest CT can display the main pathogenic factors of chronic obstructive pulmonary disease (COPD), emphysema and airway wall remodeling. This study aims to establish deep convolutional neural network (CNN) models using these two imaging markers to diagnose and grade COPD. METHODS: Subjects who underwent chest CT and pulmonary function test (PFT) from one hospital (n = 373) were retrospectively included as the training cohort, and subjects from another hospital (n = 226) were used as the external test cohort. According to the PFT results, all subjects were labeled as Global Initiative for Chronic Obstructive Lung Disease (GOLD) Grade 1, 2, 3, 4 or normal. Two DenseNet-201 CNNs were trained using CT images of lung parenchyma and bronchial wall to generate two corresponding confidence levels to indicate the possibility of COPD, then combined with logistic regression analysis. Quantitative CT was used for comparison. RESULTS: In the test cohort, CNN achieved an area under the curve of 0.899 (95%CI: 0.853-0.935) to determine the existence of COPD, and an accuracy of 81.7% (76.2-86.7%), which was significantly higher than the accuracy 68.1% (61.6%-74.2%) using quantitative CT method (p < 0.05). For three-way (normal, GOLD 1-2, and GOLD 3-4) and five-way (normal, GOLD 1, 2, 3, and 4) classifications, CNN reached accuracies of 77.4 and 67.9%, respectively. CONCLUSION: CNN can identify emphysema and airway wall remodeling on CT images to infer lung function and determine the existence and severity of COPD. It provides an alternative way to detect COPD using the extensively available chest CT. ADVANCES IN KNOWLEDGE: CNN can identify the main pathological changes of COPD (emphysema and airway wall remodeling) based on CT images, to infer lung function and determine the existence and severity of COPD. CNN reached an area under the curve of 0.853 to determine the existence of COPD in the external test cohort. The CNN approach provides an alternative and effective way for early detection of COPD using extensively used chest CT, as an important alternative to pulmonary function test.",2022,10.1259/bjr.20210637,diagnosis,True
Coronavirus disease analysis using chest X-ray images and a novel deep convolutional neural network,"BACKGROUND: The recent emergence of a highly infectious and contagious respiratory viral disease known as COVID-19 has vastly impacted human lives and overloaded the health care system. Therefore, it is crucial to develop a fast and accurate diagnostic system for the timely identification of COVID-19 infected patients and thus to help control its spread. METHODS: This work proposes a new deep CNN based technique for COVID-19 classification in X-ray images. In this regard, two novel custom CNN architectures, namely COVID-RENet-1 and COVID-RENet-2, are developed for COVID-19 specific pneumonia analysis. The proposed technique systematically employs Region and Edge-based operations along with convolution operations. The advantage of the proposed idea is validated by performing series of experimentation and comparing results with two baseline CNNs that exploited either a single type of pooling operation or strided convolution down the architecture. Additionally, the discrimination capacity of the proposed technique is assessed by benchmarking it against the state-of-the-art CNNs on radiologist's authenticated chest X-ray dataset. Implementation is available at https://github.com/PRLAB21/Coronavirus-Disease-Analysis-using-Chest-X-Ray-Images. RESULTS: The proposed classification technique shows good generalization as compared to existing CNNs by achieving promising MCC (0.96), F-score (0.98) and Accuracy (98%). This suggests that the idea of synergistically using Region and Edge-based operations aid in better exploiting the region homogeneity, textural variations, and region boundary-related information in an image, which helps to capture the pneumonia specific pattern. CONCLUSIONS: The encouraging results of the proposed classification technique on the test set with high sensitivity (0.98) and precision (0.98) suggest the effectiveness of the proposed technique. Thus, it suggests the potential use of the proposed technique in other X-ray imagery-based infectious disease analysis.",2021,10.1016/j.pdpdt.2021.102473,diagnosis,False
CoroNet: A deep neural network for detection and diagnosis of COVID-19 from chest x-ray images,"BACKGROUND AND OBJECTIVE: The novel Coronavirus also called COVID-19 originated in Wuhan, China in December 2019 and has now spread across the world. It has so far infected around 1.8 million people and claimed approximately 114,698 lives overall. As the number of cases are rapidly increasing, most of the countries are facing shortage of testing kits and resources. The limited quantity of testing kits and increasing number of daily cases encouraged us to come up with a Deep Learning model that can aid radiologists and clinicians in detecting COVID-19 cases using chest X-rays. METHODS: In this study, we propose CoroNet, a Deep Convolutional Neural Network model to automatically detect COVID-19 infection from chest X-ray images. The proposed model is based on Xception architecture pre-trained on ImageNet dataset and trained end-to-end on a dataset prepared by collecting COVID-19 and other chest pneumonia X-ray images from two different publically available databases. RESULTS: CoroNet has been trained and tested on the prepared dataset and the experimental results show that our proposed model achieved an overall accuracy of 89.6%, and more importantly the precision and recall rate for COVID-19 cases are 93% and 98.2% for 4-class cases (COVID vs Pneumonia bacterial vs pneumonia viral vs normal). For 3-class classification (COVID vs Pneumonia vs normal), the proposed model produced a classification accuracy of 95%. The preliminary results of this study look promising which can be further improved as more training data becomes available. CONCLUSION: CoroNet achieved promising results on a small prepared dataset which indicates that given more data, the proposed model can achieve better results with minimum pre-processing of data. Overall, the proposed model substantially advances the current radiology based methodology and during COVID-19 pandemic, it can be very helpful tool for clinical practitioners and radiologists to aid them in diagnosis, quantification and follow-up of COVID-19 cases.",2020,10.1016/j.cmpb.2020.105581,diagnosis,False
Could automated analysis of chest X-rays detect early bronchiectasis in children?,"Non-cystic fibrosis bronchiectasis is increasingly described in the paediatric population. While diagnosis is by high-resolution chest computed tomography (CT), chest X-rays (CXRs) remain a first-line investigation. CXRs are currently insensitive in their detection of bronchiectasis. We aim to determine if quantitative digital analysis allows CT features of bronchiectasis to be detected in contemporaneously taken CXRs. Regions of radiologically (A) normal, (B) severe bronchiectasis, (C) mild airway dilation and (D) other parenchymal abnormalities were identified in CT and mapped to corresponding CXR. An artificial neural network (ANN) algorithm was used to characterise regions of classes A, B, C and D. The algorithm was then tested in 13 subjects and compared to CT scan features. Structural changes in CT were reflected in CXR, including mild airway dilation. The areas under the receiver operator curve for ANN feature detection were 0.74 (class A), 0.71 (class B), 0.76 (class C) and 0.86 (class D). CXR analysis identified CT measures of abnormality with a better correlation than standard radiological scoring at the 99% confidence level.Conclusion: Regional abnormalities can be detected by digital analysis of CXR, which may provide a low-cost and readily available tool to indicate the need for diagnostic CT and for ongoing disease monitoring. What is Known: • Bronchiectasis is a severe chronic respiratory disorder increasingly recognised in paediatric populations. • Diagnostic computed tomography imaging is often requested only after several chest X-ray investigations. What is New: • We show that a digital analysis of chest X-ray could provide more accurate identification of bronchiectasis features.",2021,10.1007/s00431-021-04061-8,diagnosis,True
COV-DLS: Prediction of COVID-19 from X-Rays Using Enhanced Deep Transfer Learning Techniques,"In this paper, modifications in neoteric architectures such as VGG16, VGG19, ResNet50, and InceptionV3 are proposed for the classification of COVID-19 using chest X-rays. The proposed architectures termed ""COV-DLS"" consist of two phases: heading model construction and classification. The heading model construction phase utilizes four modified deep learning architectures, namely Modified-VGG16, Modified-VGG19, Modified-ResNet50, and Modified-InceptionV3. An attempt is made to modify these neoteric architectures by incorporating the average pooling and dense layers. The dropout layer is also added to prevent the overfitting problem. Two dense layers with different activation functions are also added. Thereafter, the output of these modified models is applied during the classification phase, when COV-DLS are applied on a COVID-19 chest X-ray image data set. Classification accuracy of 98.61% is achieved by Modified-VGG16, 97.22% by Modified-VGG19, 95.13% by Modified-ResNet50, and 99.31% by Modified-InceptionV3. COV-DLS outperforms existing deep learning models in terms of accuracy and F1-score.",2022,10.1155/2022/6216273,diagnosis,False
COVID Detection From Chest X-Ray Images Using Multi-Scale Attention,"Deep learning based methods have shown great promise in achieving accurate automatic detection of Coronavirus Disease (covid) - 19 from Chest X-Ray (cxr) images.However, incorporating explainability in these solutions remains relatively less explored. We present a hierarchical classification approach for separating normal, non-covid pneumonia (ncp) and covid cases using cxr images. We demonstrate that the proposed method achieves clinically consistent explainations. We achieve this using a novel multi-scale attention architecture called Multi-scale Attention Residual Learning (marl) and a new loss function based on conicity for training the proposed architecture. The proposed classification strategy has two stages. The first stage uses a model derived from DenseNet to separate pneumonia cases from normal cases while the second stage uses the marl architecture to discriminate between covid and ncp cases. With a five-fold cross validation the proposed method achieves 93%, 96.28%, and 84.51% accuracy respectively over three large, public datasets for normal vs. ncp vs. covid classification. This is competitive to the state-of-the-art methods. We also provide explanations in the form of GradCAM attributions, which are well aligned with expert annotations. The attributions are also seen to clearly indicate that marl deems the peripheral regions of the lungs to be more important in the case of covid cases while central regions are seen as more important in ncp cases. This observation matches the criteria described by radiologists in clinical literature, thereby attesting to the utility of the derived explanations.",2022,10.1109/jbhi.2022.3151171,diagnosis,False
COVID-19 Automatic Diagnosis With Radiographic Imaging: Explainable Attention Transfer Deep Neural Networks,"Researchers seek help from deep learning methods to alleviate the enormous burden of reading radiological images by clinicians during the COVID-19 pandemic. However, clinicians are often reluctant to trust deep models due to their black-box characteristics. To automatically differentiate COVID-19 and community-acquired pneumonia from healthy lungs in radiographic imaging, we propose an explainable attention-transfer classification model based on the knowledge distillation network structure. The attention transfer direction always goes from the teacher network to the student network. Firstly, the teacher network extracts global features and concentrates on the infection regions to generate attention maps. It uses a deformable attention module to strengthen the response of infection regions and to suppress noise in irrelevant regions with an expanded reception field. Secondly, an image fusion module combines attention knowledge transferred from teacher network to student network with the essential information in original input. While the teacher network focuses on global features, the student branch focuses on irregularly shaped lesion regions to learn discriminative features. Lastly, we conduct extensive experiments on public chest X-ray and CT datasets to demonstrate the explainability of the proposed architecture in diagnosing COVID-19.",2021,10.1109/jbhi.2021.3074893,diagnosis,True
"COVID-19 Case Recognition from Chest CT Images by Deep Learning, Entropy-Controlled Firefly Optimization, and Parallel Feature Fusion","In healthcare, a multitude of data is collected from medical sensors and devices, such as X-ray machines, magnetic resonance imaging, computed tomography (CT), and so on, that can be analyzed by artificial intelligence methods for early diagnosis of diseases. Recently, the outbreak of the COVID-19 disease caused many deaths. Computer vision researchers support medical doctors by employing deep learning techniques on medical images to diagnose COVID-19 patients. Various methods were proposed for COVID-19 case classification. A new automated technique is proposed using parallel fusion and optimization of deep learning models. The proposed technique starts with a contrast enhancement using a combination of top-hat and Wiener filters. Two pre-trained deep learning models (AlexNet and VGG16) are employed and fine-tuned according to target classes (COVID-19 and healthy). Features are extracted and fused using a parallel fusion approach-parallel positive correlation. Optimal features are selected using the entropy-controlled firefly optimization method. The selected features are classified using machine learning classifiers such as multiclass support vector machine (MC-SVM). Experiments were carried out using the Radiopaedia database and achieved an accuracy of 98%. Moreover, a detailed analysis is conducted and shows the improved performance of the proposed scheme.",2021,10.3390/s21217286,diagnosis,True
COVID-19 classification of X-ray images using deep neural networks,"OBJECTIVES: In the midst of the coronavirus disease 2019 (COVID-19) outbreak, chest X-ray (CXR) imaging is playing an important role in diagnosis and monitoring of patients with COVID-19. We propose a deep learning model for detection of COVID-19 from CXRs, as well as a tool for retrieving similar patients according to the model's results on their CXRs. For training and evaluating our model, we collected CXRs from inpatients hospitalized in four different hospitals. METHODS: In this retrospective study, 1384 frontal CXRs, of COVID-19 confirmed patients imaged between March and August 2020, and 1024 matching CXRs of non-COVID patients imaged before the pandemic, were collected and used to build a deep learning classifier for detecting patients positive for COVID-19. The classifier consists of an ensemble of pre-trained deep neural networks (DNNS), specifically, ReNet34, ReNet50¸ ReNet152, and vgg16, and is enhanced by data augmentation and lung segmentation. We further implemented a nearest-neighbors algorithm that uses DNN-based image embeddings to retrieve the images most similar to a given image. RESULTS: Our model achieved accuracy of 90.3%, (95% CI: 86.3-93.7%) specificity of 90% (95% CI: 84.3-94%), and sensitivity of 90.5% (95% CI: 85-94%) on a test dataset comprising 15% (350/2326) of the original images. The AUC of the ROC curve is 0.96 (95% CI: 0.93-0.97). CONCLUSION: We provide deep learning models, trained and evaluated on CXRs that can assist medical efforts and reduce medical staff workload in handling COVID-19. KEY POINTS: • A machine learning model was able to detect chest X-ray (CXR) images of patients tested positive for COVID-19 with accuracy and detection rate above 90%. • A tool was created for finding existing CXR images with imaging characteristics most similar to a given CXR, according to the model's image embeddings.",2021,10.1007/s00330-021-08050-1,diagnosis,False
COVID-19 deep classification network based on convolution and deconvolution local enhancement,"Computer Tomography (CT) detection can effectively overcome the problems of traditional detection of Corona Virus Disease 2019 (COVID-19), such as lagging detection results and wrong diagnosis results, which lead to the increase of disease infection rate and prevalence rate. The novel coronavirus pneumonia is a significant difference between the positive and negative patients with asymptomatic infections. To effectively improve the accuracy of doctors' manual judgment of positive and negative COVID-19, this paper proposes a deep classification network model of the novel coronavirus pneumonia based on convolution and deconvolution local enhancement. Through convolution and deconvolution operation, the contrast between the local lesion region and the abdominal cavity of COVID-19 is enhanced. Besides, the middle-level features that can effectively distinguish the image types are obtained. By transforming the novel coronavirus detection problem into the region of interest (ROI) feature classification problem, it can effectively determine whether the feature vector in each feature channel contains the image features of COVID-19. This paper uses an open-source COVID-CT dataset provided by Petuum researchers from the University of California, San Diego, which is collected from 143 novel coronavirus pneumonia patients and the corresponding features are preserved. The complete dataset (including original image and enhanced image) contains 1460 images. Among them, 1022 (70%) and 438 (30%) are used to train and test the performance of the proposed model, respectively. The proposed model verifies the classification precision in different convolution layers and learning rates. Besides, it is compared with most state-of-the-art models. It is found that the proposed algorithm has good classification performance. The corresponding sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and precision are 0.98, 0.96, 0.98, and 0.97, respectively.",2021,10.1016/j.compbiomed.2021.104588,diagnosis,True
COVID-19 detection from lung CT-Scans using a fuzzy integral-based CNN ensemble,"The COVID-19 pandemic has collapsed the public healthcare systems, along with severely damaging the economy of the world. The SARS-CoV-2 virus also known as the coronavirus, led to community spread, causing the death of more than a million people worldwide. The primary reason for the uncontrolled spread of the virus is the lack of provision for population-wise screening. The apparatus for RT-PCR based COVID-19 detection is scarce and the testing process takes 6-9 h. The test is also not satisfactorily sensitive (71% sensitive only). Hence, Computer-Aided Detection techniques based on deep learning methods can be used in such a scenario using other modalities like chest CT-scan images for more accurate and sensitive screening. In this paper, we propose a method that uses a Sugeno fuzzy integral ensemble of four pre-trained deep learning models, namely, VGG-11, GoogLeNet, SqueezeNet v1.1 and Wide ResNet-50-2, for classification of chest CT-scan images into COVID and Non-COVID categories. The proposed framework has been tested on a publicly available dataset for evaluation and it achieves 98.93% accuracy and 98.93% sensitivity on the same. The model outperforms state-of-the-art methods on the same dataset and proves to be a reliable COVID-19 detector. The relevant source codes for the proposed approach can be found at: https://github.com/Rohit-Kundu/Fuzzy-Integral-Covid-Detection.",2021,10.1016/j.compbiomed.2021.104895,diagnosis,True
COVID-19 detection in chest X-ray images using deep boosted hybrid learning,"The new emerging COVID-19, declared a pandemic disease, has affected millions of human lives and caused a massive burden on healthcare centers. Therefore, a quick, accurate, and low-cost computer-based tool is required to timely detect and treat COVID-19 patients. In this work, two new deep learning frameworks: Deep Hybrid Learning (DHL) and Deep Boosted Hybrid Learning (DBHL), is proposed for effective COVID-19 detection in X-ray dataset. In the proposed DHL framework, the representation learning ability of the two developed COVID-RENet-1 & 2 models is exploited individually through a machine learning (ML) classifier. In COVID-RENet models, Region and Edge-based operations are carefully applied to learn region homogeneity and extract boundaries features. While in the case of the proposed DBHL framework, COVID-RENet-1 & 2 are fine-tuned using transfer learning on the chest X-rays. Furthermore, deep feature spaces are generated from the penultimate layers of the two models and then concatenated to get a single enriched boosted feature space. A conventional ML classifier exploits the enriched feature space to achieve better COVID-19 detection performance. The proposed COVID-19 detection frameworks are evaluated on radiologist's authenticated chest X-ray data, and their performance is compared with the well-established CNNs. It is observed through experiments that the proposed DBHL framework, which merges the two-deep CNN feature spaces, yields good performance (accuracy: 98.53%, sensitivity: 0.99, F-score: 0.98, and precision: 0.98). Furthermore, a web-based interface is developed, which takes only 5-10s to detect COVID-19 in each unseen chest X-ray image. This web-predictor is expected to help early diagnosis, save precious lives, and thus positively impact society.",2021,10.1016/j.compbiomed.2021.104816,diagnosis,False
COVID-19 detection in CT and CXR images using deep learning models,"Infectious diseases pose a threat to human life and could affect the whole world in a very short time. Corona-2019 virus disease (COVID-19) is an example of such harmful diseases. COVID-19 is a pandemic of an emerging infectious disease, called coronavirus disease 2019 or COVID-19, caused by the coronavirus SARS-CoV-2, which first appeared in December 2019 in Wuhan, China, before spreading around the world on a very large scale. The continued rise in the number of positive COVID-19 cases has disrupted the health care system in many countries, creating a lot of stress for governing bodies around the world, hence the need for a rapid way to identify cases of this disease. Medical imaging is a widely accepted technique for early detection and diagnosis of the disease which includes different techniques such as Chest X-ray (CXR), Computed Tomography (CT) scan, etc. In this paper, we propose a methodology to investigate the potential of deep transfer learning in building a classifier to detect COVID-19 positive patients using CT scan and CXR images. Data augmentation technique is used to increase the size of the training dataset in order to solve overfitting and enhance generalization ability of the model. Our contribution consists of a comprehensive evaluation of a series of pre-trained deep neural networks: ResNet50, InceptionV3, VGGNet-19, and Xception, using data augmentation technique. The findings proved that deep learning is effective at detecting COVID-19 cases. From the results of the experiments it was found that by considering each modality separately, the VGGNet-19 model outperforms the other three models proposed by using the CT image dataset where it achieved 88.5% precision, 86% recall, 86.5% F1-score, and 87% accuracy while the refined Xception version gave the highest precision, recall, F1-score, and accuracy values which equal 98% using CXR images dataset. On the other hand, and by applying the average of the two modalities X-ray and CT, VGG-19 presents the best score which is 90.5% for the accuracy and the F1-score, 90.3% for the recall while the precision is 91.5%. These results enables to automatize the process of analyzing chest CT scans and X-ray images with high accuracy and can be used in cases where RT-PCR testing and materials are limited.",2022,10.1007/s10522-021-09946-7,diagnosis,True
COVID-19 detection using chest X-ray images based on a developed deep neural network,"AIM: Currently, a new coronavirus called COVID-19 is the biggest challenge of the human at 21st century. Now, the spread of this virus is such that mortality has risen strongly in all cities of countries. Therefore, it is necessary to think of a solution to handle the disease by fast and timely diagnosis. This paper proposes a method that uses chest X-ray imagery to divide 2-4 classes into 7 different Scenarios, including Bacterial, Viral, Healthy, and COVID-19 classes. The aim of this study is to propose a method that uses chest X-ray imagery to divide 2-4 classes into 7 different Scenarios, including Bacterial, Viral, Healthy, and COVID-19 classes. METHODS: 6 different databases from chest X-ray imagery that have been widely used in recent studies have been gathered for this aim. A Convolutional Neural Network-Long Short Time Memory model is designed and developed to extract features from raw data hierarchically. In order to make more realistic assumptions and use the Proposed Method in the practical field, white Gaussian noise is added to the raw chest X-ray imagery. Additionally, the proposed network is tested and investigated not only on 6 expressed databases but also on two additional databases. RESULTS: On the test set, the proposed network achieved an accuracy of more than 90% for all Scenarios excluding Scenario V, i.e. Healthy against the COVID-19 against the Viral, and also achieved 99% accuracy for separating the COVID-19 from the Healthy group. The results showed that the proposed network is robust to noise up to 1 dB. It is worth noting that the proposed network for two additional databases, which were only used as test databases, also achieved more than 90% accuracy. In addition, in comparison to the state-of-the-art pneumonia detection approaches, the final results obtained from the proposed network is so promising. CONCLUSIONS: The proposed network is effective in detecting COVID-19 and other lung infectious diseases using chest X-ray imagery and can thus assist radiologists in making rapid and accurate detections.",2022,10.1016/j.slast.2021.10.011,diagnosis,False
COVID-19 detection using deep learning models to exploit Social Mimic Optimization and structured chest X-ray images using fuzzy color and stacking approaches,"Coronavirus causes a wide variety of respiratory infections and it is an RNA-type virus that can infect both humans and animal species. It often causes pneumonia in humans. Artificial intelligence models have been helpful for successful analyses in the biomedical field. In this study, Coronavirus was detected using a deep learning model, which is a sub-branch of artificial intelligence. Our dataset consists of three classes namely: coronavirus, pneumonia, and normal X-ray imagery. In this study, the data classes were restructured using the Fuzzy Color technique as a preprocessing step and the images that were structured with the original images were stacked. In the next step, the stacked dataset was trained with deep learning models (MobileNetV2, SqueezeNet) and the feature sets obtained by the models were processed using the Social Mimic optimization method. Thereafter, efficient features were combined and classified using Support Vector Machines (SVM). The overall classification rate obtained with the proposed approach was 99.27%. With the proposed approach in this study, it is evident that the model can efficiently contribute to the detection of COVID-19 disease.",2020,10.1016/j.compbiomed.2020.103805,diagnosis,False
COVID-19 detection using federated machine learning,"The current COVID-19 pandemic threatens human life, health, and productivity. AI plays an essential role in COVID-19 case classification as we can apply machine learning models on COVID-19 case data to predict infectious cases and recovery rates using chest x-ray. Accessing patient's private data violates patient privacy and traditional machine learning model requires accessing or transferring whole data to train the model. In recent years, there has been increasing interest in federated machine learning, as it provides an effective solution for data privacy, centralized computation, and high computation power. In this paper, we studied the efficacy of federated learning versus traditional learning by developing two machine learning models (a federated learning model and a traditional machine learning model)using Keras and TensorFlow federated, we used a descriptive dataset and chest x-ray (CXR) images from COVID-19 patients. During the model training stage, we tried to identify which factors affect model prediction accuracy and loss like activation function, model optimizer, learning rate, number of rounds, and data Size, we kept recording and plotting the model loss and prediction accuracy per each training round, to identify which factors affect the model performance, and we found that softmax activation function and SGD optimizer give better prediction accuracy and loss, changing the number of rounds and learning rate has slightly effect on model prediction accuracy and prediction loss but increasing the data size did not have any effect on model prediction accuracy and prediction loss. finally, we build a comparison between the proposed models' loss, accuracy, and performance speed, the results demonstrate that the federated machine learning model has a better prediction accuracy and loss but higher performance time than the traditional machine learning model.",2021,10.1371/journal.pone.0252573,diagnosis,False
COVID-19 diagnosis and severity detection from CT-images using transfer learning and back propagation neural network,"BACKGROUND: COVID-19 diagnosis in symptomatic patients is an important factor for arranging the necessary lifesaving facilities like ICU care and ventilator support. For this purpose, we designed a computer-aided diagnosis and severity detection method by using transfer learning and a back propagation neural network. METHOD: To increase the learning capability, we used data augmentation. Most of the previously done works in this area concentrate on private datasets, but we used two publicly available datasets. The first section diagnose COVID-19 from the input CT image using the transfer learning of the pre-trained network ResNet-50. We used ResNet-50 and DenseNet-201 pre-trained networks for feature extraction and trained a back propagation neural network to classify it into High, Medium, and Low severity. RESULTS: The proposed method for COVID-19 diagnosis gave an accuracy of 98.5% compared with the state-of-the-art methods. The experimental evaluation shows that combining the ResNet-50 and DenseNet-201 features gave more accurate results with the test data. The proposed system for COVID-19 severity detection gave better average classification accuracy of 97.84% compared with the state-of-the-art methods. This enables medical practitioners to identify the resources and treatment plans correctly. CONCLUSIONS: This work is useful in the medical field as a first-line severity risk detection that is helpful for medical personnel to plan patient care and assess the need for ICU facilities and ventilator support. A computer-aided system that is helpful to make a care plan for the huge amount of patient inflow each day is sure to be an asset in these turbulent times.",2021,10.1016/j.jiph.2021.07.015,combined,True
COVID-19 diagnosis from chest X-ray images using transfer learning: Enhanced performance by debiasing dataloader,"BACKGROUND: Chest X-ray imaging has been proved as a powerful diagnostic method to detect and diagnose COVID-19 cases due to its easy accessibility, lower cost and rapid imaging time. OBJECTIVE: This study aims to improve efficacy of screening COVID-19 infected patients using chest X-ray images with the help of a developed deep convolutional neural network model (CNN) entitled nCoV-NET. METHODS: To train and to evaluate the performance of the developed model, three datasets were collected from resources of ""ChestX-ray14"", ""COVID-19 image data collection"", and ""Chest X-ray collection from Indiana University,"" respectively. Overall, 299 COVID-19 pneumonia cases and 1,522 non-COVID 19 cases are involved in this study. To overcome the probable bias due to the unbalanced cases in two classes of the datasets, ResNet, DenseNet, and VGG architectures were re-trained in the fine-tuning stage of the process to distinguish COVID-19 classes using a transfer learning method. Lastly, the optimized final nCoV-NET model was applied to the testing dataset to verify the performance of the proposed model. RESULTS: Although the performance parameters of all re-trained architectures were determined close to each other, the final nCOV-NET model optimized by using DenseNet-161 architecture in the transfer learning stage exhibits the highest performance for classification of COVID-19 cases with the accuracy of 97.1 %. The Activation Mapping method was used to create activation maps that highlights the crucial areas of the radiograph to improve causality and intelligibility. CONCLUSION: This study demonstrated that the proposed CNN model called nCoV-NET can be utilized for reliably detecting COVID-19 cases using chest X-ray images to accelerate the triaging and save critical time for disease control as well as assisting the radiologist to validate their initial diagnosis.",2021,10.3233/xst-200757,diagnosis,False
COVID-19 Diagnosis from CT Images with Convolutional Neural Network Optimized by Marine Predator Optimization Algorithm,"In recent years, almost every country in the world has struggled against the spread of Coronavirus Disease 2019. If governments and public health systems do not take action against the spread of the disease, it will have a severe impact on human life. A noteworthy technique to stop this pandemic is diagnosing COVID-19 infected patients and isolating them instantly. The present study proposes a method for the diagnosis of COVID-19 from CT images. The method is a hybrid method based on convolutional neural network which is optimized by a newly introduced metaheuristic, called marine predator optimization algorithm. This optimization method is performed to improve the system accuracy. The method is then implemented on the chest CT scans with the COVID-19-related findings (MosMedData) dataset, and the results are compared with three other methods from the literature to indicate the method's performance. The final results indicate that the proposed method with 98.11% accuracy, 98.13% precision, 98.66% sensitivity, and 97.26% F1 score has the highest performance in all indicators than the compared methods which shows its higher accuracy and reliability.",2021,10.1155/2021/5122962,diagnosis,True
COVID-19 diagnosis on CT scan images using a generative adversarial network and concatenated feature pyramid network with an attention mechanism,"OBJECTIVE: Coronavirus disease 2019 (COVID-19) has caused hundreds of thousands of infections and deaths. Efficient diagnostic methods could help curb its global spread. The purpose of this study was to develop and evaluate a method for accurately diagnosing COVID-19 based on computed tomography (CT) scans in real time. METHODS: We propose an architecture named ""concatenated feature pyramid network"" (""Concat-FPN"") with an attention mechanism, by concatenating feature maps of multiple. The proposed architecture is then used to form two networks, which we call COVID-CT-GAN and COVID-CT-DenseNet, the former for data augmentation and the latter for data classification. RESULTS: The proposed method is evaluated on 3 different numbers of magnitude of COVID-19 CT datasets. Compared with the method without GANs for data augmentation or the original network auxiliary classifier generative adversarial network, COVID-CT-GAN increases the accuracy by 2% to 3%, the recall by 2% to 4%, the precision by 1% to 3%, the F1-score by 1% to 3%, and the area under the curve by 1% to 4%. Compared with the original network DenseNet-201, COVID-CT-DenseNet increases the accuracy by 1% to 3%, the recall by 4% to 9%, the precision by 1%, the F1-score by 1% to 3%, and the area under the curve by 2%. CONCLUSION: The experimental results show that our method improves the efficiency of diagnosing COVID-19 on CT images, and helps overcome the problem of limited training data when using deep learning methods to diagnose COVID-19. SIGNIFICANCE: Our method can help clinicians build deep learning models using their private datasets to achieve automatic diagnosis of COVID-19 with a high precision.",2021,10.1002/mp.15044,diagnosis,True
COVID-19 Diagnosis Using an Enhanced Inception-ResNetV2 Deep Learning Model in CXR Images,"The COVID-19 pandemic has a significant negative effect on people's health, as well as on the world's economy. Polymerase chain reaction (PCR) is one of the main tests used to detect COVID-19 infection. However, it is expensive, time-consuming, and lacks sufficient accuracy. In recent years, convolutional neural networks have grabbed many researchers' attention in the machine learning field, due to its high diagnosis accuracy, especially the medical image recognition. Many architectures such as Inception, ResNet, DenseNet, and VGG16 have been proposed and gained an excellent performance at a low computational cost. Moreover, in a way to accelerate the training of these traditional architectures, residual connections are combined with inception architecture. Therefore, many hybrid architectures such as Inception-ResNetV2 are further introduced. This paper proposes an enhanced Inception-ResNetV2 deep learning model that can diagnose chest X-ray (CXR) scans with high accuracy. Besides, a Grad-CAM algorithm is used to enhance the visualization of the infected regions of the lungs in CXR images. Compared with state-of-the-art methods, our proposed paper proves superiority in terms of accuracy, recall, precision, and F1-measure.",2021,10.1155/2021/6658058,diagnosis,False
COVID-19 identification in chest X-ray images on flat and hierarchical classification scenarios,"BACKGROUND AND OBJECTIVE: The COVID-19 can cause severe pneumonia and is estimated to have a high impact on the healthcare system. Early diagnosis is crucial for correct treatment in order to possibly reduce the stress in the healthcare system. The standard image diagnosis tests for pneumonia are chest X-ray (CXR) and computed tomography (CT) scan. Although CT scan is the gold standard, CXR are still useful because it is cheaper, faster and more widespread. This study aims to identify pneumonia caused by COVID-19 from other types and also healthy lungs using only CXR images. METHODS: In order to achieve the objectives, we have proposed a classification schema considering the following perspectives: i) a multi-class classification; ii) hierarchical classification, since pneumonia can be structured as a hierarchy. Given the natural data imbalance in this domain, we also proposed the use of resampling algorithms in the schema in order to re-balance the classes distribution. We observed that, texture is one of the main visual attributes of CXR images, our classification schema extract features using some well-known texture descriptors and also using a pre-trained CNN model. We also explored early and late fusion techniques in the schema in order to leverage the strength of multiple texture descriptors and base classifiers at once. To evaluate the approach, we composed a database, named RYDLS-20, containing CXR images of pneumonia caused by different pathogens as well as CXR images of healthy lungs. The classes distribution follows a real-world scenario in which some pathogens are more common than others. RESULTS: The proposed approach tested in RYDLS-20 achieved a macro-avg F1-Score of 0.65 using a multi-class approach and a F1-Score of 0.89 for the COVID-19 identification in the hierarchical classification scenario. CONCLUSIONS: As far as we know, the top identification rate obtained in this paper is the best nominal rate obtained for COVID-19 identification in an unbalanced environment with more than three classes. We must also highlight the novel proposed hierarchical classification approach for this task, which considers the types of pneumonia caused by the different pathogens and lead us to the best COVID-19 recognition rate obtained here.",2020,10.1016/j.cmpb.2020.105532,diagnosis,False
COVID-19 image classification using deep features and fractional-order marine predators algorithm,"Currently, we witness the severe spread of the pandemic of the new Corona virus, COVID-19, which causes dangerous symptoms to humans and animals, its complications may lead to death. Although convolutional neural networks (CNNs) is considered the current state-of-the-art image classification technique, it needs massive computational cost for deployment and training. In this paper, we propose an improved hybrid classification approach for COVID-19 images by combining the strengths of CNNs (using a powerful architecture called Inception) to extract features and a swarm-based feature selection algorithm (Marine Predators Algorithm) to select the most relevant features. A combination of fractional-order and marine predators algorithm (FO-MPA) is considered an integration among a robust tool in mathematics named fractional-order calculus (FO). The proposed approach was evaluated on two public COVID-19 X-ray datasets which achieves both high performance and reduction of computational complexity. The two datasets consist of X-ray COVID-19 images by international Cardiothoracic radiologist, researchers and others published on Kaggle. The proposed approach selected successfully 130 and 86 out of 51 K features extracted by inception from dataset 1 and dataset 2, while improving classification accuracy at the same time. The results are the best achieved on these datasets when compared to a set of recent feature selection algorithms. By achieving 98.7%, 98.2% and 99.6%, 99% of classification accuracy and F-Score for dataset 1 and dataset 2, respectively, the proposed approach outperforms several CNNs and all recent works on COVID-19 images.",2020,10.1038/s41598-020-71294-2,diagnosis,False
COVID-19 Image Segmentation Based on Deep Learning and Ensemble Learning,"Medical imaging offers great potential for COVID-19 diagnosis and monitoring. Our work introduces an automated pipeline to segment areas of COVID-19 infection in CT scans using deep convolutional neural networks. Furthermore, we evaluate the performance impact of ensemble learning techniques (Bagging and Augmenting). Our models showed highly accurate segmentation results, in which Bagging achieved the highest dice similarity coefficient.",2021,10.3233/shti210223,diagnosis,True
COVID-19 in CXR: From Detection and Severity Scoring to Patient Disease Monitoring,"This work estimates the severity of pneumonia in COVID-19 patients and reports the findings of a longitudinal study of disease progression. It presents a deep learning model for simultaneous detection and localization of pneumonia in chest Xray (CXR) images, which is shown to generalize to COVID-19 pneumonia. The localization maps are utilized to calculate a ""Pneumonia Ratio"" which indicates disease severity. The assessment of disease severity serves to build a temporal disease extent profile for hospitalized patients. To validate the model's applicability to the patient monitoring task, we developed a validation strategy which involves a synthesis of Digital Reconstructed Radiographs (DRRs - synthetic Xray) from serial CT scans; we then compared the disease progression profiles that were generated from the DRRs to those that were generated from CT volumes.",2021,10.1109/jbhi.2021.3069169,diagnosis,True
COVID-19 infection localization and severity grading from chest X-ray images,"The immense spread of coronavirus disease 2019 (COVID-19) has left healthcare systems incapable to diagnose and test patients at the required rate. Given the effects of COVID-19 on pulmonary tissues, chest radiographic imaging has become a necessity for screening and monitoring the disease. Numerous studies have proposed Deep Learning approaches for the automatic diagnosis of COVID-19. Although these methods achieved outstanding performance in detection, they have used limited chest X-ray (CXR) repositories for evaluation, usually with a few hundred COVID-19 CXR images only. Thus, such data scarcity prevents reliable evaluation of Deep Learning models with the potential of overfitting. In addition, most studies showed no or limited capability in infection localization and severity grading of COVID-19 pneumonia. In this study, we address this urgent need by proposing a systematic and unified approach for lung segmentation and COVID-19 localization with infection quantification from CXR images. To accomplish this, we have constructed the largest benchmark dataset with 33,920 CXR images, including 11,956 COVID-19 samples, where the annotation of ground-truth lung segmentation masks is performed on CXRs by an elegant human-machine collaborative approach. An extensive set of experiments was performed using the state-of-the-art segmentation networks, U-Net, U-Net++, and Feature Pyramid Networks (FPN). The developed network, after an iterative process, reached a superior performance for lung region segmentation with Intersection over Union (IoU) of 96.11% and Dice Similarity Coefficient (DSC) of 97.99%. Furthermore, COVID-19 infections of various shapes and types were reliably localized with 83.05% IoU and 88.21% DSC. Finally, the proposed approach has achieved an outstanding COVID-19 detection performance with both sensitivity and specificity values above 99%.",2021,10.1016/j.compbiomed.2021.105002,diagnosis,False
COVID-19 lung infection segmentation with a novel two-stage cross-domain transfer learning framework,"With the global outbreak of COVID-19 in early 2020, rapid diagnosis of COVID-19 has become the urgent need to control the spread of the epidemic. In clinical settings, lung infection segmentation from computed tomography (CT) images can provide vital information for the quantification and diagnosis of COVID-19. However, accurate infection segmentation is a challenging task due to (i) the low boundary contrast between infections and the surroundings, (ii) large variations of infection regions, and, most importantly, (iii) the shortage of large-scale annotated data. To address these issues, we propose a novel two-stage cross-domain transfer learning framework for the accurate segmentation of COVID-19 lung infections from CT images. Our framework consists of two major technical innovations, including an effective infection segmentation deep learning model, called nCoVSegNet, and a novel two-stage transfer learning strategy. Specifically, our nCoVSegNet conducts effective infection segmentation by taking advantage of attention-aware feature fusion and large receptive fields, aiming to resolve the issues related to low boundary contrast and large infection variations. To alleviate the shortage of the data, the nCoVSegNet is pre-trained using a two-stage cross-domain transfer learning strategy, which makes full use of the knowledge from natural images (i.e., ImageNet) and medical images (i.e., LIDC-IDRI) to boost the final training on CT images with COVID-19 infections. Extensive experiments demonstrate that our framework achieves superior segmentation accuracy and outperforms the cutting-edge models, both quantitatively and qualitatively.",2021,10.1016/j.media.2021.102205,diagnosis,True
COVID-19 on Chest Radiographs: A Multireader Evaluation of an Artificial Intelligence System,"Background Chest radiography may play an important role in triage for coronavirus disease 2019 (COVID-19), particularly in low-resource settings. Purpose To evaluate the performance of an artificial intelligence (AI) system for detection of COVID-19 pneumonia on chest radiographs. Materials and Methods An AI system (CAD4COVID-XRay) was trained on 24 678 chest radiographs, including 1540 used only for validation while training. The test set consisted of a set of continuously acquired chest radiographs (n = 454) obtained in patients suspected of having COVID-19 pneumonia between March 4 and April 6, 2020, at one center (223 patients with positive reverse transcription polymerase chain reaction [RT-PCR] results, 231 with negative RT-PCR results). Radiographs were independently analyzed by six readers and by the AI system. Diagnostic performance was analyzed with the receiver operating characteristic curve. Results For the test set, the mean age of patients was 67 years ± 14.4 (standard deviation) (56% male). With RT-PCR test results as the reference standard, the AI system correctly classified chest radiographs as COVID-19 pneumonia with an area under the receiver operating characteristic curve of 0.81. The system significantly outperformed each reader (P < .001 using the McNemar test) at their highest possible sensitivities. At their lowest sensitivities, only one reader significantly outperformed the AI system (P = .04). Conclusion The performance of an artificial intelligence system in the detection of coronavirus disease 2019 on chest radiographs was comparable with that of six independent readers. © RSNA, 2020.",2020,10.1148/radiol.2020201874,diagnosis,False
COVID-19 pneumonia and the pulmonary vasculature: a marriage made in hell,Quantitative CT of the pulmonary vasculature is potentially important in COVID-19 associated pneumonia https://bit.ly/3vUTTRM,2021,10.1183/13993003.00811-2021,prognosis,True
COVID-19 Pneumonia Diagnosis Using a Simple 2D Deep Learning Framework With a Single Chest CT Image: Model Development and Validation,"BACKGROUND: Coronavirus disease (COVID-19) has spread explosively worldwide since the beginning of 2020. According to a multinational consensus statement from the Fleischner Society, computed tomography (CT) is a relevant screening tool due to its higher sensitivity for detecting early pneumonic changes. However, physicians are extremely occupied fighting COVID-19 in this era of worldwide crisis. Thus, it is crucial to accelerate the development of an artificial intelligence (AI) diagnostic tool to support physicians. OBJECTIVE: We aimed to rapidly develop an AI technique to diagnose COVID-19 pneumonia in CT images and differentiate it from non-COVID-19 pneumonia and nonpneumonia diseases. METHODS: A simple 2D deep learning framework, named the fast-track COVID-19 classification network (FCONet), was developed to diagnose COVID-19 pneumonia based on a single chest CT image. FCONet was developed by transfer learning using one of four state-of-the-art pretrained deep learning models (VGG16, ResNet-50, Inception-v3, or Xception) as a backbone. For training and testing of FCONet, we collected 3993 chest CT images of patients with COVID-19 pneumonia, other pneumonia, and nonpneumonia diseases from Wonkwang University Hospital, Chonnam National University Hospital, and the Italian Society of Medical and Interventional Radiology public database. These CT images were split into a training set and a testing set at a ratio of 8:2. For the testing data set, the diagnostic performance of the four pretrained FCONet models to diagnose COVID-19 pneumonia was compared. In addition, we tested the FCONet models on an external testing data set extracted from embedded low-quality chest CT images of COVID-19 pneumonia in recently published papers. RESULTS: Among the four pretrained models of FCONet, ResNet-50 showed excellent diagnostic performance (sensitivity 99.58%, specificity 100.00%, and accuracy 99.87%) and outperformed the other three pretrained models in the testing data set. In the additional external testing data set using low-quality CT images, the detection accuracy of the ResNet-50 model was the highest (96.97%), followed by Xception, Inception-v3, and VGG16 (90.71%, 89.38%, and 87.12%, respectively). CONCLUSIONS: FCONet, a simple 2D deep learning framework based on a single chest CT image, provides excellent diagnostic performance in detecting COVID-19 pneumonia. Based on our testing data set, the FCONet model based on ResNet-50 appears to be the best model, as it outperformed other FCONet models based on VGG16, Xception, and Inception-v3.",2020,10.2196/19569,diagnosis,True
"COVID-19 prognostic modeling using CT radiomic features and machine learning algorithms: Analysis of a multi-institutional dataset of 14,339 patients","BACKGROUND: We aimed to analyze the prognostic power of CT-based radiomics models using data of 14,339 COVID-19 patients. METHODS: Whole lung segmentations were performed automatically using a deep learning-based model to extract 107 intensity and texture radiomics features. We used four feature selection algorithms and seven classifiers. We evaluated the models using ten different splitting and cross-validation strategies, including non-harmonized and ComBat-harmonized datasets. The sensitivity, specificity, and area under the receiver operating characteristic curve (AUC) were reported. RESULTS: In the test dataset (4,301) consisting of CT and/or RT-PCR positive cases, AUC, sensitivity, and specificity of 0.83 ± 0.01 (CI95%: 0.81-0.85), 0.81, and 0.72, respectively, were obtained by ANOVA feature selector + Random Forest (RF) classifier. Similar results were achieved in RT-PCR-only positive test sets (3,644). In ComBat harmonized dataset, Relief feature selector + RF classifier resulted in the highest performance of AUC, reaching 0.83 ± 0.01 (CI95%: 0.81-0.85), with a sensitivity and specificity of 0.77 and 0.74, respectively. ComBat harmonization did not depict statistically significant improvement compared to a non-harmonized dataset. In leave-one-center-out, the combination of ANOVA feature selector and RF classifier resulted in the highest performance. CONCLUSION: Lung CT radiomics features can be used for robust prognostic modeling of COVID-19. The predictive power of the proposed CT radiomics model is more reliable when using a large multicentric heterogeneous dataset, and may be used prospectively in clinical setting to manage COVID-19 patients.",2022,10.1016/j.compbiomed.2022.105467,prognosis,True
COVID-19 Screening in Chest X-Ray Images Using Lung Region Priors,"Early screening of COVID-19 is essential for pandemic control, and thus to relieve stress on the health care system. Lung segmentation from chest X-ray (CXR) is a promising method for early diagnoses of pulmonary diseases. Recently, deep learning has achieved great success in supervised lung segmentation. However, how to effectively utilize the lung region in screening COVID-19 still remains a challenge due to domain shift and lack of manual pixel-level annotations. We hereby propose a multi-appearance COVID-19 screening framework by using lung region priors derived from CXR images. Firstly, we propose a multi-scale adversarial domain adaptation network (MS-AdaNet) to boost the cross-domain lung segmentation task as the prior knowledge to the classification network. Then, we construct a multi-appearance network (MA-Net), which is composed of three sub-networks to realize multi-appearance feature extraction and fusion using lung region priors. At last, we can obtain prediction results from normal, viral pneumonia, and COVID-19 using the proposed MA-Net. We extend the proposed MS-AdaNet for lung segmentation task on three different public CXR datasets. The results suggest that the MS-AdaNet outperforms contrastive methods in cross-domain lung segmentation. Moreover, experiments reveal that the proposed MA-Net achieves accuracy of 98.83 % and F1-score of 98.71 % on COVID-19 screening. The results indicate that the proposed MA-Net can obtain significant performance on COVID-19 screening.",2021,10.1109/jbhi.2021.3104629,diagnosis,False
Covid-19: automatic detection from X-ray images utilizing transfer learning with convolutional neural networks,"In this study, a dataset of X-ray images from patients with common bacterial pneumonia, confirmed Covid-19 disease, and normal incidents, was utilized for the automatic detection of the Coronavirus disease. The aim of the study is to evaluate the performance of state-of-the-art convolutional neural network architectures proposed over the recent years for medical image classification. Specifically, the procedure called Transfer Learning was adopted. With transfer learning, the detection of various abnormalities in small medical image datasets is an achievable target, often yielding remarkable results. The datasets utilized in this experiment are two. Firstly, a collection of 1427 X-ray images including 224 images with confirmed Covid-19 disease, 700 images with confirmed common bacterial pneumonia, and 504 images of normal conditions. Secondly, a dataset including 224 images with confirmed Covid-19 disease, 714 images with confirmed bacterial and viral pneumonia, and 504 images of normal conditions. The data was collected from the available X-ray images on public medical repositories. The results suggest that Deep Learning with X-ray imaging may extract significant biomarkers related to the Covid-19 disease, while the best accuracy, sensitivity, and specificity obtained is 96.78%, 98.66%, and 96.46% respectively. Since by now, all diagnostic tests show failure rates such as to raise concerns, the probability of incorporating X-rays into the diagnosis of the disease could be assessed by the medical community, based on the findings, while more research to evaluate the X-ray approach from different aspects may be conducted.",2020,10.1007/s13246-020-00865-4,diagnosis,False
COVID-AL: The diagnosis of COVID-19 with deep active learning,"The efficient diagnosis of COVID-19 plays a key role in preventing the spread of this disease. The computer-aided diagnosis with deep learning methods can perform automatic detection of COVID-19 using CT scans. However, large scale annotation of CT scans is impossible because of limited time and heavy burden on the healthcare system. To meet the challenge, we propose a weakly-supervised deep active learning framework called COVID-AL to diagnose COVID-19 with CT scans and patient-level labels. The COVID-AL consists of the lung region segmentation with a 2D U-Net and the diagnosis of COVID-19 with a novel hybrid active learning strategy, which simultaneously considers sample diversity and predicted loss. With a tailor-designed 3D residual network, the proposed COVID-AL can diagnose COVID-19 efficiently and it is validated on a large CT scan dataset collected from the CC-CCII. The experimental results demonstrate that the proposed COVID-AL outperforms the state-of-the-art active learning approaches in the diagnosis of COVID-19. With only 30% of the labeled data, the COVID-AL achieves over 95% accuracy of the deep learning method using the whole dataset. The qualitative and quantitative analysis proves the effectiveness and efficiency of the proposed COVID-AL framework.",2021,10.1016/j.media.2020.101913,diagnosis,True
COVID-CCD-Net: COVID-19 and colon cancer diagnosis system with optimized CNN hyperparameters using gradient-based optimizer,"Coronavirus disease-2019 (COVID-19) is a new types of coronavirus which have turned into a pandemic within a short time. Reverse transcription-polymerase chain reaction (RT-PCR) test is used for the diagnosis of COVID-19 in national healthcare centers. Because the number of PCR test kits is often limited, it is sometimes difficult to diagnose the disease at an early stage. However, X-ray technology is accessible nearly all over the world, and it succeeds in detecting symptoms of COVID-19 more successfully. Another disease which affects people's lives to a great extent is colorectal cancer. Tissue microarray (TMA) is a technological method which is widely used for its high performance in the analysis of colorectal cancer. Computer-assisted approaches which can classify colorectal cancer in TMA images are also needed. In this respect, the present study proposes a convolutional neural network (CNN) classification approach with optimized parameters using gradient-based optimizer (GBO) algorithm. Thanks to the proposed approach, COVID-19, normal, and viral pneumonia in various chest X-ray images can be classified accurately. Additionally, other types such as epithelial and stromal regions in epidermal growth factor receptor (EFGR) colon in TMAs can also be classified. The proposed approach was called COVID-CCD-Net. AlexNet, DarkNet-19, Inception-v3, MobileNet, ResNet-18, and ShuffleNet architectures were used in COVID-CCD-Net, and the hyperparameters of this architecture was optimized for the proposed approach. Two different medical image classification datasets, namely, COVID-19 and Epistroma, were used in the present study. The experimental findings demonstrated that proposed approach increased the classification performance of the non-optimized CNN architectures significantly and displayed a very high classification performance even in very low value of epoch.",2022,10.1007/s11517-022-02553-9,diagnosis,False
COVID-Classifier: an automated machine learning model to assist in the diagnosis of COVID-19 infection in chest X-ray images,"Chest-X ray (CXR) radiography can be used as a first-line triage process for non-COVID-19 patients with pneumonia. However, the similarity between features of CXR images of COVID-19 and pneumonia caused by other infections makes the differential diagnosis by radiologists challenging. We hypothesized that machine learning-based classifiers can reliably distinguish the CXR images of COVID-19 patients from other forms of pneumonia. We used a dimensionality reduction method to generate a set of optimal features of CXR images to build an efficient machine learning classifier that can distinguish COVID-19 cases from non-COVID-19 cases with high accuracy and sensitivity. By using global features of the whole CXR images, we successfully implemented our classifier using a relatively small dataset of CXR images. We propose that our COVID-Classifier can be used in conjunction with other tests for optimal allocation of hospital resources by rapid triage of non-COVID-19 cases.",2021,10.1038/s41598-021-88807-2,diagnosis,False
COVID-Transformer: Interpretable COVID-19 Detection Using Vision Transformer for Healthcare,"In the recent pandemic, accurate and rapid testing of patients remained a critical task in the diagnosis and control of COVID-19 disease spread in the healthcare industry. Because of the sudden increase in cases, most countries have faced scarcity and a low rate of testing. Chest X-rays have been shown in the literature to be a potential source of testing for COVID-19 patients, but manually checking X-ray reports is time-consuming and error-prone. Considering these limitations and the advancements in data science, we proposed a Vision Transformer-based deep learning pipeline for COVID-19 detection from chest X-ray-based imaging. Due to the lack of large data sets, we collected data from three open-source data sets of chest X-ray images and aggregated them to form a 30 K image data set, which is the largest publicly available collection of chest X-ray images in this domain to our knowledge. Our proposed transformer model effectively differentiates COVID-19 from normal chest X-rays with an accuracy of 98% along with an AUC score of 99% in the binary classification task. It distinguishes COVID-19, normal, and pneumonia patient's X-rays with an accuracy of 92% and AUC score of 98% in the Multi-class classification task. For evaluation on our data set, we fine-tuned some of the widely used models in literature, namely, EfficientNetB0, InceptionV3, Resnet50, MobileNetV3, Xception, and DenseNet-121, as baselines. Our proposed transformer model outperformed them in terms of all metrics. In addition, a Grad-CAM based visualization is created which makes our approach interpretable by radiologists and can be used to monitor the progression of the disease in the affected lungs, assisting healthcare.",2021,10.3390/ijerph182111086,diagnosis,False
COVID-view: Diagnosis of COVID-19 using Chest CT,"Significant work has been done towards deep learning (DL) models for automatic lung and lesion segmentation and classification of COVID-19 on chest CT data. However, comprehensive visualization systems focused on supporting the dual visual+DL diagnosis of COVID-19 are non-existent. We present COVID-view, a visualization application specially tailored for radiologists to diagnose COVID-19 from chest CT data. The system incorporates a complete pipeline of automatic lungs segmentation, localization/isolation of lung abnormalities, followed by visualization, visual and DL analysis, and measurement/quantification tools. Our system combines the traditional 2D workflow of radiologists with newer 2D and 3D visualization techniques with DL support for a more comprehensive diagnosis. COVID-view incorporates a novel DL model for classifying the patients into positive/negative COVID-19 cases, which acts as a reading aid for the radiologist using COVID-view and provides the attention heatmap as an explainable DL for the model output. We designed and evaluated COVID-view through suggestions, close feedback and conducting case studies of real-world patient data by expert radiologists who have substantial experience diagnosing chest CT scans for COVID-19, pulmonary embolism, and other forms of lung infections. We present requirements and task analysis for the diagnosis of COVID-19 that motivate our design choices and results in a practical system which is capable of handling real-world patient cases.",2022,10.1109/tvcg.2021.3114851,diagnosis,True
COVID19XrayNet: A Two-Step Transfer Learning Model for the COVID-19 Detecting Problem Based on a Limited Number of Chest X-Ray Images,"The novel coronavirus severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has caused a major pandemic outbreak recently. Various diagnostic technologies have been under active development. The novel coronavirus disease (COVID-19) may induce pulmonary failures, and chest X-ray imaging becomes one of the major confirmed diagnostic technologies. The very limited number of publicly available samples has rendered the training of the deep neural networks unstable and inaccurate. This study proposed a two-step transfer learning pipeline and a deep residual network framework COVID19XrayNet for the COVID-19 detection problem based on chest X-ray images. COVID19XrayNet firstly tunes the transferred model on a large dataset of chest X-ray images, which is further tuned using a small dataset of annotated chest X-ray images. The final model achieved 0.9108 accuracy. The experimental data also suggested that the model may be improved with more training samples being released. COVID19XrayNet, a two-step transfer learning framework designed for biomedical images.",2020,10.1007/s12539-020-00393-5,diagnosis,False
COVIDGR Dataset and COVID-SDNet Methodology for Predicting COVID-19 Based on Chest X-Ray Images,"Currently, Coronavirus disease (COVID-19), one of the most infectious diseases in the 21st century, is diagnosed using RT-PCR testing, CT scans and/or Chest X-Ray (CXR) images. CT (Computed Tomography) scanners and RT-PCR testing are not available in most medical centers and hence in many cases CXR images become the most time/cost effective tool for assisting clinicians in making decisions. Deep learning neural networks have a great potential for building COVID-19 triage systems and detecting COVID-19 patients, especially patients with low severity. Unfortunately, current databases do not allow building such systems as they are highly heterogeneous and biased towards severe cases. This article is three-fold: (i) we demystify the high sensitivities achieved by most recent COVID-19 classification models, (ii) under a close collaboration with Hospital Universitario Clínico San Cecilio, Granada, Spain, we built COVIDGR-1.0, a homogeneous and balanced database that includes all levels of severity, from normal with Positive RT-PCR, Mild, Moderate to Severe. COVIDGR-1.0 contains 426 positive and 426 negative PA (PosteroAnterior) CXR views and (iii) we propose COVID Smart Data based Network (COVID-SDNet) methodology for improving the generalization capacity of COVID-classification models. Our approach reaches good and stable results with an accuracy of [Formula: see text], [Formula: see text], [Formula: see text] in severe, moderate and mild COVID-19 severity levels. Our approach could help in the early detection of COVID-19. COVIDGR-1.0 along with the severity level labels are available to the scientific community through this link https://dasci.es/es/transferencia/open-data/covidgr/.",2020,10.1109/jbhi.2020.3037127,diagnosis,False
COVIDiag: a clinical CAD system to diagnose COVID-19 pneumonia based on CT findings,"OBJECTIVES: CT findings of COVID-19 look similar to other atypical and viral (non-COVID-19) pneumonia diseases. This study proposes a clinical computer-aided diagnosis (CAD) system using CT features to automatically discriminate COVID-19 from non-COVID-19 pneumonia patients. METHODS: Overall, 612 patients (306 COVID-19 and 306 non-COVID-19 pneumonia) were recruited. Twenty radiological features were extracted from CT images to evaluate the pattern, location, and distribution of lesions of patients in both groups. All significant CT features were fed in five classifiers namely decision tree, K-nearest neighbor, naïve Bayes, support vector machine, and ensemble to evaluate the best performing CAD system in classifying COVID-19 and non-COVID-19 cases. RESULTS: Location and distribution pattern of involvement, number of the lesion, ground-glass opacity (GGO) and crazy-paving, consolidation, reticular, bronchial wall thickening, nodule, air bronchogram, cavity, pleural effusion, pleural thickening, and lymphadenopathy are the significant features to classify COVID-19 from non-COVID-19 groups. Our proposed CAD system obtained the sensitivity, specificity, and accuracy of 0.965, 93.54%, 90.32%, and 91.94%, respectively, using ensemble (COVIDiag) classifier. CONCLUSIONS: This study proposed a COVIDiag model obtained promising results using CT radiological routine features. It can be considered an adjunct tool by the radiologists during the current COVID-19 pandemic to make an accurate diagnosis. KEY POINTS: • Location and distribution of involvement, number of lesions, GGO and crazy-paving, consolidation, reticular, bronchial wall thickening, nodule, air bronchogram, cavity, pleural effusion, pleural thickening, and lymphadenopathy are the significant features between COVID-19 from non-COVID-19 groups. • The proposed CAD system, COVIDiag, could diagnose COVID-19 pneumonia cases with an AUC of 0.965 (sensitivity = 93.54%; specificity = 90.32%; and accuracy = 91.94%). • The AUC, sensitivity, specificity, and accuracy obtained by radiologist diagnosis are 0.879, 87.10%, 88.71%, and 87.90%, respectively.",2021,10.1007/s00330-020-07087-y,diagnosis,True
COVIDiagnosis-Net: Deep Bayes-SqueezeNet based diagnosis of the coronavirus disease 2019 (COVID-19) from X-ray images,"The Coronavirus Disease 2019 (COVID-19) outbreak has a tremendous impact on global health and the daily life of people still living in more than two hundred countries. The crucial action to gain the force in the fight of COVID-19 is to have powerful monitoring of the site forming infected patients. Most of the initial tests rely on detecting the genetic material of the coronavirus, and they have a poor detection rate with the time-consuming operation. In the ongoing process, radiological imaging is also preferred where chest X-rays are highlighted in the diagnosis. Early studies express the patients with an abnormality in chest X-rays pointing to the presence of the COVID-19. On this motivation, there are several studies cover the deep learning-based solutions to detect the COVID-19 using chest X-rays. A part of the existing studies use non-public datasets, others perform on complicated Artificial Intelligent (AI) structures. In our study, we demonstrate an AI-based structure to outperform the existing studies. The SqueezeNet that comes forward with its light network design is tuned for the COVID-19 diagnosis with Bayesian optimization additive. Fine-tuned hyperparameters and augmented dataset make the proposed network perform much better than existing network designs and to obtain a higher COVID-19 diagnosis accuracy.",2020,10.1016/j.mehy.2020.109761,diagnosis,False
CovidXrayNet: Optimizing data augmentation and CNN hyperparameters for improved COVID-19 detection from CXR,"To mitigate the spread of the current coronavirus disease 2019 (COVID-19) pandemic, it is crucial to have an effective screening of infected patients to be isolated and treated. Chest X-Ray (CXR) radiological imaging coupled with Artificial Intelligence (AI) applications, in particular Convolutional Neural Network (CNN), can speed the COVID-19 diagnostic process. In this paper, we optimize the data augmentation and the CNN hyperparameters for detecting COVID-19 from CXRs in terms of validation accuracy. This optimization increases the accuracy of the popular CNN architectures such as the Visual Geometry Group network (VGG-19) and the Residual Neural Network (ResNet-50), by 11.93% and 4.97%, respectively. We then proposed CovidXrayNet model that is based on EfficientNet-B0 and our optimization results. We evaluated CovidXrayNet on two datasets, including our generated balanced COVIDcxr dataset (960 CXRs) and the benchmark COVIDx dataset (15,496 CXRs). With only 30 epochs of training, CovidXrayNet achieves state-of-the-art accuracy of 95.82% on the COVIDx dataset in the three-class classification task (COVID-19, normal or pneumonia). The CovidXRayNet model, the COVIDcxr dataset, and several optimization experiments are publicly available at https://github.com/MaramMonshi/CovidXrayNet.",2021,10.1016/j.compbiomed.2021.104375,diagnosis,False
CovXNet: A multi-dilation convolutional neural network for automatic COVID-19 and other pneumonia detection from chest X-ray images with transferable multi-receptive feature optimization,"With the recent outbreak of COVID-19, fast diagnostic testing has become one of the major challenges due to the critical shortage of test kit. Pneumonia, a major effect of COVID-19, needs to be urgently diagnosed along with its underlying reasons. In this paper, deep learning aided automated COVID-19 and other pneumonia detection schemes are proposed utilizing a small amount of COVID-19 chest X-rays. A deep convolutional neural network (CNN) based architecture, named as CovXNet, is proposed that utilizes depthwise convolution with varying dilation rates for efficiently extracting diversified features from chest X-rays. Since the chest X-ray images corresponding to COVID-19 caused pneumonia and other traditional pneumonias have significant similarities, at first, a large number of chest X-rays corresponding to normal and (viral/bacterial) pneumonia patients are used to train the proposed CovXNet. Learning of this initial training phase is transferred with some additional fine-tuning layers that are further trained with a smaller number of chest X-rays corresponding to COVID-19 and other pneumonia patients. In the proposed method, different forms of CovXNets are designed and trained with X-ray images of various resolutions and for further optimization of their predictions, a stacking algorithm is employed. Finally, a gradient-based discriminative localization is integrated to distinguish the abnormal regions of X-ray images referring to different types of pneumonia. Extensive experimentations using two different datasets provide very satisfactory detection performance with accuracy of 97.4% for COVID/Normal, 96.9% for COVID/Viral pneumonia, 94.7% for COVID/Bacterial pneumonia, and 90.2% for multiclass COVID/normal/Viral/Bacterial pneumonias. Hence, the proposed schemes can serve as an efficient tool in the current state of COVID-19 pandemic. All the architectures are made publicly available at: https://github.com/Perceptron21/CovXNet.",2020,10.1016/j.compbiomed.2020.103869,diagnosis,False
Creating a training set for artificial intelligence from initial segmentations of airways,"Airways segmentation is important for research about pulmonary disease but require a large amount of time by trained specialists. We used an openly available software to improve airways segmentations obtained from an artificial intelligence (AI) tool and retrained the tool to get a better performance. Fifteen initial airway segmentations from low-dose chest computed tomography scans were obtained with a 3D-Unet AI tool previously trained on Danish Lung Cancer Screening Trial and Erasmus-MC Sophia datasets. Segmentations were manually corrected in 3D Slicer. The corrected airway segmentations were used to retrain the 3D-Unet. Airway measurements were automatically obtained and included count, airway length and luminal diameter per generation from the segmentations. Correcting segmentations required 2-4 h per scan. Manually corrected segmentations had more branches (p < 0.001), longer airways (p < 0.001) and smaller luminal diameters (p = 0.004) than initial segmentations. Segmentations from retrained 3D-Unets trended towards more branches and longer airways compared to the initial segmentations. The largest changes were seen in airways from 6th generation onwards. Manual correction results in significantly improved segmentations and is potentially a useful and time-efficient method to improve the AI tool performance on a specific hospital or research dataset.",2021,10.1186/s41747-021-00247-9,diagnosis,True
Cross-modality (CT-MRI) prior augmented deep learning for robust lung tumor segmentation from small MR datasets,"PURPOSE: Accurate tumor segmentation is a requirement for magnetic resonance (MR)-based radiotherapy. Lack of large expert annotated MR datasets makes training deep learning models difficult. Therefore, a cross-modality (MR-CT) deep learning segmentation approach that augments training data using pseudo MR images produced by transforming expert-segmented CT images was developed. METHODS: Eighty-one T2-weighted MRI scans from 28 patients with non-small cell lung cancers (nine with pretreatment and weekly MRI and the remainder with pre-treatment MRI scans) were analyzed. Cross-modality model encoding the transformation of CT to pseudo MR images resembling T2w MRI was learned as a generative adversarial deep learning network. This model was used to translate 377 expert segmented non-small cell lung cancer CT scans from the Cancer Imaging Archive into pseudo MRI that served as additional training set. This method was benchmarked against shallow learning using random forest, standard data augmentation, and three state-of-the art adversarial learning-based cross-modality data (pseudo MR) augmentation methods. Segmentation accuracy was computed using Dice similarity coefficient (DSC), Hausdorff distance metrics, and volume ratio. RESULTS: The proposed approach produced the lowest statistical variability in the intensity distribution between pseudo and T2w MR images measured as Kullback-Leibler divergence of 0.069. This method produced the highest segmentation accuracy with a DSC of (0.75 ± 0.12) and the lowest Hausdorff distance of (9.36 mm ± 6.00 mm) on the test dataset using a U-Net structure. This approach produced highly similar estimations of tumor growth as an expert (P = 0.37). CONCLUSIONS: A novel deep learning MR segmentation was developed that overcomes the limitation of learning robust models from small datasets by leveraging learned cross-modality information using a model that explicitly incorporates knowledge of tumors in modality translation to augment segmentation training. The results show the feasibility of the approach and the corresponding improvement over the state-of-the-art methods.",2019,10.1002/mp.13695,treatment,True
CT and clinical assessment in asymptomatic and pre-symptomatic patients with early SARS-CoV-2 in outbreak settings,"OBJECTIVES: The early infection dynamics of patients with SARS-CoV-2 are not well understood. We aimed to investigate and characterize associations between clinical, laboratory, and imaging features of asymptomatic and pre-symptomatic patients with SARS-CoV-2. METHODS: Seventy-four patients with RT-PCR-proven SARS-CoV-2 infection were asymptomatic at presentation. All were retrospectively identified from 825 patients with chest CT scans and positive RT-PCR following exposure or travel risks in outbreak settings in Japan and China. CTs were obtained for every patient within a day of admission and were reviewed for infiltrate subtypes and percent with assistance from a deep learning tool. Correlations of clinical, laboratory, and imaging features were analyzed and comparisons were performed using univariate and multivariate logistic regression. RESULTS: Forty-eight of 74 (65%) initially asymptomatic patients had CT infiltrates that pre-dated symptom onset by 3.8 days. The most common CT infiltrates were ground glass opacities (45/48; 94%) and consolidation (22/48; 46%). Patient body temperature (p < 0.01), CRP (p < 0.01), and KL-6 (p = 0.02) were associated with the presence of CT infiltrates. Infiltrate volume (p = 0.01), percent lung involvement (p = 0.01), and consolidation (p = 0.043) were associated with subsequent development of symptoms. CONCLUSIONS: COVID-19 CT infiltrates pre-dated symptoms in two-thirds of patients. Body temperature elevation and laboratory evaluations may identify asymptomatic patients with SARS-CoV-2 CT infiltrates at presentation, and the characteristics of CT infiltrates could help identify asymptomatic SARS-CoV-2 patients who subsequently develop symptoms. The role of chest CT in COVID-19 may be illuminated by a better understanding of CT infiltrates in patients with early disease or SARS-CoV-2 exposure. KEY POINTS: • Forty-eight of 74 (65%) pre-selected asymptomatic patients with SARS-CoV-2 had abnormal chest CT findings. • CT infiltrates pre-dated symptom onset by 3.8 days (range 1-5). • KL-6, CRP, and elevated body temperature identified patients with CT infiltrates. Higher infiltrate volume, percent lung involvement, and pulmonary consolidation identified patients who developed symptoms.",2021,10.1007/s00330-020-07401-8,diagnosis,True
CT Image Analysis and Clinical Diagnosis of New Coronary Pneumonia Based on Improved Convolutional Neural Network,"In this paper, based on the improved convolutional neural network, in-depth analysis of the CT image of the new coronary pneumonia, using the U-Net series of deep neural networks to semantically segment the CT image of the new coronary pneumonia, to obtain the new coronary pneumonia area as the foreground and the remaining areas as the background of the binary image, provides a basis for subsequent image diagnosis. Secondly, the target-detection framework Faster RCNN extracts features from the CT image of the new coronary pneumonia tumor, obtains a higher-level abstract representation of the data, determines the lesion location of the new coronary pneumonia tumor, and gives its bounding box in the image. By generating an adversarial network to diagnose the lesion area of the CT image of the new coronary pneumonia tumor, obtaining a complete image of the new coronary pneumonia, achieving the effect of the CT image diagnosis of the new coronary pneumonia tumor, and three-dimensionally reconstructing the complete new coronary pneumonia model, filling the current the gap in this aspect, provide a basis to produce new coronary pneumonia prosthesis and improve the accuracy of diagnosis.",2021,10.1155/2021/7259414,diagnosis,True
CT Manifestations of Coronavirus Disease (COVID-19) Pneumonia and Influenza Virus Pneumonia: A Comparative Study,"OBJECTIVE. The purpose of this study was to investigate differences in CT manifestations of coronavirus disease (COVID-19) pneumonia and those of influenza virus pneumonia. MATERIALS AND METHODS. We conducted a retrospective study of 52 patients with COVID-19 pneumonia and 45 patients with influenza virus pneumonia. All patients had positive results for the respective viruses from nucleic acid testing and had complete clinical data and CT images. CT findings of pulmonary inflammation, CT score, and length of largest lesion were evaluated in all patients. Mean density, volume, and mass of lesions were further calculated using artificial intelligence software. CT findings and clinical data were evaluated. RESULTS. Between the group of patients with COVID-19 pneumonia and the group of patients with influenza virus pneumonia, the largest lesion close to the pleura (i.e., no pulmonary parenchyma between the lesion and the pleura), mucoid impaction, presence of pleural effusion, and axial distribution showed statistical difference (p < 0.05). The properties of the largest lesion, presence of ground-glass opacity, presence of consolidation, mosaic attenuation, bronchial wall thickening, centrilobular nodules, interlobular septal thickening, crazy paving pattern, air bronchogram, unilateral or bilateral distribution, and longitudinal distribution did not show significant differences (p > 0.05). In addition, no significant difference was seen in CT score, length of the largest lesion, mean density, volume, or mass of the lesions between the two groups (p > 0.05). CONCLUSION. Most lesions in patients with COVID-19 pneumonia were located in the peripheral zone and close to the pleura, whereas influenza virus pneumonia was more prone to show mucoid impaction and pleural effusion. However, differentiating between COVID-19 pneumonia and influenza virus pneumonia in clinical practice remains difficult.",2021,10.2214/ajr.20.23304,diagnosis,True
CT Quantification and Machine-learning Models for Assessment of Disease Severity and Prognosis of COVID-19 Patients,"OBJECTIVE: This study was to investigate the CT quantification of COVID-19 pneumonia and its impacts on the assessment of disease severity and the prediction of clinical outcomes in the management of COVID-19 patients. MATERIALS METHODS: Ninety-nine COVID-19 patients who were confirmed by positive nucleic acid test (NAT) of RT-PCR and hospitalized from January 19, 2020 to February 19, 2020 were collected for this retrospective study. All patients underwent arterial blood gas test, routine blood test, chest CT examination, and physical examination on admission. In addition, follow-up clinical data including the disease severity, clinical treatment, and clinical outcomes were collected for each patient. Lung volume, lesion volume, nonlesion lung volume (NLLV) (lung volume - lesion volume), and fraction of nonlesion lung volume (%NLLV) (nonlesion lung volume / lung volume) were quantified in CT images by using two U-Net models trained for segmentation of lung and COVID-19 lesions in CT images. Furthermore, we calculated 20 histogram textures for lesions volume and NLLV, respectively. To investigate the validity of CT quantification in the management of COVID-19, we built random forest (RF) models for the purpose of classification and regression to assess the disease severity (Moderate, Severe, and Critical) and to predict the need and length of ICU stay, the duration of oxygen inhalation, hospitalization, sputum NAT-positive, and patient prognosis. The performance of RF classifiers was evaluated using the area under the receiver operating characteristic curves (AUC) and that of RF regressors using the root-mean-square error. RESULTS: Patients were classified into three groups of disease severity: moderate (n = 25), severe (n = 47) and critical (n = 27), according to the clinical staging. Of which, a total of 32 patients, 1 (1/25) moderate, 6 (6/47) severe, and 25 critical (25/27), respectively, were admitted to ICU. The median values of ICU stay were 0, 0, and 12 days, the duration of oxygen inhalation 10, 15, and 28 days, the hospitalization 12, 16, and 28 days, and the sputum NAT-positive 8, 9, and 13 days, in three severity groups, respectively. The clinical outcomes were complete recovery (n = 3), partial recovery with residual pulmonary damage (n = 80), prolonged recovery (n = 15), and death (n = 1). The %NLLV in three severity groups were 92.18 ± 9.89%, 82.94 ± 16.49%, and 66.19 ± 24.15% with p value <0.05 among each two groups. The AUCs of RF classifiers using hybrid models were 0.927 and 0.929 in classification of moderate vs (severe + critical), and severe vs critical, respectively, which were significantly higher than either radiomics models or clinical models (p < 0.05). The root-mean-square errors of RF regressors were 0.88 weeks for prediction of duration of hospitalization (mean: 2.60 ± 1.01 weeks), 0.92 weeks for duration of oxygen inhalation (mean: 2.44 ± 1.08 weeks), 0.90 weeks for duration of sputum NAT-positive (mean: 1.59 ± 0.98 weeks), and 0.69 weeks for stay of ICU (mean: 1.32 ± 0.67 weeks), respectively. The AUCs for prediction of ICU treatment and prognosis (partial recovery vs prolonged recovery) were 0.945 and 0.960, respectively. CONCLUSION: CT quantification and machine-learning models show great potentials for assisting decision-making in the management of COVID-19 patients by assessing disease severity and predicting clinical outcomes.",2020,10.1016/j.acra.2020.09.004,prognosis,True
CT quantification of pneumonia lesions in early days predicts progression to severe illness in a cohort of COVID-19 patients,"Rationale: Some patients with coronavirus disease 2019 (COVID-19) rapidly develop respiratory failure or even die, underscoring the need for early identification of patients at elevated risk of severe illness. This study aims to quantify pneumonia lesions by computed tomography (CT) in the early days to predict progression to severe illness in a cohort of COVID-19 patients. Methods: This retrospective cohort study included confirmed COVID-19 patients. Three quantitative CT features of pneumonia lesions were automatically calculated using artificial intelligence algorithms, representing the percentages of ground-glass opacity volume (PGV), semi-consolidation volume (PSV), and consolidation volume (PCV) in both lungs. CT features, acute physiology and chronic health evaluation II (APACHE-II) score, neutrophil-to-lymphocyte ratio (NLR), and d-dimer, on day 0 (hospital admission) and day 4, were collected to predict the occurrence of severe illness within a 28-day follow-up using both logistic regression and Cox proportional hazard models. Results: We included 134 patients, of whom 19 (14.2%) developed any severe illness. CT features on day 0 and day 4, as well as their changes from day 0 to day 4, showed predictive capability. Changes in CT features from day 0 to day 4 performed the best in the prediction (area under the receiver operating characteristic curve = 0.93, 95% confidence interval [CI] 0.87~0.99; C-index=0.88, 95% CI 0.81~0.95). The hazard ratios of PGV and PCV were 1.39 (95% CI 1.05~1.84, P=0.023) and 1.67 (95% CI 1.17~2.38, P=0.005), respectively. CT features, adjusted for age and gender, on day 4 and in terms of changes from day 0 to day 4 outperformed APACHE-II, NLR, and d-dimer. Conclusions: CT quantification of pneumonia lesions can early and non-invasively predict the progression to severe illness, providing a promising prognostic indicator for clinical management of COVID-19.",2020,10.7150/thno.45985,prognosis,True
CT radiomics facilitates more accurate diagnosis of COVID-19 pneumonia: compared with CO-RADS,"BACKGROUND: Limited data was available for rapid and accurate detection of COVID-19 using CT-based machine learning model. This study aimed to investigate the value of chest CT radiomics for diagnosing COVID-19 pneumonia compared with clinical model and COVID-19 reporting and data system (CO-RADS), and develop an open-source diagnostic tool with the constructed radiomics model. METHODS: This study enrolled 115 laboratory-confirmed COVID-19 and 435 non-COVID-19 pneumonia patients (training dataset, n = 379; validation dataset, n = 131; testing dataset, n = 40). Key radiomics features extracted from chest CT images were selected to build a radiomics signature using least absolute shrinkage and selection operator (LASSO) regression. Clinical and clinico-radiomics combined models were constructed. The combined model was further validated in the viral pneumonia cohort, and compared with performance of two radiologists using CO-RADS. The diagnostic performance was assessed by receiver operating characteristics curve (ROC) analysis, calibration curve, and decision curve analysis (DCA). RESULTS: Eight radiomics features and 5 clinical variables were selected to construct the combined radiomics model, which outperformed the clinical model in diagnosing COVID-19 pneumonia with an area under the ROC (AUC) of 0.98 and good calibration in the validation cohort. The combined model also performed better in distinguishing COVID-19 from other viral pneumonia with an AUC of 0.93 compared with 0.75 (P = 0.03) for clinical model, and 0.69 (P = 0.008) or 0.82 (P = 0.15) for two trained radiologists using CO-RADS. The sensitivity and specificity of the combined model can be achieved to 0.85 and 0.90. The DCA confirmed the clinical utility of the combined model. An easy-to-use open-source diagnostic tool was developed using the combined model. CONCLUSIONS: The combined radiomics model outperformed clinical model and CO-RADS for diagnosing COVID-19 pneumonia, which can facilitate more rapid and accurate detection.",2021,10.1186/s12967-020-02692-3,diagnosis,True
CT texture analysis as predictive factor in metastatic lung adenocarcinoma treated with tyrosine kinase inhibitors (TKIs),"PURPOSE: To assess the predictive and prognostic value of pre-treatment CT texture features in lung adenocarcinoma treated with tyrosine kinase inhibitors (TKI). MATERIALS AND METHODS: Texture analysis was performed using commercially available software (TexRAD Ltd, Cambridge, UK) on pre-treatment contrast-enhanced CT studies from 50 patients with metastatic lung adenocarcinoma treated by TKI. Texture features were quantified on a 5-mm-thick central slice of the primary tumor and were correlated with progression-free and overall survival (PFS and OS) using an internally cross-validated machine learning approach then validated on a bootstrapped sample. RESULTS: Median PFS and OS were 10.5 and 20.7 months, respectively. A noninvasive signature based on five texture parameters predicted 6-month progression with Area Under the Curve (AUC) of 0.8 (95% CI) and 1-year progression with AUC of 0.76. A high-risk group had hazard ratios for progression of 4.63 and 5.78 when divided by median and best cut-off points, respectively. Texture signature did not correlate with OS. Available clinical variables did not correlate with PFS or with OS. CONCLUSION: Texture features seem to be associated with PFS in lung adenocarcinoma treated with TKI.",2018,10.1016/j.ejrad.2018.10.016,combined,True
CT texture analysis-based nomogram for the preoperative prediction of visceral pleural invasion in cT1N0M0 lung adenocarcinoma: an external validation cohort study,"AIM: To develop a nomogram based on computed tomography (CT) texture analysis for the preoperative prediction of visceral pleural invasion in patients with cT1N0M0 lung adenocarcinoma. MATERIALS AND METHODS: A dataset of chest CT containing lung nodules was collected from two institutions, and all surgically resected nodules were classified pathologically based on the presence of visceral pleural invasion. Each nodule on the CT image was segmented automatically by artificial-intelligence software and its CT texture features were extracted. The dataset was divided into training and external validation cohorts according to the institution, and a nomogram for predicting visceral pleural invasion was developed and validated. RESULTS: Of a total of 313 patients enrolled from two independent institutions, 63 were diagnosed with visceral pleural invasion. Three-dimensional (3D) CT long diameter, skewness, and sphericity, and chronic obstructive pulmonary disease were identified as independent predictors for visceral pleural invasion by multivariable logistic regression. The nomogram based on multivariable logistic regression showed great discriminative ability, as indicated by a C-index of 0.890 (95% confidence interval [CI]: 0.867-0.914) and 0.864 (95% CI: 0.817-0.911) for the training and external validation cohorts, respectively. Additionally, calibration of the nomogram revealed good predictive ability, as indicated by the Brier score (0.108 and 0.100 for the training and external validation cohorts, respectively). CONCLUSIONS: A nomogram was developed that could compute the probability of visceral pleural invasion in patients with cT1N0M0 lung adenocarcinoma with good calibration and discrimination. The nomogram has potential as a reliable tool for clinical evaluation and decision-making.",2022,10.1016/j.crad.2021.11.008,diagnosis,True
CT-Based COVID-19 triage: Deep multitask learning improves joint identification and severity quantification,"The current COVID-19 pandemic overloads healthcare systems, including radiology departments. Though several deep learning approaches were developed to assist in CT analysis, nobody considered study triage directly as a computer science problem. We describe two basic setups: Identification of COVID-19 to prioritize studies of potentially infected patients to isolate them as early as possible; Severity quantification to highlight patients with severe COVID-19, thus direct them to a hospital or provide emergency medical care. We formalize these tasks as binary classification and estimation of affected lung percentage. Though similar problems were well-studied separately, we show that existing methods could provide reasonable quality only for one of these setups. We employ a multitask approach to consolidate both triage approaches and propose a convolutional neural network to leverage all available labels within a single model. In contrast with the related multitask approaches, we show the benefit from applying the classification layers to the most spatially detailed feature map at the upper part of U-Net instead of the less detailed latent representation at the bottom. We train our model on approximately 1500 publicly available CT studies and test it on the holdout dataset that consists of 123 chest CT studies of patients drawn from the same healthcare system, specifically 32 COVID-19 and 30 bacterial pneumonia cases, 30 cases with cancerous nodules, and 31 healthy controls. The proposed multitask model outperforms the other approaches and achieves ROC AUC scores of 0.87±0.01 vs. bacterial pneumonia, 0.93±0.01 vs. cancerous nodules, and 0.97±0.01 vs. healthy controls in Identification of COVID-19, and achieves 0.97±0.01 Spearman Correlation in Severity quantification. We have released our code and shared the annotated lesions masks for 32 CT images of patients with COVID-19 from the test dataset.",2021,10.1016/j.media.2021.102054,diagnosis,True
CT-based deep learning model to differentiate invasive pulmonary adenocarcinomas appearing as subsolid nodules among surgical candidates: comparison of the diagnostic performance with a size-based logistic model and radiologists,"OBJECTIVES: To evaluate the deep learning models for differentiating invasive pulmonary adenocarcinomas (IACs) among subsolid nodules (SSNs) considered for resection in a retrospective diagnostic cohort in comparison with a size-based logistic model and expert radiologists. METHODS: This study included 525 patients (309 women; median, 62 years) to develop models, and an independent cohort of 101 patients (57 women; median, 66 years) was used for validation. A size-based logistic model and deep learning models using 2.5-dimension (2.5D) and three-dimension (3D) CT images were developed to discriminate IAC from less invasive pathologies. Overall performance, discrimination, and calibration were assessed. Diagnostic performances of the three thoracic radiologists were compared with those of the deep learning model. RESULTS: The overall performances of the deep learning models (Brier score, 0.122 for the 2.5D DenseNet and 0.121 for the 3D DenseNet) were superior to those of the size-based logistic model (Brier score, 0.198). The area under the receiver operating characteristic curve (AUC) of the 2.5D DenseNet (0.921) was significantly higher than that of the 3D DenseNet (0.835; p = 0.037) and the size-based logistic model (0.836; p = 0.009). At equally high sensitivities of 90%, the 2.5D DenseNet showed significantly higher specificity (88.2%; all p < 0.05) and positive predictive value (97.4%; all p < 0.05) than other models. Model calibration was poor for all models (all p < 0.05). The 2.5D DenseNet had a comparable performance with the radiologists (AUC, 0.848-0.910). CONCLUSION: The 2.5D DenseNet model could be used as a highly sensitive and specific diagnostic tool to differentiate IACs among SSNs for surgical candidates. KEY POINTS: • The deep learning model developed using 2.5D DenseNet showed higher overall performance and discrimination than the size-based logistic model for the differentiation of invasive adenocarcinomas among subsolid nodules for surgical candidates. • The 2.5D DenseNet demonstrated a thoracic radiologist-level diagnostic performance and had higher specificity (88.2%) at equal sensitivities (90%) than the size-based logistic model (specificity, 52.9%). • The 2.5D DenseNet could be used to reduce potential overtreatment for the indolent subsolid nodules or to select candidates for sublobar resection instead of the standard lobectomy.",2020,10.1007/s00330-019-06628-4,diagnosis,True
CT-Based Hand-crafted Radiomic Signatures Can Predict PD-L1 Expression Levels in Non-small Cell Lung Cancer: a Two-Center Study,"Here, we used pre-treatment CT images to develop and evaluate a radiomic signature that can predict the expression of programmed death ligand 1 (PD-L1) in non-small cell lung cancer (NSCLC). We then verified its predictive performance by cross-referencing its results with clinical characteristics. This two-center retrospective analysis included 125 patients with histologically confirmed NSCLC. A total of 1287 hand-crafted radiomic features were observed from manually determined tumor regions. Valuable features were then selected with a ridge regression-based recursive feature elimination approach. Machine learning-based prediction models were then built from this and compared each other. The final radiomic signature was built using logistic regression in the primary cohort, and then tested in a validation cohort. Finally, we compared the efficacy of the radiomic signature to the clinical model and the radiomic-clinical nomogram. Among the 125 patients, 89 were classified as having PD-L1 positive expression. However, there was no significant difference in PD-L1 expression levels determined by clinical characteristics (P = 0.109-0.955). Upon selecting 9 radiomic features, we found that the logistic regression-based prediction model performed the best (AUC = 0.96, P < 0.001). In the external cohort, our radiomic signature showed an AUC of 0.85, which outperformed both the clinical model (AUC = 0.38, P < 0.001) and the radiomics-nomogram model (AUC = 0.61, P < 0.001). Our CT-based hand-crafted radiomic signature model can effectively predict PD-L1 expression levels, providing a noninvasive means of better understanding PD-L1 expression in patients with NSCLC.",2021,10.1007/s10278-021-00484-9,diagnosis,True
CT-based radiomics and machine learning to predict spread through air space in lung adenocarcinoma,"PURPOSE: Spread through air space (STAS) is a novel invasive pattern of lung adenocarcinoma and is also a risk factor for recurrence and worse prognosis of lung adenocarcinoma. The aims of this study are to develop and validate a computed tomography (CT)‑based radiomics model for preoperative prediction of STAS in lung adenocarcinoma. METHODS AND MATERIALS: This retrospective study was approved by an institutional review board and included 462 (mean age, 58.06 years) patients with pathologically confirmed lung adenocarcinoma. STAS was identified in 90 patients (19.5%). Two experienced radiologists segmented and extracted radiomics features on preoperative thin-slice CT images with radiomics extension independently. Intraclass correlation coefficients (ICC) and Pearson's correlation were used to rule out those low reliable (ICC < 0.75) and redundant (r > 0.9) features. Univariate logistic regression was applied to select radiomics features which were associated with STAS. A radiomics-based machine learning predictive model using a random forest (RF) was developed and calibrated with fivefold cross-validation. The diagnostic performance of the model was measured by the area under the curve (AUC) of receiver operating characteristic (ROC). RESULTS: With univariate analysis, 12 radiomics features and age were found to be associated with STAS significantly. The RF model achieved an AUC of 0.754 (a sensitivity of 0.880 and a specificity of 0.588) for predicting STAS. CONCLUSION: CT-based radiomics model can preoperatively predict STAS in lung adenocarcinoma with good diagnosis performance. KEY POINTS: • CT-based radiomics and machine learning model can predict spread through air space (STAS) in lung adenocarcinoma with high accuracy. • The random forest (RF) model achieved an AUC of 0.754 (a sensitivity of 0.880 and a specificity of 0.588) for predicting STAS.",2020,10.1007/s00330-020-06694-z,diagnosis,True
CT-based radiomics for predicting the rapid progression of coronavirus disease 2019 (COVID-19) pneumonia lesions,"OBJECTIVES: To develop and validate a radiomic model to predict the rapid progression (defined as volume growth of pneumonia lesions > 50% within seven days) in patients with coronavirus disease 2019 (COVID-19). METHODS: Patients with laboratory-confirmed COVID-19 who underwent longitudinal chest CT between January 01 and February 18, 2020 were included. A total of 1316 radiomic features were extracted from the lung parenchyma window for each CT. The least absolute shrinkage and selection operator (LASSO), Relief, Las Vegas Wrapper (LVW), L1-norm-Support Vector Machine (L1-norm-SVM), and recursive feature elimination (RFE) were applied to select the features that associated with rapid progression. Four machine learning classifiers were used for modeling, including Support Vector Machine (SVM), Random Forest (RF), Logistic Regression (LR), and Decision Tree (DT). Accordingly, 20 radiomic models were developed on the basis of 296 CT scans and validated in 74 CT scans. Model performance was determined by the receiver operating characteristic curve. RESULTS: A total of 107 patients (median age, 49.0 years, interquartile range, 35-54) were evaluated. The patients underwent a total of 370 chest CT scans with a median interval of 4 days (interquartile range, 3-5 days). The combination methods of L1-norm SVM and SVM with 17 radiomic features yielded the highest performance in predicting the likelihood of rapid progression of pneumonia lesions on next CT scan, with an AUC of 0.857 (95% CI: 0.766-0.947), sensitivity of 87.5%, and specificity of 70.7%. CONCLUSIONS: Our radiomic model based on longitudinal chest CT data could predict the rapid progression of pneumonia lesions, which may facilitate the CT follow-up intervals and reduce the radiation. ADVANCES IN KNOWLEDGE: Radiomic features extracted from the current chest CT have potential in predicting the likelihood of rapid progression of pneumonia lesions on the next chest CT, which would improve clinical decision-making regarding timely treatment.",2021,10.1259/bjr.20201007,prognosis,True
CT-based radiomics for prediction of histologic subtype and metastatic disease in primary malignant lung neoplasms,"PURPOSE: As some of the most important factors for treatment decision of lung cancer (which is the deadliest neoplasm) are staging and histology, this work aimed to associate quantitative contrast-enhanced computed tomography (CT) features from malignant lung tumors with distant and nodal metastases (according to clinical TNM staging) and histopathology (according to biopsy and surgical resection) using radiomics assessment. METHODS: A local cohort of 85 patients were retrospectively (2010-2017) analyzed after approval by the institutional research review board. CT images acquired with the same protocol were semiautomatically segmented by a volumetric segmentation method. Tumors were characterized by quantitative CT features of shape, first-order, second-order, and higher-order textures. Statistical and machine learning analyses assessed the features individually and combined with clinical data. RESULTS: Univariate and multivariate analyses identified 40, 2003, and 45 quantitative features associated with distant metastasis, nodal metastasis, and histopathology (adenocarcinoma and squamous cell carcinoma), respectively. A machine learning model yielded the highest areas under the receiver operating characteristic curves of 0.92, 0.84, and 0.88 to predict the same previous patterns. CONCLUSION: Several radiomic features (including wavelet energies, information measures of correlation and maximum probability from co-occurrence matrix, busyness from neighborhood intensity-difference matrix, directionalities from Tamura's texture, and fractal dimension estimation) significantly associated with distant metastasis, nodal metastasis, and histology were discovered in this work, presenting great potential as imaging biomarkers for pathological diagnosis and target therapy decision.",2020,10.1007/s11548-019-02093-y,diagnosis,True
CT-based radiomics signatures can predict the tumor response of non-small cell lung cancer patients treated with first-line chemotherapy and targeted therapy,"OBJECTIVES: The goal of this study was to evaluate the effectiveness of radiomics signatures on pre-treatment computed tomography (CT) images of lungs to predict the tumor responses of non-small cell lung cancer (NSCLC) patients treated with first-line chemotherapy, targeted therapy, or a combination of both. MATERIALS AND METHODS: This retrospective study included 322 NSCLC patients who were treated with first-line chemotherapy, targeted therapy, or a combination of both. Of these patients, 224 were randomly assigned to a cohort to help develop the radiomics signature. A total of 1946 radiomics features were obtained from each patient's CT scan. The top-ranked features were selected by the Minimum Redundancy Maximum Relevance (MRMR) feature-ranking method and used to build a lightweight radiomics signature with the Random Forest (RF) classifier. The independent predictive (IP) features (AUC > 0.6, p value < 0.05) were further identified from the top-ranked features and used to build a refined radiomics signature by the RF classifier. Its prediction performance was tested on the validation cohort, which consisted of the remaining 98 patients. RESULTS: The initial lightweight radiomics signature constructed from 15 top-ranked features had an AUC of 0.721 (95% CI, 0.619-0.823). After six IP features were further identified and a refined radiomics signature was built, it had an AUC of 0.746 (95% CI, 0.646-0.846). CONCLUSIONS: Radiomics signatures based on pre-treatment CT scans can accurately predict tumor response in NSCLC patients after first-line chemotherapy or targeted therapy treatments. Radiomics features could be used as promising prognostic imaging biomarkers in the future. KEY POINTS: The radiomics signature extracted from baseline CT images in patients with NSCLC can predict response to first-line chemotherapy, targeted therapy, or both treatments with an AUC = 0.746 (95% CI, 0.646-0.846). The radiomics signature could be used as a new biomarker for quantitative analysis in radiology, which might provide value in decision-making and to define personalized treatments for cancer patients.",2022,10.1007/s00330-021-08277-y,treatment,True
CTumorGAN: a unified framework for automatic computed tomography tumor segmentation,"PURPOSE: Unlike the normal organ segmentation task, automatic tumor segmentation is a more challenging task because of the existence of similar visual characteristics between tumors and their surroundings, especially on computed tomography (CT) images with severe low contrast resolution, as well as the diversity and individual characteristics of data acquisition procedures and devices. Consequently, most of the recently proposed methods have become increasingly difficult to be applied on a different tumor dataset with good results, and moreover, some tumor segmentors usually fail to generalize beyond those datasets and modalities used in their original evaluation experiments. METHODS: In order to alleviate some of the problems with the recently proposed methods, we propose a novel unified and end-to-end adversarial learning framework for automatic segmentation of any kinds of tumors from CT scans, called CTumorGAN, consisting of a Generator network and a Discriminator network. Specifically, the Generator attempts to generate segmentation results that are close to their corresponding golden standards, while the Discriminator aims to distinguish between generated samples and real tumor ground truths. More importantly, we deliberately design different modules to take into account the well-known obstacles, e.g., severe class imbalance, small tumor localization, and the label noise problem with poor expert annotation quality, and then use these modules to guide the CTumorGAN training process by utilizing multi-level supervision more effectively. RESULTS: We conduct a comprehensive evaluation on diverse loss functions for tumor segmentation and find that mean square error is more suitable for the CT tumor segmentation task. Furthermore, extensive experiments with multiple evaluation criteria on three well-established datasets, including lung tumor, kidney tumor, and liver tumor databases, also demonstrate that our CTumorGAN achieves stable and competitive performance compared with the state-of-the-art approaches for CT tumor segmentation. CONCLUSION: In order to overcome those key challenges arising from CT datasets and solve some of the main problems existing in the current deep learning-based methods, we propose a novel unified CTumorGAN framework, which can be effectively generalized to address any kinds of tumor datasets with superior performance.",2020,10.1007/s00259-020-04781-3,diagnosis,True
CXR-RefineDet: Single-Shot Refinement Neural Network for Chest X-Ray Radiograph Based on Multiple Lesions Detection,"The workload of radiologists has dramatically increased in the context of the COVID-19 pandemic, causing misdiagnosis and missed diagnosis of diseases. The use of artificial intelligence technology can assist doctors in locating and identifying lesions in medical images. In order to improve the accuracy of disease diagnosis in medical imaging, we propose a lung disease detection neural network that is superior to the current mainstream object detection model in this paper. By combining the advantages of RepVGG block and Resblock in information fusion and information extraction, we design a backbone RRNet with few parameters and strong feature extraction capabilities. After that, we propose a structure called Information Reuse, which can solve the problem of low utilization of the original network output features by connecting the normalized features back to the network. Combining the network of RRNet and the improved RefineDet, we propose the overall network which was called CXR-RefineDet. Through a large number of experiments on the largest public lung chest radiograph detection dataset VinDr-CXR, it is found that the detection accuracy and inference speed of CXR-RefineDet have reached 0.1686 mAP and 6.8 fps, respectively, which is better than the two-stage object detection algorithm using a strong backbone like ResNet-50 and ResNet-101. In addition, the fast reasoning speed of CXR-RefineDet also provides the possibility for the actual implementation of the computer-aided diagnosis system.",2022,10.1155/2022/4182191,diagnosis,False
D2-CovidNet: A Deep Learning Model for COVID-19 Detection in Chest X-Ray Images,"Since the outbreak of Coronavirus disease 2019 (COVID-19), it has been spreading rapidly worldwide and has not yet been effectively controlled. Many researchers are studying novel Coronavirus pneumonia from chest X-ray images. In order to improve the detection accuracy, two modules sensitive to feature information, dual-path multiscale feature fusion module and dense depthwise separable convolution module, are proposed. Based on these two modules, a lightweight convolutional neural network model, D2-CovidNet, is designed to assist experts in diagnosing COVID-19 by identifying chest X-ray images. D2-CovidNet is tested on two public data sets, and its classification accuracy, precision, sensitivity, specificity, and F1-score are 94.56%, 95.14%, 94.02%, 96.61%, and 95.30%, respectively. Specifically, the precision, sensitivity, and specificity of the network for COVID-19 are 98.97%, 94.12%, and 99.84%, respectively. D2-CovidNet has fewer computation number and parameter number. Compared with other methods, D2-CovidNet can help diagnose COVID-19 more quickly and accurately.",2021,10.1155/2021/9952109,diagnosis,False
DCT-MIL: Deep CNN transferred multiple instance learning for COPD identification using CT images,"While many pre-defined computed tomographic (CT) measures have been utilized to characterize chronic obstructive pulmonary disease (COPD), it is still challenging to represent pathological alternations of multiple dimensions and highly spatial heterogeneity. Deep CNN transferred multiple instance learning (DCT-MIL) is proposed to identify COPD via CT images. After the lung is divided into eight sections along the axial direction, one random axial CT image is taken out from each section as one instance. With one instance as the input, the activations of neural layers of AlexNet trained by natural images are extracted as features. After dimension reduction through principle component analysis, features of all instances are input into three MIL methods: Citation k-Nearest-Neighbor (Citation-KNN), multiple instance support vector machine, and expectation-maximization diverse density. Moreover, the performance dependence of the resulted models on the depth of the neural layer where activations are extracted and the number of features is investigated. The proposed DCT-MIL achieves an exceptional performance with an accuracy of 99.29% and area under curve of 0.9826 while using 100 principle components of features extracted from the fourth convolutional layer and Citation-KNN. It outperforms not only DCT-MIL models using other settings and the pre-trained AlexNet with fine-tuning by montages of eight lung CT images, but also other state-of-art methods. Deep CNN transferred multiple instance learning is suited for identification of COPD using CT images. It can help finding subgroups with high risk of COPD from large populations through CT scans ordered doing lung cancer screening.",2020,10.1088/1361-6560/ab857d,diagnosis,True
Deciphering unclassified tumors of non-small-cell lung cancer through radiomics,"BACKGROUND: Tumors are highly heterogeneous at the phenotypic, physiologic, and genomic levels. They are categorized in terms of a differentiated appearance under a microscope. Non-small-cell lung cancer tumors are categorized into three main subgroups: adenocarcinoma, squamous cell carcinoma, and large cell carcinoma. In approximately 20% of pathology reports, they are returned unclassified or classified as not-otherwise-specified (NOS) owing to scant materials or poor tumor differentiation. METHOD: We present a radiomic interrogation of molecular spatial variations to decode unclassified NOS tumor architecture quantitatively. Twelve spatial descriptors with various displacements and directions were extracted and profiled with respect to the subgroups. The profiled descriptors were used to decipher the NOS tumor morphologic clues from the imaging phenotype perspective. This profiler was built as an extended version of a conventional support-vector-machine classifier, wherein a genetic algorithm and correlation analysis were embedded to define the molecular signatures of poorly differentiated tumors using well-differentiated-tumor information. RESULTS: Sixteen multi-class classifier models with an 81% average accuracy and descriptor subset size ranging from 12 to 144 were reported. The average area under the curve was 86.3% at a 95% confidence interval and a 0.03-0.08 standard error. Correlation analysis returned an unclassified NOS membership matrix with respect to the cell-architecture similarity score for the subgroups. The best model demonstrated 53% NOS reduction. CONCLUSION: The membership matrix is expected to assist pathologists and oncologists in cases of unresectable tumors or scant biopsy materials for histological subtyping and cancer therapy.",2017,10.1016/j.compbiomed.2017.10.029,diagnosis,True
Decoding COVID-19 pneumonia: comparison of deep learning and radiomics CT image signatures,"PURPOSE: High-dimensional image features that underlie COVID-19 pneumonia remain opaque. We aim to compare feature engineering and deep learning methods to gain insights into the image features that drive CT-based for COVID-19 pneumonia prediction, and uncover CT image features significant for COVID-19 pneumonia from deep learning and radiomics framework. METHODS: A total of 266 patients with COVID-19 and other viral pneumonia with clinical symptoms and CT signs similar to that of COVID-19 during the outbreak were retrospectively collected from three hospitals in China and the USA. All the pneumonia lesions on CT images were manually delineated by four radiologists. One hundred eighty-four patients (n = 93 COVID-19 positive; n = 91 COVID-19 negative; 24,216 pneumonia lesions from 12,001 CT image slices) from two hospitals from China served as discovery cohort for model development. Thirty-two patients (17 COVID-19 positive, 15 COVID-19 negative; 7883 pneumonia lesions from 3799 CT image slices) from a US hospital served as external validation cohort. A bi-directional adversarial network-based framework and PyRadiomics package were used to extract deep learning and radiomics features, respectively. Linear and Lasso classifiers were used to develop models predictive of COVID-19 versus non-COVID-19 viral pneumonia. RESULTS: 120-dimensional deep learning image features and 120-dimensional radiomics features were extracted. Linear and Lasso classifiers identified 32 high-dimensional deep learning image features and 4 radiomics features associated with COVID-19 pneumonia diagnosis (P < 0.0001). Both models achieved sensitivity > 73% and specificity > 75% on external validation cohort with slight superior performance for radiomics Lasso classifier. Human expert diagnostic performance improved (increase by 16.5% and 11.6% in sensitivity and specificity, respectively) when using a combined deep learning-radiomics model. CONCLUSIONS: We uncover specific deep learning and radiomics features to add insight into interpretability of machine learning algorithms and compare deep learning and radiomics models for COVID-19 pneumonia that might serve to augment human diagnostic performance.",2021,10.1007/s00259-020-05075-4,diagnosis,True
Deep Bidirectional Classification Model for COVID-19 Disease Infected Patients,"In December of 2019, a novel coronavirus (COVID-19) appeared in Wuhan city, China and has been reported in many countries with millions of people infected within only four months. Chest computed Tomography (CT) has proven to be a useful supplement to reverse transcription polymerase chain reaction (RT-PCR) and has been shown to have high sensitivity to diagnose this condition. Therefore, radiological examinations are becoming crucial in early examination of COVID-19 infection. Currently, CT findings have already been suggested as an important evidence for scientific examination of COVID-19 in Hubei, China. However, classification of patient from chest CT images is not an easy task. Therefore, in this paper, a deep bidirectional long short-term memory network with mixture density network (DBM) model is proposed. To tune the hyperparameters of the DBM model, a Memetic Adaptive Differential Evolution (MADE) algorithm is used. Extensive experiments are drawn by considering the benchmark chest-Computed Tomography (chest-CT) images datasets. Comparative analysis reveals that the proposed MADE-DBM model outperforms the competitive COVID-19 classification approaches in terms of various performance metrics. Therefore, the proposed MADE-DBM model can be used in real-time COVID-19 classification systems.",2021,10.1109/tcbb.2020.3009859,diagnosis,True
"Deep CNN models for pulmonary nodule classification: Model modification, model integration, and transfer learning","BACKGROUND: Deep learning has made spectacular achievements in analysing natural images, but it faces challenges for medical applications partly due to inadequate images. OBJECTIVE: Aiming to classify malignant and benign pulmonary nodules using CT images, we explore different strategies to utilize the state-of-the-art deep convolutional neural networks (CNN). METHODS: Experiments are conducted using the Lung Image Database Consortium image collection (LIDC-IDRI), which is a public database containing 1018 cases. Three strategies are implemented including to 1) modify some state-of-the-art CNN architectures, 2) integrate different CNNs and 3) adopt transfer learning. Totally, 11 deep CNN models are compared using the same dataset. RESULTS: Study demonstrates that, for the model modification scheme, a concise CifarNet performs better than the other modified CNNs with more complex architectures, achieving an area under ROC curve of AUC = 0.90. Integrated CNN models do not significantly improve the classification performance, but the model complexity is reduced. Transfer learning outperforms the other two schemes and ResNet with fine-tuning leads to the best performance with an AUC = 0.94, as well as the sensitivity of 91% and an overall accuracy of 88%. CONCLUSIONS: Model modification, model integration, and transfer learning can play important roles to identify and generate optimal deep CNN models in classifying pulmonary nodules based on CT images efficiently. Transfer learning is preferred when applying deep learning to medical imaging applications.",2019,10.3233/xst-180490,diagnosis,True
Deep convolutional neural network for segmentation of thoracic organs-at-risk using cropped 3D images,"PURPOSE: Automatic segmentation of organs-at-risk (OARs) is a key step in radiation treatment planning to reduce human efforts and bias. Deep convolutional neural networks (DCNN) have shown great success in many medical image segmentation applications but there are still challenges in dealing with large 3D images for optimal results. The purpose of this study is to develop a novel DCNN method for thoracic OARs segmentation using cropped 3D images. METHODS: To segment the five organs (left and right lungs, heart, esophagus and spinal cord) from the thoracic CT scans, preprocessing to unify the voxel spacing and intensity was first performed, a 3D U-Net was then trained on resampled thoracic images to localize each organ, then the original images were cropped to only contain one organ and served as the input to each individual organ segmentation network. The segmentation maps were then merged to get the final results. The network structures were optimized for each step, as well as the training and testing strategies. A novel testing augmentation with multiple iterations of image cropping was used. The networks were trained on 36 thoracic CT scans with expert annotations provided by the organizers of the 2017 AAPM Thoracic Auto-segmentation Challenge and tested on the challenge testing dataset as well as a private dataset. RESULTS: The proposed method earned second place in the live phase of the challenge and first place in the subsequent ongoing phase using a newly developed testing augmentation approach. It showed superior-than-human performance on average in terms of Dice scores (spinal cord: 0.893 ± 0.044, right lung: 0.972 ± 0.021, left lung: 0.979 ± 0.008, heart: 0.925 ± 0.015, esophagus: 0.726 ± 0.094), mean surface distance (spinal cord: 0.662 ± 0.248 mm, right lung: 0.933 ± 0.574 mm, left lung: 0.586 ± 0.285 mm, heart: 2.297 ± 0.492 mm, esophagus: 2.341 ± 2.380 mm) and 95% Hausdorff distance (spinal cord: 1.893 ± 0.627 mm, right lung: 3.958 ± 2.845 mm, left lung: 2.103 ± 0.938 mm, heart: 6.570 ± 1.501 mm, esophagus: 8.714 ± 10.588 mm). It also achieved good performance in the private dataset and reduced the editing time to 7.5 min per patient following automatic segmentation. CONCLUSIONS: The proposed DCNN method demonstrated good performance in automatic OAR segmentation from thoracic CT scans. It has the potential for eventual clinical adoption of deep learning in radiation treatment planning due to improved accuracy and reduced cost for OAR segmentation.",2019,10.1002/mp.13466,treatment,True
Deep Convolutional Neural Network-Based Computer-Aided Detection System for COVID-19 Using Multiple Lung Scans: Design and Implementation Study,"BACKGROUND: Owing to the COVID-19 pandemic and the imminent collapse of health care systems following the exhaustion of financial, hospital, and medicinal resources, the World Health Organization changed the alert level of the COVID-19 pandemic from high to very high. Meanwhile, more cost-effective and precise COVID-19 detection methods are being preferred worldwide. OBJECTIVE: Machine vision-based COVID-19 detection methods, especially deep learning as a diagnostic method in the early stages of the pandemic, have been assigned great importance during the pandemic. This study aimed to design a highly efficient computer-aided detection (CAD) system for COVID-19 by using a neural search architecture network (NASNet)-based algorithm. METHODS: NASNet, a state-of-the-art pretrained convolutional neural network for image feature extraction, was adopted to identify patients with COVID-19 in their early stages of the disease. A local data set, comprising 10,153 computed tomography scans of 190 patients with and 59 without COVID-19 was used. RESULTS: After fitting on the training data set, hyperparameter tuning, and topological alterations of the classifier block, the proposed NASNet-based model was evaluated on the test data set and yielded remarkable results. The proposed model's performance achieved a detection sensitivity, specificity, and accuracy of 0.999, 0.986, and 0.996, respectively. CONCLUSIONS: The proposed model achieved acceptable results in the categorization of 2 data classes. Therefore, a CAD system was designed on the basis of this model for COVID-19 detection using multiple lung computed tomography scans. The system differentiated all COVID-19 cases from non-COVID-19 ones without any error in the application phase. Overall, the proposed deep learning-based CAD system can greatly help radiologists detect COVID-19 in its early stages. During the COVID-19 pandemic, the use of a CAD system as a screening tool would accelerate disease detection and prevent the loss of health care resources.",2021,10.2196/27468,diagnosis,True
Deep Convolutional Neural Network-based Software Improves Radiologist Detection of Malignant Lung Nodules on Chest Radiographs,"Background Multicenter studies are required to validate the added benefit of using deep convolutional neural network (DCNN) software for detecting malignant pulmonary nodules on chest radiographs. Purpose To compare the performance of radiologists in detecting malignant pulmonary nodules on chest radiographs when assisted by deep learning-based DCNN software with that of radiologists or DCNN software alone in a multicenter setting. Materials and Methods Investigators at four medical centers retrospectively identified 600 lung cancer-containing chest radiographs and 200 normal chest radiographs. Each radiograph with a lung cancer had at least one malignant nodule confirmed by CT and pathologic examination. Twelve radiologists from the four centers independently analyzed the chest radiographs and marked regions of interest. Commercially available deep learning-based computer-aided detection software separately trained, tested, and validated with 19 330 radiographs was used to find suspicious nodules. The radiologists then reviewed the images with the assistance of DCNN software. The sensitivity and number of false-positive findings per image of DCNN software, radiologists alone, and radiologists with the use of DCNN software were analyzed by using logistic regression and Poisson regression. Results The average sensitivity of radiologists improved (from 65.1% [1375 of 2112; 95% confidence interval {CI}: 62.0%, 68.1%] to 70.3% [1484 of 2112; 95% CI: 67.2%, 73.1%], P < .001) and the number of false-positive findings per radiograph declined (from 0.2 [488 of 2400; 95% CI: 0.18, 0.22] to 0.18 [422 of 2400; 95% CI: 0.16, 0.2], P < .001) when the radiologists re-reviewed radiographs with the DCNN software. For the 12 radiologists in this study, 104 of 2400 radiographs were positively changed (from false-negative to true-positive or from false-positive to true-negative) using the DCNN, while 56 of 2400 radiographs were changed negatively. Conclusion Radiologists had better performance with deep convolutional network software for the detection of malignant pulmonary nodules on chest radiographs than without. © RSNA, 2019 Online supplemental material is available for this article. See also the editorial by Jacobson in this issue.",2020,10.1148/radiol.2019182465,diagnosis,False
Deep convolutional neural networks for COVID-19 automatic diagnosis,"This article is mainly concerned with COVID-19 diagnosis from X-ray images. The number of cases infected with COVID-19 is increasing daily, and there is a limitation in the number of test kits needed in hospitals. Therefore, there is an imperative need to implement an efficient automatic diagnosis system to alleviate COVID-19 spreading among people. This article presents a discussion of the utilization of convolutional neural network (CNN) models with different learning strategies for automatic COVID-19 diagnosis. First, we consider the CNN-based transfer learning approach for automatic diagnosis of COVID-19 from X-ray images with different training and testing ratios. Different pre-trained deep learning models in addition to a transfer learning model are considered and compared for the task of COVID-19 detection from X-ray images. Confusion matrices of these studied models are presented and analyzed. Considering the performance results obtained, ResNet models (ResNet18, ResNet50, and ResNet101) provide the highest classification accuracy on the two considered datasets with different training and testing ratios, namely 80/20, 70/30, 60/40, and 50/50. The accuracies obtained using the first dataset with 70/30 training and testing ratio are 97.67%, 98.81%, and 100% for ResNet18, ResNet50, and ResNet101, respectively. For the second dataset, the reported accuracies are 99%, 99.12%, and 99.29% for ResNet18, ResNet50, and ResNet101, respectively. The second approach is the training of a proposed CNN model from scratch. The results confirm that training of the CNN from scratch can lead to the identification of the signs of COVID-19 disease.",2021,10.1002/jemt.23713,diagnosis,False
Deep convolutional neural networks for multiplanar lung nodule detection: Improvement in small nodule identification,"PURPOSE: Early detection of lung cancer is of importance since it can increase patients' chances of survival. To detect nodules accurately during screening, radiologists would commonly take the axial, coronal, and sagittal planes into account, rather than solely the axial plane in clinical evaluation. Inspired by clinical work, the paper aims to develop an accurate deep learning framework for nodule detection by a combination of multiple planes. METHODS: The nodule detection system is designed in two stages, multiplanar nodule candidate detection, multiscale false positive (FP) reduction. At the first stage, a deeply supervised encoder-decoder network is trained by axial, coronal, and sagittal slices for the candidate detection task. All possible nodule candidates from the three different planes are merged. To further refine results, a three-dimensional multiscale dense convolutional neural network that extracts multiscale contextual information is applied to remove non-nodules. In the public LIDC-IDRI dataset, 888 computed tomography scans with 1186 nodules accepted by at least three of four radiologists are selected to train and evaluate our proposed system via a tenfold cross-validation scheme. The free-response receiver operating characteristic curve is used for performance assessment. RESULTS: The proposed system achieves a sensitivity of 94.2% with 1.0 FP/scan and a sensitivity of 96.0% with 2.0 FPs/scan. Although it is difficult to detect small nodules (i.e., <6 mm), our designed CAD system reaches a sensitivity of 93.4% (95.0%) of these small nodules at an overall FP rate of 1.0 (2.0) FPs/scan. At the nodule candidate detection stage, results show that the system with a multiplanar method is capable to detect more nodules compared to using a single plane. CONCLUSION: Our approach achieves good performance not only for small nodules but also for large lesions on this dataset. This demonstrates the effectiveness of our developed CAD system for lung nodule detection.",2021,10.1002/mp.14648,diagnosis,True
Deep convolutional neural networks with multiplane consensus labeling for lung function quantification using UTE proton MRI,"BACKGROUND: Ultrashort echo time (UTE) proton MRI has gained popularity for assessing lung structure and function in pulmonary imaging; however, the development of rapid biomarker extraction and regional quantification has lagged behind due to labor-intensive lung segmentation. PURPOSE: To evaluate a deep learning (DL) approach for automated lung segmentation to extract image-based biomarkers from functional lung imaging using 3D radial UTE oxygen-enhanced (OE) MRI. STUDY TYPE: Retrospective study aimed to evaluate a technical development. POPULATION: Forty-five human subjects, including 16 healthy volunteers, 5 asthma, and 24 patients with cystic fibrosis. FIELD STRENGTH/SEQUENCE: 1.5T MRI, 3D radial UTE (TE = 0.08 msec) sequence. ASSESSMENT: Two 3D radial UTE volumes were acquired sequentially under normoxic (21% O(2) ) and hyperoxic (100% O(2) ) conditions. Automated segmentation of the lungs using 2D convolutional encoder-decoder based DL method, and the subsequent functional quantification via adaptive K-means were compared with the results obtained from the reference method, supervised region growing. STATISTICAL TESTS: Relative to the reference method, the performance of DL on volumetric quantification was assessed using Dice coefficient with 95% confidence interval (CI) for accuracy, two-sided Wilcoxon signed-rank test for computation time, and Bland-Altman analysis on the functional measure derived from the OE images. RESULTS: The DL method produced strong agreement with supervised region growing for the right (Dice: 0.97; 95% CI = [0.96, 0.97]; P < 0.001) and left lungs (Dice: 0.96; 95% CI = [0.96, 0.97]; P < 0.001). The DL method averaged 46 seconds to generate the automatic segmentations in contrast to 1.93 hours using the reference method (P < 0.001). Bland-Altman analysis showed nonsignificant intermethod differences of volumetric (P ≥ 0.12) and functional measurements (P ≥ 0.34) in the left and right lungs. DATA CONCLUSION: DL provides rapid, automated, and robust lung segmentation for quantification of regional lung function using UTE proton MRI. LEVEL OF EVIDENCE: 2 Technical Efficacy: Stage 1 J. Magn. Reson. Imaging 2019;50:1169-1181.",2019,10.1002/jmri.26734,diagnosis,False
Deep cross-modality (MR-CT) educed distillation learning for cone beam CT lung tumor segmentation,"PURPOSE: Despite the widespread availability of in-treatment room cone beam computed tomography (CBCT) imaging, due to the lack of reliable segmentation methods, CBCT is only used for gross set up corrections in lung radiotherapies. Accurate and reliable auto-segmentation tools could potentiate volumetric response assessment and geometry-guided adaptive radiation therapies. Therefore, we developed a new deep learning CBCT lung tumor segmentation method. METHODS: The key idea of our approach called cross-modality educed distillation (CMEDL) is to use magnetic resonance imaging (MRI) to guide a CBCT segmentation network training to extract more informative features during training. We accomplish this by training an end-to-end network comprised of unpaired domain adaptation (UDA) and cross-domain segmentation distillation networks (SDNs) using unpaired CBCT and MRI datasets. UDA approach uses CBCT and MRI that are not aligned and may arise from different sets of patients. The UDA network synthesizes pseudo MRI from CBCT images. The SDN consists of teacher MRI and student CBCT segmentation networks. Feature distillation regularizes the student network to extract CBCT features that match the statistical distribution of MRI features extracted by the teacher network and obtain better differentiation of tumor from background. The UDA network was implemented with a cycleGAN improved with contextual losses separately on Unet and dense fully convolutional segmentation networks (DenseFCN). Performance comparisons were done against CBCT only using 2D and 3D networks. We also compared against an alternative framework that used UDA with MR segmentation network, whereby segmentation was done on the synthesized pseudo MRI representation. All networks were trained with 216 weekly CBCTs and 82 T2-weighted turbo spin echo MRI acquired from different patient cohorts. Validation was done on 20 weekly CBCTs from patients not used in training. Independent testing was done on 38 weekly CBCTs from patients not used in training or validation. Segmentation accuracy was measured using surface Dice similarity coefficient (SDSC) and Hausdroff distance at 95th percentile (HD95) metrics. RESULTS: The CMEDL approach significantly improved (p < 0.001) the accuracy of both Unet (SDSC of 0.83 ± 0.08; HD95 of 7.69 ± 7.86 mm) and DenseFCN (SDSC of 0.75 ± 0.13; HD95 of 11.42 ± 9.87 mm) over CBCT only 2DUnet (SDSC of 0.69 ± 0.11; HD95 of 21.70 ± 16.34 mm), 3D Unet (SDSC of 0.72 ± 0.20; HD95 15.01 ± 12.98 mm), and DenseFCN (SDSC of 0.66 ± 0.15; HD95 of 22.15 ± 17.19 mm) networks. The alternate framework using UDA with the MRI network was also more accurate than the CBCT only methods but less accurate the CMEDL approach. CONCLUSIONS: Our results demonstrate feasibility of the introduced CMEDL approach to produce reasonably accurate lung cancer segmentation from CBCT images. Further validation on larger datasets is necessary for clinical translation.",2021,10.1002/mp.14902,treatment,True
Deep Ensemble Model for Classification of Novel Coronavirus in Chest X-Ray Images,"The novel coronavirus, SARS-CoV-2, can be deadly to people, causing COVID-19. The ease of its propagation, coupled with its high capacity for illness and death in infected individuals, makes it a hazard to the community. Chest X-rays are one of the most common but most difficult to interpret radiographic examination for early diagnosis of coronavirus-related infections. They carry a considerable amount of anatomical and physiological information, but it is sometimes difficult even for the expert radiologist to derive the related information they contain. Automatic classification using deep learning models can help in better assessing these infections swiftly. Deep CNN models, namely, MobileNet, ResNet50, and InceptionV3, were applied with different variations, including training the model from the start, fine-tuning along with adjusting learned weights of all layers, and fine-tuning with learned weights along with augmentation. Fine-tuning with augmentation produced the best results in pretrained models. Out of these, two best-performing models (MobileNet and InceptionV3) selected for ensemble learning produced accuracy and FScore of 95.18% and 90.34%, and 95.75% and 91.47%, respectively. The proposed hybrid ensemble model generated with the merger of these deep models produced a classification accuracy and FScore of 96.49% and 92.97%. For test dataset, which was separately kept, the model generated accuracy and FScore of 94.19% and 88.64%. Automatic classification using deep ensemble learning can help radiologists in the correct identification of coronavirus-related infections in chest X-rays. Consequently, this swift and computer-aided diagnosis can help in saving precious human lives and minimizing the social and economic impact on society.",2021,10.1155/2021/8890226,diagnosis,False
Deep Learning Algorithm for COVID-19 Classification Using Chest X-Ray Images,"Early diagnosis of the harmful severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), along with clinical expertise, allows governments to break the transition chain and flatten the epidemic curve. Although reverse transcription-polymerase chain reaction (RT-PCR) offers quick results, chest X-ray (CXR) imaging is a more reliable method for disease classification and assessment. The rapid spread of the coronavirus disease 2019 (COVID-19) has triggered extensive research towards developing a COVID-19 detection toolkit. Recent studies have confirmed that the deep learning-based approach, such as convolutional neural networks (CNNs), provides an optimized solution for COVID-19 classification; however, they require substantial training data for learning features. Gathering this training data in a short period has been challenging during the pandemic. Therefore, this study proposes a new model of CNN and deep convolutional generative adversarial networks (DCGANs) that classify CXR images into normal, pneumonia, and COVID-19. The proposed model contains eight convolutional layers, four max-pooling layers, and two fully connected layers, which provide better results than the existing pretrained methods (AlexNet and GoogLeNet). DCGAN performs two tasks: (1) generating synthetic/fake images to overcome the challenges of an imbalanced dataset and (2) extracting deep features of all images in the dataset. In addition, it enlarges the dataset and represents the characteristics of diversity to provide a good generalization effect. In the experimental analysis, we used four distinct publicly accessible datasets of chest X-ray images (COVID-19 X-ray, COVID Chest X-ray, COVID-19 Radiography, and CoronaHack-Chest X-Ray) to train and test the proposed CNN and the existing pretrained methods. Thereafter, the proposed CNN method was trained with the four datasets based on the DCGAN synthetic images, resulting in higher accuracy (94.8%, 96.6%, 98.5%, and 98.6%) than the existing pretrained models. The overall results suggest that the proposed DCGAN-CNN approach is a promising solution for efficient COVID-19 diagnosis.",2021,10.1155/2021/9269173,diagnosis,False
Deep Learning Algorithms-Based CT Images in Glucocorticoid Therapy in Asthma Children with Small Airway Obstruction,"CT image information data under deep learning algorithms was adopted to evaluate small airway function and analyze the clinical efficacy of different glucocorticoid administration ways in asthmatic children with small airway obstruction. The Res-NET in the deep learning algorithm was used to perform feature extraction, summary classification, and other reconstruction of CT images. A deep learning network model Mask-R-CNN was constructed to enhance the ability of image reconstruction. A total of 118 children hospitalized with acute exacerbation of asthma in the hospital were recruited. After acute exacerbation treatment, 96 children with asthma were screened out for small airway obstruction, which were divided into glucocorticoid aerosol inhalation group (group A, 32 cases), glucocorticoid combined with bronchodilator aerosol inhalation group (group B, 32 cases), and oral hormone therapy group (group C, 32 cases). Asthmatic children with small airway obstruction were screened after acute exacerbation treatment and were rolled into glucocorticoid aerosol inhalation group (group A), glucocorticoid combined with bronchodilators aerosol inhalation group (group B), and oral hormone therapy group (group C). Lung function indicators (maximal mid-expiratory flow (MMEF75 and 25), 50% forced expiratory flow (FEF50), and 75% forced expiratory flow (FEF75)), FeNO level, and airway inflammation indicators (IL-6, IL-35, and eosinophilic (EOS)) were compared before and one month after treatment. The ratio of airway wall thickness to outer diameter (T/D) and the percentage of airway wall area to total airway area (WA%) were measured by e-Health high-resolution CT (HRCT). The constructed network model was used to measure the patient's coronary artery plaque and blood vessel volume, and the image was reconstructed on the Res-Net network. It was found that the MSE value of the Res-Net network was the lowest, and the efficiency was very high during the training process. T/D and WA (%) of asthmatic children with small airway obstruction after treatment were significantly lower than those before treatment (P < 0.01). After treatment, MMEF75/25 and FEF75 were significantly higher than those before treatment (P < 0.05). Lung function-related indicator FEF50 was significantly higher than that before treatment (P < 0.01). FeNO level after treatment was remarkably lower than that before treatment (P < 0.01). In addition, lung function-related indicators, airway inflammation indicators, and FeNO level improved the most in group C, followed by group B, and those improvements in group A were the least obvious, with great differences among groups (P < 0.05). In summary, the Res-Net model proposed was of certain feasibility and effectiveness for CT image segmentation and can effectively improve the clinical evaluation of patient CT image information. Glucocorticoids could improve small airway function and airway inflammation in asthmatic children with small airway obstruction, and oral corticosteroids were more effective than aerosol inhalation therapy.",2021,10.1155/2021/5317403,combined,True
Deep Learning Analysis in Prediction of COVID-19 Infection Status Using Chest CT Scan Features,"Background and aims Non-contrast chest computed tomography (CT) scanning is one of the important tools for evaluating of lung lesions. The aim of this study was to use a deep learning approach for predicting the outcome of patients with COVID-19 into two groups of critical and non-critical according to their CT features. Methods This was carried out as a retrospective study from March to April 2020 in Baqiyatallah Hospital, Tehran, Iran. From total of 1078 patients with COVID-19 pneumonia who underwent chest CT, 169 were critical cases and 909 were non-critical. Deep learning neural networks were used to classify samples into critical or non-critical ones according to the chest CT results. Results The best accuracy of prediction was seen by the presence of diffuse opacities and lesion distribution (both=0.91, 95% CI: 0.83-0.99). The largest sensitivity was achieved using lesion distribution (0.74, 95% CI: 0.55-0.93), and the largest specificity was for presence of diffuse opacities (0.95, 95% CI: 0.9-1). The total model showed an accuracy of 0.89 (95% CI: 0.79-0.99), and the corresponding sensitivity and specificity were 0.71 (95% CI: 0.51-0.91) and 0.93 (95% CI: 0.87-0.96), respectively. Conclusions The results showed that CT scan can accurately classify and predict critical and non-critical COVID-19 cases.",2021,10.1007/978-3-030-71697-4_11,diagnosis,True
Deep learning and lung ultrasound for Covid-19 pneumonia detection and severity classification,"The Covid-19 European outbreak in February 2020 has challenged the world's health systems, eliciting an urgent need for effective and highly reliable diagnostic instruments to help medical personnel. Deep learning (DL) has been demonstrated to be useful for diagnosis using both computed tomography (CT) scans and chest X-rays (CXR), whereby the former typically yields more accurate results. However, the pivoting function of a CT scan during the pandemic presents several drawbacks, including high cost and cross-contamination problems. Radiation-free lung ultrasound (LUS) imaging, which requires high expertise and is thus being underutilised, has demonstrated a strong correlation with CT scan results and a high reliability in pneumonia detection even in the early stages. In this study, we developed a system based on modern DL methodologies in close collaboration with Fondazione IRCCS Policlinico San Matteo's Emergency Department (ED) of Pavia. Using a reliable dataset comprising ultrasound clips originating from linear and convex probes in 2908 frames from 450 hospitalised patients, we conducted an investigation into detecting Covid-19 patterns and ranking them considering two severity scales. This study differs from other research projects by its novel approach involving four and seven classes. Patients admitted to the ED underwent 12 LUS examinations in different chest parts, each evaluated according to standardised severity scales. We adopted residual convolutional neural networks (CNNs), transfer learning, and data augmentation techniques. Hence, employing methodological hyperparameter tuning, we produced state-of-the-art results meeting F1 score levels, averaged over the number of classes considered, exceeding 98%, and thereby manifesting stable measurements over precision and recall.",2021,10.1016/j.compbiomed.2021.104742,diagnosis,False
Deep learning classification of lung cancer histology using CT images,"Tumor histology is an important predictor of therapeutic response and outcomes in lung cancer. Tissue sampling for pathologist review is the most reliable method for histology classification, however, recent advances in deep learning for medical image analysis allude to the utility of radiologic data in further describing disease characteristics and for risk stratification. In this study, we propose a radiomics approach to predicting non-small cell lung cancer (NSCLC) tumor histology from non-invasive standard-of-care computed tomography (CT) data. We trained and validated convolutional neural networks (CNNs) on a dataset comprising 311 early-stage NSCLC patients receiving surgical treatment at Massachusetts General Hospital (MGH), with a focus on the two most common histological types: adenocarcinoma (ADC) and Squamous Cell Carcinoma (SCC). The CNNs were able to predict tumor histology with an AUC of 0.71(p = 0.018). We also found that using machine learning classifiers such as k-nearest neighbors (kNN) and support vector machine (SVM) on CNN-derived quantitative radiomics features yielded comparable discriminative performance, with AUC of up to 0.71 (p = 0.017). Our best performing CNN functioned as a robust probabilistic classifier in heterogeneous test sets, with qualitatively interpretable visual explanations to its predictions. Deep learning based radiomics can identify histological phenotypes in lung cancer. It has the potential to augment existing approaches and serve as a corrective aid for diagnosticians.",2021,10.1038/s41598-021-84630-x,diagnosis,True
Deep learning combined with radiomics may optimize the prediction in differentiating high-grade lung adenocarcinomas in ground glass opacity lesions on CT scans,"PURPOSE: Adenocarcinoma (ADC) is the most common histological subtype of lung cancers in non-small cell lung cancer (NSCLC) in which ground glass opacifications (GGOs) found on computed tomography (CT) scans are the most common lesions. However, the presence of a micropapillary or a solid component is identified as an independent predictor of prognosis, suggesting a more extensive resection. The purpose of our study is to explore imaging phenotyping using a method combining radiomics with deep learning (RDL) to predict high-grade patterns within lung ADC. METHODS: Included in this study were 111 patients differentiated as having GGOs and pathologically confirmed ADC. Four different groups of methods were compared to classify the GGOs for the prediction of the pathological subtypes of high-grade lung ADCs in definitive hematoxylin and eosin stain, including radiomics with gray-level features, radiomics with textural features, deep learning method, and the RDL. RESULTS: We evaluated the performance of different models on 111 NSCLC patients using 4-fold cross-validation. The proposed RDL has achieved an overall accuracy of 0.913, which significantly outperforms the other methods (p < 0.01, analysis of variation, ANOVA). In addition, we also verified the generality and practical effectiveness of these models on an independent validation dataset of 28 patients. The results showed that our RDL framework with an accuracy of 0.966 significantly surpassed other methods. CONCLUSION: High-grade lung ADC based on histologic pattern spectrum in GGO lesions might be predicted by the framework combining radiomics with deep learning, which reveals advantage over radiomics alone.",2020,10.1016/j.ejrad.2020.109150,diagnosis,True
Deep learning COVID-19 detection bias: accuracy through artificial intelligence,"BACKGROUND: Detection of COVID-19 cases' accuracy is posing a conundrum for scientists, physicians, and policy-makers. As of April 23, 2020, 2.7 million cases have been confirmed, over 190,000 people are dead, and about 750,000 people are reported recovered. Yet, there is no publicly available data on tests that could be missing infections. Complicating matters and furthering anxiety are specific instances of false-negative tests. METHODS: We developed a deep learning model to improve accuracy of reported cases and to precisely predict the disease from chest X-ray scans. Our model relied on convolutional neural networks (CNNs) to detect structural abnormalities and disease categorization that were keys to uncovering hidden patterns. To do so, a transfer learning approach was deployed to perform detections from the chest anterior-posterior radiographs of patients. We used publicly available datasets to achieve this. RESULTS: Our results offer very high accuracy (96.3%) and loss (0.151 binary cross-entropy) using the public dataset consisting of patients from different countries worldwide. As the confusion matrix indicates, our model is able to accurately identify true negatives (74) and true positives (32); this deep learning model identified three cases of false-positive and one false-negative finding from the healthy patient scans. CONCLUSIONS: Our COVID-19 detection model minimizes manual interaction dependent on radiologists as it automates identification of structural abnormalities in patient's CXRs, and our deep learning model is likely to detect true positives and true negatives and weed out false positive and false negatives with > 96.3% accuracy.",2020,10.1007/s00264-020-04609-7,diagnosis,False
Deep Learning COVID-19 Features on CXR Using Limited Training Data Sets,"Under the global pandemic of COVID-19, the use of artificial intelligence to analyze chest X-ray (CXR) image for COVID-19 diagnosis and patient triage is becoming important. Unfortunately, due to the emergent nature of the COVID-19 pandemic, a systematic collection of CXR data set for deep neural network training is difficult. To address this problem, here we propose a patch-based convolutional neural network approach with a relatively small number of trainable parameters for COVID-19 diagnosis. The proposed method is inspired by our statistical analysis of the potential imaging biomarkers of the CXR radiographs. Experimental results show that our method achieves state-of-the-art performance and provides clinically interpretable saliency maps, which are useful for COVID-19 diagnosis and patient triage.",2020,10.1109/tmi.2020.2993291,diagnosis,False
Deep learning driven predictive treatment planning for adaptive radiotherapy of lung cancer,"BACKGROUND AND PURPOSE: To develop a novel deep learning algorithm of sequential analysis, Seq2Seq, for predicting weekly anatomical changes of lung tumor and esophagus during definitive radiotherapy, incorporate the potential tumor shrinkage into a predictive treatment planning paradigm, and improve the therapeutic ratio. METHODS AND MATERIALS: Seq2Seq starts with the primary tumor and esophagus observed on the planning CT to predict their geometric evolution during radiotherapy on a weekly basis, and subsequently updates the predictions with new snapshots acquired via weekly CBCTs. Seq2Seq is equipped with convolutional long short term memory to analyze the spatial-temporal changes of longitudinal images, trained and validated using a dataset including sixty patients. Predictive plans were optimized according to each weekly prediction and made ready for weekly deployment to mitigate the clinical burden of online weekly replanning. RESULTS: Seq2Seq tracks structural changes well: DICE between predicted and actual weekly tumor and esophagus were (0.83 ± 0.10, 0.79 ± 0.14, 0.78 ± 0.12, 0.77 ± 0.12, 0.75 ± 0.12, 0.71 ± 0.17), and (0.72 ± 0.16, 0.73 ± 0.11, 0.75 ± 0.08, 0.74 ± 0.09, 0.72 ± 0.14, 0.71 ± 0.14), respectively, while the average Hausdorff distances were within 2 mm. Evaluating dose to the actual weekly tumor and esophagus, a 4.2 Gy reduction in esophagus mean dose while maintaining 60 Gy tumor coverage was achieved with the predictive weekly plans, compared to the plan optimized using the initial tumor and esophagus alone, primarily due to noticeable tumor shrinkage during radiotherapy. CONCLUSION: It is feasible to predict the longitudinal changes of tumor and esophagus with the Seq2Seq, which could lead to improving the efficiency and effectiveness of lung adaptive radiotherapy.",2022,10.1016/j.radonc.2022.02.013,treatment,True
Deep Learning Enables Accurate Diagnosis of Novel Coronavirus (COVID-19) With CT Images,"A novel coronavirus (COVID-19) recently emerged as an acute respiratory syndrome, and has caused a pneumonia outbreak world-widely. As the COVID-19 continues to spread rapidly across the world, computed tomography (CT) has become essentially important for fast diagnoses. Thus, it is urgent to develop an accurate computer-aided method to assist clinicians to identify COVID-19-infected patients by CT images. Here, we have collected chest CT scans of 88 patients diagnosed with COVID-19 from hospitals of two provinces in China, 100 patients infected with bacteria pneumonia, and 86 healthy persons for comparison and modeling. Based on the data, a deep learning-based CT diagnosis system was developed to identify patients with COVID-19. The experimental results showed that our model could accurately discriminate the COVID-19 patients from the bacteria pneumonia patients with an AUC of 0.95, recall (sensitivity) of 0.96, and precision of 0.79. When integrating three types of CT images, our model achieved a recall of 0.93 with precision of 0.86 for discriminating COVID-19 patients from others. Moreover, our model could extract main lesion features, especially the ground-glass opacity (GGO), which are visually helpful for assisted diagnoses by doctors. An online server is available for online diagnoses with CT images by our server (http://biomed.nscc-gz.cn/model.php). Source codes and datasets are available at our GitHub (https://github.com/SY575/COVID19-CT).",2021,10.1109/tcbb.2021.3065361,diagnosis,True
Deep Learning Enables Automatic Classification of Emphysema Pattern at CT,"BackgroundPattern of emphysema at chest CT, scored visually by using the Fleischner Society system, is associated with physiologic impairment and mortality risk.PurposeTo determine whether participant-level emphysema pattern could predict impairment and mortality when classified by using a deep learning method.Materials and MethodsThis retrospective analysis of Genetic Epidemiology of COPD (COPDGene) study participants enrolled between 2007 and 2011 included those with baseline CT, visual emphysema scores, and survival data through 2018. Participants were partitioned into nonoverlapping sets of 2407 for algorithm training, 100 for validation and parameter tuning, and 7143 for testing. A deep learning algorithm using convolutional neural network and long short-term memory architectures was trained to classify pattern of emphysema according to Fleischner criteria. Deep learning scores were compared with visual scores and clinical parameters including pulmonary function tests. Cox proportional hazard models were used to evaluate relationships between emphysema scores and survival. The algorithm was also tested by using CT and clinical data in 1962 participants enrolled in the Evaluation of COPD Longitudinally to Identify Predictive Surrogate End-points (ECLIPSE) study.ResultsA total of 7143 COPDGene participants (mean age ± standard deviation, 59.8 years ± 8.9; 3734 men and 3409 women) were evaluated. Deep learning emphysema classifications were associated with impaired pulmonary function tests, 6-minute walk distance, and St George's Respiratory Questionnaire at univariate analysis (P < .001 for each). Testing in the ECLIPSE cohort showed similar associations (P < .001). In the COPDGene test cohort, deep learning emphysema classification improved the fit of linear mixed models in the prediction of these clinical parameters compared with visual scoring (P < .001). Compared with participants without emphysema, mortality was greater in participants classified by the deep learning algorithm as having any grade of emphysema (adjusted hazard ratios were 1.5, 1.7, 2.9, 5.3, and 9.7, respectively, for trace, mild, moderate, confluent, and advanced destructive emphysema; P < .05).ConclusionDeep learning automation of the Fleischner grade of emphysema at chest CT is associated with clinical measures of pulmonary insufficiency and the risk of mortality.© RSNA, 2019Online supplemental material is available for this article.",2020,10.1148/radiol.2019191022,diagnosis,True
Deep Learning for Classification and Localization of COVID-19 Markers in Point-of-Care Lung Ultrasound,"Deep learning (DL) has proved successful in medical imaging and, in the wake of the recent COVID-19 pandemic, some works have started to investigate DL-based solutions for the assisted diagnosis of lung diseases. While existing works focus on CT scans, this paper studies the application of DL techniques for the analysis of lung ultrasonography (LUS) images. Specifically, we present a novel fully-annotated dataset of LUS images collected from several Italian hospitals, with labels indicating the degree of disease severity at a frame-level, video-level, and pixel-level (segmentation masks). Leveraging these data, we introduce several deep models that address relevant tasks for the automatic analysis of LUS images. In particular, we present a novel deep network, derived from Spatial Transformer Networks, which simultaneously predicts the disease severity score associated to a input frame and provides localization of pathological artefacts in a weakly-supervised way. Furthermore, we introduce a new method based on uninorms for effective frame score aggregation at a video-level. Finally, we benchmark state of the art deep models for estimating pixel-level segmentations of COVID-19 imaging biomarkers. Experiments on the proposed dataset demonstrate satisfactory results on all the considered tasks, paving the way to future research on DL for the assisted diagnosis of COVID-19 from LUS data.",2020,10.1109/tmi.2020.2994459,diagnosis,False
Deep learning for COVID-19 detection based on CT images,"COVID-19 has tremendously impacted patients and medical systems globally. Computed tomography images can effectively complement the reverse transcription-polymerase chain reaction testing. This study adopted a convolutional neural network for COVID-19 testing. We examined the performance of different pre-trained models on CT testing and identified that larger, out-of-field datasets boost the testing power of the models. This suggests that a priori knowledge of the models from out-of-field training is also applicable to CT images. The proposed transfer learning approach proves to be more successful than the current approaches described in literature. We believe that our approach has achieved the state-of-the-art performance in identification thus far. Based on experiments with randomly sampled training datasets, the results reveal a satisfactory performance by our model. We investigated the relevant visual characteristics of the CT images used by the model; these may assist clinical doctors in manual screening.",2021,10.1038/s41598-021-93832-2,diagnosis,True
Deep Learning for Detecting Pneumothorax on Chest Radiographs after Needle Biopsy: Clinical Implementation,"Background Accurate detection of pneumothorax on chest radiographs, the most common complication of percutaneous transthoracic needle biopsies (PTNBs), is not always easy in practice. A computer-aided detection (CAD) system may help detect pneumothorax. Purpose To investigate whether a deep learning-based CAD system can improve detection performance for pneumothorax on chest radiographs after PTNB in clinical practice. Materials and Methods A CAD system for post-PTNB pneumothorax detection on chest radiographs was implemented in an institution in February 2020. This retrospective cohort study consecutively included chest radiographs interpreted with CAD assistance (CAD-applied group; February 2020 to November 2020) and those interpreted before implementation (non-CAD group; January 2018 to January 2020). The reference standard was defined by consensus reading by two radiologists. The diagnostic accuracy for pneumothorax was compared between the two groups using generalized estimating equations. Matching was performed according to whether the radiograph reader and PTNB operator were the same using the greedy method. Results A total of 676 radiographs from 655 patients (mean age: 67 years ± 11; 390 men) in the CAD-applied group and 676 radiographs from 664 patients (mean age: 66 years ± 12; 400 men) in the non-CAD group were included. The incidence of pneumothorax was 18.2% (123 of 676 radiographs) in the CAD-applied group and 22.5% (152 of 676 radiographs) in the non-CAD group (P = .05). The CAD-applied group showed higher sensitivity (85.4% vs 67.1%), negative predictive value (96.8% vs 91.3%), and accuracy (96.8% vs 92.3%) than the non-CAD group (all P < .001). The sensitivity for a small amount of pneumothorax improved in the CAD-applied group (pneumothorax of <10%: 74.5% vs 51.4%, P = .009; pneumothorax of 10%-15%: 92.7% vs 70.2%, P = .008). Among patients with pneumothorax, 34 of 655 (5.0%) in the non-CAD group and 16 of 664 (2.4%) in the CAD-applied group (P = .009) required subsequent drainage catheter insertion. Conclusion A deep learning-based computer-aided detection system improved the detection performance for pneumothorax on chest radiographs after lung biopsy. © RSNA, 2022 See also the editorial by Schiebler and Hartung in this issue.",2022,10.1148/radiol.211706,diagnosis,False
Deep learning for diagnosis of COVID-19 using 3D CT scans,"A new pneumonia-type coronavirus, COVID-19, recently emerged in Wuhan, China. COVID-19 has subsequently infected many people and caused many deaths worldwide. Isolating infected people is one of the methods of preventing the spread of this virus. CT scans provide detailed imaging of the lungs and assist radiologists in diagnosing COVID-19 in hospitals. However, a person's CT scan contains hundreds of slides, and the diagnosis of COVID-19 using such scans can lead to delays in hospitals. Artificial intelligence techniques could assist radiologists with rapidly and accurately detecting COVID-19 infection from these scans. This paper proposes an artificial intelligence (AI) approach to classify COVID-19 and normal CT volumes. The proposed AI method uses the ResNet-50 deep learning model to predict COVID-19 on each CT image of a 3D CT scan. Then, this AI method fuses image-level predictions to diagnose COVID-19 on a 3D CT volume. We show that the proposed deep learning model provides 96% AUC value for detecting COVID-19 on CT scans.",2021,10.1016/j.compbiomed.2021.104306,diagnosis,True
Deep learning for lung cancer prognostication: A retrospective multi-cohort radiomics study,"BACKGROUND: Non-small-cell lung cancer (NSCLC) patients often demonstrate varying clinical courses and outcomes, even within the same tumor stage. This study explores deep learning applications in medical imaging allowing for the automated quantification of radiographic characteristics and potentially improving patient stratification. METHODS AND FINDINGS: We performed an integrative analysis on 7 independent datasets across 5 institutions totaling 1,194 NSCLC patients (age median = 68.3 years [range 32.5-93.3], survival median = 1.7 years [range 0.0-11.7]). Using external validation in computed tomography (CT) data, we identified prognostic signatures using a 3D convolutional neural network (CNN) for patients treated with radiotherapy (n = 771, age median = 68.0 years [range 32.5-93.3], survival median = 1.3 years [range 0.0-11.7]). We then employed a transfer learning approach to achieve the same for surgery patients (n = 391, age median = 69.1 years [range 37.2-88.0], survival median = 3.1 years [range 0.0-8.8]). We found that the CNN predictions were significantly associated with 2-year overall survival from the start of respective treatment for radiotherapy (area under the receiver operating characteristic curve [AUC] = 0.70 [95% CI 0.63-0.78], p < 0.001) and surgery (AUC = 0.71 [95% CI 0.60-0.82], p < 0.001) patients. The CNN was also able to significantly stratify patients into low and high mortality risk groups in both the radiotherapy (p < 0.001) and surgery (p = 0.03) datasets. Additionally, the CNN was found to significantly outperform random forest models built on clinical parameters-including age, sex, and tumor node metastasis stage-as well as demonstrate high robustness against test-retest (intraclass correlation coefficient = 0.91) and inter-reader (Spearman's rank-order correlation = 0.88) variations. To gain a better understanding of the characteristics captured by the CNN, we identified regions with the most contribution towards predictions and highlighted the importance of tumor-surrounding tissue in patient stratification. We also present preliminary findings on the biological basis of the captured phenotypes as being linked to cell cycle and transcriptional processes. Limitations include the retrospective nature of this study as well as the opaque black box nature of deep learning networks. CONCLUSIONS: Our results provide evidence that deep learning networks may be used for mortality risk stratification based on standard-of-care CT images from NSCLC patients. This evidence motivates future research into better deciphering the clinical and biological basis of deep learning networks as well as validation in prospective data.",2018,10.1371/journal.pmed.1002711,prognosis,True
Deep learning for lung disease segmentation on CT: Which reconstruction kernel should be used?,"PURPOSE: The purpose of this study was to determine whether a single reconstruction kernel or both high and low frequency kernels should be used for training deep learning models for the segmentation of diffuse lung disease on chest computed tomography (CT). MATERIALS AND METHODS: Two annotated datasets of COVID-19 pneumonia (323,960 slices) and interstitial lung disease (ILD) (4,284 slices) were used. Annotated CT images were used to train a U-Net architecture to segment disease. All CT slices were reconstructed using both a lung kernel (LK) and a mediastinal kernel (MK). Three different trainings, resulting in three different models were compared for each disease: training on LK only, MK only or LK+MK images. Dice similarity scores (DSC) were compared using the Wilcoxon signed-rank test. RESULTS: Models only trained on LK images performed better on LK images than on MK images (median DSC = 0.62 [interquartile range (IQR): 0.54, 0.69] vs. 0.60 [IQR: 0.50, 0.70], P < 0.001 for COVID-19 and median DSC = 0.62 [IQR: 0.56, 0.69] vs. 0.50 [IQR 0.43, 0.57], P < 0.001 for ILD). Similarly, models only trained on MK images performed better on MK images (median DSC = 0.62 [IQR: 0.53, 0.68] vs. 0.54 [IQR: 0.47, 0.63], P < 0.001 for COVID-19 and 0.69 [IQR: 0.61, 0.73] vs. 0.63 [IQR: 0.53, 0.70], P < 0.001 for ILD). Models trained on both kernels performed better or similarly than those trained on only one kernel. For COVID-19, median DSC was 0.67 (IQR: =0.59, 0.73) when applied on LK images and 0.67 (IQR: 0.60, 0.74) when applied on MK images (P < 0.001 for both). For ILD, median DSC was 0.69 (IQR: 0.63, 0.73) when applied on LK images (P = 0.006) and 0.68 (IQR: 0.62, 0.72) when applied on MK images (P > 0.99). CONCLUSION: Reconstruction kernels impact the performance of deep learning-based models for lung disease segmentation. Training on both LK and MK images improves the performance.",2021,10.1016/j.diii.2021.10.001,diagnosis,True
Deep Learning for Malignancy Risk Estimation of Pulmonary Nodules Detected at Low-Dose Screening CT,"Background Accurate estimation of the malignancy risk of pulmonary nodules at chest CT is crucial for optimizing management in lung cancer screening. Purpose To develop and validate a deep learning (DL) algorithm for malignancy risk estimation of pulmonary nodules detected at screening CT. Materials and Methods In this retrospective study, the DL algorithm was developed with 16 077 nodules (1249 malignant) collected -between 2002 and 2004 from the National Lung Screening Trial. External validation was performed in the following three -cohorts -collected between 2004 and 2010 from the Danish Lung Cancer Screening Trial: a full cohort containing all 883 nodules (65 -malignant) and two cancer-enriched cohorts with size matching (175 nodules, 59 malignant) and without size matching (177 -nodules, 59 malignant) of benign nodules selected at random. Algorithm performance was measured by using the area under the receiver operating characteristic curve (AUC) and compared with that of the Pan-Canadian Early Detection of Lung Cancer (PanCan) model in the full cohort and a group of 11 clinicians composed of four thoracic radiologists, five radiology residents, and two pulmonologists in the cancer-enriched cohorts. Results The DL algorithm significantly outperformed the PanCan model in the full cohort (AUC, 0.93 [95% CI: 0.89, 0.96] vs 0.90 [95% CI: 0.86, 0.93]; P = .046). The algorithm performed comparably to thoracic radiologists in cancer-enriched cohorts with both random benign nodules (AUC, 0.96 [95% CI: 0.93, 0.99] vs 0.90 [95% CI: 0.81, 0.98]; P = .11) and size-matched benign nodules (AUC, 0.86 [95% CI: 0.80, 0.91] vs 0.82 [95% CI: 0.74, 0.89]; P = .26). Conclusion The deep learning algorithm showed excellent performance, comparable to thoracic radiologists, for malignancy risk estimation of pulmonary nodules detected at screening CT. This algorithm has the potential to provide reliable and reproducible malignancy risk scores for clinicians, which may help optimize management in lung cancer screening. © RSNA, 2021 Online supplemental material is available for this article. See also the editorial by Tammemägi in this issue.",2021,10.1148/radiol.2021204433,diagnosis,True
Deep learning for predicting COVID-19 malignant progression,"As COVID-19 is highly infectious, many patients can simultaneously flood into hospitals for diagnosis and treatment, which has greatly challenged public medical systems. Treatment priority is often determined by the symptom severity based on first assessment. However, clinical observation suggests that some patients with mild symptoms may quickly deteriorate. Hence, it is crucial to identify patient early deterioration to optimize treatment strategy. To this end, we develop an early-warning system with deep learning techniques to predict COVID-19 malignant progression. Our method leverages CT scans and the clinical data of outpatients and achieves an AUC of 0.920 in the single-center study. We also propose a domain adaptation approach to improve the generalization of our model and achieve an average AUC of 0.874 in the multicenter study. Moreover, our model automatically identifies crucial indicators that contribute to the malignant progression, including Troponin, Brain natriuretic peptide, White cell count, Aspartate aminotransferase, Creatinine, and Hypersensitive C-reactive protein.",2021,10.1016/j.media.2021.102096,diagnosis,True
A semi-automatic approach for epicardial adipose tissue segmentation and quantification on cardiac CT scans,"Many studies have shown that epicardial fat is associated with a higher risk of heart diseases. Accurate epicardial adipose tissue quantification is still an open research issue. Considering that manual approaches are generally user-dependent and time-consuming, computer-assisted tools can considerably improve the result repeatability as well as reduce the time required for performing an accurate segmentation. Unfortunately, fully automatic strategies might not always identify the Region of Interest (ROI) correctly. Moreover, they could require user interaction for handling unexpected events. This paper proposes a semi-automatic method for Epicardial Fat Volume (EFV) segmentation and quantification. Unlike supervised Machine Learning approaches, the method does not require any initial training or modeling phase to set up the system. As a further key novelty, the method also yields a subdivision into quartiles of the adipose tissue density. Quartile-based analysis conveys information about fat densities distribution, enabling an in-depth study towards a possible correlation between fat amounts, fat distribution, and heart diseases. Experimental tests were performed on 50 Calcium Score (CaSc) series and 95 Coronary Computed Tomography Angiography (CorCTA) series. Area-based and distance-based metrics were used to evaluate the segmentation accuracy, by obtaining Dice Similarity Coefficient (DSC) = 93.74% and Mean Absolute Distance (MAD) = 2.18 for CaSc, as well as DSC = 92.48% and MAD = 2.87 for CorCTA. Moreover, the Pearson and Spearman coefficients were computed for quantifying the correlation between the ground-truth EFV and the corresponding automated measurement, by obtaining 0.9591 and 0.9490 for CaSc, and 0.9513 and 0.9319 for CorCTA, respectively. In conclusion, the proposed EFV quantification and analysis method represents a clinically useable tool assisting the cardiologist to gain insights into a specific clinical scenario and leading towards personalized diagnosis and therapy.",2019,10.1016/j.compbiomed.2019.103424,diagnosis,True
Artificial intelligence based automatic quantification of epicardial adipose tissue suitable for large scale population studies,"To develop a fully automatic model capable of reliably quantifying epicardial adipose tissue (EAT) volumes and attenuation in large scale population studies to investigate their relation to markers of cardiometabolic risk. Non-contrast cardiac CT images from the SCAPIS study were used to train and test a convolutional neural network based model to quantify EAT by: segmenting the pericardium, suppressing noise-induced artifacts in the heart chambers, and, if image sets were incomplete, imputing missing EAT volumes. The model achieved a mean Dice coefficient of 0.90 when tested against expert manual segmentations on 25 image sets. Tested on 1400 image sets, the model successfully segmented 99.4% of the cases. Automatic imputation of missing EAT volumes had an error of less than 3.1% with up to 20% of the slices in image sets missing. The most important predictors of EAT volumes were weight and waist, while EAT attenuation was predicted mainly by EAT volume. A model with excellent performance, capable of fully automatic handling of the most common challenges in large scale EAT quantification has been developed. In studies of the importance of EAT in disease development, the strong co-variation with anthropometric measures needs to be carefully considered.",2021,10.1038/s41598-021-03150-w,diagnosis,True
Relationship of epicardial fat volume from noncontrast CT with impaired myocardial flow reserve by positron emission tomography,"BACKGROUND: Impaired myocardial flow reserve (MFR) is a marker of coronary vascular dysfunction with prognostic significance. OBJECTIVES: We aimed to investigate the relationship between epicardial fat volume (EFV) measured from noncontrast CT and impaired MFR derived from rest-stress Rb-82 positron emission tomography (PET). METHODS: We retrospectively studied 85 consecutive patients without known coronary artery disease who underwent rest-stress Rb-82 myocardial PET/CT and were subsequently referred for invasive coronary angiography. EFV was computed from noncontrast CT by validated software and indexed to body surface area (EFVi, cm3/m2). Global stress and rest MFR were automatically derived from PET. Patient age, sex, cardiovascular risk factors, coronary calcium score (CCS), and EFVi were combined by boosted ensemble machine learning algorithm into a novel composite risk score, using 10-fold cross-validation, to predict impaired global MFR (MFR ≤2.0) by PET. RESULTS: Patients with impaired MFR (44 of 85; 52%) were older (71 vs. 65 years; P = .03) and had higher frequency of CCS (≥400; P = .02) with significantly higher EFVi (63.1 ± 20.4 vs. 51.3 ± 14.1 cm3/m2; P = .003). On multivariate logistic regression (with age, sex, number of risk factors, CCS, and EFVi), EFVi was the only independent predictor of impaired MFR (odds ratio, 7.39; P = .02). The machine learning composite risk score significantly improved risk reclassification of impaired MFR compared to CCS or EFVi alone (integrated discrimination improvement = 0.19; P = .007 and IDI = 0.22; P = .002, respectively). CONCLUSIONS: Increased EFVi and composite risk score combining EFVi and CCS significantly improve identification of impaired global MFR by PET.",2015,10.1016/j.jcct.2015.03.005,diagnosis,True
Adaptive Fruitfly Based Modified Region Growing Algorithm for Cardiac Fat Segmentation Using Optimal Neural Network,"Epicardial adipose tissue is a visceral fat that has remained an entity of concern for decades owing to its high correlation with coronary heart disease. It continues to stump medical practitioners on the pretext of its relevance with pericardial fat and its dependence on a numerous other parameters including ethnicity and physique of an individual. This calls for a fool-proof algorithm that promises accurate classification and segmentation, hence an immaculate prediction. CT is immensely popular and widely preferred for diagnosis. Implementation of an improvised algorithm in CT would be a natural necessity. This research work proposes a Fruitfly Algorithm based Modified region growing algorithm is applied to the acquired CT images to segment fat accurately. The proposed methodology promises image registration and classification in order to segment two cardiac fats namely epicardial, pericardial and mediastinal. The main contributions are (1) Fat feature extraction: Construction of GLCM features CT image (2) Development of GWO based optimal neural network for classification; (3) Modeling the fat segmentation using modified region growing algorithm with Fruitfly optimization. The entire experimentation has been implemented in MATLAB simulation environment and final result is expected to flaunt a definite distinction between cardiac mediastinal and epicardial fats. Parallely, the accuracy, sensitivity, specificity, FPR and FNR have been stated and contrasted methodically with the existing methodology. This venture aims at spurring the healthcare industry towards smarter computational techniques that multiplies efficiency manifold.",2019,10.1007/s10916-019-1227-3,diagnosis,True
Deep Learning for Quantification of Epicardial and Thoracic Adipose Tissue From Non-Contrast CT,"Epicardial adipose tissue (EAT) is a visceral fat deposit related to coronary artery disease. Fully automated quantification of EAT volume in clinical routine could be a timesaving and reliable tool for cardiovascular risk assessment. We propose a new fully automated deep learning framework for EAT and thoracic adipose tissue (TAT) quantification from non-contrast coronary artery calcium computed tomography (CT) scans. The first multi-task convolutional neural network (ConvNet) is used to determine heart limits and perform segmentation of heart and adipose tissues. The second ConvNet, combined with a statistical shape model, allows for pericardium detection. EAT and TAT segmentations are then obtained from outputs of both ConvNets. We evaluate the performance of the method on CT data sets from 250 asymptomatic individuals. Strong agreement between automatic and expert manual quantification is obtained for both EAT and TAT with median Dice score coefficients of 0.823 (inter-quartile range (IQR): 0.779-0.860) and 0.905 (IQR: 0.862-0.928), respectively; with excellent correlations of 0.924 and 0.945 for EAT and TAT volumes. Computations are performed in <6 s on a standard personal computer for one CT scan. Therefore, the proposed method represents a tool for rapid fully automated quantification of adipose tissue and may improve cardiovascular risk stratification in patients referred for routine CT calcium scans.",2018,10.1109/tmi.2018.2804799,prognosis,True
Deep learning for semi-automated unidirectional measurement of lung tumor size in CT,"BACKGROUND: Performing Response Evaluation Criteria in Solid Tumor (RECISTS) measurement is a non-trivial task requiring much expertise and time. A deep learning-based algorithm has the potential to assist with rapid and consistent lesion measurement. PURPOSE: The aim of this study is to develop and evaluate deep learning (DL) algorithm for semi-automated unidirectional CT measurement of lung lesions. METHODS: This retrospective study included 1617 lung CT images from 8 publicly open datasets. A convolutional neural network was trained using 1373 training and validation images annotated by two radiologists. Performance of the DL algorithm was evaluated 244 test images annotated by one radiologist. DL algorithm's measurement consistency with human radiologist was evaluated using Intraclass Correlation Coefficient (ICC) and Bland-Altman plotting. Bonferroni's method was used to analyze difference in their diagnostic behavior, attributed by tumor characteristics. Statistical significance was set at p < 0.05. RESULTS: The DL algorithm yielded ICC score of 0.959 with human radiologist. Bland-Altman plotting suggested 240 (98.4 %) measurements realized within the upper and lower limits of agreement (LOA). Some measurements outside the LOA revealed difference in clinical reasoning between DL algorithm and human radiologist. Overall, the algorithm marginally overestimated the size of lesion by 2.97 % compared to human radiologists. Further investigation indicated tumor characteristics may be associated with the DL algorithm's diagnostic behavior of over or underestimating the lesion size compared to human radiologist. CONCLUSIONS: The DL algorithm for unidirectional measurement of lung tumor size demonstrated excellent agreement with human radiologist.",2021,10.1186/s40644-021-00413-7,prognosis,True
Deep Learning for the Classification of Small (≤2 cm) Pulmonary Nodules on CT Imaging: A Preliminary Study,"RATIONALE AND OBJECTIVES: We aimed to present a deep learning-based malignancy prediction model (CT-lungNET) that is simpler and faster to use in the diagnosis of small (≤2 cm) pulmonary nodules on nonenhanced chest CT and to preliminarily evaluate its performance and usefulness for human reviewers. MATERIALS AND METHODS: A total of 173 whole nonenhanced chest CT images containing 208 pulmonary nodules (94 malignant and 11 benign nodules) ranging in size from 5 mm to 20 mm were collected. Pathologically confirmed nodules or nodules that remained unchanged for more than 1 year were included, and 30 benign and 30 malignant nodules were randomly assigned into the test set. We designed CT-lungNET to include three convolutional layers followed by two fully-connected layers and compared its diagnostic performance and processing time with those of AlexNET by using the area under the receiver operating curve (AUROC). An observer performance test was conducted involving eight human reviewers of four different groups (medical students, physicians, radiologic residents, and thoracic radiologists) at test 1 and test 2, referring to the CT-lungNET's malignancy prediction rate with pairwise comparison receiver operating curve analysis. RESULTS: CT-lungNET showed an improved AUROC (0.85; 95% confidence interval: 0.74-0.93), compared to that of the AlexNET (0.82; 95% confidence interval: 0.71-0.91). The processing speed per one image slice for CT-lungNET was about 10 times faster than that for AlexNET (0.90 vs. 8.79 seconds). During the observer performance test, the classification performance of nonradiologists was increased with the aid of CTlungNET, (mean AUC improvement: 0.13; range: 0.03-0.19) but not significantly so in the radiologists group (mean AUC improvement: 0.02; range: -0.02 to 0.07). CONCLUSION: CT-lungNET was able to provide better classification results with a significantly shorter amount of processing time as compared to AlexNET in the diagnosis of small pulmonary nodules on nonenhanced chest CT. In this preliminary observer performance test, CT-lungNET may have a role acting as a second reviewer for less experienced reviewers, resulting in enhanced performance in the diagnosis of early lung cancer.",2020,10.1016/j.acra.2019.05.018,diagnosis,True
Deep learning model for distinguishing novel coronavirus from other chest related infections in X-ray images,"Novel Coronavirus is deadly for humans and animals. The ease of its dispersion, coupled with its tremendous capability for ailment and death in infected people, makes it a risk to society. The chest X-ray is conventional but hard to interpret radiographic test for initial diagnosis of coronavirus from other related infections. It bears a considerable amount of information on physiological and anatomical features. To extract relevant information from it can occasionally become challenging even for a professional radiologist. In this regard, deep-learning models can help in swift, accurate and reliable outcomes. Existing datasets are small and suffer from the balance issue. In this paper, we prepare a relatively larger and well-balanced dataset as compared to the available datasets. Furthermore, we analyze deep learning models, namely, AlexNet, SqueezeNet, DenseNet201, MobileNetV2 and InceptionV3 with numerous variations such as training the models from scratch, fine-tuning without pre-trained weights, fine-tuning along with updating pre-trained weights of all layers, and fine-tuning with pre-trained weights along with applying augmentation. Our results show that fine-tuning with augmentation generates best results in pre-trained models. Finally, we have made architectural adjustments in MobileNetV2 and InceptionV3 models to learn more intricate features, which are then merged in our proposed ensemble model. The performance of our model is statistically analyzed against other models using four different performance metrics with paired two-sided t-test on 5 different splits of training and test sets of our dataset. We find that it is statistically better than its competing methods for the four metrics. Thus, the computer-aided classification based on the proposed model can assist radiologists in identifying coronavirus from other related infections in chest X-rays with higher accuracy. This can help in a reliable and speedy diagnosis, thereby saving valuable lives and mitigating the adverse impact on the socioeconomics of our community.",2021,10.1016/j.compbiomed.2021.104401,diagnosis,False
"Deep learning model for the automatic classification of COVID-19 pneumonia, non-COVID-19 pneumonia, and the healthy: a multi-center retrospective study","This retrospective study aimed to develop and validate a deep learning model for the classification of coronavirus disease-2019 (COVID-19) pneumonia, non-COVID-19 pneumonia, and the healthy using chest X-ray (CXR) images. One private and two public datasets of CXR images were included. The private dataset included CXR from six hospitals. A total of 14,258 and 11,253 CXR images were included in the 2 public datasets and 455 in the private dataset. A deep learning model based on EfficientNet with noisy student was constructed using the three datasets. The test set of 150 CXR images in the private dataset were evaluated by the deep learning model and six radiologists. Three-category classification accuracy and class-wise area under the curve (AUC) for each of the COVID-19 pneumonia, non-COVID-19 pneumonia, and healthy were calculated. Consensus of the six radiologists was used for calculating class-wise AUC. The three-category classification accuracy of our model was 0.8667, and those of the six radiologists ranged from 0.5667 to 0.7733. For our model and the consensus of the six radiologists, the class-wise AUC of the healthy, non-COVID-19 pneumonia, and COVID-19 pneumonia were 0.9912, 0.9492, and 0.9752 and 0.9656, 0.8654, and 0.8740, respectively. Difference of the class-wise AUC between our model and the consensus of the six radiologists was statistically significant for COVID-19 pneumonia (p value = 0.001334). Thus, an accurate model of deep learning for the three-category classification could be constructed; the diagnostic performance of our model was significantly better than that of the consensus interpretation by the six radiologists for COVID-19 pneumonia.",2022,10.1038/s41598-022-11990-3,diagnosis,False
Deep learning model to quantify left atrium volume on routine non-contrast chest CT and predict adverse outcomes,"BACKGROUND: Low-dose computed tomography (LDCT) are performed routinely for lung cancer screening. However, a large amount of nonpulmonary data from these scans remains unassessed. We aimed to validate a deep learning model to automatically segment and measure left atrial (LA) volumes from routine NCCT and evaluate prediction of cardiovascular outcomes. METHODS: We retrospectively evaluated 273 patients (median age 69 years, 55.5% male) who underwent LDCT for lung cancer screening. LA volumes were quantified by three expert cardiothoracic radiologists and a prototype AI algorithm. LA volumes were then indexed to the body surface area (BSA). Expert and AI LA volume index (LAVi) were compared and used to predict cardiovascular outcomes within five years. Logistic regression with appropriate univariate statistics were used for modelling outcomes. RESULTS: There was excellent correlation between AI and expert results with an LAV intraclass correlation of 0.950 (0.936-0.960). Bland-Altman plot demonstrated the AI underestimated LAVi by a mean 5.86 ​mL/m(2). AI-LAVi was associated with new-onset atrial fibrillation (AUC 0.86; OR 1.12, 95% CI 1.08-1.18, p ​< ​0.001), HF hospitalization (AUC 0.90; OR 1.07, 95% CI 1.04-1.13, p ​< ​0.001), and MACCE (AUC 0.68; OR 1.04, 95% CI 1.01-1.07, p ​= ​0.01). CONCLUSION: This novel deep learning algorithm for automated measurement of LA volume on lung cancer screening scans had excellent agreement with manual quantification. AI-LAVi is significantly associated with increased risk of new-onset atrial fibrillation, HF hospitalization, and major adverse cardiac and cerebrovascular events within 5 years.",2022,10.1016/j.jcct.2021.12.005,prognosis,True
Deep Learning on MRI Images for Diagnosis of Lung Cancer Spinal Bone Metastasis,"This paper aimed to explore the adoption of deep learning algorithms in lung cancer spinal bone metastasis diagnosis. Comprehensive analysis was carried out with the aid of AdaBoost algorithm and Chan-Vese (CV) algorithm. 87 patients with lung cancer spinal bone metastasis were taken as research subjects, and comprehensive evaluation was made in terms of preliminary classification of images, segmentation results, Dice index, and Jaccard coefficient. After the case of misjudgment on whether there was hot spot was excluded, the initial classification accuracy of the AdaBoost algorithm can reach 96.55%. True positive rate (TPR) was 2.3%, and false negative rate (FNR) was 1.15%. 45 MRI images with hot spots were utilized as test set to detect the segmentation accuracy of CV, maximum between-cluster variance method (OTSU), and region growing algorithm. The results showed that the Dice index and Jaccard coefficient of the CV algorithm were 0.8591 and 0.8002, respectively, which were considerably superior to OTSU (0.6125 and 0.5541) and region growing algorithm (0.7293 and 0.6598). In summary, the AdaBoost algorithm was adopted for image preliminary classification, and CV algorithm for image segmentation was ideal for the diagnosis of lung cancer spinal bone metastasis and it was worthy of clinical promotion.",2021,10.1155/2021/5294379,diagnosis,False
Deep learning predicts cardiovascular disease risks from lung cancer screening low dose computed tomography,"Cancer patients have a higher risk of cardiovascular disease (CVD) mortality than the general population. Low dose computed tomography (LDCT) for lung cancer screening offers an opportunity for simultaneous CVD risk estimation in at-risk patients. Our deep learning CVD risk prediction model, trained with 30,286 LDCTs from the National Lung Cancer Screening Trial, achieves an area under the curve (AUC) of 0.871 on a separate test set of 2,085 subjects and identifies patients with high CVD mortality risks (AUC of 0.768). We validate our model against ECG-gated cardiac CT based markers, including coronary artery calcification (CAC) score, CAD-RADS score, and MESA 10-year risk score from an independent dataset of 335 subjects. Our work shows that, in high-risk patients, deep learning can convert LDCT for lung cancer screening into a dual-screening quantitative tool for CVD risk estimation.",2021,10.1038/s41467-021-23235-4,prognosis,True
Deep learning predicts epidermal growth factor receptor mutation subtypes in lung adenocarcinoma,"PURPOSE: This study aimed to explore the predictive ability of deep learning (DL) for the common epidermal growth factor receptor (EGFR) mutation subtypes in patients with lung adenocarcinoma. METHODS: A total of 665 patients with lung adenocarcinoma (528/137) were recruited from two different institutions. In the training set, an 18-layer convolutional neural network (CNN) and fivefold cross-validation strategy were used to establish a CNN model. Subsequently, an independent external validation cohort from the other institution was used to evaluate the predictive efficacy of the CNN model. Grad-weighted class activation mapping (Grad-CAM) technology was used for the visual interpretation of the CNN model. In addition, this study also compared the prediction abilities of the radiomics and CNN models. Receiver operating characteristic (ROC) curves, accuracy and precision values, and recall and F1-score were used to evaluate the effectiveness of the CNN model and compare its performance with that of the radiomics model. RESULTS: In the validation set, the micro- and macroaverage values of the area under the ROC curve of the CNN model to identify the three EGFR subtypes were 0.78 and 0.79, respectively. All evaluation indicators of the CNN model were better than those of the radiomics model. CONCLUSIONS: Our study confirmed the potential of DL for predicting the EGFR mutation status in lung adenocarcinoma. The imaging phenotypes of the three mutation subtypes were found to be different, which can provide a basis for choosing more accurate and personalized treatment in patients with lung adenocarcinoma.",2021,10.1002/mp.15307,prognosis,True
Deep Learning Predicts Lung Cancer Treatment Response from Serial Medical Imaging,"PURPOSE: Tumors are continuously evolving biological systems, and medical imaging is uniquely positioned to monitor changes throughout treatment. Although qualitatively tracking lesions over space and time may be trivial, the development of clinically relevant, automated radiomics methods that incorporate serial imaging data is far more challenging. In this study, we evaluated deep learning networks for predicting clinical outcomes through analyzing time series CT images of patients with locally advanced non-small cell lung cancer (NSCLC).Experimental Design: Dataset A consists of 179 patients with stage III NSCLC treated with definitive chemoradiation, with pretreatment and posttreatment CT images at 1, 3, and 6 months follow-up (581 scans). Models were developed using transfer learning of convolutional neural networks (CNN) with recurrent neural networks (RNN), using single seed-point tumor localization. Pathologic response validation was performed on dataset B, comprising 89 patients with NSCLC treated with chemoradiation and surgery (178 scans). RESULTS: Deep learning models using time series scans were significantly predictive of survival and cancer-specific outcomes (progression, distant metastases, and local-regional recurrence). Model performance was enhanced with each additional follow-up scan into the CNN model (e.g., 2-year overall survival: AUC = 0.74, P < 0.05). The models stratified patients into low and high mortality risk groups, which were significantly associated with overall survival [HR = 6.16; 95% confidence interval (CI), 2.17-17.44; P < 0.001]. The model also significantly predicted pathologic response in dataset B (P = 0.016). CONCLUSIONS: We demonstrate that deep learning can integrate imaging scans at multiple timepoints to improve clinical outcome predictions. AI-based noninvasive radiomics biomarkers can have a significant impact in the clinic given their low cost and minimal requirements for human input.",2019,10.1158/1078-0432.Ccr-18-2495,treatment,True
Deep learning representations to support COVID-19 diagnosis on CT slices,"INTRODUCTION: The coronavirus disease 2019 (COVID-19) has become a significant public health problem worldwide. In this context, CT-scan automatic analysis has emerged as a COVID-19 complementary diagnosis tool allowing for radiological finding characterization, patient categorization, and disease follow-up. However, this analysis depends on the radiologist's expertise, which may result in subjective evaluations. OBJECTIVE: To explore deep learning representations, trained from thoracic CT-slices, to automatically distinguish COVID-19 disease from control samples. MATERIALS AND METHODS: Two datasets were used: SARS-CoV-2 CT Scan (Set-1) and FOSCAL clinic's dataset (Set-2). The deep representations took advantage of supervised learning models previously trained on the natural image domain, which were adjusted following a transfer learning scheme. The deep classification was carried out: (a) via an end-to-end deep learning approach and (b) via random forest and support vector machine classifiers by feeding the deep representation embedding vectors into these classifiers. RESULTS: The end-to-end classification achieved an average accuracy of 92.33% (89.70% precision) for Set-1 and 96.99% (96.62% precision) for Set-2. The deep feature embedding with a support vector machine achieved an average accuracy of 91.40% (95.77% precision) and 96.00% (94.74% precision) for Set-1 and Set-2, respectively. CONCLUSION: Deep representations have achieved outstanding performance in the identification of COVID-19 cases on CT scans demonstrating good characterization of the COVID-19 radiological patterns. These representations could potentially support the COVID-19 diagnosis in clinical settings.",2022,10.7705/biomedica.5927,diagnosis,True
Deep Learning to Assess Long-term Mortality From Chest Radiographs,"IMPORTANCE: Chest radiography is the most common diagnostic imaging test in medicine and may also provide information about longevity and prognosis. OBJECTIVE: To develop and test a convolutional neural network (CNN) (named CXR-risk) to predict long-term mortality, including noncancer death, from chest radiographs. DESIGN, SETTING, AND PARTICIPANTS: In this prognostic study, CXR-risk CNN development (n = 41 856) and testing (n = 10 464) used data from the screening radiography arm of the Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial (PLCO) (n = 52 320), a community cohort of asymptomatic nonsmokers and smokers (aged 55-74 years) enrolled at 10 US sites from November 8, 1993, through July 2, 2001. External testing used data from the screening radiography arm of the National Lung Screening Trial (NLST) (n = 5493), a community cohort of heavy smokers (aged 55-74 years) enrolled at 21 US sites from August 2002, through April 2004. Data analysis was performed from January 1, 2018, to May 23, 2019. EXPOSURE: Deep learning CXR-risk score (very low, low, moderate, high, and very high) based on CNN analysis of the enrollment radiograph. MAIN OUTCOMES AND MEASURES: All-cause mortality. Prognostic value was assessed in the context of radiologists' diagnostic findings (eg, lung nodule) and standard risk factors (eg, age, sex, and diabetes) and for cause-specific mortality. RESULTS: Among 10 464 PLCO participants (mean [SD] age, 62.4 [5.4] years; 5405 men [51.6%]; median follow-up, 12.2 years [interquartile range, 10.5-12.9 years]) and 5493 NLST test participants (mean [SD] age, 61.7 [5.0] years; 3037 men [55.3%]; median follow-up, 6.3 years [interquartile range, 6.0-6.7 years]), there was a graded association between CXR-risk score and mortality. The very high-risk group had mortality of 53.0% (PLCO) and 33.9% (NLST), which was higher compared with the very low-risk group (PLCO: unadjusted hazard ratio [HR], 18.3 [95% CI, 14.5-23.2]; NLST: unadjusted HR, 15.2 [95% CI, 9.2-25.3]; both P < .001). This association was robust to adjustment for radiologists' findings and risk factors (PLCO: adjusted HR [aHR], 4.8 [95% CI, 3.6-6.4]; NLST: aHR, 7.0 [95% CI, 4.0-12.1]; both P < .001). Comparable results were seen for lung cancer death (PLCO: aHR, 11.1 [95% CI, 4.4-27.8]; NLST: aHR, 8.4 [95% CI, 2.5-28.0]; both P ≤ .001) and for noncancer cardiovascular death (PLCO: aHR, 3.6 [95% CI, 2.1-6.2]; NLST: aHR, 47.8 [95% CI, 6.1-374.9]; both P < .001) and respiratory death (PLCO: aHR, 27.5 [95% CI, 7.7-97.8]; NLST: aHR, 31.9 [95% CI, 3.9-263.5]; both P ≤ .001). CONCLUSIONS AND RELEVANCE: In this study, the deep learning CXR-risk score stratified the risk of long-term mortality based on a single chest radiograph. Individuals at high risk of mortality may benefit from prevention, screening, and lifestyle interventions.",2019,10.1001/jamanetworkopen.2019.7416,prognosis,False
Deep learning to detect acute respiratory distress syndrome on chest radiographs: a retrospective study with external validation,"BACKGROUND: Acute respiratory distress syndrome (ARDS) is a common, but under-recognised, critical illness syndrome associated with high mortality. An important factor in its under-recognition is the variability in chest radiograph interpretation for ARDS. We sought to train a deep convolutional neural network (CNN) to detect ARDS findings on chest radiographs. METHODS: CNNs were pretrained on 595 506 radiographs from two centres to identify common chest findings (eg, opacity and effusion), and then trained on 8072 radiographs annotated for ARDS by multiple physicians using various transfer learning approaches. The best performing CNN was tested on chest radiographs in an internal and external cohort, including a subset reviewed by six physicians, including a chest radiologist and physicians trained in intensive care medicine. Chest radiograph data were acquired from four US hospitals. FINDINGS: In an internal test set of 1560 chest radiographs from 455 patients with acute hypoxaemic respiratory failure, a CNN could detect ARDS with an area under the receiver operator characteristics curve (AUROC) of 0·92 (95% CI 0·89-0·94). In the subgroup of 413 images reviewed by at least six physicians, its AUROC was 0·93 (95% CI 0·88-0·96), sensitivity 83·0% (95% CI 74·0-91·1), and specificity 88·3% (95% CI 83·1-92·8). Among images with zero of six ARDS annotations (n=155), the median CNN probability was 11%, with six (4%) assigned a probability above 50%. Among images with six of six ARDS annotations (n=27), the median CNN probability was 91%, with two (7%) assigned a probability below 50%. In an external cohort of 958 chest radiographs from 431 patients with sepsis, the AUROC was 0·88 (95% CI 0·85-0·91). When radiographs annotated as equivocal were excluded, the AUROC was 0·93 (0·92-0·95). INTERPRETATION: A CNN can be trained to achieve expert physician-level performance in ARDS detection on chest radiographs. Further research is needed to evaluate the use of these algorithms to support real-time identification of ARDS patients to ensure fidelity with evidence-based care or to support ongoing ARDS research. FUNDING: National Institutes of Health, Department of Defense, and Department of Veterans Affairs.",2021,10.1016/s2589-7500(21)00056-x,diagnosis,False
Deep Learning to Estimate Biological Age From Chest Radiographs,"OBJECTIVES: The goal of this study was to assess whether a deep learning estimate of age from a chest radiograph image (CXR-Age) can predict longevity beyond chronological age. BACKGROUND: Chronological age is an imperfect measure of longevity. Biological age, a measure of overall health, may improve personalized care. This paper proposes a new way to estimate biological age using a convolutional neural network that takes as input a CXR image and outputs a chest x-ray age (in years) as a measure of long-term mortality risk. METHODS: CXR-Age was developed using CXR from 116,035 individuals and validated in 2 held-out testing sets: 1) 75% of the CXR arm of PLCO (Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial) (N = 40,967); and 2) the CXR arm of NLST (National Lung Screening Trial) (N = 5,414). CXR-Age was compared to chronological age and a multivariable regression model of chronological age, risk factors, and radiograph findings to predict all-cause and cardiovascular mortality with a maximum 23 years and 13 years of follow-up, respectively. The primary outcome was observed mortality; results are provided for the testing datasets only. RESULTS: In the PLCO testing dataset, a 5-year increase in CXR-Age carried a higher risk of all-cause mortality than a 5-year increase in chronological age (CXR-Age hazard ratio [HR]: 2.26 [95% confidence interval (CI): 2.24 to 2.29] vs. chronological age HR: 1.77 [95% CI: 1.75 to 1.78]; p < 0.001). A similar pattern was found for cardiovascular mortality (CXR-Age cause-specific HR: 2.45 per 5 years [95% CI: 2.34 to 2.56] vs. chronological age HR: 1.82 per 5 years [95% CI: 1.74 to 1.90]). Similar results were seen for both outcomes in the NLST external testing dataset. Adding CXR-Age to the multivariable model resulted in significant improvements for predicting both outcomes in both testing datasets (p < 0.001 for all comparisons). CONCLUSIONS: Based on a CXR image, CXR-Age predicted long-term all-cause and cardiovascular mortality.",2021,10.1016/j.jcmg.2021.01.008,diagnosis,False
Deep Learning Using Chest Radiographs to Identify High-Risk Smokers for Lung Cancer Screening Computed Tomography: Development and Validation of a Prediction Model,"BACKGROUND: Lung cancer screening with chest computed tomography (CT) reduces lung cancer death. Centers for Medicare & Medicaid Services (CMS) eligibility criteria for lung cancer screening with CT require detailed smoking information and miss many incident lung cancers. An automated deep-learning approach based on chest radiograph images may identify more smokers at high risk for lung cancer who could benefit from screening with CT. OBJECTIVE: To develop and validate a convolutional neural network (CXR-LC) that predicts long-term incident lung cancer using data commonly available in the electronic medical record (EMR) (chest radiograph, age, sex, and whether currently smoking). DESIGN: Risk prediction study. SETTING: U.S. lung cancer screening trials. PARTICIPANTS: The CXR-LC model was developed in the PLCO (Prostate, Lung, Colorectal, and Ovarian) Cancer Screening Trial (n = 41 856). The final CXR-LC model was validated in additional PLCO smokers (n = 5615, 12-year follow-up) and NLST (National Lung Screening Trial) heavy smokers (n = 5493, 6-year follow-up). Results are reported for validation data sets only. MEASUREMENTS: Up to 12-year lung cancer incidence predicted by CXR-LC. RESULTS: The CXR-LC model had better discrimination (area under the receiver-operating characteristic curve [AUC]) for incident lung cancer than CMS eligibility (PLCO AUC, 0.755 vs. 0.634; P < 0.001). The CXR-LC model's performance was similar to that of PLCO(M2012), a state-of-the-art risk score with 11 inputs, in both the PLCO data set (CXR-LC AUC of 0.755 vs. PLCO(M2012) AUC of 0.751) and the NLST data set (0.659 vs. 0.650). When compared in equal-sized screening populations, CXR-LC was more sensitive than CMS eligibility in the PLCO data set (74.9% vs. 63.8%; P = 0.012) and missed 30.7% fewer incident lung cancers. On decision curve analysis, CXR-LC had higher net benefit than CMS eligibility and similar benefit to PLCO(M2012). LIMITATION: Validation in lung cancer screening trials and not a clinical setting. CONCLUSION: The CXR-LC model identified smokers at high risk for incident lung cancer, beyond CMS eligibility and using information commonly available in the EMR. PRIMARY FUNDING SOURCE: None.",2020,10.7326/m20-1868,prognosis,False
Deep learning with robustness to missing data: A novel approach to the detection of COVID-19,"In the context of the current global pandemic and the limitations of the RT-PCR test, we propose a novel deep learning architecture, DFCN (Denoising Fully Connected Network). Since medical facilities around the world differ enormously in what laboratory tests or chest imaging may be available, DFCN is designed to be robust to missing input data. An ablation study extensively evaluates the performance benefits of the DFCN as well as its robustness to missing inputs. Data from 1088 patients with confirmed RT-PCR results are obtained from two independent medical facilities. The data includes results from 27 laboratory tests and a chest x-ray scored by a deep learning model. Training and test datasets are taken from different medical facilities. Data is made publicly available. The performance of DFCN in predicting the RT-PCR result is compared with 3 related architectures as well as a Random Forest baseline. All models are trained with varying levels of masked input data to encourage robustness to missing inputs. Missing data is simulated at test time by masking inputs randomly. DFCN outperforms all other models with statistical significance using random subsets of input data with 2-27 available inputs. When all 28 inputs are available DFCN obtains an AUC of 0.924, higher than any other model. Furthermore, with clinically meaningful subsets of parameters consisting of just 6 and 7 inputs respectively, DFCN achieves higher AUCs than any other model, with values of 0.909 and 0.919.",2021,10.1371/journal.pone.0255301,diagnosis,False
Deep learning-based algorithm for lung cancer detection on chest radiographs using the segmentation method,"We developed and validated a deep learning (DL)-based model using the segmentation method and assessed its ability to detect lung cancer on chest radiographs. Chest radiographs for use as a training dataset and a test dataset were collected separately from January 2006 to June 2018 at our hospital. The training dataset was used to train and validate the DL-based model with five-fold cross-validation. The model sensitivity and mean false positive indications per image (mFPI) were assessed with the independent test dataset. The training dataset included 629 radiographs with 652 nodules/masses and the test dataset included 151 radiographs with 159 nodules/masses. The DL-based model had a sensitivity of 0.73 with 0.13 mFPI in the test dataset. Sensitivity was lower in lung cancers that overlapped with blind spots such as pulmonary apices, pulmonary hila, chest wall, heart, and sub-diaphragmatic space (0.50-0.64) compared with those in non-overlapped locations (0.87). The dice coefficient for the 159 malignant lesions was on average 0.52. The DL-based model was able to detect lung cancers on chest radiographs, with low mFPI.",2022,10.1038/s41598-021-04667-w,diagnosis,False
Deep Learning-based Artificial Intelligence Improves Accuracy of Error-prone Lung Nodules,"Introduction: Early detection of lung cancer is one way to improve outcomes. Improving the detection of nodules on chest CT scans is important. Previous artificial intelligence (AI) modules show rapid advantages, which improves the performance of detecting lung nodules in some datasets. However, they have a high false-positive (FP) rate. Its effectiveness in clinical practice has not yet been fully proven. We aimed to use AI assistance in CT scans to decrease FP. Materials and methods: CT images of 60 patients were obtained. Five senior doctors who were blinded to these cases participated in this study for the detection of lung nodules. Two doctors performed manual detection and labeling of lung nodules without AI assistance. Another three doctors used AI assistance to detect and label lung nodules before manual interpretation. The AI program is based on a deep learning framework. Results: In total, 266 nodules were identified. For doctors without AI assistance, the FP was 0.617-0.650/scan and the sensitivity was 59.2-67.0%. For doctors with AI assistance, the FP was 0.067 to 0.2/scan and the sensitivity was 59.2-77.3% This AI-assisted program significantly reduced FP. The error-prone characteristics of lung nodules were central locations, ground-glass appearances, and small sizes. The AI-assisted program improved the detection of error-prone nodules. Conclusions: Detection of lung nodules is important for lung cancer treatment. When facing a large number of CT scans, error-prone nodules are a great challenge for doctors. The AI-assisted program improved the performance of detecting lung nodules, especially for error-prone nodules.",2022,10.7150/ijms.69400,diagnosis,False
Deep Learning-Based Chest CT Image Features in Diagnosis of Lung Cancer,"This study was to evaluate the diagnostic value of deep learning-optimized chest CT in the patients with lung cancer. 90 patients who were diagnosed with lung cancer by surgery or puncture in hospital were selected as the research subjects. The Mask Region Convolutional Neural Network (Mask-RCNN) model was a typical end-to-end image segmentation model, and Dual Path Network (DPN) was used in nodule detection. The results showed that the accuracy of DPN algorithm model in detecting lung lesions in lung cancer patients was 88.74%, the accuracy of CT diagnosis of lung cancer was 88.37%, the sensitivity was 82.91%, and the specificity was 87.43%. Deep learning-based CT examination combined with serum tumor detection, factoring into Neurospecific enolase (N S E), cytokeratin 19 fragment (CYFRA21), Carcinoembryonic antigen (CEA), and squamous cell carcinoma (SCC) antigen, improved the accuracy to 97.94%, the sensitivity to 98.12%, and the specificity to 100%, all showing significant differences (P < 0.05). In conclusion, this study provides a scientific basis for improving the diagnostic efficiency of CT imaging in lung cancer and theoretical support for subsequent lung cancer diagnosis and treatment.",2022,10.1155/2022/4153211,diagnosis,True
Deep Learning-Based Computed Tomography Imaging to Diagnose the Lung Nodule and Treatment Effect of Radiofrequency Ablation,"This study aimed to detect and diagnose the lung nodules as early as possible to effectively treat them, thereby reducing the burden on the medical system and patients. A lung computed tomography (CT) image segmentation algorithm was constructed based on the deep learning convolutional neural network (CNN). The clinical data of 69 patients with lung nodules diagnosed by needle biopsy and pathological comprehensive diagnosis at hospital were collected for specific analysis. The CT image segmentation algorithm was used to distinguish the nature and volume of lung nodules and compared with other computer aided design (CAD) software (Philips ISP). 69 patients with lung nodules were treated by radiofrequency ablation (RFA). The results showed that the diagnostic sensitivity of the CT image segmentation algorithm based on the CNN was obviously higher than that of the Philips ISP for solid nodules <5 mm (63 cases vs. 33 cases) (P < 0.05); it was the same result for the subsolid nodule <5 mm (33 case vs. 5 cases) (P < 0.05) that was slightly higher for solid and subsolid nodules with a diameter of 5-10 mm (37 cases vs. 28 cases) (P < 0.05). In addition, the CNN algorithm can reach all detection for calcified nodules and pleural nodules (7 cases; 5 cases), and the diagnostic sensitivities were much better than those of Philips ISP (2 cases; 3 cases) (P < 0.05). Patients with pulmonary nodules treated by RFA were in good postoperative condition, with a half-year survival rate of 100% and a one-year survival rate of 72.4%. Therefore, it could be concluded that the CT image segmentation algorithm based on the CNN could effectively detect and diagnose the lung nodules early, and the RFA could effectively treat the lung nodules.",2021,10.1155/2021/6556266,diagnosis,True
Deep Learning-Based Computer-Aided Pneumothorax Detection Using Chest X-ray Images,"Pneumothorax is a thoracic disease leading to failure of the respiratory system, cardiac arrest, or in extreme cases, death. Chest X-ray (CXR) imaging is the primary diagnostic imaging technique for the diagnosis of pneumothorax. A computerized diagnosis system can detect pneumothorax in chest radiographic images, which provide substantial benefits in disease diagnosis. In the present work, a deep learning neural network model is proposed to detect the regions of pneumothoraces in the chest X-ray images. The model incorporates a Mask Regional Convolutional Neural Network (Mask RCNN) framework and transfer learning with ResNet101 as a backbone feature pyramid network (FPN). The proposed model was trained on a pneumothorax dataset prepared by the Society for Imaging Informatics in Medicine in association with American college of Radiology (SIIM-ACR). The present work compares the operation of the proposed MRCNN model based on ResNet101 as an FPN with the conventional model based on ResNet50 as an FPN. The proposed model had lower class loss, bounding box loss, and mask loss as compared to the conventional model based on ResNet50 as an FPN. Both models were simulated with a learning rate of 0.0004 and 0.0006 with 10 and 12 epochs, respectively.",2022,10.3390/s22062278,diagnosis,False
Deep learning-based detection system for multiclass lesions on chest radiographs: comparison with observer readings,"OBJECTIVE: To investigate the feasibility of a deep learning-based detection (DLD) system for multiclass lesions on chest radiograph, in comparison with observers. METHODS: A total of 15,809 chest radiographs were collected from two tertiary hospitals (7204 normal and 8605 abnormal with nodule/mass, interstitial opacity, pleural effusion, or pneumothorax). Except for the test set (100 normal and 100 abnormal (nodule/mass, 70; interstitial opacity, 10; pleural effusion, 10; pneumothorax, 10)), radiographs were used to develop a DLD system for detecting multiclass lesions. The diagnostic performance of the developed model and that of nine observers with varying experiences were evaluated and compared using area under the receiver operating characteristic curve (AUROC), on a per-image basis, and jackknife alternative free-response receiver operating characteristic figure of merit (FOM) on a per-lesion basis. The false-positive fraction was also calculated. RESULTS: Compared with the group-averaged observations, the DLD system demonstrated significantly higher performances on image-wise normal/abnormal classification and lesion-wise detection with pattern classification (AUROC, 0.985 vs. 0.958; p = 0.001; FOM, 0.962 vs. 0.886; p < 0.001). In lesion-wise detection, the DLD system outperformed all nine observers. In the subgroup analysis, the DLD system exhibited consistently better performance for both nodule/mass (FOM, 0.913 vs. 0.847; p < 0.001) and the other three abnormal classes (FOM, 0.995 vs. 0.843; p < 0.001). The false-positive fraction of all abnormalities was 0.11 for the DLD system and 0.19 for the observers. CONCLUSIONS: The DLD system showed the potential for detection of lesions and pattern classification on chest radiographs, performing normal/abnormal classifications and achieving high diagnostic performance. KEY POINTS: • The DLD system was feasible for detection with pattern classification of multiclass lesions on chest radiograph. • The DLD system had high performance of image-wise classification as normal or abnormal chest radiographs (AUROC, 0.985) and showed especially high specificity (99.0%). • In lesion-wise detection of multiclass lesions, the DLD system outperformed all 9 observers (FOM, 0.962 vs. 0.886; p < 0.001).",2020,10.1007/s00330-019-06532-x,diagnosis,False
Deep learning-based differentiation of invasive adenocarcinomas from preinvasive or minimally invasive lesions among pulmonary subsolid nodules,"OBJECTIVES: To evaluate a deep learning-based model using model-generated segmentation masks to differentiate invasive pulmonary adenocarcinoma (IPA) from preinvasive lesions or minimally invasive adenocarcinoma (MIA) on CT, making comparisons with radiologist-derived measurements of solid portion size. METHODS: Four hundred eleven subsolid nodules (SSNs) (120 preinvasive lesions or MIAs and 291 IPAs) in 333 patients who underwent surgery between June 2010 and August 2016 were retrospectively included to develop the model (370 SSNs in 293 patients for training and 41 SSNs in 40 patients for tuning). Ninety SSNs of 2 cm or smaller (45 preinvasive lesions or MIAs and 45 IPAs) resected in 2018 formed a validation set. Six radiologists measured the solid portion of each nodule. Performances of the model and radiologists were assessed using receiver operating characteristics curve analysis. RESULTS: The deep learning model differentiated IPA from preinvasive lesions or MIA with areas under the curve (AUCs) of 0.914, 0.956, and 0.833 for the training, tuning, and validation sets, respectively. The mean AUC of the radiologists was 0.835 in the validation set, without significant differences between radiologists and the model (p = 0.97). The sensitivity, specificity, and accuracy of the model were 71% (32/45), 87% (39/45), and 79% (71/90), respectively, whereas the corresponding values of the radiologists were 75.2% (203/270), 76.7% (207/270), and 75.9% (410/540) with a 5-mm threshold for the solid portion size. CONCLUSIONS: The performance of the model for differentiating IPA from preinvasive lesions or MIA was comparable to that of the radiologists' measurements of solid portion size. KEY POINTS: • A deep learning-based model differentiated IPA from preinvasive lesions or MIA with AUCs of 0.914 and 0.956 for the training and tuning sets, respectively. • In the validation set including subsolid nodules of 2 cm or smaller, the model showed an AUC of 0.833, being on par with the performance of the solid portion size measurements made by the radiologists (AUC, 0.835; p = 0.97). • SSNs with a solid portion measuring > 10 mm on CT showed a high probability of being IPA (positive predictive value, 93.5-100.0%).",2021,10.1007/s00330-020-07620-z,diagnosis,True
Deep Learning-Based Internal Target Volume (ITV) Prediction Using Cone-Beam CT Images in Lung Stereotactic Body Radiotherapy,"Purpose:This study aims to develop a deep learning (DL)-based (Mask R-CNN) method to predict the internal target volume (ITV) in cone beam computed tomography (CBCT) images for lung stereotactic body radiotherapy (SBRT) patients and to evaluate the prediction accuracy of the model using 4DCT as ground truth. Methods and Materials: This study enrolled 78 phantom cases and 156 patient cases who received SBRT treatment. We used a novel DL model (Mask R-CNN) to identify and delineate lung tumor ITV in CBCT images. The results of the DL-based method were compared quantitatively with the ground truth (4DCT) using 4 metrics, including Dice Similarity Coefficient (DSC), Relative Volume Index (RVI), 3D Motion Range (R(3D)), and Hausdorff Surface Distance (HD). Paired t-tests were used to determine the differences between the DL-based method and manual contouring. Results: The DSC value for 4DCT(MIP) versus CBCT is 0.86 ± 0.16 and for 4DCT(AVG) versus CBCT is 0.83 ± 0.18, indicating a high similarity of tumor delineation in CBCT and 4DCT. The mean Accuracy Precision (mAP), R(3D), RVI, and HD values for phantom evaluation are 0.94 ± 0.04, 1.37 ± 0.36, 0.79 ± 0.02, and 6.79 ± 0.68, respectively. For patient evaluation, the mAP, R(3D), RVI, and HD achieved averaged values of 0.74 ± 0.23, 2.39 ± 1.59, 1.27 ± 0.47, and 17.00 ± 19.89, respectively. These results showed a good correlation between predicted ITV and manually contoured ITV. The phantom p-value for RVI, R(3D), and HD are 0.75, 0.08, 0.86, and patient p-value are 0.53, 0.07, 0.28, respectively. These p-values for phantom and patient showed no significant difference between the predicted ITV and physician's manual contouring. Conclusion:The current improved method (Mask R-CNN) yielded a good similarity between predicted ITV in CBCT and the manual contouring in 4DCT, thus indicating great potential for using CBCT for patient ITV contouring.",2022,10.1177/15330338211073380,treatment,True
Deep learning-based multi-view fusion model for screening 2019 novel coronavirus pneumonia: A multicentre study,"PURPOSE: To develop a deep learning-based method to assist radiologists to fast and accurately identify patients with COVID-19 by CT images. METHODS: We retrospectively collected chest CT images of 495 patients from three hospitals in China. 495 datasets were randomly divided into 395 cases (80%, 294 of COVID-19, 101 of other pneumonia) of the training set, 50 cases (10%, 37 of COVID-19, 13 of other pneumonia) of the validation set and 50 cases (10%, 37 of COVID-19, 13 of other pneumonia) of the testing set. We trained a multi-view fusion model using deep learning network to screen patients with COVID-19 using CT images with the maximum lung regions in axial, coronal and sagittal views. The performance of the proposed model was evaluated by both the validation and testing sets. RESULTS: The multi-view deep learning fusion model achieved the area under the receiver-operating characteristics curve (AUC) of 0.732, accuracy of 0.700, sensitivity of 0.730 and specificity of 0.615 in validation set. In the testing set, we can achieve AUC, accuracy, sensitivity and specificity of 0.819, 0.760, 0.811 and 0.615 respectively. CONCLUSIONS: Based on deep learning method, the proposed diagnosis model trained on multi-view images of chest CT images showed great potential to improve the efficacy of diagnosis and mitigate the heavy workload of radiologists for the initial screening of COVID-19 pneumonia.",2020,10.1016/j.ejrad.2020.109041,diagnosis,True
Deep learning-based pulmonary nodule detection: Effect of slab thickness in maximum intensity projections at the nodule candidate detection stage,"BACKGROUND AND OBJECTIVE: To investigate the effect of the slab thickness in maximum intensity projections (MIPs) on the candidate detection performance of a deep learning-based computer-aided detection (DL-CAD) system for pulmonary nodule detection in CT scans. METHODS: The public LUNA16 dataset includes 888 CT scans with 1186 nodules annotated by four radiologists. From those scans, MIP images were reconstructed with slab thicknesses of 5 to 50 mm (at 5 mm intervals) and 3 to 13 mm (at 2 mm intervals). The architecture in the nodule candidate detection part of the DL-CAD system was trained separately using MIP images with various slab thicknesses. Based on ten-fold cross-validation, the sensitivity and the F(2) score were determined to evaluate the performance of using each slab thickness at the nodule candidate detection stage. The free-response receiver operating characteristic (FROC) curve was used to assess the performance of the whole DL-CAD system that took the results combined from 16 MIP slab thickness settings. RESULTS: At the nodule candidate detection stage, the combination of results from 16 MIP slab thickness settings showed a high sensitivity of 98.0% with 46 false positives (FPs) per scan. Regarding a single MIP slab thickness of 10 mm, the highest sensitivity of 90.0% with 8 FPs/scan was reached before false positive reduction. The sensitivity increased (82.8% to 90.0%) for slab thickness of 1 to 10 mm and decreased (88.7% to 76.6%) for slab thickness of 15-50 mm. The number of FPs was decreasing with increasing slab thickness, but was stable at 5 FPs/scan at a slab thickness of 30 mm or more. After false positive reduction, the DL-CAD system, utilizing 16 MIP slab thickness settings, had the sensitivity of 94.4% with 1 FP/scan. CONCLUSIONS: The utilization of multi-MIP images could improve the performance at the nodule candidate detection stage, even for the whole DL-CAD system. For a single slab thickness of 10 mm, the highest sensitivity for pulmonary nodule detection was reached at the nodule candidate detection stage, similar to the slab thickness usually applied by radiologists.",2020,10.1016/j.cmpb.2020.105620,diagnosis,True
Deep learning-based real-time volumetric imaging for lung stereotactic body radiation therapy: a proof of concept study,"Due to the inter- and intra- variation of respiratory motion, it is highly desired to provide real-time volumetric images during the treatment delivery of lung stereotactic body radiation therapy (SBRT) for accurate and active motion management. In this proof-of-concept study, we propose a novel generative adversarial network integrated with perceptual supervision to derive instantaneous volumetric images from a single 2D projection. Our proposed network, named TransNet, consists of three modules, i.e. encoding, transformation and decoding modules. Rather than only using image distance loss between the generated 3D images and the ground truth 3D CT images to supervise the network, perceptual loss in feature space is integrated into loss function to force the TransNet to yield accurate lung boundary. Adversarial supervision is also used to improve the realism of generated 3D images. We conducted a simulation study on 20 patient cases, who had received lung SBRT treatments in our institution and undergone 4D-CT simulation, and evaluated the efficacy and robustness of our method for four different projection angles, i.e. 0°, 30°, 60° and 90°. For each 3D CT image set of a breathing phase, we simulated its 2D projections at these angles. For each projection angle, a patient's 3D CT images of 9 phases and the corresponding 2D projection data were used to train our network for that specific patient, with the remaining phase used for testing. The mean absolute error of the 3D images obtained by our method are 99.3 ± 14.1 HU. The peak signal-to-noise ratio and structural similarity index metric within the tumor region of interest are 15.4 ± 2.5 dB and 0.839 ± 0.090, respectively. The center of mass distance between the manual tumor contours on the 3D images obtained by our method and the manual tumor contours on the corresponding 3D phase CT images are within 2.6 mm, with a mean value of 1.26 mm averaged over all the cases. Our method has also been validated in a simulated challenging scenario with increased respiratory motion amplitude and tumor shrinkage, and achieved acceptable results. Our experimental results demonstrate the feasibility and efficacy of our 2D-to-3D method for lung cancer patients, which provides a potential solution for in-treatment real-time on-board volumetric imaging for tumor tracking and dose delivery verification to ensure the effectiveness of lung SBRT treatment.",2020,10.1088/1361-6560/abc303,treatment,False
Deep learning-based segmentation of the lung in MR-images acquired by a stack-of-spirals trajectory at ultra-short echo-times,"BACKGROUND: Functional lung MRI techniques are usually associated with time-consuming post-processing, where manual lung segmentation represents the most cumbersome part. The aim of this study was to investigate whether deep learning-based segmentation of lung images which were scanned by a fast UTE sequence exploiting the stack-of-spirals trajectory can provide sufficiently good accuracy for the calculation of functional parameters. METHODS: In this study, lung images were acquired in 20 patients suffering from cystic fibrosis (CF) and 33 healthy volunteers, by a fast UTE sequence with a stack-of-spirals trajectory and a minimum echo-time of 0.05 ms. A convolutional neural network was then trained for semantic lung segmentation using 17,713 2D coronal slices, each paired with a label obtained from manual segmentation. Subsequently, the network was applied to 4920 independent 2D test images and results were compared to a manual segmentation using the Sørensen-Dice similarity coefficient (DSC) and the Hausdorff distance (HD). Obtained lung volumes and fractional ventilation values calculated from both segmentations were compared using Pearson's correlation coefficient and Bland Altman analysis. To investigate generalizability to patients outside the CF collective, in particular to those exhibiting larger consolidations inside the lung, the network was additionally applied to UTE images from four patients with pneumonia and one with lung cancer. RESULTS: The overall DSC for lung tissue was 0.967 ± 0.076 (mean ± standard deviation) and HD was 4.1 ± 4.4 mm. Lung volumes derived from manual and deep learning based segmentations as well as values for fractional ventilation exhibited a high overall correlation (Pearson's correlation coefficent = 0.99 and 1.00). For the additional cohort with unseen pathologies / consolidations, mean DSC was 0.930 ± 0.083, HD = 12.9 ± 16.2 mm and the mean difference in lung volume was 0.032 ± 0.048 L. CONCLUSIONS: Deep learning-based image segmentation in stack-of-spirals based lung MRI allows for accurate estimation of lung volumes and fractional ventilation values and promises to replace the time-consuming step of manual image segmentation in the future.",2021,10.1186/s12880-021-00608-1,prognosis,False
Deep Mining External Imperfect Data for Chest X-Ray Disease Screening,"Deep learning approaches have demonstrated remarkable progress in automatic Chest X-ray analysis. The data-driven feature of deep models requires training data to cover a large distribution. Therefore, it is substantial to integrate knowledge from multiple datasets, especially for medical images. However, learning a disease classification model with extra Chest X-ray (CXR) data is yet challenging. Recent researches have demonstrated that performance bottleneck exists in joint training on different CXR datasets, and few made efforts to address the obstacle. In this paper, we argue that incorporating an external CXR dataset leads to imperfect training data, which raises the challenges. Specifically, the imperfect data is in two folds: domain discrepancy, as the image appearances vary across datasets; and label discrepancy, as different datasets are partially labeled. To this end, we formulate the multi-label thoracic disease classification problem as weighted independent binary tasks according to the categories. For common categories shared across domains, we adopt task-specific adversarial training to alleviate the feature differences. For categories existing in a single dataset, we present uncertainty-aware temporal ensembling of model predictions to mine the information from the missing labels further. In this way, our framework simultaneously models and tackles the domain and label discrepancies, enabling superior knowledge mining ability. We conduct extensive experiments on three datasets with more than 360,000 Chest X-ray images. Our method outperforms other competing models and sets state-of-the-art performance on the official NIH test set with 0.8349 AUC, demonstrating its effectiveness of utilizing the external dataset to improve the internal classification.",2020,10.1109/tmi.2020.3000949,diagnosis,False
Deep Mining Generation of Lung Cancer Malignancy Models from Chest X-ray Images,"Lung cancer is the leading cause of cancer death and morbidity worldwide. Many studies have shown machine learning models to be effective in detecting lung nodules from chest X-ray images. However, these techniques have yet to be embraced by the medical community due to several practical, ethical, and regulatory constraints stemming from the ""black-box"" nature of deep learning models. Additionally, most lung nodules visible on chest X-rays are benign; therefore, the narrow task of computer vision-based lung nodule detection cannot be equated to automated lung cancer detection. Addressing both concerns, this study introduces a novel hybrid deep learning and decision tree-based computer vision model, which presents lung cancer malignancy predictions as interpretable decision trees. The deep learning component of this process is trained using a large publicly available dataset on pathological biomarkers associated with lung cancer. These models are then used to inference biomarker scores for chest X-ray images from two independent data sets, for which malignancy metadata is available. Next, multi-variate predictive models were mined by fitting shallow decision trees to the malignancy stratified datasets and interrogating a range of metrics to determine the best model. The best decision tree model achieved sensitivity and specificity of 86.7% and 80.0%, respectively, with a positive predictive value of 92.9%. Decision trees mined using this method may be considered as a starting point for refinement into clinically useful multi-variate lung cancer malignancy models for implementation as a workflow augmentation tool to improve the efficiency of human radiologists.",2021,10.3390/s21196655,diagnosis,False
Deep multi-instance transfer learning for pneumothorax classification in chest X-ray images,"PURPOSE: Pneumothorax is a life-threatening emergency that requires immediate treatment. Frontal-view chest X-ray images are typically used for pneumothorax detection in clinical practice. However, manual review of radiographs is time-consuming, labor-intensive, and highly dependent on the experience of radiologists, which may lead to misdiagnosis. Here, we aim to develop a reliable automatic classification method to assist radiologists in rapidly and accurately diagnosing pneumothorax in frontal chest radiographs. METHODS: A novel residual neural network (ResNet)-based two-stage deep-learning strategy is proposed for pneumothorax identification: local feature learning (LFL) followed by global multi-instance learning (GMIL). Most of the nonlesion regions in the images are removed for learning discriminative features. Two datasets are used for large-scale validation: a private dataset (27 955 frontal-view chest X-ray images) and a public dataset (the National Institutes of Health [NIH] ChestX-ray14; 112 120 frontal-view X-ray images). The model performance of the identification was evaluated using the accuracy, precision, recall, specificity, F1-score, receiver operating characteristic (ROC), and area under ROC curve (AUC). Fivefold cross-validation is conducted on the datasets, and then the mean and standard deviation of the above-mentioned metrics are calculated to assess the overall performance of the model. RESULTS: The experimental results demonstrate that the proposed learning strategy can achieve state-of-the-art performance on the NIH dataset with an accuracy, AUC, precision, recall, specificity, and F1-score of 94.4% ± 0.7%, 97.3% ± 0.5%, 94.2% ± 0.3%, 94.6% ± 1.5%, 94.2% ± 0.4%, and 94.4% ± 0.7%, respectively. CONCLUSIONS: The experimental results demonstrate that our proposed CAD system is an efficient assistive tool in the identification of pneumothorax.",2022,10.1002/mp.15328,diagnosis,False
Deep radiomics-based survival prediction in patients with chronic obstructive pulmonary disease,"Heterogeneous clinical manifestations and progression of chronic obstructive pulmonary disease (COPD) affect patient health risk assessment, stratification, and management. Pulmonary function tests are used to diagnose and classify the severity of COPD, but they cannot fully represent the type or range of pathophysiologic abnormalities of the disease. To evaluate whether deep radiomics from chest computed tomography (CT) images can predict mortality in patients with COPD, we designed a convolutional neural network (CNN) model for extracting representative features from CT images and then performed random survival forest to predict survival in COPD patients. We trained CNN-based binary classifier based on six-minute walk distance results (> 440 m or not) and extracted high-throughput image features (i.e., deep radiomics) directly from the last fully connected layer of it. The various sizes of fully connected layers and combinations of deep features were experimented using a discovery cohort with 344 patients from the Korean Obstructive Lung Disease cohort and an external validation cohort with 102 patients from Penang General Hospital in Malaysia. In the integrative analysis of discovery and external validation cohorts, with combining 256 deep features from the coronal slice of the vertebral body and two sagittal slices of the left/right lung, deep radiomics for survival prediction achieved concordance indices of 0.8008 (95% CI, 0.7642-0.8373) and 0.7156 (95% CI, 0.7024-0.7288), respectively. Deep radiomics from CT images could be used to predict mortality in COPD patients.",2021,10.1038/s41598-021-94535-4,prognosis,True
Deep Reinforcement Learning for Fractionated Radiotherapy in Non-Small Cell Lung Carcinoma,"Lung cancer is by far the leading cause of cancer death among both men and women. Radiation therapy is one of the main approaches to lung cancer treatment, and its planning is crucial for the therapy outcome. However, the current practice that uniformly delivers the dose does not take into account the patient-specific tumour features that may affect treatment success. Since radiation therapy is by its very nature a sequential procedure, Deep Reinforcement Learning (DRL) is a well-suited methodology to overcome this limitation. In this respect, in this work we present a DRL controller optimizing the daily dose fraction delivered to the patient on the basis of CT scans collected over time during the therapy, offering a personalized treatment not only for volume adaptation, as currently intended, but also for daily fractionation. Furthermore, this contribution introduces a virtual radiotherapy environment based on a set of ordinary differential equations modelling the tissue radiosensitivity by combining both the effect of the radiotherapy treatment and cell growth. Their parameters are estimated from CT scans routinely collected using the Particle Swarm Optimization algorithm. This permits the DRL to learn the optimal behaviour through an iterative trial and error process with the environment. We performed several experiments considering three rewards functions modelling treatment strategies with different tissue aggressiveness and two exploration strategies for the exploration-exploitation dilemma. The results show that our DRL approach can adapt to radiation therapy treatment, optimizing its behaviour according to the different reward functions and outperforming the current clinical practice.",2021,10.1016/j.artmed.2021.102137,treatment,True
Deep Residual Separable Convolutional Neural Network for lung tumor segmentation,"Lung cancer is one of the deadliest types of cancers. Computed Tomography (CT) is a widely used technique to detect tumors present inside the lungs. Delineation of such tumors is particularly essential for analysis and treatment purposes. With the advancement in hardware technologies, Machine Learning and Deep Learning methods are outperforming the traditional methods in the field of medical imaging. In order to delineate lung cancer tumors, we have proposed a deep learning-based methodology which includes a maximum intensity projection based pre-processing method, two novel deep learning networks and an ensemble strategy. The two proposed networks named Deep Residual Separable Convolutional Neural Network 1 and 2 (DRS-CNN1 and DRS-CNN2) achieved better performance over the state-of-the-art U-net network and other segmentation networks. For fair comparison, we have evaluated the performances of all networks on Medical Segmentation Decathlon (MSD) and StructSeg 2019 datasets. The DRS-CNN2 achieved a mean Dice Similarity Coefficient (DSC) of 0.649, mean 95 Hausdorff Distance (HD95) of 18.26, mean Sensitivity 0.737 and a mean Precision of 0.765 on independent test sets.",2022,10.1016/j.compbiomed.2021.105161,diagnosis,True
Deep segmentation networks predict survival of non-small cell lung cancer,"Non-small-cell lung cancer (NSCLC) represents approximately 80-85% of lung cancer diagnoses and is the leading cause of cancer-related death worldwide. Recent studies indicate that image-based radiomics features from positron emission tomography/computed tomography (PET/CT) images have predictive power for NSCLC outcomes. To this end, easily calculated functional features such as the maximum and the mean of standard uptake value (SUV) and total lesion glycolysis (TLG) are most commonly used for NSCLC prognostication, but their prognostic value remains controversial. Meanwhile, convolutional neural networks (CNN) are rapidly emerging as a new method for cancer image analysis, with significantly enhanced predictive power compared to hand-crafted radiomics features. Here we show that CNNs trained to perform the tumor segmentation task, with no other information than physician contours, identify a rich set of survival-related image features with remarkable prognostic value. In a retrospective study on pre-treatment PET-CT images of 96 NSCLC patients before stereotactic-body radiotherapy (SBRT), we found that the CNN segmentation algorithm (U-Net) trained for tumor segmentation in PET and CT images, contained features having strong correlation with 2- and 5-year overall and disease-specific survivals. The U-Net algorithm has not seen any other clinical information (e.g. survival, age, smoking history, etc.) than the images and the corresponding tumor contours provided by physicians. In addition, we observed the same trend by validating the U-Net features against an extramural data set provided by Stanford Cancer Institute. Furthermore, through visualization of the U-Net, we also found convincing evidence that the regions of metastasis and recurrence appear to match with the regions where the U-Net features identified patterns that predicted higher likelihoods of death. We anticipate our findings will be a starting point for more sophisticated non-intrusive patient specific cancer prognosis determination. For example, the deep learned PET/CT features can not only predict survival but also visualize high-risk regions within or adjacent to the primary tumor and hence potentially impact therapeutic outcomes by optimal selection of therapeutic strategy or first-line therapy adjustment.",2019,10.1038/s41598-019-53461-2,prognosis,True
Deep semantic lung segmentation for tracking potential pulmonary perfusion biomarkers in chronic obstructive pulmonary disease (COPD): The multi-ethnic study of atherosclerosis COPD study,"BACKGROUND: Chronic obstructive pulmonary disease (COPD) is associated with high morbidity and mortality. Identification of imaging biomarkers for phenotyping is necessary for future treatment and therapy monitoring. However, translation of visual analytic pipelines into clinics or their use in large-scale studies is significantly slowed by time-consuming postprocessing steps. PURPOSE: To implement an automated tool chain for regional quantification of pulmonary microvascular blood flow in order to reduce analysis time and user variability. STUDY TYPE: Prospective. POPULATION: In all, 90 MRI scans of 63 patients, of which 31 had a COPD with a mean Global Initiative for Chronic Obstructive Lung Disease status of 1.9 ± 0.64 (μ ± σ). FIELD STRENGTH/SEQUENCE: 1.5T dynamic gadolinium-enhanced MRI measurement using 4D dynamic contrast material-enhanced (DCE) time-resolved angiography acquired in a single breath-hold in inspiration. [Correction added on August 20, 2019, after first online publication: The field strength in the preceding sentence was corrected.] ASSESSMENT: We built a 3D convolutional neural network for semantic segmentation using 29 manually segmented perfusion maps. All five lobes of the lung are denoted, including the middle lobe. Evaluation was performed on 61 independent cases from two sites of the Multi-Ethnic Study of Arteriosclerosis (MESA)-COPD study. We publish our implementation of a model-free deconvolution filter according to Sourbron et al for 4D DCE MRI scans as open source. STATISTICAL TEST: Cross-validation 29/61 (# training / # testing), intraclass correlation coefficient (ICC), Spearman ρ, Pearson r, Sørensen-Dice coefficient, and overlap. RESULTS: Segmentations and derived clinical parameters were processed in ~90 seconds per case on a Xeon E5-2637v4 workstation with Tesla P40 GPUs. Clinical parameters and predicted segmentations exhibit high concordance with the ground truth regarding median perfusion for all lobes with an ICC of 0.99 and a Sørensen-Dice coefficient of 93.4 ± 2.8 (μ ± σ). DATA CONCLUSION: We present a robust end-to-end pipeline that allows for the extraction of perfusion-based biomarkers for all lung lobes in 4D DCE MRI scans by combining model-free deconvolution with deep learning. LEVEL OF EVIDENCE: 3 Technical Efficacy: Stage 2 J. Magn. Reson. Imaging 2020;51:571-579.",2020,10.1002/jmri.26853,prognosis,False
Deep Transfer Learning for COVID-19 Prediction: Case Study for Limited Data Problems,"OBJECTIVE: Automatic prediction of COVID-19 using deep convolution neural networks based pre-trained transfer models and Chest X-ray images. METHODS: This research employs the advantages of computer vision and medical image analysis to develop an automated model that has the clinical potential for early detection of the disease. Using Deep Learning models, the research aims at evaluating the effectiveness and accuracy of different convolutional neural networks models in the automatic diagnosis of COVID-19 from X-ray images as compared to diagnosis performed by experts in the medical community. RESULTS: Due to the fact that the dataset available for COVID-19 is still limited, the best model to use is the InceptionNetV3. Performance results show that the InceptionNetV3 model yielded the highest accuracy of 98.63% (with data augmentation) and 98.90% (without data augmentation) among the three models designed. However, as the dataset gets bigger, the Inception ResNetV2 and NASNetlarge will do a better job of classification. All the performed networks tend to over-fit when data augmentation is not used, this is due to the small amount of data used for training and validation. CONCLUSION: A deep transfer learning is proposed to detecting the COVID-19 automatically from chest X-ray by training it with X-ray images gotten from both COVID-19 patients and people with normal chest X-rays. The study is aimed at helping doctors in making decisions in their clinical practice due its high performance and effectiveness, the study also gives an insight to how transfer learning was used to automatically detect the COVID-19.",2021,10.2174/1573405616666201123120417,diagnosis,False
Deep transfer learning to quantify pleural effusion severity in chest X-rays,"PURPOSE: The detection of pleural effusion in chest radiography is crucial for doctors to make timely treatment decisions for patients with chronic obstructive pulmonary disease. We used the MIMIC-CXR database to develop a deep learning model to quantify pleural effusion severity in chest radiographs. METHODS: The Medical Information Mart for Intensive Care Chest X-ray (MIMIC-CXR) dataset was divided into patients 'with' or 'without' chronic obstructive pulmonary disease (COPD). The label of pleural effusion severity was obtained from the extracted COPD radiology reports and classified into four categories: no effusion, small effusion, moderate effusion, and large effusion. A total of 200 datasets were randomly sampled to manually check each item and determine whether the tags are correct. A professional doctor re-tagged these items as a verification cohort without knowing their previous tags. The learning models include eight common network structures including Resnet, DenseNet, and GoogleNET. Three data processing methods (no sampling, downsampling, and upsampling) and two loss algorithms (focal loss and cross-entropy loss) were used for unbalanced data. The Neural Network Intelligence tool was applied to train the model. Receiver operating characteristic curves, Area under the curve, and confusion matrix were employed to evaluate the model results. Grad-CAM was used for model interpretation. RESULTS: Among the 8533 patients, 15,620 chest X-rays with clearly marked pleural effusion severity were obtained (no effusion, 5685; small effusion, 4877; moderate effusion, 3657; and large effusion, 1401). The error rate of the manual check label was 6.5%, and the error rate of the doctor's relabeling was 11.0%. The highest accuracy rate of the optimized model was 73.07. The micro-average AUCs of the testing and validation cohorts was 0.89 and 0.90, respectively, and their macro-average AUCs were 0.86 and 0.89, respectively. The AUC of the distinguishing results of each class and the other three classes were 0.95 and 0.94, 0.76 and 0.83, 0.85 and 0.83, and 0.87 and 0.93. CONCLUSION: The deep transfer learning model can grade the severity of pleural effusion.",2022,10.1186/s12880-022-00827-0,diagnosis,False
"Deep-chest: Multi-classification deep learning model for diagnosing COVID-19, pneumonia, and lung cancer chest diseases","Corona Virus Disease (COVID-19) has been announced as a pandemic and is spreading rapidly throughout the world. Early detection of COVID-19 may protect many infected people. Unfortunately, COVID-19 can be mistakenly diagnosed as pneumonia or lung cancer, which with fast spread in the chest cells, can lead to patient death. The most commonly used diagnosis methods for these three diseases are chest X-ray and computed tomography (CT) images. In this paper, a multi-classification deep learning model for diagnosing COVID-19, pneumonia, and lung cancer from a combination of chest x-ray and CT images is proposed. This combination has been used because chest X-ray is less powerful in the early stages of the disease, while a CT scan of the chest is useful even before symptoms appear, and CT can precisely detect the abnormal features that are identified in images. In addition, using these two types of images will increase the dataset size, which will increase the classification accuracy. To the best of our knowledge, no other deep learning model choosing between these diseases is found in the literature. In the present work, the performance of four architectures are considered, namely: VGG19-CNN, ResNet152V2, ResNet152V2 + Gated Recurrent Unit (GRU), and ResNet152V2 + Bidirectional GRU (Bi-GRU). A comprehensive evaluation of different deep learning architectures is provided using public digital chest x-ray and CT datasets with four classes (i.e., Normal, COVID-19, Pneumonia, and Lung cancer). From the results of the experiments, it was found that the VGG19 +CNN model outperforms the three other proposed models. The VGG19+CNN model achieved 98.05% accuracy (ACC), 98.05% recall, 98.43% precision, 99.5% specificity (SPC), 99.3% negative predictive value (NPV), 98.24% F1 score, 97.7% Matthew's correlation coefficient (MCC), and 99.66% area under the curve (AUC) based on X-ray and CT images.",2021,10.1016/j.compbiomed.2021.104348,diagnosis,True
Deep-COVID: Predicting COVID-19 from chest X-ray images using deep transfer learning,"The COVID-19 pandemic is causing a major outbreak in more than 150 countries around the world, having a severe impact on the health and life of many people globally. One of the crucial step in fighting COVID-19 is the ability to detect the infected patients early enough, and put them under special care. Detecting this disease from radiography and radiology images is perhaps one of the fastest ways to diagnose the patients. Some of the early studies showed specific abnormalities in the chest radiograms of patients infected with COVID-19. Inspired by earlier works, we study the application of deep learning models to detect COVID-19 patients from their chest radiography images. We first prepare a dataset of 5000 Chest X-rays from the publicly available datasets. Images exhibiting COVID-19 disease presence were identified by board-certified radiologist. Transfer learning on a subset of 2000 radiograms was used to train four popular convolutional neural networks, including ResNet18, ResNet50, SqueezeNet, and DenseNet-121, to identify COVID-19 disease in the analyzed chest X-ray images. We evaluated these models on the remaining 3000 images, and most of these networks achieved a sensitivity rate of 98% ( ± 3%), while having a specificity rate of around 90%. Besides sensitivity and specificity rates, we also present the receiver operating characteristic (ROC) curve, precision-recall curve, average prediction, and confusion matrix of each model. We also used a technique to generate heatmaps of lung regions potentially infected by COVID-19 and show that the generated heatmaps contain most of the infected areas annotated by our board certified radiologist. While the achieved performance is very encouraging, further analysis is required on a larger set of COVID-19 images, to have a more reliable estimation of accuracy rates. The dataset, model implementations (in PyTorch), and evaluations, are all made publicly available for research community at https://github.com/shervinmin/DeepCovid.git.",2020,10.1016/j.media.2020.101794,diagnosis,False
Deep-learning algorithms for the interpretation of chest radiographs to aid in the triage of COVID-19 patients: A multicenter retrospective study,"The recent medical applications of deep-learning (DL) algorithms have demonstrated their clinical efficacy in improving speed and accuracy of image interpretation. If the DL algorithm achieves a performance equivalent to that achieved by physicians in chest radiography (CR) diagnoses with Coronavirus disease 2019 (COVID-19) pneumonia, the automatic interpretation of the CR with DL algorithms can significantly reduce the burden on clinicians and radiologists in sudden surges of suspected COVID-19 patients. The aim of this study was to evaluate the efficacy of the DL algorithm for detecting COVID-19 pneumonia on CR compared with formal radiology reports. This is a retrospective study of adult patients that were diagnosed as positive COVID-19 cases based on the reverse transcription polymerase chain reaction among all the patients who were admitted to five emergency departments and one community treatment center in Korea from February 18, 2020 to May 1, 2020. The CR images were evaluated with a publicly available DL algorithm. For reference, CR images without chest computed tomography (CT) scans classified as positive for COVID-19 pneumonia were used given that the radiologist identified ground-glass opacity, consolidation, or other infiltration in retrospectively reviewed CR images. Patients with evidence of pneumonia on chest CT scans were also classified as COVID-19 pneumonia positive outcomes. The overall sensitivity and specificity of the DL algorithm for detecting COVID-19 pneumonia on CR were 95.6%, and 88.7%, respectively. The area under the curve value of the DL algorithm for the detection of COVID-19 with pneumonia was 0.921. The DL algorithm demonstrated a satisfactory diagnostic performance comparable with that of formal radiology reports in the CR-based diagnosis of pneumonia in COVID-19 patients. The DL algorithm may offer fast and reliable examinations that can facilitate patient screening and isolation decisions, which can reduce the medical staff workload during COVID-19 pandemic situations.",2020,10.1371/journal.pone.0242759,diagnosis,False
DeepCOVID-XR: An Artificial Intelligence Algorithm to Detect COVID-19 on Chest Radiographs Trained and Tested on a Large U.S. Clinical Data Set,"Background There are characteristic findings of coronavirus disease 2019 (COVID-19) on chest images. An artificial intelligence (AI) algorithm to detect COVID-19 on chest radiographs might be useful for triage or infection control within a hospital setting, but prior reports have been limited by small data sets, poor data quality, or both. Purpose To present DeepCOVID-XR, a deep learning AI algorithm to detect COVID-19 on chest radiographs, that was trained and tested on a large clinical data set. Materials and Methods DeepCOVID-XR is an ensemble of convolutional neural networks developed to detect COVID-19 on frontal chest radiographs, with reverse-transcription polymerase chain reaction test results as the reference standard. The algorithm was trained and validated on 14 788 images (4253 positive for COVID-19) from sites across the Northwestern Memorial Health Care System from February 2020 to April 2020 and was then tested on 2214 images (1192 positive for COVID-19) from a single hold-out institution. Performance of the algorithm was compared with interpretations from five experienced thoracic radiologists on 300 random test images using the McNemar test for sensitivity and specificity and the DeLong test for the area under the receiver operating characteristic curve (AUC). Results A total of 5853 patients (mean age, 58 years ± 19 [standard deviation]; 3101 women) were evaluated across data sets. For the entire test set, accuracy of DeepCOVID-XR was 83%, with an AUC of 0.90. For 300 random test images (134 positive for COVID-19), accuracy of DeepCOVID-XR was 82%, compared with that of individual radiologists (range, 76%-81%) and the consensus of all five radiologists (81%). DeepCOVID-XR had a significantly higher sensitivity (71%) than one radiologist (60%, P < .001) and significantly higher specificity (92%) than two radiologists (75%, P < .001; 84%, P = .009). AUC of DeepCOVID-XR was 0.88 compared with the consensus AUC of 0.85 (P = .13 for comparison). With consensus interpretation as the reference standard, the AUC of DeepCOVID-XR was 0.95 (95% CI: 0.92, 0.98). Conclusion DeepCOVID-XR, an artificial intelligence algorithm, detected coronavirus disease 2019 on chest radiographs with a performance similar to that of experienced thoracic radiologists in consensus. © RSNA, 2020 Supplemental material is available for this article. See also the editorial by van Ginneken in this issue.",2021,10.1148/radiol.2020203511,diagnosis,False
"DenResCov-19: A deep transfer learning network for robust automatic classification of COVID-19, pneumonia, and tuberculosis from X-rays","The global pandemic of coronavirus disease 2019 (COVID-19) is continuing to have a significant effect on the well-being of the global population, thus increasing the demand for rapid testing, diagnosis, and treatment. As COVID-19 can cause severe pneumonia, early diagnosis is essential for correct treatment, as well as to reduce the stress on the healthcare system. Along with COVID-19, other etiologies of pneumonia and Tuberculosis (TB) constitute additional challenges to the medical system. Pneumonia (viral as well as bacterial) kills about 2 million infants every year and is consistently estimated as one of the most important factor of childhood mortality (according to the World Health Organization). Chest X-ray (CXR) and computed tomography (CT) scans are the primary imaging modalities for diagnosing respiratory diseases. Although CT scans are the gold standard, they are more expensive, time consuming, and are associated with a small but significant dose of radiation. Hence, CXR have become more widespread as a first line investigation. In this regard, the objective of this work is to develop a new deep transfer learning pipeline, named DenResCov-19, to diagnose patients with COVID-19, pneumonia, TB or healthy based on CXR images. The pipeline consists of the existing DenseNet-121 and the ResNet-50 networks. Since the DenseNet and ResNet have orthogonal performances in some instances, in the proposed model we have created an extra layer with convolutional neural network (CNN) blocks to join these two models together to establish superior performance as compared to the two individual networks. This strategy can be applied universally in cases where two competing networks are observed. We have tested the performance of our proposed network on two-class (pneumonia and healthy), three-class (COVID-19 positive, healthy, and pneumonia), as well as four-class (COVID-19 positive, healthy, TB, and pneumonia) classification problems. We have validated that our proposed network has been able to successfully classify these lung-diseases on our four datasets and this is one of our novel findings. In particular, the AUC-ROC are 99.60, 96.51, 93.70, 96.40% and the F1 values are 98.21, 87.29, 76.09, 83.17% on our Dataset X-Ray 1, 2, 3, and 4 (DXR1, DXR2, DXR3, DXR4), respectively.",2021,10.1016/j.compmedimag.2021.102008,diagnosis,False
DenseCapsNet: Detection of COVID-19 from X-ray images using a capsule neural network,"At present, the global pandemic as it relates to novel coronavirus pneumonia is still a very difficult situation. Due to the recent outbreak of novel coronavirus pneumonia, novel chest X-ray (CXR) images that can be used for deep learning analysis are very rare. To solve this problem, we propose a deep learning framework that integrates a convolutional neural network and a capsule network. DenseCapsNet, a new deep learning framework, is formed by the fusion of a dense convolutional network (DenseNet) and the capsule neural network (CapsNet), leveraging their respective advantages and reducing the dependence of convolutional neural networks on a large amount of data. Using 750 CXR images of lungs of healthy patients as well as those of patients with other pneumonia and novel coronavirus pneumonia, the method can obtain an accuracy of 90.7% and an F1 score of 90.9%, and the sensitivity for detecting COVID-19 can reach 96%. These results show that the deep fusion neural network DenseCapsNet has good performance in novel coronavirus pneumonia CXR radiography detection.",2021,10.1016/j.compbiomed.2021.104399,diagnosis,False
Densely connected attention network for diagnosing COVID-19 based on chest CT,"BACKGROUND: To fully enhance the feature extraction capabilities of deep learning models, so as to accurately diagnose coronavirus disease 2019 (COVID-19) based on chest CT images, a densely connected attention network (DenseANet) was constructed by utilizing the self-attention mechanism in deep learning. METHODS: During the construction of the DenseANet, we not only densely connected attention features within and between the feature extraction blocks with the same scale, but also densely connected attention features with different scales at the end of the deep model, thereby further enhancing the high-order features. In this way, as the depth of the deep model increases, the spatial attention features generated by different layers can be densely connected and gradually transferred to deeper layers. The DenseANet takes CT images of the lung fields segmented by an improved U-Net as inputs and outputs the probability of the patients suffering from COVID-19. RESULTS: Compared with exiting attention networks, DenseANet can maximize the utilization of self-attention features at different depths in the model. A five-fold cross-validation experiment was performed on a dataset containing 2993 CT scans of 2121 patients, and experiments showed that the DenseANet can effectively locate the lung lesions of patients infected with SARS-CoV-2, and distinguish COVID-19, common pneumonia and normal controls with an average of 96.06% Acc and 0.989 AUC. CONCLUSIONS: The DenseANet we proposed can generate strong attention features and achieve the best diagnosis results. In addition, the proposed method of densely connecting attention features can be easily extended to other advanced deep learning methods to improve their performance in related tasks.",2021,10.1016/j.compbiomed.2021.104857,diagnosis,True
Design of lung nodules segmentation and recognition algorithm based on deep learning,"BACKGROUND: Accurate segmentation and recognition algorithm of lung nodules has great important value of reference for early diagnosis of lung cancer. An algorithm is proposed for 3D CT sequence images in this paper based on 3D Res U-Net segmentation network and 3D ResNet50 classification network. The common convolutional layers in encoding and decoding paths of U-Net are replaced by residual units while the loss function is changed to Dice loss after using cross entropy loss to accelerate network convergence. Since the lung nodules are small and rich in 3D information, the ResNet50 is improved by replacing the 2D convolutional layers with 3D convolutional layers and reducing the sizes of some convolution kernels, 3D ResNet50 network is obtained for the diagnosis of benign and malignant lung nodules. RESULTS: 3D Res U-Net was trained and tested on 1044 CT subcases in the LIDC-IDRI database. The segmentation result shows that the Dice coefficient of 3D Res U-Net is above 0.8 for the segmentation of lung nodules larger than 10 mm in diameter. 3D ResNet50 was trained and tested on 2960 lung nodules in the LIDC-IDRI database. The classification result shows that the diagnostic accuracy of 3D ResNet50 is 87.3% and AUC is 0.907. CONCLUSION: The 3D Res U-Net module improves segmentation performance significantly with the comparison of 3D U-Net model based on residual learning mechanism. 3D Res U-Net can identify small nodules more effectively and improve its segmentation accuracy for large nodules. Compared with the original network, the classification performance of 3D ResNet50 is significantly improved, especially for small benign nodules.",2021,10.1186/s12859-021-04234-0,diagnosis,True
Detailed identification of epidermal growth factor receptor mutations in lung adenocarcinoma: Combining radiomics with machine learning,"PURPOSE: To investigate the use of radiomics in the in-depth identification of epidermal growth factor receptor (EGFR) mutation status in patients with lung adenocarcinoma. METHODS: Computed tomography images of 438 patients with lung adenocarcinoma were collected in two different institutions, and 496 radiomic features were extracted. In the training set, lasso logistic regression was used to establish radiomic signatures. Combining radiomic index and clinical features, five machine learning methods, and a tenfold cross-validation strategy were used to establish combined models for EGFR(+) vs EGFR(-) , and 19Del vs L858R, groups. The predictive power of the models was then evaluated using an independent external validation cohort. RESULTS: In the EGFR(+) vs EGFR(-) and 19Del vs L858R groups, radiomic signatures consisting of 12 and 7 radiomic features were established, respectively; the area under the curves (AUCs) of the lasso logistic regression model on the validation set was 0.76 and 0.71, respectively. After inclusion of the clinical features, the maximum AUC of combined models on the validation set was 0.79 and 0.74, respectively. Logistic regression analysis showed good performance in the two groups, with AUCs of 0.79 and 0.71 on the validation set. Additionally, the AUC of combined models in the EGFR(+) vs EGFR(-) group was higher than that of the 19Del vs L858R group. CONCLUSIONS: Our study shows the potential of radiomics to predict EGFR mutation status. There are imaging phenotypic differences between EGFR(+) and EGFR(-) , and between 19Del and L858R; these can be used to allow patients with lung adenocarcinoma to choose more appropriate and personalized treatment options.",2020,10.1002/mp.14238,prognosis,True
Detecting COVID-19 in Chest X-Ray Images via MCFF-Net,"COVID-19 is a respiratory disease caused by severe acute respiratory syndrome coronavirus (SARS-CoV-2). Due to the rapid spread of COVID-19 around the world, the number of COVID-19 cases continues to increase, and lots of countries are facing tremendous pressure on both public and medical resources. Although RT-PCR is the most widely used detection technology with COVID-19 detection, it still has some limitations, such as high cost, being time-consuming, and having low sensitivity. According to the characteristics of chest X-ray (CXR) images, we design the Parallel Channel Attention Feature Fusion Module (PCAF), as well as a new structure of convolutional neural network MCFF-Net proposed based on PCAF. In order to improve the recognition efficiency, the network adopts 3 classifiers: 1-FC, GAP-FC, and Conv1-GAP. The experimental results show that the overall accuracy of MCFF-Net66-Conv1-GAP model is 94.66% for 4-class classification. Simultaneously, the classification accuracy, precision, sensitivity, specificity, and F1-score of COVID-19 are 100%. MCFF-Net may not only assist clinicians in making appropriate decisions for COVID-19 diagnosis but also mitigate the lack of testing kits.",2021,10.1155/2021/3604900,diagnosis,False
Detecting Pneumonia using Convolutions and Dynamic Capsule Routing for Chest X-ray Images,"An entity's existence in an image can be depicted by the activity instantiation vector from a group of neurons (called capsule). Recently, multi-layered capsules, called CapsNet, have proven to be state-of-the-art for image classification tasks. This research utilizes the prowess of this algorithm to detect pneumonia from chest X-ray (CXR) images. Here, an entity in the CXR image can help determine if the patient (whose CXR is used) is suffering from pneumonia or not. A simple model of capsules (also known as Simple CapsNet) has provided results comparable to best Deep Learning models that had been used earlier. Subsequently, a combination of convolutions and capsules is used to obtain two models that outperform all models previously proposed. These models-Integration of convolutions with capsules (ICC) and Ensemble of convolutions with capsules (ECC)-detect pneumonia with a test accuracy of 95.33% and 95.90%, respectively. The latter model is studied in detail to obtain a variant called EnCC, where n = 3, 4, 8, 16. Here, the E4CC model works optimally and gives test accuracy of 96.36%. All these models had been trained, validated, and tested on 5857 images from Mendeley.",2020,10.3390/s20041068,diagnosis,False
Detecting Racial/Ethnic Health Disparities Using Deep Learning From Frontal Chest Radiography,"PURPOSE: The aim of this study was to assess racial/ethnic and socioeconomic disparities in the difference between atherosclerotic vascular disease prevalence measured by a multitask convolutional neural network (CNN) deep learning model using frontal chest radiographs (CXRs) and the prevalence reflected by administrative hierarchical condition category codes in two cohorts of patients with coronavirus disease 2019 (COVID-19). METHODS: A CNN model, previously published, was trained to predict atherosclerotic disease from ambulatory frontal CXRs. The model was then validated on two cohorts of patients with COVID-19: 814 ambulatory patients from a suburban location (presenting from March 14, 2020, to October 24, 2020, the internal ambulatory cohort) and 485 hospitalized patients from an inner-city location (hospitalized from March 14, 2020, to August 12, 2020, the external hospitalized cohort). The CNN model predictions were validated against electronic health record administrative codes in both cohorts and assessed using the area under the receiver operating characteristic curve (AUC). The CXRs from the ambulatory cohort were also reviewed by two board-certified radiologists and compared with the CNN-predicted values for the same cohort to produce a receiver operating characteristic curve and the AUC. The atherosclerosis diagnosis discrepancy, Δ(vasc), referring to the difference between the predicted value and presence or absence of the vascular disease HCC categorical code, was calculated. Linear regression was performed to determine the association of Δ(vasc) with the covariates of age, sex, race/ethnicity, language preference, and social deprivation index. Logistic regression was used to look for an association between the presence of any hierarchical condition category codes with Δ(vasc) and other covariates. RESULTS: The CNN prediction for vascular disease from frontal CXRs in the ambulatory cohort had an AUC of 0.85 (95% confidence interval, 0.82-0.89) and in the hospitalized cohort had an AUC of 0.69 (95% confidence interval, 0.64-0.75) against the electronic health record data. In the ambulatory cohort, the consensus radiologists' reading had an AUC of 0.89 (95% confidence interval, 0.86-0.92) relative to the CNN. Multivariate linear regression of Δ(vasc) in the ambulatory cohort demonstrated significant negative associations with non-English-language preference (β = -0.083, P < .05) and Black or Hispanic race/ethnicity (β = -0.048, P < .05) and positive associations with age (β = 0.005, P < .001) and sex (β = 0.044, P < .05). For the hospitalized cohort, age was also significant (β = 0.003, P < .01), as was social deprivation index (β = 0.002, P < .05). The Δ(vasc) variable (odds ratio [OR], 0.34), Black or Hispanic race/ethnicity (OR, 1.58), non-English-language preference (OR, 1.74), and site (OR, 0.22) were independent predictors of having one or more hierarchical condition category codes (P < .01 for all) in the combined patient cohort. CONCLUSIONS: A CNN model was predictive of aortic atherosclerosis in two cohorts (one ambulatory and one hospitalized) with COVID-19. The discrepancy between the CNN model and the administrative code, Δ(vasc), was associated with language preference in the ambulatory cohort; in the hospitalized cohort, this discrepancy was associated with social deprivation index. The absence of administrative code(s) was associated with Δ(vasc) in the combined cohorts, suggesting that Δ(vasc) is an independent predictor of health disparities. This may suggest that biomarkers extracted from routine imaging studies and compared with electronic health record data could play a role in enhancing value-based health care for traditionally underserved or disadvantaged patients for whom barriers to care exist.",2022,10.1016/j.jacr.2021.09.010,prognosis,False
Detection and analysis of COVID-19 in medical images using deep learning techniques,"The main purpose of this work is to investigate and compare several deep learning enhanced techniques applied to X-ray and CT-scan medical images for the detection of COVID-19. In this paper, we used four powerful pre-trained CNN models, VGG16, DenseNet121, ResNet50,and ResNet152, for the COVID-19 CT-scan binary classification task. The proposed Fast.AI ResNet framework was designed to find out the best architecture, pre-processing, and training parameters for the models largely automatically. The accuracy and F1-score were both above 96% in the diagnosis of COVID-19 using CT-scan images. In addition, we applied transfer learning techniques to overcome the insufficient data and to improve the training time. The binary and multi-class classification of X-ray images tasks were performed by utilizing enhanced VGG16 deep transfer learning architecture. High accuracy of 99% was achieved by enhanced VGG16 in the detection of X-ray images from COVID-19 and pneumonia. The accuracy and validity of the algorithms were assessed on X-ray and CT-scan well-known public datasets. The proposed methods have better results for COVID-19 diagnosis than other related in literature. In our opinion, our work can help virologists and radiologists to make a better and faster diagnosis in the struggle against the outbreak of COVID-19.",2021,10.1038/s41598-021-99015-3,diagnosis,False
Detection Methods of COVID-19,"Since being first detected in China, coronavirus disease 2019 (COVID-19) has spread rapidly across the world, triggering a global pandemic with no viable cure in sight. As a result, national responses have focused on the effective minimization of the spread. Border control measures and travel restrictions have been implemented in a number of countries to limit the import and export of the virus. The detection of COVID-19 is a key task for physicians. The erroneous results of early laboratory tests and their delays led researchers to focus on different options. Information obtained from computed tomography (CT) and radiological images is important for clinical diagnosis. Therefore, it is worth developing a rapid method of detection of viral diseases through the analysis of radiographic images. We propose a novel method of detection of COVID-19. The purpose is to provide clinical decision support to healthcare workers and researchers. The article is to support researchers working on early detection of COVID-19 as well as similar viral diseases.",2020,10.1177/2472630320962002,diagnosis,False
Detection of coronavirus disease from X-ray images using deep learning and transfer learning algorithms,"OBJECTIVE: This study aims to employ the advantages of computer vision and medical image analysis to develop an automated model that has the clinical potential for early detection of novel coronavirus (COVID-19) infected disease. METHOD: This study applied transfer learning method to develop deep learning models for detecting COVID-19 disease. Three existing state-of-the-art deep learning models namely, Inception ResNetV2, InceptionNetV3 and NASNetLarge, were selected and fine-tuned to automatically detect and diagnose COVID-19 disease using chest X-ray images. A dataset involving 850 images with the confirmed COVID-19 disease, 500 images of community-acquired (non-COVID-19) pneumonia cases and 915 normal chest X-ray images was used in this study. RESULTS: Among the three models, InceptionNetV3 yielded the best performance with accuracy levels of 98.63% and 99.02% with and without using data augmentation in model training, respectively. All the performed networks tend to overfitting (with high training accuracy) when data augmentation is not used, this is due to the limited amount of image data used for training and validation. CONCLUSION: This study demonstrated that a deep transfer learning is feasible to detect COVID-19 disease automatically from chest X-ray by training the learning model with chest X-ray images mixed with COVID-19 patients, other pneumonia affected patients and people with healthy lungs, which may help doctors more effectively make their clinical decisions. The study also gives an insight to how transfer learning was used to automatically detect the COVID-19 disease. In future studies, as the amount of available dataset increases, different convolution neutral network models could be designed to achieve the goal more efficiently.",2020,10.3233/xst-200720,diagnosis,False
Detection of COVID-19 from Chest CT Images Using CNN with MLP Hybrid Model,"COVID-19 when left undetected can lead to a hazardous infection spread, leading to an unfortunate loss of life. It's of utmost importance to diagnose COVID-19 in Infected patients at the earliest, to avoid further complications. RT-PCR, the gold standard method is routinely used for the diagnosis of COVID-19 infection. Yet, this method comes along with few limitations such as its time-consuming nature, a scarcity of trained manpower, sophisticated laboratory equipment and the possibility of false positive and negative results. Physicians and global health care centers use CT scan as an alternate for the diagnosis of COVID-19. But this process of detection too, might demand more manual work, effort and time. Thus, automating the detection of COVID-19 using an intelligent system has been a recent research topic, in the view of pandemic. This will also help in saving the physician's time for carrying out further treatment. In this paper, a hybrid learning model has been proposed to identify the COVID-19 infection using CT scan images. The Convolutional Neural Network (CNN) was used for feature extraction and Multilayer Perceptron was used for classification. This hybrid learning model's results were also compared with traditional CNN and MLP models in terms of Accuracy, F1-Score, Precision and Recall. This Hybrid CNN-MLP model showed an Accuracy of 94.89% when compared with CNN and MLP giving 86.95% and 80.77% respectively.",2021,10.3233/shti210617,diagnosis,False
Detection of COVID-19 from Chest X-Ray Images Using Convolutional Neural Networks,"The detection of severe acute respiratory syndrome coronavirus 2 (SARS CoV-2), which is responsible for coronavirus disease 2019 (COVID-19), using chest X-ray images has life-saving importance for both patients and doctors. In addition, in countries that are unable to purchase laboratory kits for testing, this becomes even more vital. In this study, we aimed to present the use of deep learning for the high-accuracy detection of COVID-19 using chest X-ray images. Publicly available X-ray images (1583 healthy, 4292 pneumonia, and 225 confirmed COVID-19) were used in the experiments, which involved the training of deep learning and machine learning classifiers. Thirty-eight experiments were performed using convolutional neural networks, 10 experiments were performed using five machine learning models, and 14 experiments were performed using the state-of-the-art pre-trained networks for transfer learning. Images and statistical data were considered separately in the experiments to evaluate the performances of models, and eightfold cross-validation was used. A mean sensitivity of 93.84%, mean specificity of 99.18%, mean accuracy of 98.50%, and mean receiver operating characteristics-area under the curve scores of 96.51% are achieved. A convolutional neural network without pre-processing and with minimized layers is capable of detecting COVID-19 in a limited number of, and in imbalanced, chest X-ray images.",2020,10.1177/2472630320958376,diagnosis,False
Detection of COVID-19 from Chest X-ray Images Using Deep Convolutional Neural Networks,"The COVID-19 global pandemic has wreaked havoc on every aspect of our lives. More specifically, healthcare systems were greatly stretched to their limits and beyond. Advances in artificial intelligence have enabled the implementation of sophisticated applications that can meet clinical accuracy requirements. In this study, customized and pre-trained deep learning models based on convolutional neural networks were used to detect pneumonia caused by COVID-19 respiratory complications. Chest X-ray images from 368 confirmed COVID-19 patients were collected locally. In addition, data from three publicly available datasets were used. The performance was evaluated in four ways. First, the public dataset was used for training and testing. Second, data from the local and public sources were combined and used to train and test the models. Third, the public dataset was used to train the model and the local data were used for testing only. This approach adds greater credibility to the detection models and tests their ability to generalize to new data without overfitting the model to specific samples. Fourth, the combined data were used for training and the local dataset was used for testing. The results show a high detection accuracy of 98.7% with the combined dataset, and most models handled new data with an insignificant drop in accuracy.",2021,10.3390/s21175940,diagnosis,False
Detection of COVID-19 from CT Lung Scans Using Transfer Learning,"This paper aims to investigate the use of transfer learning architectures in the detection of COVID-19 from CT lung scans. The study evaluates the performances of various transfer learning architectures, as well as the effects of the standard Histogram Equalization and Contrast Limited Adaptive Histogram Equalization. The findings of this study suggest that transfer learning-based frameworks are an alternative to the contemporary methods used to detect the presence of the virus in patients. The highest performing model, the VGG-19 implemented with the Contrast Limited Adaptive Histogram Equalization, on a SARS-CoV-2 dataset, achieved an accuracy and recall of 95.75% and 97.13%, respectively.",2021,10.1155/2021/5527923,diagnosis,True
Detection of COVID-19 in Chest X-ray Images: A Big Data Enabled Deep Learning Approach,"Coronavirus disease (COVID-19) spreads from one person to another rapidly. A recently discovered coronavirus causes it. COVID-19 has proven to be challenging to detect and cure at an early stage all over the world. Patients showing symptoms of COVID-19 are resulting in hospitals becoming overcrowded, which is becoming a significant challenge. Deep learning's contribution to big data medical research has been enormously beneficial, offering new avenues and possibilities for illness diagnosis techniques. To counteract the COVID-19 outbreak, researchers must create a classifier distinguishing between positive and negative corona-positive X-ray pictures. In this paper, the Apache Spark system has been utilized as an extensive data framework and applied a Deep Transfer Learning (DTL) method using Convolutional Neural Network (CNN) three architectures -InceptionV3, ResNet50, and VGG19-on COVID-19 chest X-ray images. The three models are evaluated in two classes, COVID-19 and normal X-ray images, with 100 percent accuracy. But in COVID/Normal/pneumonia, detection accuracy was 97 percent for the inceptionV3 model, 98.55 percent for the ResNet50 Model, and 98.55 percent for the VGG19 model, respectively.",2021,10.3390/ijerph181910147,diagnosis,False
Detection of COVID-19 Using Deep Learning Algorithms on Chest Radiographs,"PURPOSE: To evaluate the performance of a deep learning (DL) algorithm for the detection of COVID-19 on chest radiographs (CXR). MATERIALS AND METHODS: In this retrospective study, a DL model was trained on 112,120 CXR images with 14 labeled classifiers (ChestX-ray14) and fine-tuned using initial CXR on hospital admission of 509 patients, who had undergone COVID-19 reverse transcriptase-polymerase chain reaction (RT-PCR). The test set consisted of a CXR on presentation of 248 individuals suspected of COVID-19 pneumonia between February 16 and March 3, 2020 from 4 centers (72 RT-PCR positives and 176 RT-PCR negatives). The CXR were independently reviewed by 3 radiologists and using the DL algorithm. Diagnostic performance was compared with radiologists' performance and was assessed by area under the receiver operating characteristics (AUC). RESULTS: The median age of the subjects in the test set was 61 (interquartile range: 39 to 79) years (51% male). The DL algorithm achieved an AUC of 0.81, sensitivity of 0.85, and specificity of 0.72 in detecting COVID-19 using RT-PCR as the reference standard. On subgroup analyses, the model achieved an AUC of 0.79, sensitivity of 0.80, and specificity of 0.74 in detecting COVID-19 in patients presented with fever or respiratory systems and an AUC of 0.87, sensitivity of 0.85, and specificity of 0.81 in distinguishing COVID-19 from other forms of pneumonia. The algorithm significantly outperforms human readers (P<0.001 using DeLong test) with higher sensitivity (P=0.01 using McNemar test). CONCLUSIONS: A DL algorithm (COV19NET) for the detection of COVID-19 on chest radiographs can potentially be an effective tool in triaging patients, particularly in resource-stretched health-care systems.",2020,10.1097/rti.0000000000000559,diagnosis,False
Detection of COVID-19 Using Transfer Learning and Grad-CAM Visualization on Indigenously Collected X-ray Dataset,"The COVID-19 outbreak began in December 2019 and has dreadfully affected our lives since then. More than three million lives have been engulfed by this newest member of the corona virus family. With the emergence of continuously mutating variants of this virus, it is still indispensable to successfully diagnose the virus at early stages. Although the primary technique for the diagnosis is the PCR test, the non-contact methods utilizing the chest radiographs and CT scans are always preferred. Artificial intelligence, in this regard, plays an essential role in the early and accurate detection of COVID-19 using pulmonary images. In this research, a transfer learning technique with fine tuning was utilized for the detection and classification of COVID-19. Four pre-trained models i.e., VGG16, DenseNet-121, ResNet-50, and MobileNet were used. The aforementioned deep neural networks were trained using the dataset (available on Kaggle) of 7232 (COVID-19 and normal) chest X-ray images. An indigenous dataset of 450 chest X-ray images of Pakistani patients was collected and used for testing and prediction purposes. Various important parameters, e.g., recall, specificity, F1-score, precision, loss graphs, and confusion matrices were calculated to validate the accuracy of the models. The achieved accuracies of VGG16, ResNet-50, DenseNet-121, and MobileNet are 83.27%, 92.48%, 96.49%, and 96.48%, respectively. In order to display feature maps that depict the decomposition process of an input image into various filters, a visualization of the intermediate activations is performed. Finally, the Grad-CAM technique was applied to create class-specific heatmap images in order to highlight the features extracted in the X-ray images. Various optimizers were used for error minimization purposes. DenseNet-121 outperformed the other three models in terms of both accuracy and prediction.",2021,10.3390/s21175813,diagnosis,False
Detection of COVID-19 With CT Images Using Hybrid Complex Shearlet Scattering Networks,"With the ongoing worldwide coronavirus disease 2019 (COVID-19) pandemic, it is desirable to develop effective algorithms to automatically detect COVID-19 with chest computed tomography (CT) images. Recently, a considerable number of methods based on deep learning have indeed been proposed. However, training an accurate deep learning model requires a large-scale chest CT dataset, which is hard to collect due to the high contagiousness of COVID-19. To achieve improved detection performance, this paper proposes a hybrid framework that fuses the complex shearlet scattering transform (CSST) and a suitable convolutional neural network into a single model. The introduced CSST cascades complex shearlet transforms with modulus nonlinearities and low-pass filter convolutions to compute a sparse and locally invariant image representation. The features computed from the input chest CT images are discriminative for COVID-19 detection. Furthermore, a wide residual network with a redesigned residual block (WR2N) is developed to learn more granular multiscale representations by applying it to scattering features. The combination of model-based CSST and data-driven WR2N leads to a more convenient neural network for image representation, where the idea is to learn only the image parts that the CSST cannot handle instead of all parts. Experiments on two public datasets demonstrate the superiority of our method. We can obtain more accurate results than several state-of-the-art COVID-19 classification methods in terms of measures such as accuracy, the F1-score, and the area under the receiver operating characteristic curve.",2022,10.1109/jbhi.2021.3132157,diagnosis,True
Detection of Lung Cancer on Computed Tomography Using Artificial Intelligence Applications Developed by Deep Learning Methods and the Contribution of Deep Learning to the Classification of Lung Carcinoma,"BACKGROUND: Every year, lung cancer contributes to a high percentage deaths in the world. Early detection of lung cancer is important for its effective treatment, and non-invasive rapid methods are usually used for diagnosis. INTRODUCTION: In this study, we aimed to detect lung cancer using deep learning methods and determine the contribution of deep learning to the classification of lung carcinoma using a convolutional neural network (CNN). METHODS: A total of 301 patients diagnosed with lung carcinoma pathologies in our hospital were included in the study. In the thorax, Computed Tomography (CT) was performed for diagnostic purposes prior to the treatment. After tagging the section images, tumor detection, small and non-small cell lung carcinoma differentiation, adenocarcinoma-squamous cell lung carcinoma differentiation, and adenocarcinoma-squamous cell-small cell lung carcinoma differentiation were sequentially performed using deep CNN methods. RESULTS: In total, 301 lung carcinoma images were used to detect tumors, and the model obtained with the deep CNN system exhibited 0.93 sensitivity, 0.82 precision, and 0.87 F1 score in detecting lung carcinoma. In the differentiation of small cell-non-small cell lung carcinoma, the sensitivity, precision and F1 score of the CNN model at the test stage were 0.92, 0.65, and 0.76, respectively. In the adenocarcinoma-squamous cancer differentiation, the sensitivity, precision, and F1 score were 0.95, 0.80, and 0.86, respectively. The patients were finally grouped as small cell lung carcinoma, adenocarcinoma, and squamous cell lung carcinoma, and the CNN model was used to determine whether it could differentiate these groups. The sensitivity, specificity, and F1 score of this model were 0.90, 0.44, and 0.59, respectively, in this differentiation. CONCLUSION: In this study, we successfully detected tumors and differentiated between adenocarcinoma- squamous cell carcinoma groups with the deep learning method using the CNN model. Due to their non-invasive nature and the success of the deep learning methods, they should be integrated into radiology to diagnose lung carcinoma.",2021,10.2174/1573405617666210204210500,diagnosis,True
Detection of pulmonary ground-glass opacity based on deep learning computer artificial intelligence,"BACKGROUND: A deep learning computer artificial intelligence system is helpful for early identification of ground glass opacities (GGOs). METHODS: Images from the Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI) database were used in AlexNet and GoogLeNet to detect pulmonary nodules, and 221 GGO images provided by Xinhua Hospital were used in ResNet50 for detecting GGOs. We used computed tomography image radial reorganization to create the input image of the three-dimensional features, and used the extracted features for deep learning, network training, testing, and analysis. RESULTS: In the final evaluation results, we found that the accuracy of identification of lung nodule could reach 88.0%, with an F-score of 0.891. In terms of performance and accuracy, our method was better than the existing solutions. The GGO nodule classification achieved the best F-score of 0.87805. We propose a preprocessing method of red, green, and blue (RGB) superposition in the region of interest to effectively increase the differentiation between nodules and normal tissues, and that is the innovation of our research. CONCLUSIONS: The method of deep learning proposed in this study is more sensitive than other systems in recent years, and the average false positive is lower than that of others.",2019,10.1186/s12938-019-0627-4,diagnosis,True
Detection of pulmonary nodules based on a multiscale feature 3D U-Net convolutional neural network of transfer learning,"A new computer-aided detection scheme is proposed, the 3D U-Net convolutional neural network, based on multiscale features of transfer learning to automatically detect pulmonary nodules from the thoracic region containing background and noise. The test results can be used as reference information for doctors to assist in the detection of early lung cancer. The proposed scheme is composed of three major steps: First, the pulmonary parenchyma area is segmented by various methods. Then, the 3D U-Net convolutional neural network model with a multiscale feature structure is built. The network model structure is subsequently fine-tuned by the transfer learning method based on weight, and the optimal parameters are selected in the network model. Finally, datasets are extracted to train the fine-tuned 3D U-Net network model to detect pulmonary nodules. The five-fold cross-validation method is used to obtain the experimental results for the LUNA16 and TIANCHI17 datasets. The experimental results show that the scheme not only has obvious advantages in the detection of medium and large-sized nodules but also has an accuracy rate of more than 70% for the detection of small-sized nodules. The scheme provides automatic and accurate detection of pulmonary nodules that reduces the overfitting rate and training time and improves the efficiency of the algorithm. It can assist doctors in the diagnosis of lung cancer and can be extended to other medical image detection and recognition fields.",2020,10.1371/journal.pone.0235672,diagnosis,True
Detection of pulmonary nodules in CT images based on fuzzy integrated active contour model and hybrid parametric mixture model,"The segmentation and detection of various types of nodules in a Computer-aided detection (CAD) system present various challenges, especially when (1) the nodule is connected to a vessel and they have very similar intensities; (2) the nodule with ground-glass opacity (GGO) characteristic possesses typical weak edges and intensity inhomogeneity, and hence it is difficult to define the boundaries. Traditional segmentation methods may cause problems of boundary leakage and ""weak"" local minima. This paper deals with the above mentioned problems. An improved detection method which combines a fuzzy integrated active contour model (FIACM)-based segmentation method, a segmentation refinement method based on Parametric Mixture Model (PMM) of juxta-vascular nodules, and a knowledge-based C-SVM (Cost-sensitive Support Vector Machines) classifier, is proposed for detecting various types of pulmonary nodules in computerized tomography (CT) images. Our approach has several novel aspects: (1) In the proposed FIACM model, edge and local region information is incorporated. The fuzzy energy is used as the motivation power for the evolution of the active contour. (2) A hybrid PMM Model of juxta-vascular nodules combining appearance and geometric information is constructed for segmentation refinement of juxta-vascular nodules. Experimental results of detection for pulmonary nodules show desirable performances of the proposed method.",2013,10.1155/2013/515386,diagnosis,True
Detection of the location of pneumothorax in chest X-rays using small artificial neural networks and a simple training process,"The purpose of this study was to evaluate the diagnostic performance achieved by using fully-connected small artificial neural networks (ANNs) and a simple training process, the Kim-Monte Carlo algorithm, to detect the location of pneumothorax in chest X-rays. A total of 1,000 chest X-ray images with pneumothorax were taken randomly from NIH (the National Institutes of Health) public image database and used as the training and test sets. Each X-ray image with pneumothorax was divided into 49 boxes for pneumothorax localization. For each of the boxes in the chest X-ray images contained in the test set, the area under the receiver operating characteristic (ROC) curve (AUC) was 0.882, and the sensitivity and specificity were 80.6% and 83.0%, respectively. In addition, a common currently used deep-learning method for image recognition, the convolution neural network (CNN), was also applied to the same dataset for comparison purposes. The performance of the fully-connected small ANN was better than that of the CNN. Regarding the diagnostic performances of the CNN with different activation functions, the CNN with a sigmoid activation function for fully-connected hidden nodes was better than the CNN with the rectified linear unit (RELU) activation function. This study showed that our approach can accurately detect the location of pneumothorax in chest X-rays, significantly reduce the time delay incurred when diagnosing urgent diseases such as pneumothorax, and increase the effectiveness of clinical practice and patient care.",2021,10.1038/s41598-021-92523-2,diagnosis,False
Determination of disease severity in COVID-19 patients using deep learning in chest X-ray images,"PURPOSE: Chest X-ray plays a key role in diagnosis and management of COVID-19 patients and imaging features associated with clinical elements may assist with the development or validation of automated image analysis tools. We aimed to identify associations between clinical and radiographic features as well as to assess the feasibility of deep learning applied to chest X-rays in the setting of an acute COVID-19 outbreak. METHODS: A retrospective study of X-rays, clinical, and laboratory data was performed from 48 SARS-CoV-2 RT-PCR positive patients (age 60±17 years, 15 women) between February 22 and March 6, 2020 from a tertiary care hospital in Milan, Italy. Sixty-five chest X-rays were reviewed by two radiologists for alveolar and interstitial opacities and classified by severity on a scale from 0 to 3. Clinical factors (age, symptoms, comorbidities) were investigated for association with opacity severity and also with placement of central line or endotracheal tube. Deep learning models were then trained for two tasks: lung segmentation and opacity detection. Imaging characteristics were compared to clinical datapoints using the unpaired student's t-test or Mann-Whitney U test. Cohen's kappa analysis was used to evaluate the concordance of deep learning to conventional radiologist interpretation. RESULTS: Fifty-six percent of patients presented with alveolar opacities, 73% had interstitial opacities, and 23% had normal X-rays. The presence of alveolar or interstitial opacities was statistically correlated with age (P = 0.008) and comorbidities (P = 0.005). The extent of alveolar or interstitial opacities on baseline X-ray was significantly associated with the presence of endotracheal tube (P = 0.0008 and P = 0.049) or central line (P = 0.003 and P = 0.007). In comparison to human interpretation, the deep learning model achieved a kappa concordance of 0.51 for alveolar opacities and 0.71 for interstitial opacities. CONCLUSION: Chest X-ray analysis in an acute COVID-19 outbreak showed that the severity of opacities was associated with advanced age, comorbidities, as well as acuity of care. Artificial intelligence tools based upon deep learning of COVID-19 chest X-rays are feasible in the acute outbreak setting.",2021,10.5152/dir.2020.20205,prognosis,False
Determining the invasiveness of ground-glass nodules using a 3D multi-task network,"OBJECTIVES: The aim of this study was to determine the invasiveness of ground-glass nodules (GGNs) using a 3D multi-task deep learning network. METHODS: We propose a novel architecture based on 3D multi-task learning to determine the invasiveness of GGNs. In total, 770 patients with 909 GGNs who underwent lung CT scans were enrolled. The patients were divided into the training (n = 626) and test sets (n = 144). In the test set, invasiveness was classified using deep learning into three categories: atypical adenomatous hyperplasia (AAH) and adenocarcinoma in situ (AIS), minimally invasive adenocarcinoma (MIA), and invasive pulmonary adenocarcinoma (IA). Furthermore, binary classifications (AAH/AIS/MIA vs. IA) were made by two thoracic radiologists and compared with the deep learning results. RESULTS: In the three-category classification task, the sensitivity, specificity, and accuracy were 65.41%, 82.21%, and 64.9%, respectively. In the binary classification task, the sensitivity, specificity, accuracy, and area under the ROC curve (AUC) values were 69.57%, 95.24%, 87.42%, and 0.89, respectively. In the visual assessment of GGN invasiveness of binary classification by the two thoracic radiologists, the sensitivity, specificity, and accuracy of the senior and junior radiologists were 58.93%, 90.51%, and 81.35% and 76.79%, 55.47%, and 61.66%, respectively. CONCLUSIONS: The proposed multi-task deep learning model achieved good classification results in determining the invasiveness of GGNs. This model may help to select patients with invasive lesions who need surgery and the proper surgical methods. KEY POINTS: • The proposed multi-task model has achieved good classification results for the invasiveness of GGNs. • The proposed network includes a classification and segmentation branch to learn global and regional features, respectively. • The multi-task model could assist doctors in selecting patients with invasive lesions who need surgery and choosing appropriate surgical methods.",2021,10.1007/s00330-021-07794-0,prognosis,True
"Determining Top Fully Connected Layer's Hidden Neuron Count for Transfer Learning, Using Knowledge Distillation: a Case Study on Chest X-Ray Classification of Pneumonia and COVID-19","Deep convolutional neural network (CNN)-assisted classification of images is one of the most discussed topics in recent years. Continuously innovation of neural network architectures is making it more correct and efficient every day. But training a neural network from scratch is very time-consuming and requires a lot of sophisticated computational equipment and power. So, using some pre-trained neural network as feature extractor for any image classification task or ""transfer learning"" is a very popular approach that saves time and computational power for practical use of CNNs. In this paper, an efficient way of building full model from any pre-trained model with high accuracy and low memory is proposed using knowledge distillation. Using the distilled knowledge of the last layer of pre-trained networks passes through fully connected layers with different hidden layers, followed by Softmax layer. The accuracies of student networks are mildly lesser than the whole models, but accuracy of student models clearly indicates the accuracy of the real network. In this way, the best number of hidden layers for dense layer for that pre-trained network with best accuracy and no-overfitting can be found with less time. Here, VGG16 and VGG19 (pre-trained upon ""ImageNet"" dataset) is tested upon chest X-rays (pneumonia and COVID-19). For finding the best total number of hidden layers, it saves nearly 44 min for VGG19 and 36 min and 37 s for VGG16 feature extractor.",2021,10.1007/s10278-021-00518-2,diagnosis,False
Development and clinical application of deep learning model for lung nodules screening on CT images,"Lung cancer screening based on low-dose CT (LDCT) has now been widely applied because of its effectiveness and ease of performance. Radiologists who evaluate a large LDCT screening images face enormous challenges, including mechanical repetition and boring work, the easy omission of small nodules, lack of consistent criteria, etc. It requires an efficient method for helping radiologists improve nodule detection accuracy with efficiency and cost-effectiveness. Many novel deep neural network-based systems have demonstrated the potential for use in the proposed technique to detect lung nodules. However, the effectiveness of clinical practice has not been fully recognized or proven. Therefore, the aim of this study to develop and assess a deep learning (DL) algorithm in identifying pulmonary nodules (PNs) on LDCT and investigate the prevalence of the PNs in China. Radiologists and algorithm performance were assessed using the FROC score, ROC-AUC, and average time consumption. Agreement between the reference standard and the DL algorithm in detecting positive nodules was assessed per-study by Bland-Altman analysis. The Lung Nodule Analysis (LUNA) public database was used as the external test. The prevalence of NCPNs was investigated as well as other detailed information regarding the number of pulmonary nodules, their location, and characteristics, as interpreted by two radiologists.",2020,10.1038/s41598-020-70629-3,diagnosis,True
"Development and clinical implementation of tailored image analysis tools for COVID-19 in the midst of the pandemic: The synergetic effect of an open, clinically embedded software development platform and machine learning","PURPOSE: During the emerging COVID-19 pandemic, radiology departments faced a substantial increase in chest CT admissions coupled with the novel demand for quantification of pulmonary opacities. This article describes how our clinic implemented an automated software solution for this purpose into an established software platform in 10 days. The underlying hypothesis was that modern academic centers in radiology are capable of developing and implementing such tools by their own efforts and fast enough to meet the rapidly increasing clinical needs in the wake of a pandemic. METHOD: Deep convolutional neural network algorithms for lung segmentation and opacity quantification on chest CTs were trained using semi-automatically and manually created ground-truth (N(total) = 172). The performance of the in-house method was compared to an externally developed algorithm on a separate test subset (N = 66). RESULTS: The final algorithm was available at day 10 and achieved human-like performance (Dice coefficient = 0.97). For opacity quantification, a slight underestimation was seen both for the in-house (1.8 %) and for the external algorithm (0.9 %). In contrast to the external reference, the underestimation for the in-house algorithm showed no dependency on total opacity load, making it more suitable for follow-up. CONCLUSIONS: The combination of machine learning and a clinically embedded software development platform enabled time-efficient development, instant deployment, and rapid adoption in clinical routine. The algorithm for fully automated lung segmentation and opacity quantification that we developed in the midst of the COVID-19 pandemic was ready for clinical use within just 10 days and achieved human-level performance even in complex cases.",2020,10.1016/j.ejrad.2020.109233,diagnosis,True
Development and evaluation of an artificial intelligence system for COVID-19 diagnosis,"Early detection of COVID-19 based on chest CT enables timely treatment of patients and helps control the spread of the disease. We proposed an artificial intelligence (AI) system for rapid COVID-19 detection and performed extensive statistical analysis of CTs of COVID-19 based on the AI system. We developed and evaluated our system on a large dataset with more than 10 thousand CT volumes from COVID-19, influenza-A/B, non-viral community acquired pneumonia (CAP) and non-pneumonia subjects. In such a difficult multi-class diagnosis task, our deep convolutional neural network-based system is able to achieve an area under the receiver operating characteristic curve (AUC) of 97.81% for multi-way classification on test cohort of 3,199 scans, AUC of 92.99% and 93.25% on two publicly available datasets, CC-CCII and MosMedData respectively. In a reader study involving five radiologists, the AI system outperforms all of radiologists in more challenging tasks at a speed of two orders of magnitude above them. Diagnosis performance of chest x-ray (CXR) is compared to that of CT. Detailed interpretation of deep network is also performed to relate system outputs with CT presentations. The code is available at https://github.com/ChenWWWeixiang/diagnosis_covid19 .",2020,10.1038/s41467-020-18685-1,diagnosis,True
Development and prospective validation of COVID-19 chest X-ray screening model for patients attending emergency departments,"Chest X-rays (CXRs) are the first-line investigation in patients presenting to emergency departments (EDs) with dyspnoea and are a valuable adjunct to clinical management of COVID-19 associated lung disease. Artificial intelligence (AI) has the potential to facilitate rapid triage of CXRs for further patient testing and/or isolation. In this work we develop an AI algorithm, CovIx, to differentiate normal, abnormal, non-COVID-19 pneumonia, and COVID-19 CXRs using a multicentre cohort of 293,143 CXRs. The algorithm is prospectively validated in 3289 CXRs acquired from patients presenting to ED with symptoms of COVID-19 across four sites in NHS Greater Glasgow and Clyde. CovIx achieves area under receiver operating characteristic curve for COVID-19 of 0.86, with sensitivity and F1-score up to 0.83 and 0.71 respectively, and performs on-par with four board-certified radiologists. AI-based algorithms can identify CXRs with COVID-19 associated pneumonia, as well as distinguish non-COVID pneumonias in symptomatic patients presenting to ED. Pre-trained models and inference scripts are freely available at https://github.com/beringresearch/bravecx-covid .",2021,10.1038/s41598-021-99986-3,diagnosis,False
Development and validation of a clinically applicable deep learning strategy (HONORS) for pulmonary nodule classification at CT: A retrospective multicentre study,"PURPOSE: To propose a practical strategy for the clinical application of deep learning algorithm, i.e., Hierarchical-Ordered Network-ORiented Strategy (HONORS), and a new approach to pulmonary nodule classification in various clinical scenarios, i.e., Filter-Guided Pyramid NETwork (FGP-NET). MATERIALS AND METHODS: We developed and validated FGP-NET on a collection of 2106 pulmonary nodules on computed tomography images which combined screened and clinically detected nodules, and performed external test (n = 341). The area under the curves (AUCs) of FGP-NET were assessed. A comparison study with a group of 126 skilled radiologists was conducted. On top of FGP-NET, we built up our HONORS which was composed of two solutions. In the Human Free Solution, we used the high sensitivity operating point for screened nodules, but the high specificity operating point for clinically detected nodules. In the Human-Machine Coupling Solution, we used the Youden point. RESULTS: FGP-NET achieved AUCs of 0.969 and 0.847 for internal and external test. The AUCs of the subsets of the external test set ranged from 0.890 to 0.942. The average sensitivity and specificity of the 126 radiologists were 72.2 ± 15.1 % and 71.7 ± 15.5 %, respectively, while a higher sensitivity (93.3 %) but a relatively inferior specificity (64.0 %) were achieved by FGP-NET. HONORS-guided FGP-NET identified benign nodules with high sensitivity (sensitivity,95.5 %; specificity, 72.5 %) in the screened nodules, and identified malignant nodules with high specificity (sensitivity, 31.0 %; specificity, 97.5 %) in the clinically detected nodules. These nodules could be reliably diagnosed without any intervention from radiologists, via the Human Free Solution. The remaining ambiguous nodules were diagnosed with high performance, which however required manual confirmation by radiologists, via the Human-Machine Coupling Solution. CONCLUSIONS: FGP-NET performed comparably to skilled radiologists in terms of diagnosing pulmonary nodules. HONORS, due to its high performance, might reliably contribute a second opinion, aiding in optimizing the clinical workflow.",2021,10.1016/j.lungcan.2021.03.008,diagnosis,True
Development and Validation of a Deep Learning-based Automatic Detection Algorithm for Active Pulmonary Tuberculosis on Chest Radiographs,"BACKGROUND: Detection of active pulmonary tuberculosis on chest radiographs (CRs) is critical for the diagnosis and screening of tuberculosis. An automated system may help streamline the tuberculosis screening process and improve diagnostic performance. METHODS: We developed a deep learning-based automatic detection (DLAD) algorithm using 54c221 normal CRs and 6768 CRs with active pulmonary tuberculosis that were labeled and annotated by 13 board-certified radiologists. The performance of DLAD was validated using 6 external multicenter, multinational datasets. To compare the performances of DLAD with physicians, an observer performance test was conducted by 15 physicians including nonradiology physicians, board-certified radiologists, and thoracic radiologists. Image-wise classification and lesion-wise localization performances were measured using area under the receiver operating characteristic (ROC) curves and area under the alternative free-response ROC curves, respectively. Sensitivities and specificities of DLAD were calculated using 2 cutoffs (high sensitivity [98%] and high specificity [98%]) obtained through in-house validation. RESULTS: DLAD demonstrated classification performance of 0.977-1.000 and localization performance of 0.973-1.000. Sensitivities and specificities for classification were 94.3%-100% and 91.1%-100% using the high-sensitivity cutoff and 84.1%-99.0% and 99.1%-100% using the high-specificity cutoff. DLAD showed significantly higher performance in both classification (0.993 vs 0.746-0.971) and localization (0.993 vs 0.664-0.925) compared to all groups of physicians. CONCLUSIONS: Our DLAD demonstrated excellent and consistent performance in the detection of active pulmonary tuberculosis on CR, outperforming physicians, including thoracic radiologists.",2019,10.1093/cid/ciy967,diagnosis,False
Development and Validation of a Modified Three-Dimensional U-Net Deep-Learning Model for Automated Detection of Lung Nodules on Chest CT Images From the Lung Image Database Consortium and Japanese Datasets,"RATIONALE AND OBJECTIVES: A more accurate lung nodule detection algorithm is needed. We developed a modified three-dimensional (3D) U-net deep-learning model for the automated detection of lung nodules on chest CT images. The purpose of this study was to evaluate the accuracy of the developed modified 3D U-net deep-learning model. MATERIALS AND METHODS: In this Health Insurance Portability and Accountability Act-compliant, Institutional Review Board-approved retrospective study, the 3D U-net based deep-learning model was trained using the Lung Image Database Consortium and Image Database Resource Initiative dataset. For internal model validation, we used 89 chest CT scans that were not used for model training. For external model validation, we used 450 chest CT scans taken at an urban university hospital in Japan. Each case included at least one nodule of >5 mm identified by an experienced radiologist. We evaluated model accuracy using the competition performance metric (CPM) (average sensitivity at 1/8, 1/4, 1/2, 1, 2, 4, and 8 false-positives per scan). The 95% confidence interval (CI) was computed by bootstrapping 1000 times. RESULTS: In the internal validation, the CPM was 94.7% (95% CI: 89.1%-98.6%). In the external validation, the CPM was 83.3% (95% CI: 79.4%-86.1%). CONCLUSION: The modified 3D U-net deep-learning model showed high performance in both internal and external validation.",2022,10.1016/j.acra.2020.07.030,diagnosis,True
Development and Validation of a Predictive Radiomics Model for Clinical Outcomes in Stage I Non-small Cell Lung Cancer,"PURPOSE: To develop and validate a radiomics signature that can predict the clinical outcomes for patients with stage I non-small cell lung cancer (NSCLC). METHODS AND MATERIALS: We retrospectively analyzed contrast-enhanced computed tomography images of patients from a training cohort (n = 147) treated with surgery and an independent validation cohort (n = 295) treated with stereotactic ablative radiation therapy. Twelve radiomics features with established strategies for filtering and preprocessing were extracted. The random survival forests (RSF) method was used to build models from subsets of the 12 candidate features based on their survival relevance and generate a mortality risk index for each observation in the training set. An optimal model was selected, and its ability to predict clinical outcomes was evaluated in the validation set using predicted mortality risk indexes. RESULTS: The optimal RSF model, consisting of 2 predictive features, kurtosis and the gray level co-occurrence matrix feature homogeneity2, allowed for significant risk stratification (log-rank P < .0001) and remained an independent predictor of overall survival after adjusting for age, tumor volume and histologic type, and Karnofsky performance status (hazard ratio [HR] 1.27; P < 2e-16) in the training set. The resultant mortality risk indexes were significantly associated with overall survival in the validation set (log-rank P = .0173; HR 1.02, P = .0438). They were also significant for distant metastasis (log-rank P < .05; HR 1.04, P = .0407) and were borderline significant for regional recurrence on univariate analysis (log-rank P < .05; HR 1.04, P = .0617). CONCLUSIONS: Our radiomics model accurately predicted several clinical outcomes and allowed pretreatment risk stratification in stage I NSCLC, allowing the choice of treatment to be tailored to each patient's individual risk profile.",2018,10.1016/j.ijrobp.2017.10.046,diagnosis,True
"Development and validation of a preoperative CT-based radiomic nomogram to predict pathology invasiveness in patients with a solitary pulmonary nodule: a machine learning approach, multicenter, diagnostic study","OBJECTIVES: To develop and validate a preoperative CT-based nomogram combined with radiomic and clinical-radiological signatures to distinguish preinvasive lesions from pulmonary invasive lesions. METHODS: This was a retrospective, diagnostic study conducted from August 1, 2018, to May 1, 2020, at three centers. Patients with a solitary pulmonary nodule were enrolled in the GDPH center and were divided into two groups (7:3) randomly: development (n = 149) and internal validation (n = 54). The SYSMH center and the ZSLC Center formed an external validation cohort of 170 patients. The least absolute shrinkage and selection operator (LASSO) algorithm and logistic regression analysis were used to feature signatures and transform them into models. RESULTS: The study comprised 373 individuals from three independent centers (female: 225/373, 60.3%; median [IQR] age, 57.0 [48.0-65.0] years). The AUCs for the combined radiomic signature selected from the nodular area and the perinodular area were 0.93, 0.91, and 0.90 in the three cohorts. The nomogram combining the clinical and combined radiomic signatures could accurately predict interstitial invasion in patients with a solitary pulmonary nodule (AUC, 0.94, 0.90, 0.92) in the three cohorts, respectively. The radiomic nomogram outperformed any clinical or radiomic signature in terms of clinical predictive abilities, according to a decision curve analysis and the Akaike information criteria. CONCLUSIONS: This study demonstrated that a nomogram constructed by identified clinical-radiological signatures and combined radiomic signatures has the potential to precisely predict pathology invasiveness. KEY POINTS: • The radiomic signature from the perinodular area has the potential to predict pathology invasiveness of the solitary pulmonary nodule. • The new radiomic nomogram was useful in clinical decision-making associated with personalized surgical intervention and therapeutic regimen selection in patients with early-stage non-small-cell lung cancer.",2022,10.1007/s00330-021-08268-z,diagnosis,True
Development and Validation of a Risk Stratification Model of Pulmonary Ground-Glass Nodules Based on Complementary Lung-RADS 1.1 and Deep Learning Scores,"PURPOSE: To assess the value of novel deep learning (DL) scores combined with complementary lung imaging reporting and data system 1.1 (cLung-RADS 1.1) in managing the risk stratification of ground-glass nodules (GGNs) and therefore improving the efficiency of lung cancer (LC) screening in China. MATERIALS AND METHODS: Overall, 506 patients with 561 GGNs on routine computed tomography images, obtained between January 2017 and March 2021, were enrolled in this single-center, retrospective Chinese study. Moreover, the cLung-RADS 1.1 was previously validated, and the DL algorithms were based on a multi-stage, three-dimensional DL-based convolutional neural network. Therefore, the DL-based cLung-RADS 1.1 model was created using a combination of the risk scores of DL and category of cLung-RADS 1.1. The recall rate, precision, accuracy, per-class F1 score, weighted average F1 score (F1(weighted)), Matthews correlation coefficient (MCC), and area under the curve (AUC) were used to evaluate the performance of DL-based cLung-RADS 1.1. RESULTS: The percentage of neoplastic lesions appeared as GGNs in our study was 95.72% (537/561) after long-period follow-up.Compared to cLung-RADS 1.1 model or DL model, The DL-based cLung-RADS 1.1 model achieved the excellent performance with F1 scores of 95.96% and 95.58%, F1(weighted) values of 97.49 and 96.62%, accuracies of 92.38 and 91.77%, and MCCs of 32.43 and 37.15% in the training and validation tests, respectively. The combined model achieved the best AUCs of 0.753 (0.526-0.980) and 0.734 (0.585-0.884) for the training and validation tests, respectively. CONCLUSION: The DL-based cLung-RADS 1.1 model shows the best performance in risk stratification management of GGNs, which demonstrates substantial promise for developing a more effective personalized lung neoplasm management paradigm for LC screening in China.",2022,10.3389/fpubh.2022.891306,diagnosis,True
Development and Validation of Deep Learning-based Automatic Detection Algorithm for Malignant Pulmonary Nodules on Chest Radiographs,"Purpose To develop and validate a deep learning-based automatic detection algorithm (DLAD) for malignant pulmonary nodules on chest radiographs and to compare its performance with physicians including thoracic radiologists. Materials and Methods For this retrospective study, DLAD was developed by using 43 292 chest radiographs (normal radiograph-to-nodule radiograph ratio, 34 067:9225) in 34 676 patients (healthy-to-nodule ratio, 30 784:3892; 19 230 men [mean age, 52.8 years; age range, 18-99 years]; 15 446 women [mean age, 52.3 years; age range, 18-98 years]) obtained between 2010 and 2015, which were labeled and partially annotated by 13 board-certified radiologists, in a convolutional neural network. Radiograph classification and nodule detection performances of DLAD were validated by using one internal and four external data sets from three South Korean hospitals and one U.S. hospital. For internal and external validation, radiograph classification and nodule detection performances of DLAD were evaluated by using the area under the receiver operating characteristic curve (AUROC) and jackknife alternative free-response receiver-operating characteristic (JAFROC) figure of merit (FOM), respectively. An observer performance test involving 18 physicians, including nine board-certified radiologists, was conducted by using one of the four external validation data sets. Performances of DLAD, physicians, and physicians assisted with DLAD were evaluated and compared. Results According to one internal and four external validation data sets, radiograph classification and nodule detection performances of DLAD were a range of 0.92-0.99 (AUROC) and 0.831-0.924 (JAFROC FOM), respectively. DLAD showed a higher AUROC and JAFROC FOM at the observer performance test than 17 of 18 and 15 of 18 physicians, respectively (P < .05), and all physicians showed improved nodule detection performances with DLAD (mean JAFROC FOM improvement, 0.043; range, 0.006-0.190; P < .05). Conclusion This deep learning-based automatic detection algorithm outperformed physicians in radiograph classification and nodule detection performance for malignant pulmonary nodules on chest radiographs, and it enhanced physicians' performances when used as a second reader. © RSNA, 2018 Online supplemental material is available for this article.",2019,10.1148/radiol.2018180237,diagnosis,True
Development and Validation of Machine Learning Models to Predict Epidermal Growth Factor Receptor Mutation in Non-Small Cell Lung Cancer: A Multi-Center Retrospective Radiomics Study,"OBJECTIVE: To develop and validate a generalized prediction model that can classify epidermal growth factor receptor (EGFR) mutation status in non-small cell lung cancer patients. METHODS: A total of 346 patients (296 in the training cohort and 50 in the validation cohort) from four centers were included in this retrospective study. First, 1085 features were extracted using IBEX from the computed tomography images. The features were screened using the intraclass correlation coefficient, hypothesis tests and least absolute shrinkage and selection operator. Logistic regression (LR), decision tree (DT), random forest (RF), and support vector machine (SVM) were used to build a radiomics model for classification. The models were evaluated using the following metrics: area under the curve (AUC), calibration curve (CAL), decision curve analysis (DCA), concordance index (C-index), and Brier score. RESULTS: Sixteen features were selected, and models were built using LR, DT, RF, and SVM. In the training cohort, the AUCs was .723, .842, .995, and .883; In the validation cohort, the AUCs were .658, 0567, .88, and .765. RF model with the best AUC, its CAL, C-index (training cohort=.998; validation cohort=.883), and Brier score (training cohort=.007; validation cohort=0.137) showed a satisfactory predictive accuracy; DCA indicated that the RF model has better clinical application value. CONCLUSION: Machine learning models based on computed tomography images can be used to evaluate EGFR status in patients with non-small cell lung cancer, and the RF model outperformed LR, DT, and SVM.",2022,10.1177/10732748221092926,prognosis,True
Development and Validation of Machine Learning-based Model for the Prediction of Malignancy in Multiple Pulmonary Nodules: Analysis from Multicentric Cohorts,"PURPOSE: Nodule evaluation is challenging and critical to diagnose multiple pulmonary nodules (MPNs). We aimed to develop and validate a machine learning-based model to estimate the malignant probability of MPNs to guide decision-making. EXPERIMENTAL DESIGN: A boosted ensemble algorithm (XGBoost) was used to predict malignancy using the clinicoradiologic variables of 1,739 nodules from 520 patients with MPNs at a Chinese center. The model (PKU-M model) was trained using 10-fold cross-validation in which hyperparameters were selected and fine-tuned. The model was validated and compared with solitary pulmonary nodule (SPN) models, clinicians, and a computer-aided diagnosis (CADx) system in an independent transnational cohort and a prospective multicentric cohort. RESULTS: The PKU-M model showed excellent discrimination [area under the curve; AUC (95% confidence interval (95% CI)), 0.909 (0.854-0.946)] and calibration (Brier score, 0.122) in the development cohort. External validation (583 nodules) revealed that the AUC of the PKU-M model was 0.890 (0.859-0.916), higher than those of the Brock model [0.806 (0.771-0.838)], PKU model [0.780 (0.743-0.817)], Mayo model [0.739 (0.697-0.776)], and VA model [0.682 (0.640-0.722)]. Prospective comparison (200 nodules) showed that the AUC of the PKU-M model [0.871 (0.815-0.915)] was higher than that of surgeons [0.790 (0.711-0.852), 0.741 (0.662-0.804), and 0.727 (0.650-0.788)], radiologist [0.748 (0.671-0.814)], and the CADx system [0.757 (0.682-0.818)]. Furthermore, the model outperformed the clinicians with an increase of 14.3% in sensitivity and 7.8% in specificity. CONCLUSIONS: After its development using machine learning algorithms, validation using transnational multicentric cohorts, and prospective comparison with clinicians and the CADx system, this novel prediction model for MPNs presented solid performance as a convenient reference to help decision-making.",2021,10.1158/1078-0432.Ccr-20-4007,prognosis,True
Development of a deep learning-based method to diagnose pulmonary ground-glass nodules by sequential computed tomography imaging,"BACKGROUND: Early identification of the malignant propensity of pulmonary ground-glass nodules (GGNs) can relieve the pressure from tracking lesions and personalized treatment adaptation. The purpose of this study was to develop a deep learning-based method using sequential computed tomography (CT) imaging for diagnosing pulmonary GGNs. METHODS: This diagnostic study retrospectively enrolled 762 patients with GGNs from West China Hospital of Sichuan University between July 2009 and March 2019. All patients underwent surgical resection and at least two consecutive time-point CT scans. We developed a deep learning-based method to identify GGNs using sequential CT imaging on a training set consisting of 1524 CT sections from 508 patients and then evaluated 256 patients in the testing set. Afterwards, an observer study was conducted to compare the diagnostic performance between the deep learning model and two trained radiologists in the testing set. We further performed stratified analysis to further relieve the impact of histological types, nodule size, time interval between two CTs, and the component of GGNs. Receiver operating characteristic (ROC) analysis was used to assess the performance of all models. RESULTS: The deep learning model that used integrated DL-features from initial and follow-up CT images yielded the best diagnostic performance, with an area under the curve of 0.841. The observer study showed that the accuracies for the deep learning model, junior radiologist, and senior radiologist were 77.17%, 66.89%, and 77.03%, respectively. Stratified analyses showed that the deep learning model and radiologists exhibited higher performance in the subgroup of nodule sizes larger than 10 mm. With a longer time interval between two CTs, the deep learning model yielded higher diagnostic accuracy, but no general rules were yielded for radiologists. Different densities of components did not affect the performance of the deep learning model. In contrast, the radiologists were affected by the nodule component. CONCLUSIONS: Deep learning can achieve diagnostic performance on par with or better than radiologists in identifying pulmonary GGNs.",2022,10.1111/1759-7714.14305,diagnosis,True
Development of a predictive radiomics model for lymph node metastases in pre-surgical CT-based stage IA non-small cell lung cancer,"OBJECTIVES: To develop and validate predictive models using clinical parameters, radiomic features and a combination of both for lymph node metastasis (LNM) in pre-surgical CT-based stage IA non-small cell lung cancer (NSCLC) patients. METHODS: This retrospective study included 649 pre-surgical CT-based stage IA NSCLC patients from our hospital. One hundred and thirty-eight (21 %) of the 649 patients had LNM after surgery. A total of 396 radiomic features were extracted from the venous phase contrast enhanced computed tomography (CECT). The training group included 455 patients (97 with and 358 without LNM) and the testing group included 194 patients (41 with and 153 without LNM). The least absolute shrinkage and selection operator (LASSO) algorithm was used for radiomic feature selection. The random forest (RF) was used for model development. Three models (a clinical model, a radiomics model, and a combined model) were developed to predict LNM in early stage NSCLC patients. The area under the receiver operating characteristic (ROC) curve (AUC) value and decision curve analysis were used to evaluate the performance in LNM status (with or without LNM) using the three models. RESULTS: The ROC analysis (also decision curve analysis) showed predictive performance for LNM of the radiomics model (AUC values for training and testing, respectively 0.898 and 0.851) and of the combined model (0.911 and 0.860, respectively). Both performed better than the clinical model (0.739 and 0.614, respectively; delong test p-values both<0.001). CONCLUSION: A radiomics model using the venous phase of CE-CT has potential for predicting LNM in pre-surgical CT-based stage IA NSCLC patients.",2020,10.1016/j.lungcan.2019.11.003,prognosis,True
Development of a quantitative segmentation model to assess the effect of comorbidity on patients with COVID-19,"BACKGROUND: The coronavirus disease 2019 (COVID-19) has brought a global disaster. Quantitative lesions may provide the radiological evidence of the severity of pneumonia and further to assess the effect of comorbidity on patients with COVID-19. METHODS: 294 patients with COVID-19 were enrolled from February, 24, 2020 to June, 1, 2020 from six centers. Multi-task Unet network was used to segment the whole lung and lesions from chest CT images. This deep learning method was pre-trained in 650 CT images (550 in primary dataset and 100 in test dataset) with COVID-19 or community-acquired pneumonia and Dice coefficients in test dataset were calculated. 50 CT scans of 50 patients (15 with comorbidity and 35 without comorbidity) were random selected to mark lesions manually. The results will be compared with the automatic segmentation model. Eight quantitative parameters were calculated based on the segmentation results to evaluate the effect of comorbidity on patients with COVID-19. RESULTS: Quantitative segmentation model was proved to be effective and accurate with all Dice coefficients more than 0.85 and all accuracies more than 0.95. Of the 294 patients, 52 (17.7%) patients were reported having at least one comorbidity; 14 (4.8%) having more than one comorbidity. Patients with any comorbidity were older (P < 0.001), had longer incubation period (P < 0.001), were more likely to have abnormal laboratory findings (P < 0.05), and be in severity status (P < 0.001). More lesions (including larger volume of lesion, consolidation, and ground-glass opacity) were shown in patients with any comorbidity than patients without comorbidity (all P < 0.001). More lesions were found on CT images in patients with more comorbidities. The median volumes of lesion, consolidation, and ground-glass opacity in diabetes mellitus group were largest among the groups with single comorbidity that had the incidence rate of top three. CONCLUSIONS: Multi-task Unet network can make quantitative CT analysis of lesions to assess the effect of comorbidity on patients with COVID-19, further to provide the radiological evidence of the severity of pneumonia. More lesions (including GGO and consolidation) were found in CT images of cases with comorbidity. The more comorbidities patients have, the more lesions CT images show.",2020,10.1186/s40001-020-00450-1,prognosis,True
Development of computer-aided model to differentiate COVID-19 from pulmonary edema in lung CT scan: EDECOVID-net,"The efforts made to prevent the spread of COVID-19 face specific challenges in diagnosing COVID-19 patients and differentiating them from patients with pulmonary edema. Although systemically administered pulmonary vasodilators and acetazolamide are of great benefit for treating pulmonary edema, they should not be used to treat COVID-19 as they carry the risk of several adverse consequences, including worsening the matching of ventilation and perfusion, impaired carbon dioxide transport, systemic hypotension, and increased work of breathing. This study proposes a machine learning-based method (EDECOVID-net) that automatically differentiates the COVID-19 symptoms from pulmonary edema in lung CT scans using radiomic features. To the best of our knowledge, EDECOVID-net is the first method to differentiate COVID-19 from pulmonary edema and a helpful tool for diagnosing COVID-19 at early stages. The EDECOVID-net has been proposed as a new machine learning-based method with some advantages, such as having simple structure and few mathematical calculations. In total, 13 717 imaging patches, including 5759 COVID-19 and 7958 edema images, were extracted using a CT incision by a specialist radiologist. The EDECOVID-net can distinguish the patients with COVID-19 from those with pulmonary edema with an accuracy of 0.98. In addition, the accuracy of the EDECOVID-net algorithm is compared with other machine learning methods, such as VGG-16 (Acc = 0.94), VGG-19 (Acc = 0.96), Xception (Acc = 0.95), ResNet101 (Acc = 0.97), and DenseNet20l (Acc = 0.97).",2022,10.1016/j.compbiomed.2021.105172,diagnosis,True
Development of severity and mortality prediction models for covid-19 patients at emergency department including the chest x-ray,"OBJECTIVES: To develop prognosis prediction models for COVID-19 patients attending an emergency department (ED) based on initial chest X-ray (CXR), demographics, clinical and laboratory parameters. METHODS: All symptomatic confirmed COVID-19 patients admitted to our hospital ED between February 24th and April 24th 2020 were recruited. CXR features, clinical and laboratory variables and CXR abnormality indices extracted by a convolutional neural network (CNN) diagnostic tool were considered potential predictors on this first visit. The most serious individual outcome defined the three severity level: 0) home discharge or hospitalization ≤ 3 days, 1) hospital stay >3 days and 2) intensive care requirement or death. Severity and in-hospital mortality multivariable prediction models were developed and internally validated. The Youden index was used for the optimal threshold selection of the classification model. RESULTS: A total of 440 patients were enrolled (median 64 years; 55.9% male); 13.6% patients were discharged, 64% hospitalized, 6.6% required intensive care and 15.7% died. The severity prediction model included oxygen saturation/inspired oxygen fraction (SatO2/FiO2), age, C-reactive protein (CRP), lymphocyte count, extent score of lung involvement on CXR (ExtScoreCXR), lactate dehydrogenase (LDH), D-dimer level and platelets count, with AUC-ROC = 0.94 and AUC-PRC = 0.88. The mortality prediction model included age, SatO2/FiO2, CRP, LDH, CXR extent score, lymphocyte count and D-dimer level, with AUC-ROC = 0.97 and AUC-PRC = 0.78. The addition of CXR CNN-based indices did not improve significantly the predictive metrics. CONCLUSION: The developed and internally validated severity and mortality prediction models could be useful as triage tools in ED for patients with COVID-19 or other virus infections with similar behaviour.",2022,10.1016/j.rxeng.2021.09.004,prognosis,False
DIAG a Diagnostic Web Application Based on Lung CT Scan Images and Deep Learning,"Coronavirus disease is a pandemic that has infected millions of people around the world. Lung CT-scans are effective diagnostic tools, but radiologists can quickly become overwhelmed by the flow of infected patients. Therefore, automated image interpretation needs to be achieved. Deep learning (DL) can support critical medical tasks including diagnostics, and DL algorithms have successfully been applied to the classification and detection of many diseases. This work aims to use deep learning methods that can classify patients between Covid-19 positive and healthy patient. We collected 4 available datasets, and tested our convolutional neural networks (CNNs) on different distributions to investigate the generalizability of our models. In order to clearly explain the predictions, Grad-CAM and Fast-CAM visualization methods were used. Our approach reaches more than 92% accuracy on 2 different distributions. In addition, we propose a computer aided diagnosis web application for Covid-19 diagnosis. The results suggest that our proposed deep learning tool can be integrated to the Covid-19 detection process and be useful for a rapid patient management.",2021,10.3233/shti210175,diagnosis,True
Diagnosis of Coronavirus Disease 2019 (COVID-19) With Structured Latent Multi-View Representation Learning,"Recently, the outbreak of Coronavirus Disease 2019 (COVID-19) has spread rapidly across the world. Due to the large number of infected patients and heavy labor for doctors, computer-aided diagnosis with machine learning algorithm is urgently needed, and could largely reduce the efforts of clinicians and accelerate the diagnosis process. Chest computed tomography (CT) has been recognized as an informative tool for diagnosis of the disease. In this study, we propose to conduct the diagnosis of COVID-19 with a series of features extracted from CT images. To fully explore multiple features describing CT images from different views, a unified latent representation is learned which can completely encode information from different aspects of features and is endowed with promising class structure for separability. Specifically, the completeness is guaranteed with a group of backward neural networks (each for one type of features), while by using class labels the representation is enforced to be compact within COVID-19/community-acquired pneumonia (CAP) and also a large margin is guaranteed between different types of pneumonia. In this way, our model can well avoid overfitting compared to the case of directly projecting high-dimensional features into classes. Extensive experimental results show that the proposed method outperforms all comparison methods, and rather stable performances are observed when varying the number of training data.",2020,10.1109/tmi.2020.2992546,diagnosis,True
Diagnosis of Coronavirus Disease 2019 Pneumonia by Using Chest Radiography: Value of Artificial Intelligence,"Background Radiologists are proficient in differentiating between chest radiographs with and without symptoms of pneumonia but have found it more challenging to differentiate coronavirus disease 2019 (COVID-19) pneumonia from non-COVID-19 pneumonia on chest radiographs. Purpose To develop an artificial intelligence algorithm to differentiate COVID-19 pneumonia from other causes of abnormalities at chest radiography. Materials and Methods In this retrospective study, a deep neural network, CV19-Net, was trained, validated, and tested on chest radiographs in patients with and without COVID-19 pneumonia. For the chest radiographs positive for COVID-19, patients with reverse transcription polymerase chain reaction results positive for severe acute respiratory syndrome coronavirus 2 with findings positive for pneumonia between February 1, 2020, and May 30, 2020, were included. For the non-COVID-19 chest radiographs, patients with pneumonia who underwent chest radiography between October 1, 2019, and December 31, 2019, were included. Area under the receiver operating characteristic curve (AUC), sensitivity, and specificity were calculated to characterize diagnostic performance. To benchmark the performance of CV19-Net, a randomly sampled test data set composed of 500 chest radiographs in 500 patients was evaluated by the CV19-Net and three experienced thoracic radiologists. Results A total of 2060 patients (5806 chest radiographs; mean age, 62 years ± 16 [standard deviation]; 1059 men) with COVID-19 pneumonia and 3148 patients (5300 chest radiographs; mean age, 64 years ± 18; 1578 men) with non-COVID-19 pneumonia were included and split into training and validation and test data sets. For the test set, CV19-Net achieved an AUC of 0.92 (95% CI: 0.91, 0.93). This corresponded to a sensitivity of 88% (95% CI: 87, 89) and a specificity of 79% (95% CI: 77, 80) by using a high-sensitivity operating threshold, or a sensitivity of 78% (95% CI: 77, 79) and a specificity of 89% (95% CI: 88, 90) by using a high-specificity operating threshold. For the 500 sampled chest radiographs, CV19-Net achieved an AUC of 0.94 (95% CI: 0.93, 0.96) compared with an AUC of 0.85 (95% CI: 0.81, 0.88) achieved by radiologists. Conclusion CV19-Net was able to differentiate coronavirus disease 2019-related pneumonia from other types of pneumonia, with performance exceeding that of experienced thoracic radiologists. © RSNA, 2021 Online supplemental material is available for this article.",2021,10.1148/radiol.2020202944,diagnosis,False
Diagnosis of COVID-19 using CT scan images and deep learning techniques,"Early diagnosis of the coronavirus disease in 2019 (COVID-19) is essential for controlling this pandemic. COVID-19 has been spreading rapidly all over the world. There is no vaccine available for this virus yet. Fast and accurate COVID-19 screening is possible using computed tomography (CT) scan images. The deep learning techniques used in the proposed method is based on a convolutional neural network (CNN). Our manuscript focuses on differentiating the CT scan images of COVID-19 and non-COVID 19 CT using different deep learning techniques. A self-developed model named CTnet-10 was designed for the COVID-19 diagnosis, having an accuracy of 82.1%. Also, other models that we tested are DenseNet-169, VGG-16, ResNet-50, InceptionV3, and VGG-19. The VGG-19 proved to be superior with an accuracy of 94.52% as compared to all other deep learning models. Automated diagnosis of COVID-19 from the CT scan pictures can be used by the doctors as a quick and efficient method for COVID-19 screening.",2021,10.1007/s10140-020-01886-y,diagnosis,True
Diagnosis of Invasive Lung Adenocarcinoma Based on Chest CT Radiomic Features of Part-Solid Pulmonary Nodules: A Multicenter Study,"Background Solid components of part-solid nodules (PSNs) at CT are reflective of invasive adenocarcinoma, but studies describing radiomic features of PSNs and the perinodular region are lacking. Purpose To develop and to validate radiomic signatures diagnosing invasive lung adenocarcinoma in PSNs compared with the Brock, clinical-semantic features, and volumetric models. Materials and Methods This retrospective multicenter study (https://ClinicalTrials.gov, NCT03872362) included 291 patients (median age, 60 years; interquartile range, 55-65 years; 191 women) from January 2013 to October 2017 with 297 PSN lung adenocarcinomas split into training (n = 229) and test (n = 68) data sets. Radiomic features were extracted from the different regions (gross tumor volume [GTV], solid, ground-glass, and perinodular). Random-forest models were trained using clinical-semantic, volumetric, and radiomic features, and an online nodule calculator was used to compute the Brock model. Performances of models were evaluated using standard metrics such as area under the curve (AUC), accuracy, and calibration. The integrated discrimination improvement was applied to assess model performance changes after the addition of perinodular features. Results The radiomics model based on ground-glass and solid features yielded an AUC of 0.98 (95% confidence interval [CI]: 0.96, 1.00) on the test data set, which was significantly higher than the Brock (AUC, 0.83 [95% CI: 0.72, 0.94]; P = .007), clinical-semantic (AUC, 0.90 [95% CI: 0.83, 0.98]; P = .03), volumetric GTV (AUC, 0.87 [95% CI: 0.78, 0.96]; P = .008), and radiomics GTV (AUC, 0.88 [95% CI: 0.80, 0.96]; P = .01) models. It also achieved the best accuracy (93% [95% CI: 84%, 98%]). Both this model and the model with added perinodular features showed good calibration, whereas adding perinodular features did not improve the performance (integrated discrimination improvement, -0.02; P = .56). Conclusion Separating ground-glass and solid CT radiomic features of part-solid nodules was useful in diagnosing the invasiveness of lung adenocarcinoma, yielding a better predictive performance than the Brock, clinical-semantic, volumetric, and radiomics gross tumor volume models. Online supplemental material is available for this article. See also the editorial by Nishino in this issue. Published under a CC BY 4.0 license.",2020,10.1148/radiol.2020192431,diagnosis,True
Diagnostic Accuracy and Performance of Artificial Intelligence in Detecting Lung Nodules in Patients With Complex Lung Disease: A Noninferiority Study,"OBJECTIVES: The aim of the study is to investigate the performance of artificial intelligence (AI) convolutional neural networks (CNN) in detecting lung nodules on chest computed tomography of patients with complex lung disease, and demonstrate its noninferiority when compared against an experienced radiologist through clinically relevant assessments. METHODS: A CNN prototype was used to retrospectively evaluate 103 complex lung disease cases and 40 control cases without reported nodules. Computed tomography scans were blindly evaluated by an expert thoracic radiologist; a month after initial analyses, 20 positive cases were re-evaluated with the assistance of AI. For clinically relevant applications: (1) AI was asked to classify each patient into nodules present or absent and (2) AI results were compared against standard radiology reports. Standard statistics were performed to determine detection performance. RESULTS: AI was, on average, 27 seconds faster than the expert and detected 8.4% of nodules that would have been missed. AI had a sensitivity of 67.7%, similar to an accuracy reported for experienced radiologists. AI correctly classified each patient (nodules present/absent) with a sensitivity of 96.1%. When matched against radiology reports, AI performed with a sensitivity of 89.4%. Control group assessment demonstrated an overall specificity of 82.5%. When aided by AI, the expert decreased the average assessment time per case from 2:44 minutes to 35.7 seconds, while reporting an overall increase in confidence. CONCLUSION: In a group of patients with complex lung disease, the sensitivity of AI is similar to an experienced radiologist and the tool helps detect previously missed nodules. AI also helps experts analyze for lung nodules faster and more confidently, a feature that is beneficial to patients and favorable to hospitals due to increased patient load and need for shorter turnaround times.",2022,10.1097/rti.0000000000000613,diagnosis,True
Diagnostic Approach for Accurate Diagnosis of COVID-19 Employing Deep Learning and Transfer Learning Techniques through Chest X-ray Images Clinical Data in E-Healthcare,"COVID-19 is a transferable disease that is also a leading cause of death for a large number of people worldwide. This disease, caused by SARS-CoV-2, spreads very rapidly and quickly affects the respiratory system of the human being. Therefore, it is necessary to diagnosis this disease at the early stage for proper treatment, recovery, and controlling the spread. The automatic diagnosis system is significantly necessary for COVID-19 detection. To diagnose COVID-19 from chest X-ray images, employing artificial intelligence techniques based methods are more effective and could correctly diagnosis it. The existing diagnosis methods of COVID-19 have the problem of lack of accuracy to diagnosis. To handle this problem we have proposed an efficient and accurate diagnosis model for COVID-19. In the proposed method, a two-dimensional Convolutional Neural Network (2DCNN) is designed for COVID-19 recognition employing chest X-ray images. Transfer learning (TL) pre-trained ResNet-50 model weight is transferred to the 2DCNN model to enhanced the training process of the 2DCNN model and fine-tuning with chest X-ray images data for final multi-classification to diagnose COVID-19. In addition, the data augmentation technique transformation (rotation) is used to increase the data set size for effective training of the R2DCNNMC model. The experimental results demonstrated that the proposed (R2DCNNMC) model obtained high accuracy and obtained 98.12% classification accuracy on CRD data set, and 99.45% classification accuracy on CXI data set as compared to baseline methods. This approach has a high performance and could be used for COVID-19 diagnosis in E-Healthcare systems.",2021,10.3390/s21248219,diagnosis,False
Diagnostic classification of coronavirus disease 2019 (COVID-19) and other pneumonias using radiomics features in CT chest images,"We propose a classification method using the radiomics features of CT chest images to identify patients with coronavirus disease 2019 (COVID-19) and other pneumonias. The chest CT images of two groups of participants (90 COVID-19 patients who were confirmed as positive by nucleic acid test of RT-PCR and 90 other pneumonias patients) were collected, and the two groups of data were manually drawn to outline the region of interest (ROI) of pneumonias. The radiomics method was used to extract textural features and histogram features of the ROI and obtain a radiomics features vector from each sample. Then, we divided the data into two independent radiomic cohorts for training (70 COVID-19 patients and 70 other pneumonias patients), and validation (20 COVID-19 patients and 20 other pneumonias patients) by using support vector machine (SVM). This model used 20 rounds of tenfold cross-validation for training. Finally, single-shot testing of the final model was performed on the independent validation cohort. In the COVID-19 patients, correlation analysis (multiple comparison correction-Bonferroni correction, P < 0.05/7) was also conducted to determine whether the textural and histogram features were correlated with the laboratory test index of blood, i.e., blood oxygen, white blood cell, lymphocytes, neutrophils, C-reactive protein, hypersensitive C-reactive protein, and erythrocyte sedimentation rate. The final model showed good discrimination on the independent validation cohort, with an accuracy of 89.83%, sensitivity of 94.22%, specificity of 85.44%, and AUC of 0.940. This proved that the radiomics features were highly distinguishable, and this SVM model can effectively identify and diagnose patients with COVID-19 and other pneumonias. The correlation analysis results showed that some textural features were positively correlated with WBC, and NE, and also negatively related to SPO2H and NE. Our results showed that radiomic features can classify COVID-19 patients and other pneumonias patients. The SVM model can achieve an excellent diagnosis of COVID-19.",2021,10.1038/s41598-021-97497-9,diagnosis,True
Diagnostic classification of solitary pulmonary nodules using dual time (18)F-FDG PET/CT image texture features in granuloma-endemic regions,"Lung cancer, the most commonly diagnosed cancer worldwide, usually presents as solid pulmonary nodules (SPNs) on early diagnostic images. Classification of malignant disease at this early timepoint is critical for improving the success of surgical resection and increasing 5-year survival rates. (18)F-fluorodeoxyglucose ((18)F-FDG) PET/CT has demonstrated value for SPNs diagnosis with high sensitivity to detect malignant SPNs, but lower specificity in diagnosing malignant SPNs in populations with endemic infectious lung disease. This study aimed to determine whether quantitative heterogeneity derived from various texture features on dual time FDG PET/CT images (DTPI) can differentiate between malignant and benign SPNs in patients from granuloma-endemic regions. Machine learning methods were employed to find optimal discrimination between malignant and benign nodules. Machine learning models trained by texture features on DTPI images achieved significant improvements over standard clinical metrics and visual interpretation for discriminating benign from malignant SPNs, especially by texture features on delayed FDG PET/CT images.",2017,10.1038/s41598-017-08764-7,diagnosis,True
Diagnostic performance for pulmonary adenocarcinoma on CT: comparison of radiologists with and without three-dimensional convolutional neural network,"OBJECTIVES: To compare diagnostic performance for pulmonary invasive adenocarcinoma among radiologists with and without three-dimensional convolutional neural network (3D-CNN). METHODS: Enrolled were 285 patients with adenocarcinoma in situ (AIS, n = 75), minimally invasive adenocarcinoma (MIA, n = 58), and invasive adenocarcinoma (IVA, n = 152). A 3D-CNN model was constructed with seven convolution-pooling and two max-pooling layers and fully connected layers, in which batch normalization, residual connection, and global average pooling were used. Only the flipping process was performed for augmentation. The output layer comprised two nodes for two conditions (AIS/MIA and IVA) according to prognosis. Diagnostic performance of the 3D-CNN model in 285 patients was calculated using nested 10-fold cross-validation. In 90 of 285 patients, results from each radiologist (R1, R2, and R3; with 9, 14, and 26 years of experience, respectively) with and without the 3D-CNN model were statistically compared. RESULTS: Without the 3D-CNN model, accuracy, sensitivity, and specificity of the radiologists were as follows: R1, 70.0%, 52.1%, and 90.5%; R2, 72.2%, 75%, and 69%; and R3, 74.4%, 89.6%, and 57.1%, respectively. With the 3D-CNN model, accuracy, sensitivity, and specificity of the radiologists were as follows: R1, 72.2%, 77.1%, and 66.7%; R2, 74.4%, 85.4%, and 61.9%; and R3, 74.4%, 93.8%, and 52.4%, respectively. Diagnostic performance of each radiologist with and without the 3D-CNN model had no significant difference (p > 0.88), but the accuracy of R1 and R2 was significantly higher with than without the 3D-CNN model (p < 0.01). CONCLUSIONS: The 3D-CNN model can support a less-experienced radiologist to improve diagnostic accuracy for pulmonary invasive adenocarcinoma without deteriorating any diagnostic performances. KEY POINTS: • The 3D-CNN model is a non-invasive method for predicting pulmonary invasive adenocarcinoma in CT images with high sensitivity. • Diagnostic accuracy by a less-experienced radiologist was better with the 3D-CNN model than without the model.",2021,10.1007/s00330-020-07339-x,diagnosis,True
Diagnostic performance of artificial intelligence model for pneumonia from chest radiography,"OBJECTIVE: The chest X-ray (CXR) is the most readily available and common imaging modality for the assessment of pneumonia. However, detecting pneumonia from chest radiography is a challenging task, even for experienced radiologists. An artificial intelligence (AI) model might help to diagnose pneumonia from CXR more quickly and accurately. We aim to develop an AI model for pneumonia from CXR images and to evaluate diagnostic performance with external dataset. METHODS: To train the pneumonia model, a total of 157,016 CXR images from the National Institutes of Health (NIH) and the Korean National Tuberculosis Association (KNTA) were used (normal vs. pneumonia = 120,722 vs.36,294). An ensemble model of two neural networks with DenseNet classifies each CXR image into pneumonia or not. To test the accuracy of the models, a separate external dataset of pneumonia CXR images (n = 212) from a tertiary university hospital (Gachon University Gil Medical Center GUGMC, Incheon, South Korea) was used; the diagnosis of pneumonia was based on both the chest CT findings and clinical information, and the performance evaluated using the area under the receiver operating characteristic curve (AUC). Moreover, we tested the change of the AI probability score for pneumonia using the follow-up CXR images (7 days after the diagnosis of pneumonia, n = 100). RESULTS: When the probability scores of the models that have a threshold of 0.5 for pneumonia, two models (models 1 and 4) having different pre-processing parameters on the histogram equalization distribution showed best AUC performances of 0.973 and 0.960, respectively. As expected, the ensemble model of these two models performed better than each of the classification models with 0.983 AUC. Furthermore, the AI probability score change for pneumonia showed a significant difference between improved cases and aggravated cases (Δ = -0.06 ± 0.14 vs. 0.06 ± 0.09, for 85 improved cases and 15 aggravated cases, respectively, P = 0.001) for CXR taken as a 7-day follow-up. CONCLUSIONS: The ensemble model combined two different classification models for pneumonia that performed at 0.983 AUC for an external test dataset from a completely different data source. Furthermore, AI probability scores showed significant changes between cases of different clinical prognosis, which suggest the possibility of increased efficiency and performance of the CXR reading at the diagnosis and follow-up evaluation for pneumonia.",2021,10.1371/journal.pone.0249399,diagnosis,False
Diagnostic Value of Deep Learning-Based CT Feature for Severe Pulmonary Infection,"The study aimed to explore the diagnostic value of computed tomography (CT) images based on cavity convolution U-Net algorithm for patients with severe pulmonary infection. A new lung CT image segmentation algorithm (U-Net+ deep convolution (DC)) was proposed based on U-Net network and compared with convolutional neural network (CNN) algorithm. Then, it was applied to CT image diagnosis of 100 patients with severe lung infection in The Second Affiliated Hospital of Fujian Medical University hospital and compared with traditional methods, and its sensitivity, specificity, and accuracy were compared. It was found that the single training time and loss of U-Net + DC algorithm were reduced by 59.4% and 9.8%, respectively, compared with CNN algorithm, while Dice increased by 3.6%. The lung contour segmented by the proposed model was smooth, which was the closest to the gold standard. Fungal infection, bacterial infection, viral infection, tuberculosis infection, and mixed infection accounted for 28%, 18%, 7%, 7%, and 40%, respectively. 36%, 38%, 26%, 17%, and 20% of the patients had ground-glass shadow, solid shadow, nodule or mass shadow, reticular or linear shadow, and hollow shadow in CT, respectively. The incidence of various CT characteristics in patients with fungal and bacterial infections was statistically significant (P < 0.05). The specificity (94.32%) and accuracy (97.22%) of CT image diagnosis based on U-Net + DC algorithm were significantly higher than traditional diagnostic method (75.74% and 74.23%), and the differences were statistically significant (P < 0.05). The network of the algorithm in this study demonstrated excellent image segmentation effect. The CT image based on the U-Net + DC algorithm can be used for the diagnosis of patients with severe pulmonary infection, with high diagnostic value.",2021,10.1155/2021/5359084,prognosis,True
Different CT slice thickness and contrast-enhancement phase in radiomics models on the differential performance of lung adenocarcinoma,"BACKGROUND: To investigate the effects of computed tomography (CT) reconstruction slice thickness and contrast-enhancement phase on the differential diagnosis performance of radiomic signature in lung adenocarcinoma. METHODS: A total of 187 patients who had been pathologically confirmed with lung adenocarcinoma and nonadenocarcinoma were divided into a training cohort (n = 149) and validation cohort (n = 38). All the patients underwent contrast-enhanced CT and the images were reconstructed with different slice thickness. The radiomic features were extracted from different slice thickness and scan phase. The logistic regression (LR) algorithm was used to build a machine learning model for each group. The area under the curve (AUC) obtained from the receiver operating characteristic (ROC) curve and DeLong test was used to evaluate its discriminating performance. RESULTS: Finally, 34 image features and five semantic features were selected to establish a radiomics model. Based on the three contrast-enhanced CT phases and four reconstruction slice thickness, 12 groups of radiomics models showed good discrimination ability with the AUCs range from 0.9287 to 0.9631, sensitivity range from 0.8349 to 0.9083, specificity range from 0.825 to 0.925 in the training group. Similar results were observed in the validation group. However, there was no statistical significance between the different CT scan phase groups and different slice thickness (p > 0.05). CONCLUSIONS: The radiomic analysis of contrast-enhanced CT can be used for the differential diagnosis of lung adenocarcinoma. Moreover, different slice thickness and contrast-enhanced scan phase did not affect the discriminating ability in the radiomics models.",2022,10.1111/1759-7714.14459,diagnosis,True
Differentiating Central Lung Tumors from Atelectasis with Contrast-Enhanced CT-Based Radiomics Features,"OBJECTIVES: To evaluate the utility of radiomics features in differentiating central lung cancers and atelectasis on contrast-enhanced computed tomography (CT) images. This study is retrospective. MATERIALS AND METHODS: In this study, 36 patients with central pulmonary cancer and atelectasis between July 2013 and June 2018 were identified. A total of 1,653 2D and 2,327 3D radiomics features were extracted from segmented lung cancers and atelectasis on contrast-enhanced CT. The refined features were investigated for usefulness in classifying lung cancer and atelectasis according to the information gain, and 10 models were trained based on these features. The classification model is trained and tested at the region level and pixel level, respectively. RESULTS: Among all the extracted features, 334 2D features and 1,507 3D features had an information gain (IG) greater than 0.1. The highest accuracy (AC) of the region classifiers was 0.9375. The best Dice score, Hausdorff distance, and voxel AC were 0.2076, 45.28, and 0.8675, respectively. CONCLUSIONS: Radiomics features derived from contrast-enhanced CT images can differentiate lung cancers and atelectasis at the regional and voxel levels.",2021,10.1155/2021/5522452,diagnosis,True
Differentiation Between Anteroposterior and Posteroanterior Chest X-Ray View Position With Convolutional Neural Networks,"PURPOSE: Detection and validation of the chest X-ray view position with use of convolutional neural networks to improve meta-information for data cleaning within a hospital data infrastructure. MATERIAL AND METHODS: Within this paper we developed a convolutional neural network which automatically detects the anteroposterior and posteroanterior view position of a chest radiograph. We trained two different network architectures (VGG variant and ResNet-34) with data published by the RSNA (26 684 radiographs, class distribution 46 % AP, 54 % PA) and validated these on a self-compiled dataset with data from the University Hospital Essen (4507, radiographs, class distribution 55 % PA, 45 % AP) labeled by a human reader. For visualization and better understanding of the network predictions, a Grad-CAM was generated for each network decision. The network results were evaluated based on the accuracy, the area under the curve (AUC), and the F1-score against the human reader labels. Also a final performance comparison between model predictions and DICOM labels was performed. RESULTS: The ensemble models reached accuracy and F1-scores greater than 95 %. The AUC reaches more than 0.99 for the ensemble models. The Grad-CAMs provide insight as to which anatomical structures contributed to a decision by the networks which are comparable with the ones a radiologist would use. Furthermore, the trained models were able to generalize over mislabeled examples, which was found by comparing the human reader labels to the predicted labels as well as the DICOM labels. CONCLUSION: The results show that certain incorrectly entered meta-information of radiological images can be effectively corrected by deep learning in order to increase data quality in clinical application as well as in research. KEY POINTS: · The predictions for both view positions are accurate with respect to external validation data.. · The networks based their decisions on anatomical structures and key points that were in-line with prior knowledge and human understanding.. · Final models were able to detect labeling errors within the test dataset.. CITATION FORMAT: · Hosch R, Kroll L, Nensa F et al. Differentiation Between Anteroposterior and Posteroanterior Chest X-Ray View Position With Convolutional Neural Networks. Fortschr Röntgenstr 2021; 193: 168 - 176.",2021,10.1055/a-1183-5227,diagnosis,False
Differentiation between immune checkpoint inhibitor-related and radiation pneumonitis in lung cancer by CT radiomics and machine learning,"PURPOSE: Consolidation immunotherapy after completion of chemoradiotherapy has become the standard of care for unresectable locally advanced non-small cell lung cancer and can induce potentially severe and life-threatening adverse events, including both immune checkpoint inhibitor-related pneumonitis (CIP) and radiation pneumonitis (RP), which are very challenging for radiologists to diagnose. Differentiating between CIP and RP has significant implications for clinical management such as the treatments for pneumonitis and the decision to continue or restart immunotherapy. The purpose of this study is to differentiate between CIP and RP by a CT radiomics approach. METHODS: We retrospectively collected the CT images and clinical information of patients with pneumonitis who received immune checkpoint inhibitor (ICI) only (n = 28), radiotherapy (RT) only (n = 31), and ICI+RT (n = 14). Three kinds of radiomic features (intensity histogram, gray-level co-occurrence matrix [GLCM] based, and bag-of-words [BoW] features) were extracted from CT images, which characterize tissue texture at different scales. Classification models, including logistic regression, random forest, and linear SVM, were first developed and tested in patients who received ICI or RT only with 10-fold cross-validation and further tested in patients who received ICI+RT using clinicians' diagnosis as a reference. RESULTS: Using 10-fold cross-validation, the classification models built on the intensity histogram features, GLCM-based features, and BoW features achieved an area under curve (AUC) of 0.765, 0.848, and 0.937, respectively. The best model was then applied to the patients receiving combination treatment, achieving an AUC of 0.896. CONCLUSIONS: This study demonstrates the promising potential of radiomic analysis of CT images for differentiating between CIP and RP in lung cancer, which could be a useful tool to attribute the cause of pneumonitis in patients who receive both ICI and RT.",2022,10.1002/mp.15451,diagnosis,True
Differentiation of Benign from Malignant Pulmonary Nodules by Using a Convolutional Neural Network to Determine Volume Change at Chest CT,"Background Deep learning may help to improve computer-aided detection of volume (CADv) measurement of pulmonary nodules at chest CT. Purpose To determine the efficacy of a deep learning method for improving CADv for measuring the solid and ground-glass opacity (GGO) volumes of a nodule, doubling time (DT), and the change in volume at chest CT. Materials and Methods From January 2014 to December 2016, patients with pulmonary nodules at CT were retrospectively reviewed. CADv without and with a convolutional neural network (CNN) automatically determined total nodule volume change per day and DT. Area under the curves (AUCs) on a per-nodule basis and diagnostic accuracy on a per-patient basis were compared among all indexes from CADv with and without CNN for differentiating benign from malignant nodules. Results The CNN training set was 294 nodules in 217 patients, the validation set was 41 nodules in 32 validation patients, and the test set was 290 nodules in 188 patients. A total of 170 patients had 290 nodules (mean size ± standard deviation, 11 mm ± 5; range, 4-29 mm) diagnosed as 132 malignant nodules and 158 benign nodules. There were 132 solid nodules (46%), 106 part-solid nodules (36%), and 52 ground-glass nodules (18%). The test set results showed that the diagnostic performance of the CNN with CADv for total nodule volume change per day was larger than DT of CADv with CNN (AUC, 0.94 [95% confidence interval {CI}: 0.90, 0.96] vs 0.67 [95% CI: 0.60, 0.74]; P < .001) and CADv without CNN (total nodule volume change per day: AUC, 0.69 [95% CI: 0.62, 0.75]; P < .001; DT: AUC, 0.58 [95% CI: 0.51, 0.65]; P < .001). The accuracy of total nodule volume change per day of CADv with CNN was significantly higher than that of CADv without CNN (P < .001) and DT of both methods (P < .001). Conclusion Convolutional neural network is useful for improving accuracy of computer-aided detection of volume measurement and nodule differentiation capability at CT for patients with pulmonary nodules. © RSNA, 2020 Online supplemental material is available for this article.",2020,10.1148/radiol.2020191740,diagnosis,True
Discrimination between transient and persistent subsolid pulmonary nodules on baseline CT using deep transfer learning,"OBJECTIVES: To develop and validate a deep learning model to discriminate transient from persistent subsolid nodules (SSNs) on baseline CT. METHODS: A cohort of 1414 SSNs, consisting of 319 transient SSNs in 168 individuals and 1095 persistent SSNs in 816 individuals, were identified on chest CT. The cohort was assigned by examination date into a development set of 996 SSNs, a tuning set of 212 SSNs, and a validation set of 206 SSNs. Our model was built by transfer learning, which was transferred from a well-performed deep learning model for pulmonary nodule classification. The performance of the model was compared with that of two experienced radiologists. Each nodule was categorized by Lung CT Screening Reporting and Data System (Lung-RADS) to further evaluate the performance and the potential clinical benefit of the model. Two methods were employed to visualize the learned features. RESULTS: Our model achieved an AUC of 0.926 on the validation set with an accuracy of 0.859, a sensitivity of 0.863, and a specificity of 0.858, and outperformed the radiologists. The model performed the best among Lung-RADS 2 nodules and maintained well performance among Lung-RADS 4 nodules. Feature visualization demonstrated the model's effectiveness in extracting features from images. CONCLUSIONS: The transfer learning model presented good performance on the discrimination between transient and persistent SSNs. A reliable diagnosis on nodule persistence can be achieved at baseline CT; thus, an early diagnosis as well as better patient care is available. KEY POINTS: • Deep learning can be used for the discrimination between transient and persistent subsolid nodules. • A transfer learning model can achieve good performance when it is transferred from a model with a similar task. • With the assistance of deep learning model, a reliable diagnosis on nodule persistence can be achieved at baseline CT, which can bring a better patient care strategy.",2020,10.1007/s00330-020-07071-6,prognosis,True
Disease Progression Detection via Deep Sequence Learning of Successive Radiographic Scans,"The highly rapid spread of the current pandemic has quickly overwhelmed hospitals all over the world and motivated extensive research to address a wide range of emerging problems. The unforeseen influx of COVID-19 patients to hospitals has made it inevitable to deploy a rapid and accurate triage system, monitor progression, and predict patients at higher risk of deterioration in order to make informed decisions regarding hospital resource management. Disease detection in radiographic scans, severity estimation, and progression and prognosis prediction have been extensively studied with the help of end-to-end methods based on deep learning. The majority of recent works have utilized a single scan to determine severity or predict progression of the disease. In this paper, we present a method based on deep sequence learning to predict improvement or deterioration in successive chest X-ray scans and build a mathematical model to determine individual patient disease progression profile using successive scans. A deep convolutional neural network pretrained on a diverse lung disease dataset was used as a feature extractor to generate the sequences. We devised three strategies for sequence modeling in order to obtain both fine-grained and coarse-grained features and construct sequences of different lengths. We also devised a strategy to quantify positive or negative change in successive scans, which was then combined with age-related risk factors to construct disease progression profile for COVID-19 patients. The age-related risk factors allowed us to model rapid deterioration and slower recovery in older patients. Experiments conducted on two large datasets showed that the proposed method could accurately predict disease progression. With the best feature extractor, the proposed method was able to achieve AUC of 0.98 with the features obtained from radiographs. Furthermore, the proposed patient profiling method accurately estimated the health profile of patients.",2022,10.3390/ijerph19010480,prognosis,False
Disease Staging and Prognosis in Smokers Using Deep Learning in Chest Computed Tomography,"RATIONALE: Deep learning is a powerful tool that may allow for improved outcome prediction. OBJECTIVES: To determine if deep learning, specifically convolutional neural network (CNN) analysis, could detect and stage chronic obstructive pulmonary disease (COPD) and predict acute respiratory disease (ARD) events and mortality in smokers. METHODS: A CNN was trained using computed tomography scans from 7,983 COPDGene participants and evaluated using 1,000 nonoverlapping COPDGene participants and 1,672 ECLIPSE participants. Logistic regression (C statistic and the Hosmer-Lemeshow test) was used to assess COPD diagnosis and ARD prediction. Cox regression (C index and the Greenwood-Nam-D'Agnostino test) was used to assess mortality. MEASUREMENTS AND MAIN RESULTS: In COPDGene, the C statistic for the detection of COPD was 0.856. A total of 51.1% of participants in COPDGene were accurately staged and 74.95% were within one stage. In ECLIPSE, 29.4% were accurately staged and 74.6% were within one stage. In COPDGene and ECLIPSE, the C statistics for ARD events were 0.64 and 0.55, respectively, and the Hosmer-Lemeshow P values were 0.502 and 0.380, respectively, suggesting no evidence of poor calibration. In COPDGene and ECLIPSE, CNN predicted mortality with fair discrimination (C indices, 0.72 and 0.60, respectively), and without evidence of poor calibration (Greenwood-Nam-D'Agnostino P values, 0.307 and 0.331, respectively). CONCLUSIONS: A deep-learning approach that uses only computed tomography imaging data can identify those smokers who have COPD and predict who are most likely to have ARD events and those with the highest mortality. At a population level CNN analysis may be a powerful tool for risk assessment.",2018,10.1164/rccm.201705-0860OC,prognosis,True
Distinguishing granulomas from adenocarcinomas by integrating stable and discriminating radiomic features on non-contrast computed tomography scans,"OBJECTIVE: To identify stable and discriminating radiomic features on non-contrast CT scans to develop more generalisable radiomic classifiers for distinguishing granulomas from adenocarcinomas. METHODS: In total, 412 patients with adenocarcinomas and granulomas from three institutions were retrospectively included. Segmentations of the lung nodules were performed manually by an expert radiologist in a 2D axial view. Radiomic features were extracted from intra- and perinodular regions. A total of 145 patients were used as part of the training set (S(tr)), whereas 205 patients were used as part of test set I (S(te)(1)) and 62 patients were used as part of independent test set II (S(te)(2)). To mitigate the variation of CT acquisition parameters, we defined 'stable' radiomic features as those for which the feature expression remains relatively unchanged between different sites, as assessed using a Wilcoxon rank-sum test. These stable features were used to develop more generalisable radiomic classifiers that were more resilient to variations in lung CT scans. Features were ranked based on two criteria, firstly based on discriminability (i.e. maximising AUC) alone and subsequently based on maximising both feature stability and discriminability. Different machine-learning classifiers (Linear discriminant analysis, Quadratic discriminant analysis, Support vector machines and random forest) were trained with features selected using the two different criteria and then compared on the two independent test sets for distinguishing granulomas from adenocarcinomas, in terms of area under the receiver operating characteristic curve. RESULTS: In the test sets, classifiers constructed using the criteria involving maximising feature stability and discriminability simultaneously achieved higher AUC compared with the discriminating alone criteria (S(te)(1) [n = 205]: maximum AUCs of 0.85versus . 0.80; p-value = 0.047 and S(te)(2) [n = 62]: maximum AUCs of 0.87 versus. 0.79; p-value = 0.021). These differences held for features extracted from scans with <3 mm slice thickness (AUC = 0.88 versus. 0.80; p-value = 0.039, n = 100) and for the ≥3 mm cases (AUC = 0.81 versus. 0.76; p-value = 0.034, n = 105). In both experiments, shape and peritumoural texture features had a higher stability compared with intratumoural texture features. CONCLUSIONS: Our study suggests that explicitly accounting for both stability and discriminability results in more generalisable radiomic classifiers to distinguish adenocarcinomas from granulomas on non-contrast CT scans. Our results also showed that peritumoural texture and shape features were less affected by the scanner parameters compared with intratumoural texture features; however, they were also less discriminating compared with intratumoural features.",2021,10.1016/j.ejca.2021.02.008,diagnosis,True
Does non-COVID-19 lung lesion help? investigating transferability in COVID-19 CT image segmentation,"BACKGROUND AND OBJECTIVE: Coronavirus disease 2019 (COVID-19) is a highly contagious virus spreading all around the world. Deep learning has been adopted as an effective technique to aid COVID-19 detection and segmentation from computed tomography (CT) images. The major challenge lies in the inadequate public COVID-19 datasets. Recently, transfer learning has become a widely used technique that leverages the knowledge gained while solving one problem and applying it to a different but related problem. However, it remains unclear whether various non-COVID19 lung lesions could contribute to segmenting COVID-19 infection areas and how to better conduct this transfer procedure. This paper provides a way to understand the transferability of non-COVID19 lung lesions and a better strategy to train a robust deep learning model for COVID-19 infection segmentation. METHODS: Based on a publicly available COVID-19 CT dataset and three public non-COVID19 datasets, we evaluate four transfer learning methods using 3D U-Net as a standard encoder-decoder method. i) We introduce the multi-task learning method to get a multi-lesion pre-trained model for COVID-19 infection. ii) We propose and compare four transfer learning strategies with various performance gains and training time costs. Our proposed Hybrid-encoder Learning strategy introduces a Dedicated-encoder and an Adapted-encoder to extract COVID-19 infection features and general lung lesion features, respectively. An attention-based Selective Fusion unit is designed for dynamic feature selection and aggregation. RESULTS: Experiments show that trained with limited data, proposed Hybrid-encoder strategy based on multi-lesion pre-trained model achieves a mean DSC, NSD, Sensitivity, F1-score, Accuracy and MCC of 0.704, 0.735, 0.682, 0.707, 0.994 and 0.716, respectively, with better genetalization and lower over-fitting risks for segmenting COVID-19 infection. CONCLUSIONS: The results reveal the benefits of transferring knowledge from non-COVID19 lung lesions, and learning from multiple lung lesion datasets can extract more general features, leading to accurate and robust pre-trained models. We further show the capability of the encoder to learn feature representations of lung lesions, which improves segmentation accuracy and facilitates training convergence. In addition, our proposed Hybrid-encoder learning method incorporates transferred lung lesion features from non-COVID19 datasets effectively and achieves significant improvement. These findings promote new insights into transfer learning for COVID-19 CT image segmentation, which can also be further generalized to other medical tasks.",2021,10.1016/j.cmpb.2021.106004,diagnosis,True
Dosimetric Factors and Radiomics Features Within Different Regions of Interest in Planning CT Images for Improving the Prediction of Radiation Pneumonitis,"PURPOSE: This study aimed to establish machine learning models using dosimetric factors and radiomics features within 5 regions of interest (ROIs) in treatment planning computed tomography images to improve the prediction of symptomatic radiation pneumonitis (RP) (grade ≥2). METHODS AND MATERIALS: This study retrospectively collected data on 79 patients with lung cancer (25 RP ≥2) who underwent chemoradiotherapy between 2015 and 2018. We defined 5 ROIs in planning computed tomography images: gross tumor volume (GTV), planning tumor volume (PTV), PTV-GTV, total lung (TL)-GTV, and TL-PTV. We calculated the mean dose, V5, V10, V20, and V30 within TL-GTV and TL-PTV and the mean dose within the other ROIs. A total of 1924 radiomics features were extracted from all 5 ROIs. We selected the best predictors for classifying 2 groups of patients using a sequential backward elimination support vector machine model. A permutation test was used to assess its statistical significance (P < .05). RESULTS: The best predictors for symptomatic RP were the combination of 11 radiomics features, 5 dosimetric factors, age, and T stage, achieving an area under the curve (AUC) of 0.94 (95% confidence interval [CI], 0.85-1) (accuracy, 90%; sensitivity, 80% [95% CI, 44%-96%]; specificity, 95% [95% CI, 73%-100%]; P = 8 × 10(-4)). The clinical characteristics, dosimetric factors, and their combination showed limited predictive power (accuracy, 63.3%, 70%, and 70%; AUC [95% CI]: 0.73 [0.54-0.92], 0.53 [0.31-0.75], and 0.72 [0.51-0.92], respectively). The radiomics features of PTV-GTV and TL-PTV outperformed those of the other ROIs (accuracy, 76.7% and 76.7%; AUC [95% CI]: 0.82 [0.65-0.99] and 0.80 [0.59-1], respectively). CONCLUSIONS: Combining dosimetric factors and radiomics features within different ROIs can improve the prediction of symptomatic RP. Our results can help physicians adjust the radiation dose distribution of the dose-sensitive lungs and target volumes based on personalized RP estimates.",2021,10.1016/j.ijrobp.2021.01.049,prognosis,True
Dosimetric Study of Deep Learning-Guided ITV Prediction in Cone-beam CT for Lung Stereotactic Body Radiotherapy,"PURPOSE: The purpose of this study was to evaluate the accuracy of a lung stereotactic body radiotherapy (SBRT) treatment plan with the target of a newly predicted internal target volume (ITV(predict)) and the feasibility of its clinical application. ITV(predict) was automatically generated by our in-house deep learning model according to the cone-beam CT (CBCT) image database. METHOD: A retrospective study of 45 patients who underwent SBRT was involved, and Mask R-CNN based algorithm model helped to predict the internal target volume (ITV) using the CBCT image database. The geometric accuracy of ITV(predict) was verified by the Dice Similarity Coefficient (DSC), 3D Motion Range (R(3D)), Relative Volume Index (RVI), and Hausdorff Distance (HD). The PTV(predict) was generated by ITV(predict), which was registered and then projected on free-breath CT (FBCT) images. The PTV(FBCT) was margined from the GTV on FBCT images gross tumor volume on free-breath CT (GTV(FBCT)). Treatment plans with the target of Predict planning target volume on CBCT images (PTV(predict)) and planning target volume on free-breath CT (PTV(FBCT)) were respectively re-established, and the dosimetric parameters included the ratio of the volume of patients receiving at least the prescribed dose to the volume of PTV (R(100%)), the ratio of the volume of patients receiving at least 50% of the prescribed dose to the volume of PTV in the Radiation Therapy Oncology Group (RTOG) 0813 Trial (R(50%)), Gradient Index (GI), and the maximum dose 2 cm from the PTV (D(2cm)), which were evaluated via Plan(4DCT), plan which based on PTV(predict) (Plan(predict)), and plan which based on PTV(FBCT) (Plan(FBCT)). RESULT: The geometric results showed that there existed a good correlation between ITV(predict) and ITV on the 4-dimensional CT [ITV(4DCT); DSC= 0.83 ±0.18]. However, the average volume of ITV(predict) was 10% less than that of ITV(4DCT) (p = 0.333). No significant difference in dose coverage was found in V(100%) for the ITV with 99.98 ± 0.04% in the ITV(4DCT) vs. 97.56 ± 4.71% in the ITV(predict) (p = 0.162). Dosimetry parameters of PTV, including R(100%), R(50%), GI and D(2cm) showed no statistically significant difference between each plan (p > 0.05). CONCLUSION: Dosimetric parameters of Plan(predict) are clinically comparable to those of the original Plan(4DCT.) This study confirmed that the treatment plan based on ITV(predict) produced by our model could automatically meet clinical requirements. Thus, for patients undergoing lung SBRT, the model has great potential for using CBCT images for ITV contouring which can be used in treatment planning.",2022,10.3389/fpubh.2022.860135,treatment,True
DR-MIL: deep represented multiple instance learning distinguishes COVID-19 from community-acquired pneumonia in CT images,"BACKGROUND AND OBJECTIVE: Given that the novel coronavirus disease 2019 (COVID-19) has become a pandemic, a method to accurately distinguish COVID-19 from community-acquired pneumonia (CAP) is urgently needed. However, the spatial uncertainty and morphological diversity of COVID-19 lesions in the lungs, and subtle differences with respect to CAP, make differential diagnosis non-trivial. METHODS: We propose a deep represented multiple instance learning (DR-MIL) method to fulfill this task. A 3D volumetric CT scan of one patient is treated as one bag and ten CT slices are selected as the initial instances. For each instance, deep features are extracted from the pre-trained ResNet-50 with fine-tuning and represented as one deep represented instance score (DRIS). Each bag with a DRIS for each initial instance is then input into a citation k-nearest neighbor search to generate the final prediction. A total of 141 COVID-19 and 100 CAP CT scans were used. The performance of DR-MIL is compared with other potential strategies and state-of-the-art models. RESULTS: DR-MIL displayed an accuracy of 95% and an area under curve of 0.943, which were superior to those observed for comparable methods. COVID-19 and CAP exhibited significant differences in both the DRIS and the spatial pattern of lesions (p<0.001). As a means of content-based image retrieval, DR-MIL can identify images used as key instances, references, and citers for visual interpretation. CONCLUSIONS: DR-MIL can effectively represent the deep characteristics of COVID-19 lesions in CT images and accurately distinguish COVID-19 from CAP in a weakly supervised manner. The resulting DRIS is a useful supplement to visual interpretation of the spatial pattern of lesions when screening for COVID-19.",2021,10.1016/j.cmpb.2021.106406,diagnosis,True
Drawing insights from COVID-19-infected patients using CT scan images and machine learning techniques: a study on 200 patients,"As the whole world is witnessing what novel coronavirus (COVID-19) can do to the mankind, it presents several unique features also. In the absence of specific vaccine for COVID-19, it is essential to detect the disease at an early stage and isolate an infected patient. Till today there is a global shortage of testing labs and testing kits for COVID-19. This paper discusses about the role of machine learning techniques for getting important insights like whether lung computed tomography (CT) scan should be the first screening/alternative test for real-time reverse transcriptase-polymerase chain reaction (RT-PCR), is COVID-19 pneumonia different from other viral pneumonia and if yes how to distinguish it using lung CT scan images from the carefully selected data of lung CT scan COVID-19-infected patients from the hospitals of Italy, China, Moscow and India? For training and testing the proposed system, custom vision software of Microsoft azure based on machine learning techniques is used. An overall accuracy of almost 91% is achieved for COVID-19 classification using the proposed methodology.",2020,10.1007/s11356-020-10133-3,diagnosis,True
Dual attention multiple instance learning with unsupervised complementary loss for COVID-19 screening,"Chest computed tomography (CT) based analysis and diagnosis of the Coronavirus Disease 2019 (COVID-19) plays a key role in combating the outbreak of the pandemic that has rapidly spread worldwide. To date, the disease has infected more than 18 million people with over 690k deaths reported. Reverse transcription polymerase chain reaction (RT-PCR) is the current gold standard for clinical diagnosis but may produce false positives; thus, chest CT based diagnosis is considered more viable. However, accurate screening is challenging due to the difficulty in annotation of infected areas, curation of large datasets, and the slight discrepancies between COVID-19 and other viral pneumonia. In this study, we propose an attention-based end-to-end weakly supervised framework for the rapid diagnosis of COVID-19 and bacterial pneumonia based on multiple instance learning (MIL). We further incorporate unsupervised contrastive learning for improved accuracy with attention applied both in spatial and latent contexts, herein we propose Dual Attention Contrastive based MIL (DA-CMIL). DA-CMIL takes as input several patient CT slices (considered as bag of instances) and outputs a single label. Attention based pooling is applied to implicitly select key slices in the latent space, whereas spatial attention learns slice spatial context for interpretable diagnosis. A contrastive loss is applied at the instance level to encode similarity of features from the same patient against representative pooled patient features. Empirical results show that our algorithm achieves an overall accuracy of 98.6% and an AUC of 98.4%. Moreover, ablation studies show the benefit of contrastive learning with MIL.",2021,10.1016/j.media.2021.102105,diagnosis,True
Dual energy CT image prediction on primary tumor of lung cancer for nodal metastasis using deep learning,"Lymph node metastasis (LNM) identification is the most clinically important tasks related to survival and recurrence from lung cancer. However, the preoperative prediction of nodal metastasis remains a challenge to determine surgical plans and pretreatment decisions in patients with cancers. We proposed a novel deep prediction method with a size-related damper block for nodal metastasis (Nmet) identification from the primary tumor in lung cancer generated by gemstone spectral imaging (GSI) dual-energy computer tomography (CT). The best model is the proposed method trained by the 40 keV dataset achieves an accuracy of 86 % and a Kappa value of 72 % for Nmet prediction. In the experiment, we have 11 different monochromatic images from 40∼140 keV (the interval is 10 keV) for each patient. When we used the model of 40 keV dataset, there has significant difference in other energy levels (unit of keV). Therefore, we apply in 5-fold cross-validation to explain the lower keV is more efficient to predict Nmet of the primary tumor. The result shows that tumor heterogeneity and size contributed to the proposed model to estimate whether absence or presence of nodal metastasis from the primary tumor.",2021,10.1016/j.compmedimag.2021.101935,prognosis,True
Dual-branch combination network (DCN): Towards accurate diagnosis and lesion segmentation of COVID-19 using CT images,"The recent global outbreak and spread of coronavirus disease (COVID-19) makes it an imperative to develop accurate and efficient diagnostic tools for the disease as medical resources are getting increasingly constrained. Artificial intelligence (AI)-aided tools have exhibited desirable potential; for example, chest computed tomography (CT) has been demonstrated to play a major role in the diagnosis and evaluation of COVID-19. However, developing a CT-based AI diagnostic system for the disease detection has faced considerable challenges, which is mainly due to the lack of adequate manually-delineated samples for training, as well as the requirement of sufficient sensitivity to subtle lesions in the early infection stages. In this study, we developed a dual-branch combination network (DCN) for COVID-19 diagnosis that can simultaneously achieve individual-level classification and lesion segmentation. To focus the classification branch more intensively on the lesion areas, a novel lesion attention module was developed to integrate the intermediate segmentation results. Furthermore, to manage the potential influence of different imaging parameters from individual facilities, a slice probability mapping method was proposed to learn the transformation from slice-level to individual-level classification. We conducted experiments on a large dataset of 1202 subjects from ten institutes in China. The results demonstrated that 1) the proposed DCN attained a classification accuracy of 96.74% on the internal dataset and 92.87% on the external validation dataset, thereby outperforming other models; 2) DCN obtained comparable performance with fewer samples and exhibited higher sensitivity, especially in subtle lesion detection; and 3) DCN provided good interpretability on the loci of infection compared to other deep models due to its classification guided by high-level semantic information. An online CT-based diagnostic platform for COVID-19 derived from our proposed framework is now available.",2021,10.1016/j.media.2020.101836,diagnosis,True
Dual-Sampling Attention Network for Diagnosis of COVID-19 From Community Acquired Pneumonia,"The coronavirus disease (COVID-19) is rapidly spreading all over the world, and has infected more than 1,436,000 people in more than 200 countries and territories as of April 9, 2020. Detecting COVID-19 at early stage is essential to deliver proper healthcare to the patients and also to protect the uninfected population. To this end, we develop a dual-sampling attention network to automatically diagnose COVID-19 from the community acquired pneumonia (CAP) in chest computed tomography (CT). In particular, we propose a novel online attention module with a 3D convolutional network (CNN) to focus on the infection regions in lungs when making decisions of diagnoses. Note that there exists imbalanced distribution of the sizes of the infection regions between COVID-19 and CAP, partially due to fast progress of COVID-19 after symptom onset. Therefore, we develop a dual-sampling strategy to mitigate the imbalanced learning. Our method is evaluated (to our best knowledge) upon the largest multi-center CT data for COVID-19 from 8 hospitals. In the training-validation stage, we collect 2186 CT scans from 1588 patients for a 5-fold cross-validation. In the testing stage, we employ another independent large-scale testing dataset including 2796 CT scans from 2057 patients. Results show that our algorithm can identify the COVID-19 images with the area under the receiver operating characteristic curve (AUC) value of 0.944, accuracy of 87.5%, sensitivity of 86.9%, specificity of 90.1%, and F1-score of 82.0%. With this performance, the proposed algorithm could potentially aid radiologists with COVID-19 diagnosis from CAP, especially in the early stage of the COVID-19 outbreak.",2020,10.1109/tmi.2020.2995508,diagnosis,True
DUDA-Net: a double U-shaped dilated attention network for automatic infection area segmentation in COVID-19 lung CT images,"PURPOSE: The global health crisis caused by coronavirus disease 2019 (COVID-19) is a common threat facing all humankind. In the process of diagnosing COVID-19 and treating patients, automatic COVID-19 lesion segmentation from computed tomography images helps doctors and patients intuitively understand lung infection. To effectively quantify lung infections, a convolutional neural network for automatic lung infection segmentation based on deep learning is proposed. METHOD: This new type of COVID-19 lesion segmentation network is based on a U-Net backbone. First, a coarse segmentation network is constructed to extract the lung areas. Second, in the encoding and decoding process of the fine segmentation network, a new soft attention mechanism, namely the dilated convolutional attention (DCA) mechanism, is introduced to enable the network to focus on better quantitative information to strengthen the network's segmentation ability in the subtle areas of the lesions. RESULTS: The experimental results show that the average Dice similarity coefficient (DSC), sensitivity (SEN), specificity (SPE) and area under the curve of DUDA-Net are 87.06%, 90.85%, 99.59% and 0.965, respectively. In addition, the introduction of a cascade U-shaped network scheme and DCA mechanism can improve the DSC by 24.46% and 14.33%, respectively. CONCLUSION: The proposed DUDA-Net approach can automatically segment COVID-19 lesions with excellent performance, which indicates that the proposed method is of great clinical significance. In addition, the introduction of a coarse segmentation network and DCA mechanism can improve the COVID-19 segmentation performance.",2021,10.1007/s11548-021-02418-w,diagnosis,True
Dynamic deformable attention network (DDANet) for COVID-19 lesions semantic segmentation,"Deep learning based medical image segmentation is an important step within diagnosis, which relies strongly on capturing sufficient spatial context without requiring too complex models that are hard to train with limited labelled data. Training data is in particular scarce for segmenting infection regions of CT images of COVID-19 patients. Attention models help gather contextual information within deep networks and benefit semantic segmentation tasks. The recent criss-cross-attention module aims to approximate global self-attention while remaining memory and time efficient by separating horizontal and vertical self-similarity computations. However, capturing attention from all non-local locations can adversely impact the accuracy of semantic segmentation networks. We propose a new Dynamic Deformable Attention Network (DDANet) that enables a more accurate contextual information computation in a similarly efficient way. Our novel technique is based on a deformable criss-cross attention block that learns both attention coefficients and attention offsets in a continuous way. A deep U-Net (Schlemper et al., 2019) segmentation network that employs this attention mechanism is able to capture attention from pertinent non-local locations and also improves the performance on semantic segmentation tasks compared to criss-cross attention within a U-Net on a challenging COVID-19 lesion segmentation task. Our validation experiments show that the performance gain of the recursively applied dynamic deformable attention blocks comes from their ability to capture dynamic and precise attention context. Our DDANet achieves Dice scores of 73.4% and 61.3% for Ground-glass opacity and consolidation lesions for COVID-19 segmentation and improves the accuracy by 4.9% points compared to a baseline U-Net and 24.4% points compared to current state of art methods (Fan et al., 2020).",2021,10.1016/j.jbi.2021.103816,diagnosis,True
E-TBNet: Light Deep Neural Network for Automatic Detection of Tuberculosis with X-ray DR Imaging,"Currently, the tuberculosis (TB) detection model based on chest X-ray images has the problem of excessive reliance on hardware computing resources, high equipment performance requirements, and being harder to deploy in low-cost personal computer and embedded devices. An efficient tuberculosis detection model is proposed to achieve accurate, efficient, and stable tuberculosis screening on devices with lower hardware levels. Due to the particularity of the chest X-ray images of TB patients, there are fewer labeled data, and the deep neural network model is difficult to fully train. We first analyzed the data distribution characteristics of two public TB datasets, and found that the two-stage tuberculosis identification (first divide, then classify) is insufficient. Secondly, according to the particularity of the detection image(s), the basic residual module was optimized and improved, and this is regarded as a crucial component of this article's network. Finally, an efficient attention mechanism was introduced, which was used to fuse the channel features. The network architecture was optimally designed and adjusted according to the correct and sufficient experimental content. In order to evaluate the performance of the network, it was compared with other lightweight networks under personal computer and Jetson Xavier embedded devices. The experimental results show that the recall rate and accuracy of the E-TBNet proposed in this paper are better than those of classic lightweight networks such as SqueezeNet and ShuffleNet, and it also has a shorter reasoning time. E-TBNet will be more advantageous to deploy on equipment with low levels of hardware.",2022,10.3390/s22030821,diagnosis,False
"Early prediction of in-hospital death of COVID-19 patients: a machine-learning model based on age, blood analyses, and chest x-ray score","An early-warning model to predict in-hospital mortality on admission of COVID-19 patients at an emergency department (ED) was developed and validated using a machine-learning model. In total, 2782 patients were enrolled between March 2020 and December 2020, including 2106 patients (first wave) and 676 patients (second wave) in the COVID-19 outbreak in Italy. The first-wave patients were divided into two groups with 1474 patients used to train the model, and 632 to validate it. The 676 patients in the second wave were used to test the model. Age, 17 blood analytes, and Brescia chest X-ray score were the variables processed using a random forests classification algorithm to build and validate the model. Receiver operating characteristic (ROC) analysis was used to assess the model performances. A web-based death-risk calculator was implemented and integrated within the Laboratory Information System of the hospital. The final score was constructed by age (the most powerful predictor), blood analytes (the strongest predictors were lactate dehydrogenase, D-dimer, neutrophil/lymphocyte ratio, C-reactive protein, lymphocyte %, ferritin std, and monocyte %), and Brescia chest X-ray score (https://bdbiomed.shinyapps.io/covid19score/). The areas under the ROC curve obtained for the three groups (training, validating, and testing) were 0.98, 0.83, and 0.78, respectively. The model predicts in-hospital mortality on the basis of data that can be obtained in a short time, directly at the ED on admission. It functions as a web-based calculator, providing a risk score which is easy to interpret. It can be used in the triage process to support the decision on patient allocation.",2021,10.7554/eLife.70640,prognosis,False
Early prediction of severity in coronavirus disease (COVID-19) using quantitative CT imaging,"PURPOSE: To evaluate whether the extent of COVID-19 pneumonia on CT scans using quantitative CT imaging obtained early in the illness can predict its future severity. METHODS: We conducted a retrospective single-center study on confirmed COVID-19 patients between January 18, 2020 and March 5, 2020. A quantitative AI algorithm was used to evaluate each patient's CT scan to determine the proportion of the lungs with pneumonia (VR) and the rate of change (RAR) in VR from scan to scan. Patients were classified as being in the severe or non-severe group based on their final symptoms. Penalized B-splines regression modeling was used to examine the relationship between mean VR and days from onset of symptoms in the two groups, with 95% and 99% confidence intervals. RESULTS: Median VR max was 18.6% (IQR 9.1-32.7%) in 21 patients in the severe group, significantly higher (P < 0.0001) than in the 53 patients in non-severe group (1.8% (IQR 0.4-5.7%)). RAR was increasing with a median RAR of 2.1% (IQR 0.4-5.5%) in severe and 0.4% (IQR 0.1-0.9%) in non-severe group, which was significantly different (P < 0.0001). Penalized B-spline analyses showed positive relationships between VR and days from onset of symptom. The 95% confidence limits of the predicted means for the two groups diverged 5 days after the onset of initial symptoms with a threshold of 11.9%. CONCLUSION: Five days after the initial onset of symptoms, CT could predict the patients who later developed severe symptoms with 95% confidence.",2021,10.1016/j.clinimag.2021.02.003,prognosis,True
Early survival prediction in non-small cell lung cancer from PET/CT images using an intra-tumor partitioning method,"PURPOSE: To explore prognostic and predictive values of a novel quantitative feature set describing intra-tumor heterogeneity in patients with lung cancer treated with concurrent and sequential chemoradiotherapy. METHODS: Longitudinal PET-CT images of 30 patients with non-small cell lung cancer were analysed. To describe tumor cell heterogeneity, the tumors were partitioned into one to ten concentric regions depending on their sizes, and, for each region, the change in average intensity between the two scans was calculated for PET and CT images separately to form the proposed feature set. To validate the prognostic value of the proposed method, radiomics analysis was performed and a combination of the proposed novel feature set and the classic radiomic features was evaluated. A feature selection algorithm was utilized to identify the optimal features, and a linear support vector machine was trained for the task of overall survival prediction in terms of area under the receiver operating characteristic curve (AUROC). RESULTS: The proposed novel feature set was found to be prognostic and even outperformed the radiomics approach with a significant difference (AUROC(SALoP) = 0.90 vs. AUROC(radiomic) = 0.71) when feature selection was not employed, whereas with feature selection, a combination of the novel feature set and radiomics led to the highest prognostic values. CONCLUSION: A novel feature set designed for capturing intra-tumor heterogeneity was introduced. Judging by their prognostic power, the proposed features have a promising potential for early survival prediction.",2019,10.1016/j.ejmp.2019.03.024,prognosis,True
Early tumor response prediction for lung cancer patients using novel longitudinal pattern features from sequential PET/CT image scans,"PURPOSE: A new set of quantitative features that capture intensity changes in PET/CT images over time and space is proposed for assessing the tumor response early during chemoradiotherapy. The hypothesis whether the new features, combined with machine learning, improve outcome prediction is tested. METHODS: The proposed method is based on dividing the tumor volume into successive zones depending on the distance to the tumor border. Mean intensity changes are computed within each zone, for CT and PET scans separately, and used as image features for tumor response assessment. Doing so, tumors are described by accounting for temporal and spatial changes at the same time. Using linear support vector machines, the new features were tested on 30 non-small cell lung cancer patients who underwent sequential or concurrent chemoradiotherapy. Prediction of 2-years overall survival was based on two PET-CT scans, acquired before the start and during the first 3 weeks of treatment. The predictive power of the newly proposed longitudinal pattern features was compared to that of previously proposed radiomics features and radiobiological parameters. RESULTS: The highest areas under the receiver operating characteristic curves were 0.98 and 0.93 for patients treated with sequential and concurrent chemoradiotherapy, respectively. Results showed an overall comparable performance with respect to radiomics features and radiobiological parameters. CONCLUSIONS: A novel set of quantitative image features, based on underlying tumor physiology, was computed from PET/CT scans and successfully employed to distinguish between early responders and non-responders to chemoradiotherapy.",2018,10.1016/j.ejmp.2018.09.003,prognosis,True
ECM-CSD: An Efficient Classification Model for Cancer Stage Diagnosis in CT Lung Images Using FCM and SVM Techniques,"As is eminent, lung cancer is one of the death frightening syndromes among people in present cases. The earlier diagnosis and treatment of lung cancer can increase the endurance rate of the affected people. But, the structure of the cancer cell makes the diagnosis process more challenging, in which the most of the cells are superimposed. By adopting the efficient image processing techniques, the diagnosis process can be made effective, earlier and accurate, where the time aspect is extremely decisive. With those considerations, the main objective of this work is to propose a region based Fuzzy C-Means Clustering (FCM) technique for segmenting the lung cancer region and the Support Vector Machine (SVM) based classification for diagnosing the cancer stage, which helps in clinical practice in significant way to increase the morality rate. Moreover, the proposed ECM-CSD (Efficient Classification Model for Cancer Stage Diagnosis) uses Computed Tomography (CT) lung images for processing, since it poses higher imaging sensitivity, resolution with good isotopic acquisition in lung nodule identification. With those images, the pre-processing has been made with Gaussian Filter for smoothing and Gabor Filter for enhancement. Following, based on the extracted image features, the effective segmentation of lung nodules is performed using the FCM based clustering. And, the stages of cancer are identified based on the SVM classification technique. Further, the model is analyzed with MATLAB tool by incorporating the LIDC-IDRI lung CT images clinical dataset. The comparative experiments show the efficiency of the proposed model in terms of the performance evaluation factors like increased accuracy and reduced error rate.",2019,10.1007/s10916-019-1190-z,diagnosis,True
Effect of CT image acquisition parameters on diagnostic performance of radiomics in predicting malignancy of pulmonary nodules of different sizes,"OBJECTIVES: To investigate the effect of CT image acquisition parameters on the performance of radiomics in classifying benign and malignant pulmonary nodules (PNs) with respect to nodule size. METHODS: We retrospectively collected CT images of 696 patients with PNs from March 2015 to March 2018. PNs were grouped by nodule diameter: T1a (diameter ≤ 1.0 cm), T1b (1.0 cm < diameter ≤ 2.0 cm), and T1c (2.0 cm < diameter ≤ 3.0 cm). CT images were divided into four settings according to slice-thickness-convolution-kernels: setting 1 (slice thickness/reconstruction type: 1.25 mm sharp), setting 2 (5 mm sharp), setting 3 (5 mm smooth), and random setting. We created twelve groups from two interacting conditions. Each PN was segmented and had 1160 radiomics features extracted. Non-redundant features with high predictive ability in training were selected to build a distinct model under each of the twelve subsets. RESULTS: The performance (AUCs) on predicting PN malignancy were as follows: T1a group: 0.84, 0.64, 0.68, and 0.68; T1b group: 0.68, 0.74, 0.76, and 0.70; T1c group: 0.66, 0.64, 0.63, and 0.70, for the setting 1, setting 2, setting 3, and random setting, respectively. In the T1a group, the AUC of radiomics model in setting 1 was statistically significantly higher than all others; In the T1b group, AUCs of radiomics models in setting 3 were statistically significantly higher than some; and in the T1c group, there were no statistically significant differences among models. CONCLUSIONS: For PNs less than 1 cm, CT image acquisition parameters have a significant influence on diagnostic performance of radiomics in predicting malignancy, and a model created using images reconstructed with thin section and a sharp kernel algorithm achieved the best performance. For PNs larger than 1 cm, CT reconstruction parameters did not affect diagnostic performance substantially. KEY POINTS: • CT image acquisition parameters have a significant influence on the diagnostic performance of radiomics in pulmonary nodules less than 1 cm. • In pulmonary nodules less than 1 cm, a radiomics model created by using images reconstructed with thin section and a sharp kernel algorithm achieved the best diagnostic performance. • For PNs larger than 1 cm, CT image acquisition parameters do not affect diagnostic performance substantially.",2022,10.1007/s00330-021-08274-1,diagnosis,True
Effect of CT Reconstruction Algorithm on the Diagnostic Performance of Radiomics Models: A Task-Based Approach for Pulmonary Subsolid Nodules,"OBJECTIVE: We investigated whether the diagnostic performance of machine learning-based radiomics models for the discrimination of invasive pulmonary adenocarcinomas (IPAs) among subsolid nodules (SSNs) was affected by the proportion of images reconstructed with filtered back projection (FBP) and model-based iterative reconstruction (MBIR) in datasets used for feature extraction. MATERIALS AND METHODS: This retrospective study included 60 patients (23 men and 37 women; mean age, 61.4 years) with 69 SSNs (54 part-solid and 15 pure ground-glass nodules). Preoperative CT scans were reconstructed with both FBP and MBIR. A total of 860 radiomics features were obtained from the entire nodule volume, and 70 resampled nodule datasets with an increasing proportion of nodules with MBIR-derived features (from 0/69 to 69/69) were prepared. After feature selection using neighborhood component analysis, support vector machines (SVMs) and an ensemble model were used as classifiers for the differentiation of IPAs. The diagnostic performances of all blending proportions of reconstruction algorithms were calculated and analyzed. RESULTS: The ROC AUC and the diagnostic accuracy of the radiomics models decreased significantly as the number of nodules with MBIR-derived features increased, and this relationship followed cubic functions (R(2) = 0.993 and 0.926 for SVM; R(2) = 0.993 and 0.975 for the ensemble model; p < 0.001). The magnitude of variation in AUC due to the reconstruction algorithm heterogeneity was 0.39 for SVM and 0.39 for the ensemble model. CONCLUSION: Inclusion of CT scans reconstructed with MBIR for radiomics modeling can significantly decrease diagnostic performance for the identification of IPAs.",2019,10.2214/ajr.18.20018,diagnosis,True
Effect of CT reconstruction settings on the performance of a deep learning based lung nodule CAD system,"PURPOSE: To study the effect of different reconstruction parameter settings on the performance of a commercially available deep learning based pulmonary nodule CAD system. MATERIALS AND METHODS: We performed a retrospective analysis of 24 chest CT scans, reconstructed at 16 different reconstruction settings for two different iterative reconstruction algorithms (SAFIRE and ADMIRE) varying in slice thickness, kernel size and iterative reconstruction level strength using a commercially available deep learning pulmonary nodule CAD system. The DL-CAD software was evaluated at 25 different sensitivity threshold settings and nodules detected by the DL-CAD software were matched against a reference standard based on the consensus reading of three radiologists. RESULTS: A total of 384 CT reconstructions was analysed from 24 patients, resulting in a total of 5786 found nodules. We matched the detected nodules against the reference standard, defined by a team of thoracic radiologists, and showed a gradual drop in recall, and an improvement in precision when the iterative strength levels were increased for a constant kernel size. The optimal DL-CAD threshold setting for use in our clinical workflow was found to be 0.88 with an F(2) of 0.73 ± 0.053. CONCLUSIONS: The DL-CAD system behaves differently on IR data than on FBP data, there is a gradual drop in recall, and growth in precision when the iterative strength levels are increased. As a result, caution should be taken when implementing deep learning software in a hospital with multiple CT scanners and different reconstruction protocols. To the best of our knowledge, this is the first study that demonstrates this result from a DL-CAD system on clinical data.",2021,10.1016/j.ejrad.2021.109526,diagnosis,True
Effect of machine learning methods on predicting NSCLC overall survival time based on Radiomics analysis,"BACKGROUND: To investigate the effect of machine learning methods on predicting the Overall Survival (OS) for non-small cell lung cancer based on radiomics features analysis. METHODS: A total of 339 radiomic features were extracted from the segmented tumor volumes of pretreatment computed tomography (CT) images. These radiomic features quantify the tumor phenotypic characteristics on the medical images using tumor shape and size, the intensity statistics and the textures. The performance of 5 feature selection methods and 8 machine learning methods were investigated for OS prediction. The predicted performance was evaluated with concordance index between predicted and true OS for the non-small cell lung cancer patients. The survival curves were evaluated by the Kaplan-Meier algorithm and compared by the log-rank tests. RESULTS: The gradient boosting linear models based on Cox's partial likelihood method using the concordance index feature selection method obtained the best performance (Concordance Index: 0.68, 95% Confidence Interval: 0.62~ 0.74). CONCLUSIONS: The preliminary results demonstrated that certain machine learning and radiomics analysis method could predict OS of non-small cell lung cancer accuracy.",2018,10.1186/s13014-018-1140-9,prognosis,True
Effective and Reliable Framework for Lung Nodules Detection from CT Scan Images,"Lung cancer is considered more serious among other prevailing cancer types. One of the reasons for it is that it is usually not diagnosed until it has spread and by that time it becomes very difficult to treat. Early detection of lung cancer can significantly increase the chances of survival of a cancer patient. An effective nodule detection system can play a key role in early detection of lung cancer thus increasing the chances of successful treatment. In this research work, we have proposed a novel classification framework for nodule classification. The framework consists of multiple phases that include image contrast enhancement, segmentation, optimal feature extraction, followed by employment of these features for training and testing of Support Vector Machine. We have empirically tested the efficacy of our technique by utilizing the well-known Lung Image Consortium Database (LIDC) dataset. The empirical results suggest that the technique is highly effective for reducing the false positive rates. We were able to receive an impressive sensitivity rate of 97.45%.",2019,10.1038/s41598-019-41510-9,diagnosis,True
Effective deep learning approaches for predicting COVID-19 outcomes from chest computed tomography volumes,"The rapid evolution of the novel coronavirus disease (COVID-19) pandemic has resulted in an urgent need for effective clinical tools to reduce transmission and manage severe illness. Numerous teams are quickly developing artificial intelligence approaches to these problems, including using deep learning to predict COVID-19 diagnosis and prognosis from chest computed tomography (CT) imaging data. In this work, we assess the value of aggregated chest CT data for COVID-19 prognosis compared to clinical metadata alone. We develop a novel patient-level algorithm to aggregate the chest CT volume into a 2D representation that can be easily integrated with clinical metadata to distinguish COVID-19 pneumonia from chest CT volumes from healthy participants and participants with other viral pneumonia. Furthermore, we present a multitask model for joint segmentation of different classes of pulmonary lesions present in COVID-19 infected lungs that can outperform individual segmentation models for each task. We directly compare this multitask segmentation approach to combining feature-agnostic volumetric CT classification feature maps with clinical metadata for predicting mortality. We show that the combination of features derived from the chest CT volumes improve the AUC performance to 0.80 from the 0.52 obtained by using patients' clinical data alone. These approaches enable the automated extraction of clinically relevant features from chest CT volumes for risk stratification of COVID-19 patients.",2022,10.1038/s41598-022-05532-0,prognosis,True
Efficient and Effective Training of COVID-19 Classification Networks With Self-Supervised Dual-Track Learning to Rank,"Coronavirus Disease 2019 (COVID-19) has rapidly spread worldwide since first reported. Timely diagnosis of COVID-19 is crucial both for disease control and patient care. Non-contrast thoracic computed tomography (CT) has been identified as an effective tool for the diagnosis, yet the disease outbreak has placed tremendous pressure on radiologists for reading the exams and may potentially lead to fatigue-related mis-diagnosis. Reliable automatic classification algorithms can be really helpful; however, they usually require a considerable number of COVID-19 cases for training, which is difficult to acquire in a timely manner. Meanwhile, how to effectively utilize the existing archive of non-COVID-19 data (the negative samples) in the presence of severe class imbalance is another challenge. In addition, the sudden disease outbreak necessitates fast algorithm development. In this work, we propose a novel approach for effective and efficient training of COVID-19 classification networks using a small number of COVID-19 CT exams and an archive of negative samples. Concretely, a novel self-supervised learning method is proposed to extract features from the COVID-19 and negative samples. Then, two kinds of soft-labels ('difficulty' and 'diversity') are generated for the negative samples by computing the earth mover's distances between the features of the negative and COVID-19 samples, from which data 'values' of the negative samples can be assessed. A pre-set number of negative samples are selected accordingly and fed to the neural network for training. Experimental results show that our approach can achieve superior performance using about half of the negative samples, substantially reducing model training time.",2020,10.1109/jbhi.2020.3018181,diagnosis,True
Efficient COVID-19 Segmentation from CT Slices Exploiting Semantic Segmentation with Integrated Attention Mechanism,"Coronavirus (COVID-19) is a pandemic, which caused suddenly unexplained pneumonia cases and caused a devastating effect on global public health. Computerized tomography (CT) is one of the most effective tools for COVID-19 screening. Since some specific patterns such as bilateral, peripheral, and basal predominant ground-glass opacity, multifocal patchy consolidation, crazy-paving pattern with a peripheral distribution can be observed in CT images and these patterns have been declared as the findings of COVID-19 infection. For patient monitoring, diagnosis and segmentation of COVID-19, which spreads into the lung, expeditiously and accurately from CT, will provide vital information about the stage of the disease. In this work, we proposed a SegNet-based network using the attention gate (AG) mechanism for the automatic segmentation of COVID-19 regions in CT images. AGs can be easily integrated into standard convolutional neural network (CNN) architectures with a minimum computing load as well as increasing model precision and predictive accuracy. Besides, the success of the proposed network has been evaluated based on dice, Tversky, and focal Tversky loss functions to deal with low sensitivity arising from the small lesions. The experiments were carried out using a fivefold cross-validation technique on a COVID-19 CT segmentation database containing 473 CT images. The obtained sensitivity, specificity, and dice scores were reported as 92.73%, 99.51%, and 89.61%, respectively. The superiority of the proposed method has been highlighted by comparing with the results reported in previous studies and it is thought that it will be an auxiliary tool that accurately detects automatic COVID-19 regions from CT images.",2021,10.1007/s10278-021-00434-5,diagnosis,True
Efficient Deep Network Architectures for Fast Chest X-Ray Tuberculosis Screening and Visualization,"Automated diagnosis of tuberculosis (TB) from chest X-Rays (CXR) has been tackled with either hand-crafted algorithms or machine learning approaches such as support vector machines (SVMs) and convolutional neural networks (CNNs). Most deep neural network applied to the task of tuberculosis diagnosis have been adapted from natural image classification. These models have a large number of parameters as well as high hardware requirements, which makes them prone to overfitting and harder to deploy in mobile settings. We propose a simple convolutional neural network optimized for the problem which is faster and more efficient than previous models but preserves their accuracy. Moreover, the visualization capabilities of CNNs have not been fully investigated. We test saliency maps and grad-CAMs as tuberculosis visualization methods, and discuss them from a radiological perspective.",2019,10.1038/s41598-019-42557-4,diagnosis,False
Efficient Framework for Detection of COVID-19 Omicron and Delta Variants Based on Two Intelligent Phases of CNN Models,"INTRODUCTION: While the COVID-19 pandemic was waning in most parts of the world, a new wave of COVID-19 Omicron and Delta variants in Central Asia and the Middle East caused a devastating crisis and collapse of health-care systems. As the diagnostic methods for this COVID-19 variant became more complex, health-care centers faced a dramatic increase in patients. Thus, the need for less expensive and faster diagnostic methods led researchers and specialists to work on improving diagnostic testing. METHOD: Inspired by the COVID-19 diagnosis methods, the latest and most efficient deep learning algorithms in the field of extracting X-ray and CT scan image features were used to identify COVID-19 in the early stages of the disease. RESULTS: We presented a general framework consisting of two models which are developed by convolutional neural network (CNN) using the concept of transfer learning and parameter optimization. The proposed phase of the framework was evaluated on the test dataset and yielded remarkable results and achieved a detection sensitivity, specificity, and accuracy of 0.99, 0.986, and 0.988, for the first phase and 0.997, 0.9976, and 0.997 for the second phase, respectively. In all cases, the whole framework was able to successfully classify COVID-19 and non-COVID-19 cases from CT scans and X-ray images. CONCLUSION: Since the proposed framework was based on two deep learning models that used two radiology modalities, it was able to significantly assist radiologists in detecting COVID-19 in the early stages. The use of models with this feature can be considered as a powerful and reliable tool, compared to the previous models used in the past pandemics.",2022,10.1155/2022/4838009,diagnosis,True
Elaboration of a multimodal MRI-based radiomics signature for the preoperative prediction of the histological subtype in patients with non-small-cell lung cancer,"BACKGROUND: Non-invasive discrimination between lung squamous cell carcinoma (LUSC) and lung adenocarcinoma (LUAD) subtypes of non-small-cell lung cancer (NSCLC) could be very beneficial to the patients unfit for the invasive diagnostic procedures. The aim of this study was to investigate the feasibility of utilizing the multimodal magnetic resonance imaging (MRI) radiomics and clinical features in classifying NSCLC. This retrospective study involved 148 eligible patients with postoperative pathologically confirmed NSCLC. The study was conducted in three steps: (1) feature extraction was performed using the online freely available package with the multimodal MRI data; (2) feature selection was performed using the Student's t test and support vector machine (SVM)-based recursive feature elimination method with the training cohort (n = 100), and the performance of these selected features was evaluated using both the training and the validation cohorts (n = 48) with a non-linear SVM classifier; (3) a Radscore model was then generated using logistic regression algorithm; (4) Integrating the Radscore with the semantic clinical features, a radiomics-clinical nomogram was developed, and its overall performance was evaluated with both cohorts. RESULTS: Thirteen optimal features achieved favorable discrimination performance with both cohorts, with area under the curve (AUC) of 0.819 and 0.824, respectively. The radiomics-clinical nomogram integrating the Radscore with the independent clinical predictors exhibited more favorable discriminative power, with AUC improved to 0.901 and 0.872 in both cohorts, respectively. The Hosmer-Lemeshow test and decision curve analysis results furtherly showed good predictive precision and clinical usefulness of the nomogram. CONCLUSION: Non-invasive histological subtype stratification of NSCLC can be done favorably using multimodal MRI radiomics features. Integrating the radiomics features with the clinical features could further improve the performance of the histological subtype stratification in patients with NSCLC.",2020,10.1186/s12938-019-0744-0,diagnosis,False
Emphysema quantification using low-dose computed tomography with deep learning-based kernel conversion comparison,"OBJECTIVE: This study determined the effect of dose reduction and kernel selection on quantifying emphysema using low-dose computed tomography (LDCT) and evaluated the efficiency of a deep learning-based kernel conversion technique in normalizing kernels for emphysema quantification. METHODS: A sample of 131 participants underwent LDCT and standard-dose computed tomography (SDCT) at 1- to 2-year intervals. LDCT images were reconstructed with B31f and B50f kernels, and SDCT images were reconstructed with B30f kernels. A deep learning model was used to convert the LDCT image from a B50f kernel to a B31f kernel. Emphysema indices (EIs), lung attenuation at 15th percentile (perc15), and mean lung density (MLD) were calculated. Comparisons among the different kernel types for both LDCT and SDCT were performed using Friedman's test and Bland-Altman plots. RESULTS: All values of LDCT B50f were significantly different compared with the values of LDCT B31f and SDCT B30f (p < 0.05). Although there was a statistical difference, the variation of the values of LDCT B50f significantly decreased after kernel normalization. The 95% limits of agreement between the SDCT and LDCT kernels (B31f and converted B50f) ranged from - 2.9 to 4.3% and from - 3.2 to 4.4%, respectively. However, there were no significant differences in EIs and perc15 between SDCT and LDCT converted B50f in the non-chronic obstructive pulmonary disease (COPD) participants (p > 0.05). CONCLUSION: The deep learning-based CT kernel conversion of sharp kernel in LDCT significantly reduced variation in emphysema quantification, and could be used for emphysema quantification. KEY POINTS: • Low-dose computed tomography with smooth kernel showed adequate performance in quantifying emphysema compared with standard-dose CT. • Emphysema quantification is affected by kernel selection and the application of a sharp kernel resulted in a significant overestimation of emphysema. • Deep learning-based kernel normalization of sharp kernel significantly reduced variation in emphysema quantification.",2020,10.1007/s00330-020-07020-3,diagnosis,True
End-to-end automatic differentiation of the coronavirus disease 2019 (COVID-19) from viral pneumonia based on chest CT,"PURPOSE: In the absence of a virus nucleic acid real-time reverse transcriptase-polymerase chain reaction (RT-PCR) test and experienced radiologists, clinical diagnosis is challenging for viral pneumonia with clinical symptoms and CT signs similar to that of coronavirus disease 2019 (COVID-19). We developed an end-to-end automatic differentiation method based on CT images to identify COVID-19 pneumonia patients in real time. METHODS: From January 18 to February 23, 2020, we conducted a retrospective study and enrolled 201 patients from two hospitals in China who underwent chest CT and RT-PCR tests, of which 98 patients tested positive for COVID-19 (118 males and 83 females, with an average age of 42 years). Patient CT images from one hospital were divided among training, validation and test datasets with an 80%:10%:10% ratio. An end-to-end representation learning method using a large-scale bi-directional generative adversarial network (BigBiGAN) architecture was designed to extract semantic features from the CT images. The semantic feature matrix was input for linear classifier construction. Patients from the other hospital were used for external validation. Differentiation accuracy was evaluated using a receiver operating characteristic curve. RESULTS: Based on the 120-dimensional semantic features extracted by BigBiGAN from each image, the linear classifier results indicated that the area under the curve (AUC) in the training, validation and test datasets were 0.979, 0.968 and 0.972, respectively, with an average sensitivity of 92% and specificity of 91%. The AUC for external validation was 0.850, with a sensitivity of 80% and specificity of 75%. Publicly available architecture and computing resources were used throughout the study to ensure reproducibility. CONCLUSION: This study provides an efficient recognition method for coronavirus disease 2019 pneumonia, using an end-to-end design to implement targeted and effective isolation for the containment of this communicable disease.",2020,10.1007/s00259-020-04929-1,diagnosis,True
End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography,"With an estimated 160,000 deaths in 2018, lung cancer is the most common cause of cancer death in the United States(1). Lung cancer screening using low-dose computed tomography has been shown to reduce mortality by 20-43% and is now included in US screening guidelines(1-6). Existing challenges include inter-grader variability and high false-positive and false-negative rates(7-10). We propose a deep learning algorithm that uses a patient's current and prior computed tomography volumes to predict the risk of lung cancer. Our model achieves a state-of-the-art performance (94.4% area under the curve) on 6,716 National Lung Cancer Screening Trial cases, and performs similarly on an independent clinical validation set of 1,139 cases. We conducted two reader studies. When prior computed tomography imaging was not available, our model outperformed all six radiologists with absolute reductions of 11% in false positives and 5% in false negatives. Where prior computed tomography imaging was available, the model performance was on-par with the same radiologists. This creates an opportunity to optimize the screening process via computer assistance and automation. While the vast majority of patients remain unscreened, we show the potential for deep learning models to increase the accuracy, consistency and adoption of lung cancer screening worldwide.",2019,10.1038/s41591-019-0447-x,diagnosis,True
End-to-End Non-Small-Cell Lung Cancer Prognostication Using Deep Learning Applied to Pretreatment Computed Tomography,"PURPOSE: Clinical TNM staging is a key prognostic factor for patients with lung cancer and is used to inform treatment and monitoring. Computed tomography (CT) plays a central role in defining the stage of disease. Deep learning applied to pretreatment CTs may offer additional, individualized prognostic information to facilitate more precise mortality risk prediction and stratification. METHODS: We developed a fully automated imaging-based prognostication technique (IPRO) using deep learning to predict 1-year, 2-year, and 5-year mortality from pretreatment CTs of patients with stage I-IV lung cancer. Using six publicly available data sets from The Cancer Imaging Archive, we performed a retrospective five-fold cross-validation using pretreatment CTs of 1,689 patients, of whom 1,110 were diagnosed with non-small-cell lung cancer and had available TNM staging information. We compared the association of IPRO and TNM staging with patients' survival status and assessed an Ensemble risk score that combines IPRO and TNM staging. Finally, we evaluated IPRO's ability to stratify patients within TNM stages using hazard ratios (HRs) and Kaplan-Meier curves. RESULTS: IPRO showed similar prognostic power (concordance index [C-index] 1-year: 0.72, 2-year: 0.70, 5-year: 0.68) compared with that of TNM staging (C-index 1-year: 0.71, 2-year: 0.71, 5-year: 0.70) in predicting 1-year, 2-year, and 5-year mortality. The Ensemble risk score yielded superior performance across all time points (C-index 1-year: 0.77, 2-year: 0.77, 5-year: 0.76). IPRO stratified patients within TNM stages, discriminating between highest- and lowest-risk quintiles in stages I (HR: 8.60), II (HR: 5.03), III (HR: 3.18), and IV (HR: 1.91). CONCLUSION: Deep learning applied to pretreatment CT combined with TNM staging enhances prognostication and risk stratification in patients with lung cancer.",2021,10.1200/cci.21.00096,prognosis,True
Ensemble Deep Learning and Internet of Things-Based Automated COVID-19 Diagnosis Framework,"Coronavirus disease (COVID-19) is a viral infection caused by SARS-CoV-2. The modalities such as computed tomography (CT) have been successfully utilized for the early stage diagnosis of COVID-19 infected patients. Recently, many researchers have utilized deep learning models for the automated screening of COVID-19 suspected cases. An ensemble deep learning and Internet of Things (IoT) based framework is proposed for screening of COVID-19 suspected cases. Three well-known pretrained deep learning models are ensembled. The medical IoT devices are utilized to collect the CT scans, and automated diagnoses are performed on IoT servers. The proposed framework is compared with thirteen competitive models over a four-class dataset. Experimental results reveal that the proposed ensembled deep learning model yielded 98.98% accuracy. Moreover, the model outperforms all competitive models in terms of other performance metrics achieving 98.56% precision, 98.58% recall, 98.75% F-score, and 98.57% AUC. Therefore, the proposed framework can improve the acceleration of COVID-19 diagnosis.",2022,10.1155/2022/7377502,diagnosis,True
Ensemble Learning Framework with GLCM Texture Extraction for Early Detection of Lung Cancer on CT Images,"Lung cancer has emerged as a major cause of death among all demographics worldwide, largely caused by a proliferation of smoking habits. However, early detection and diagnosis of lung cancer through technological improvements can save the lives of millions of individuals affected globally. Computerized tomography (CT) scan imaging is a proven and popular technique in the medical field, but diagnosing cancer with only CT scans is a difficult task even for doctors and experts. This is why computer-assisted diagnosis has revolutionized disease diagnosis, especially cancer detection. This study looks at 20 CT scan images of lungs. In a preprocessing step, we chose the best filter to be applied to medical CT images between median, Gaussian, 2D convolution, and mean. From there, it was established that the median filter is the most appropriate. Next, we improved image contrast by applying adaptive histogram equalization. Finally, the preprocessed image with better quality is subjected to two optimization algorithms, fuzzy c-means and k-means clustering. The performance of these algorithms was then compared. Fuzzy c-means showed the highest accuracy of 98%. The feature was extracted using Gray Level Cooccurrence Matrix (GLCM). In classification, a comparison between three algorithms-bagging, gradient boosting, and ensemble (SVM, MLPNN, DT, logistic regression, and KNN)-was performed. Gradient boosting performed the best among these three, having an accuracy of 90.9%.",2022,10.1155/2022/2733965,diagnosis,True
Estimating heterogeneous survival treatment effects of lung cancer screening approaches: A causal machine learning analysis,"The National Lung Screening Trial (NLST) found that low-dose computed tomography (LDCT) screening provided lung cancer (LC) mortality benefit compared to chest radiography (CXR). Considerable research concerns identifying the differential treatment effects that may exist in certain subpopulations. We shed light on several important issues in existing research and highlight the need for further investigation of the heterogeneous comparative effect of LDCT versus CXR, using more flexible and rigorous statistical approaches. We used a high-performance Bayesian machine learning approach designed for censored survival data, accelerated failure time Bayesian additive regression trees model (AFT-BART), to flexibly capture the relationships between the failure time and predictors. We then used the counterfactual framework to draw Markov chain Monte Carlo samples of the individual treatment effect for each participant. Using these posterior samples, we explored the possible treatment effect heterogeneity via a stepwise binary tree approach. When re-analyzed with AFT-BART, LDCT did not have a statistically significant LC or overall mortality benefit compared to CXR. The Asian and Black (particularly those with pack-year ≥ 37 years and without emphysema) NLST population were shown to have enhanced overall mortality benefit from LDCT than the population average. Although inconclusive for LC mortality benefit, Asians, Blacks and Whites with history of chronic obstructive pulmonary disease showed a small trend towards benefit from LDCT. Causal inference with flexible machine learning modeling can provide valuable knowledge for informing treatment decision and planning targeted clinical trials emphasizing personalized medicine approaches.",2021,10.1016/j.annepidem.2021.06.008,prognosis,False
Evaluate the Malignancy of Pulmonary Nodules Using the 3-D Deep Leaky Noisy-OR Network,"Automatic diagnosing lung cancer from computed tomography scans involves two steps: detect all suspicious lesions (pulmonary nodules) and evaluate the whole-lung/pulmonary malignancy. Currently, there are many studies about the first step, but few about the second step. Since the existence of nodule does not definitely indicate cancer, and the morphology of nodule has a complicated relationship with cancer, the diagnosis of lung cancer demands careful investigations on every suspicious nodule and integration of information of all nodules. We propose a 3-D deep neural network to solve this problem. The model consists of two modules. The first one is a 3-D region proposal network for nodule detection, which outputs all suspicious nodules for a subject. The second one selects the top five nodules based on the detection confidence, evaluates their cancer probabilities, and combines them with a leaky noisy-OR gate to obtain the probability of lung cancer for the subject. The two modules share the same backbone network, a modified U-net. The overfitting caused by the shortage of the training data is alleviated by training the two modules alternately. The proposed model won the first place in the Data Science Bowl 2017 competition.",2019,10.1109/tnnls.2019.2892409,diagnosis,True
Evaluating Deep Neural Network Architectures with Transfer Learning for Pneumonitis Diagnosis,"Pneumonitis is an infectious disease that causes the inflammation of the air sac. It can be life-threatening to the very young and elderly. Detection of pneumonitis from X-ray images is a significant challenge. Early detection and assistance with diagnosis can be crucial. Recent developments in the field of deep learning have significantly improved their performance in medical image analysis. The superior predictive performance of the deep learning methods makes them ideal for pneumonitis classification from chest X-ray images. However, training deep learning models can be cumbersome and resource-intensive. Reusing knowledge representations of public models trained on large-scale datasets through transfer learning can help alleviate these challenges. In this paper, we compare various image classification models based on transfer learning with well-known deep learning architectures. The Kaggle chest X-ray dataset was used to evaluate and compare our models. We apply basic data augmentation and fine-tune our feed-forward classification head on the models pretrained on the ImageNet dataset. We observed that the DenseNet201 model outperforms other models with an AUROC score of 0.966 and a recall score of 0.99. We also visualize the class activation maps from the DenseNet201 model to interpret the patterns recognized by the model for prediction.",2021,10.1155/2021/8036304,diagnosis,False
Evaluating the clinical trends and benefits of low-dose computed tomography in lung cancer patients,"BACKGROUND: Despite guideline recommendations, utilization of low-dose computed tomography (LDCT) for lung cancer screening remains low. The driving factors behind these low rates and the real-world effect of LDCT utilization on lung cancer outcomes remain limited. METHODS: We identified patients diagnosed with non-small cell lung cancer (NSCLC) from 2015 to 2017 within the Veterans Health Administration. Multivariable logistic regression assessed the influence of LDCT screening on stage at diagnosis. Lead time correction using published LDCT lead times was performed. Cancer-specific mortality (CSM) was evaluated using Fine-Gray regression with non-cancer death as a competing risk. A lasso machine learning model identified important predictors for receiving LDCT screening. RESULTS: Among 4664 patients, mean age was 67.8 with 58-month median follow-up, 95% CI = [7-71], and 118 patients received ≥1 screening LDCT before NSCLC diagnosis. From 2015 to 2017, LDCT screening increased (0.1%-6.6%, mean = 1.3%). Compared with no screening, patients with ≥1 LDCT were more than twice as likely to present with stage I disease at diagnosis (odds ratio [OR] 2.16 [95% CI 1.46-3.20]) and less than half as likely to present with stage IV (OR 0.38 [CI 0.21-0.70]). Screened patients had lower risk of CSM even after adjusting for LDCT lead time (subdistribution hazard ratio 0.60 [CI 0.42-0.85]). The machine learning model achieved an area under curve of 0.87 and identified diagnosis year and region as the most important predictors for receiving LDCT. White, non-Hispanic patients were more likely to receive LDCT screening, whereas minority, older, female, and unemployed patients were less likely. CONCLUSIONS: Utilization of LDCT screening is increasing, although remains low. Consistent with randomized data, LDCT-screened patients were diagnosed at earlier stages and had lower CSM. LDCT availability appeared to be the main predictor of utilization. Providing access to more patients, including those in diverse racial and socioeconomic groups, should be a priority.",2021,10.1002/cam4.4229,prognosis,True
Evaluation of a novel deep learning-based classifier for perifissural nodules,"OBJECTIVES: To evaluate the performance of a novel convolutional neural network (CNN) for the classification of typical perifissural nodules (PFN). METHODS: Chest CT data from two centers in the UK and The Netherlands (1668 unique nodules, 1260 individuals) were collected. Pulmonary nodules were classified into subtypes, including ""typical PFNs"" on-site, and were reviewed by a central clinician. The dataset was divided into a training/cross-validation set of 1557 nodules (1103 individuals) and a test set of 196 nodules (158 individuals). For the test set, three radiologically trained readers classified the nodules into three nodule categories: typical PFN, atypical PFN, and non-PFN. The consensus of the three readers was used as reference to evaluate the performance of the PFN-CNN. Typical PFNs were considered as positive results, and atypical PFNs and non-PFNs were grouped as negative results. PFN-CNN performance was evaluated using the ROC curve, confusion matrix, and Cohen's kappa. RESULTS: Internal validation yielded a mean AUC of 91.9% (95% CI 90.6-92.9) with 78.7% sensitivity and 90.4% specificity. For the test set, the reader consensus rated 45/196 (23%) of nodules as typical PFN. The classifier-reader agreement (k = 0.62-0.75) was similar to the inter-reader agreement (k = 0.64-0.79). Area under the ROC curve was 95.8% (95% CI 93.3-98.4), with a sensitivity of 95.6% (95% CI 84.9-99.5), and specificity of 88.1% (95% CI 81.8-92.8). CONCLUSION: The PFN-CNN showed excellent performance in classifying typical PFNs. Its agreement with radiologically trained readers is within the range of inter-reader agreement. Thus, the CNN-based system has potential in clinical and screening settings to rule out perifissural nodules and increase reader efficiency. KEY POINTS: • Agreement between the PFN-CNN and radiologically trained readers is within the range of inter-reader agreement. • The CNN model for the classification of typical PFNs achieved an AUC of 95.8% (95% CI 93.3-98.4) with 95.6% (95% CI 84.9-99.5) sensitivity and 88.1% (95% CI 81.8-92.8) specificity compared to the consensus of three readers.",2021,10.1007/s00330-020-07509-x,diagnosis,True
Evaluation of Glucocorticoid Therapy in Asthma Children with Small Airway Obstruction Based on CT Features of Deep Learning,"This study was aimed at exploring the treatment of asthma children with small airway obstruction in CT imaging features of deep learning and glucocorticoid. A total of 145 patients meeting the requirements in hospital were included in this study, and they were randomly assigned to receive aerosolized glucocorticoid (n = 45), aerosolized glucocorticoid combined with bronchodilator (n = 50), or oral steroids (n = 50) for 4 weeks after discharge. The lung function and fractional exhaled nitric oxide (FENO) indexes of the three groups were measured, respectively, and then the effective rates were compared to evaluate the clinical efficacy of glucocorticoids with different administration methods and combined medications in the short-term maintenance treatment after acute exacerbation of asthma. Deep learning algorithm was used for CT image segmentation. The CT image is sent to the workbench for processing on the workbench, and then the convolution operation is performed on each input pixel point during the image processing. After 4 weeks of maintenance treatment, FEF50 %, FEF75 %, and MMEF75/25 increased significantly, and FENO decreased significantly (P < 0.01). The improvement results of FEF50 %, FEF75 %, MMEF75/25, and FENO after maintenance treatment were as follows: the oral hormone group was the most effective, followed by the combined atomization inhalation group, and the hormone atomization inhalation group was the least effective. The differences among them were statistically significant (P < 0.05). The accuracy of artificial intelligence segmentation algorithm was 81%. All the hormones were more effective than local medication in the treatment of small airway function and airway inflammation. In the treatment of aerosol inhalation, the hormone combined with bronchiectasis drug was the most effective in improving small airway obstruction and reducing airway inflammation compared with single drug inhalation. Deep learning CT images are simple, noninvasive, and intuitively observe lung changes in asthma with small airway functional obstruction. Asthma with small airway functional obstruction has high clinical diagnosis and evaluation value.",2021,10.1155/2021/7936548,treatment,True
Evaluation of the feasibility of explainable computer-aided detection of cardiomegaly on chest radiographs using deep learning,"We examined the feasibility of explainable computer-aided detection of cardiomegaly in routine clinical practice using segmentation-based methods. Overall, 793 retrospectively acquired posterior-anterior (PA) chest X-ray images (CXRs) of 793 patients were used to train deep learning (DL) models for lung and heart segmentation. The training dataset included PA CXRs from two public datasets and in-house PA CXRs. Two fully automated segmentation-based methods using state-of-the-art DL models for lung and heart segmentation were developed. The diagnostic performance was assessed and the reliability of the automatic cardiothoracic ratio (CTR) calculation was determined using the mean absolute error and paired t-test. The effects of thoracic pathological conditions on performance were assessed using subgroup analysis. One thousand PA CXRs of 1000 patients (480 men, 520 women; mean age 63 ± 23 years) were included. The CTR values derived from the DL models and diagnostic performance exhibited excellent agreement with reference standards for the whole test dataset. Performance of segmentation-based methods differed based on thoracic conditions. When tested using CXRs with lesions obscuring heart borders, the performance was lower than that for other thoracic pathological findings. Thus, segmentation-based methods using DL could detect cardiomegaly; however, the feasibility of computer-aided detection of cardiomegaly without human intervention was limited.",2021,10.1038/s41598-021-96433-1,diagnosis,False
Expert knowledge-infused deep learning for automatic lung nodule detection,"BACKGROUND: Computer aided detection (CADe) of pulmonary nodules from computed tomography (CT) is crucial for early diagnosis of lung cancer. Self-learned features obtained by training datasets via deep learning have facilitated CADe of the nodules. However, the complexity of CT lung images renders a challenge of extracting effective features by self-learning only. This condition is exacerbated for limited size of datasets. On the other hand, the engineered features have been widely studied. OBJECTIVE: We proposed a novel nodule CADe which aims to relieve the challenge by the use of available engineered features to prevent convolution neural networks (CNN) from overfitting under dataset limitation and reduce the running-time complexity of self-learning. METHODS: The CADe methodology infuses adequately the engineered features, particularly texture features, into the deep learning process. RESULTS: The methodology was validated on 208 patients with at least one juxta-pleural nodule from the public LIDC-IDRI database. Results demonstrated that the methodology achieves a sensitivity of 88% with 1.9 false positives per scan and a sensitivity of 94.01% with 4.01 false positives per scan. CONCLUSIONS: The methodology shows high performance compared with the state-of-the-art results, in terms of accuracy and efficiency, from both existing CNN-based approaches and engineered feature-based classifications.",2019,10.3233/xst-180426,diagnosis,False
Explainable Artificial Intelligence for Bias Detection in COVID CT-Scan Classifiers,"PROBLEM: An application of Explainable Artificial Intelligence Methods for COVID CT-Scan classifiers is presented. MOTIVATION: It is possible that classifiers are using spurious artifacts in dataset images to achieve high performances, and such explainable techniques can help identify this issue. AIM: For this purpose, several approaches were used in tandem, in order to create a complete overview of the classificatios. METHODOLOGY: The techniques used included GradCAM, LIME, RISE, Squaregrid, and direct Gradient approaches (Vanilla, Smooth, Integrated). MAIN RESULTS: Among the deep neural networks architectures evaluated for this image classification task, VGG16 was shown to be most affected by biases towards spurious artifacts, while DenseNet was notably more robust against them. Further impacts: Results further show that small differences in validation accuracies can cause drastic changes in explanation heatmaps for DenseNet architectures, indicating that small changes in validation accuracy may have large impacts on the biases learned by the networks. Notably, it is important to notice that the strong performance metrics achieved by all these networks (Accuracy, F1 score, AUC all in the 80 to 90% range) could give users the erroneous impression that there is no bias. However, the analysis of the explanation heatmaps highlights the bias.",2021,10.3390/s21165657,diagnosis,True
Explainable COVID-19 Detection Using Chest CT Scans and Deep Learning,"This paper explores how well deep learning models trained on chest CT images can diagnose COVID-19 infected people in a fast and automated process. To this end, we adopted advanced deep network architectures and proposed a transfer learning strategy using custom-sized input tailored for each deep architecture to achieve the best performance. We conducted extensive sets of experiments on two CT image datasets, namely, the SARS-CoV-2 CT-scan and the COVID19-CT. The results show superior performances for our models compared with previous studies. Our best models achieved average accuracy, precision, sensitivity, specificity, and F1-score values of 99.4%, 99.6%, 99.8%, 99.6%, and 99.4% on the SARS-CoV-2 dataset, and 92.9%, 91.3%, 93.7%, 92.2%, and 92.5% on the COVID19-CT dataset, respectively. For better interpretability of the results, we applied visualization techniques to provide visual explanations for the models' predictions. Feature visualizations of the learned features show well-separated clusters representing CT images of COVID-19 and non-COVID-19 cases. Moreover, the visualizations indicate that our models are not only capable of identifying COVID-19 cases but also provide accurate localization of the COVID-19-associated regions, as indicated by well-trained radiologists.",2021,10.3390/s21020455,diagnosis,True
Explainable DCNN based chest X-ray image analysis and classification for COVID-19 pneumonia detection,"To speed up the discovery of COVID-19 disease mechanisms by X-ray images, this research developed a new diagnosis platform using a deep convolutional neural network (DCNN) that is able to assist radiologists with diagnosis by distinguishing COVID-19 pneumonia from non-COVID-19 pneumonia in patients based on chest X-ray classification and analysis. Such a tool can save time in interpreting chest X-rays and increase the accuracy and thereby enhance our medical capacity for the detection and diagnosis of COVID-19. The explainable method is also used in the DCNN to select instances of the X-ray dataset images to explain the behavior of training-learning models to achieve higher prediction accuracy. The average accuracy of our method is above 96%, which can replace manual reading and has the potential to be applied to large-scale rapid screening of COVID-9 for widely use cases.",2021,10.1038/s41598-021-95680-6,diagnosis,False
Explainable Deep Learning for Pulmonary Disease and Coronavirus COVID-19 Detection from X-rays,"BACKGROUND AND OBJECTIVE: Coronavirus disease (COVID-19) is an infectious disease caused by a new virus never identified before in humans. This virus causes respiratory disease (for instance, flu) with symptoms such as cough, fever and, in severe cases, pneumonia. The test to detect the presence of this virus in humans is performed on sputum or blood samples and the outcome is generally available within a few hours or, at most, days. Analysing biomedical imaging the patient shows signs of pneumonia. In this paper, with the aim of providing a fully automatic and faster diagnosis, we propose the adoption of deep learning for COVID-19 detection from X-rays. METHOD: In particular, we propose an approach composed by three phases: the first one to detect if in a chest X-ray there is the presence of a pneumonia. The second one to discern between COVID-19 and pneumonia. The last step is aimed to localise the areas in the X-ray symptomatic of the COVID-19 presence. RESULTS AND CONCLUSION: Experimental analysis on 6,523 chest X-rays belonging to different institutions demonstrated the effectiveness of the proposed approach, with an average time for COVID-19 detection of approximately 2.5 seconds and an average accuracy equal to 0.97.",2020,10.1016/j.cmpb.2020.105608,diagnosis,False
Explaining Deep Features Using Radiologist-Defined Semantic Features and Traditional Quantitative Features,"Quantitative features are generated from a tumor phenotype by various data characterization, feature-extraction approaches and have been used successfully as a biomarker. These features give us information about a nodule, for example, nodule size, pixel intensity, histogram-based information, and texture information from wavelets or a convolution kernel. Semantic features, on the other hand, can be generated by an experienced radiologist and consist of the common characteristics of a tumor, for example, location of a tumor, fissure, or pleural wall attachment, presence of fibrosis or emphysema, concave cut on nodule surface. These features have been derived for lung nodules by our group. Semantic features have also shown promise in predicting malignancy. Deep features from images are generally extracted from the last layers before the classification layer of a convolutional neural network (CNN). By training with the use of different types of images, the CNN learns to recognize various patterns and textures. But when we extract deep features, there is no specific naming approach for them, other than denoting them by the feature column number (position of a neuron in a hidden layer). In this study, we tried to relate and explain deep features with respect to traditional quantitative features and semantic features. We discovered that 26 deep features from the Vgg-S neural network and 12 deep features from our trained CNN could be explained by semantic or traditional quantitative features. From this, we concluded that those deep features can have a recognizable definition via semantic or quantitative features.",2019,10.18383/j.tom.2018.00034,diagnosis,True
Exploiting Global Structure Information to Improve Medical Image Segmentation,"In this paper, we propose a method to enhance the performance of segmentation models for medical images. The method is based on convolutional neural networks that learn the global structure information, which corresponds to anatomical structures in medical images. Specifically, the proposed method is designed to learn the global boundary structures via an autoencoder and constrain a segmentation network through a loss function. In this manner, the segmentation model performs the prediction in the learned anatomical feature space. Unlike previous studies that considered anatomical priors by using a pre-trained autoencoder to train segmentation networks, we propose a single-stage approach in which the segmentation network and autoencoder are jointly learned. To verify the effectiveness of the proposed method, the segmentation performance is evaluated in terms of both the overlap and distance metrics on the lung area and spinal cord segmentation tasks. The experimental results demonstrate that the proposed method can enhance not only the segmentation performance but also the robustness against domain shifts.",2021,10.3390/s21093249,diagnosis,False
Exploiting Multiple Optimizers with Transfer Learning Techniques for the Identification of COVID-19 Patients,"Due to the rapid spread of COVID-19 and its induced death worldwide, it is imperative to develop a reliable tool for the early detection of this disease. Chest X-ray is currently accepted to be one of the reliable means for such a detection purpose. However, most of the available methods utilize large training data, and there is a need for improvement in the detection accuracy due to the limited boundary segment of the acquired images for symptom identifications. In this study, a robust and efficient method based on transfer learning techniques is proposed to identify normal and COVID-19 patients by employing small training data. Transfer learning builds accurate models in a timesaving way. First, data augmentation was performed to help the network for memorization of image details. Next, five state-of-the-art transfer learning models, AlexNet, MobileNetv2, ShuffleNet, SqueezeNet, and Xception, with three optimizers, Adam, SGDM, and RMSProp, were implemented at various learning rates, 1e-4, 2e-4, 3e-4, and 4e-4, to reduce the probability of overfitting. All the experiments were performed on publicly available datasets with several analytical measurements attained after execution with a 10-fold cross-validation method. The results suggest that MobileNetv2 with Adam optimizer at a learning rate of 3e-4 provides an average accuracy, recall, precision, and F-score of 97%, 96.5%, 97.5%, and 97%, respectively, which are higher than those of all other combinations. The proposed method is competitive with the available literature, demonstrating that it could be used for the early detection of COVID-19 patients.",2020,10.1155/2020/8889412,diagnosis,False
Exploiting Shared Knowledge From Non-COVID Lesions for Annotation-Efficient COVID-19 CT Lung Infection Segmentation,"The novel Coronavirus disease (COVID-19) is a highly contagious virus and has spread all over the world, posing an extremely serious threat to all countries. Automatic lung infection segmentation from computed tomography (CT) plays an important role in the quantitative analysis of COVID-19. However, the major challenge lies in the inadequacy of annotated COVID-19 datasets. Currently, there are several public non-COVID lung lesion segmentation datasets, providing the potential for generalizing useful information to the related COVID-19 segmentation task. In this paper, we propose a novel relation-driven collaborative learning model to exploit shared knowledge from non-COVID lesions for annotation-efficient COVID-19 CT lung infection segmentation. The model consists of a general encoder to capture general lung lesion features based on multiple non-COVID lesions, and a target encoder to focus on task-specific features based on COVID-19 infections. We develop a collaborative learning scheme to regularize feature-level relation consistency of given input and encourage the model to learn more general and discriminative representation of COVID-19 infections. Extensive experiments demonstrate that trained with limited COVID-19 data, exploiting shared knowledge from non-COVID lesions can further improve state-of-the-art performance with up to 3.0% in dice similarity coefficient and 4.2% in normalized surface dice. In addition, experimental results on large scale 2D dataset with CT slices show that our method significantly outperforms cutting-edge segmentation methods metrics. Our method promotes new insights into annotation-efficient deep learning and illustrates strong potential for real-world applications in the global fight against COVID-19 in the absence of sufficient high-quality annotations.",2021,10.1109/jbhi.2021.3106341,diagnosis,True
Exploring the effect of image enhancement techniques on COVID-19 detection using chest X-ray images,"Computer-aided diagnosis for the reliable and fast detection of coronavirus disease (COVID-19) has become a necessity to prevent the spread of the virus during the pandemic to ease the burden on the healthcare system. Chest X-ray (CXR) imaging has several advantages over other imaging and detection techniques. Numerous works have been reported on COVID-19 detection from a smaller set of original X-ray images. However, the effect of image enhancement and lung segmentation of a large dataset in COVID-19 detection was not reported in the literature. We have compiled a large X-ray dataset (COVQU) consisting of 18,479 CXR images with 8851 normal, 6012 non-COVID lung infections, and 3616 COVID-19 CXR images and their corresponding ground truth lung masks. To the best of our knowledge, this is the largest public COVID positive database and the lung masks. Five different image enhancement techniques: histogram equalization (HE), contrast limited adaptive histogram equalization (CLAHE), image complement, gamma correction, and balance contrast enhancement technique (BCET) were used to investigate the effect of image enhancement techniques on COVID-19 detection. A novel U-Net model was proposed and compared with the standard U-Net model for lung segmentation. Six different pre-trained Convolutional Neural Networks (CNNs) (ResNet18, ResNet50, ResNet101, InceptionV3, DenseNet201, and ChexNet) and a shallow CNN model were investigated on the plain and segmented lung CXR images. The novel U-Net model showed an accuracy, Intersection over Union (IoU), and Dice coefficient of 98.63%, 94.3%, and 96.94%, respectively for lung segmentation. The gamma correction-based enhancement technique outperforms other techniques in detecting COVID-19 from the plain and the segmented lung CXR images. Classification performance from plain CXR images is slightly better than the segmented lung CXR images; however, the reliability of network performance is significantly improved for the segmented lung images, which was observed using the visualization technique. The accuracy, precision, sensitivity, F1-score, and specificity were 95.11%, 94.55%, 94.56%, 94.53%, and 95.59% respectively for the segmented lung images. The proposed approach with very reliable and comparable performance will boost the fast and robust COVID-19 detection using chest X-ray images.",2021,10.1016/j.compbiomed.2021.104319,diagnosis,False
External validation of a convolutional neural network artificial intelligence tool to predict malignancy in pulmonary nodules,"BACKGROUND: Estimation of the risk of malignancy in pulmonary nodules detected by CT is central in clinical management. The use of artificial intelligence (AI) offers an opportunity to improve risk prediction. Here we compare the performance of an AI algorithm, the lung cancer prediction convolutional neural network (LCP-CNN), with that of the Brock University model, recommended in UK guidelines. METHODS: A dataset of incidentally detected pulmonary nodules measuring 5-15 mm was collected retrospectively from three UK hospitals for use in a validation study. Ground truth diagnosis for each nodule was based on histology (required for any cancer), resolution, stability or (for pulmonary lymph nodes only) expert opinion. There were 1397 nodules in 1187 patients, of which 234 nodules in 229 (19.3%) patients were cancer. Model discrimination and performance statistics at predefined score thresholds were compared between the Brock model and the LCP-CNN. RESULTS: The area under the curve for LCP-CNN was 89.6% (95% CI 87.6 to 91.5), compared with 86.8% (95% CI 84.3 to 89.1) for the Brock model (p≤0.005). Using the LCP-CNN, we found that 24.5% of nodules scored below the lowest cancer nodule score, compared with 10.9% using the Brock score. Using the predefined thresholds, we found that the LCP-CNN gave one false negative (0.4% of cancers), whereas the Brock model gave six (2.5%), while specificity statistics were similar between the two models. CONCLUSION: The LCP-CNN score has better discrimination and allows a larger proportion of benign nodules to be identified without missing cancers than the Brock model. This has the potential to substantially reduce the proportion of surveillance CT scans required and thus save significant resources.",2020,10.1136/thoraxjnl-2019-214104,diagnosis,True
External validation of radiomics-based predictive models in low-dose CT screening for early lung cancer diagnosis,"PURPOSE: Low-dose CT screening allows early lung cancer detection, but is affected by frequent false positive results, inter/intra observer variation and uncertain diagnoses of lung nodules. Radiomics-based models have recently been introduced to overcome these issues, but limitations in demonstrating their generalizability on independent datasets are slowing their introduction to clinic. The aim of this study is to evaluate two radiomics-based models to classify malignant pulmonary nodules in low-dose CT screening, and to externally validate them on an independent cohort. The effect of a radiomics features harmonization technique is also investigated to evaluate its impact on the classification of lung nodules from a multicenter data. METHODS: Pulmonary nodules from two independent cohorts were considered in this study; the first cohort (110 subjects, 113 nodules) was used to train prediction models, and the second cohort (72 nodules) to externally validate them. Literature-based radiomics features were extracted and, after feature selection, used as predictive variables in models for malignancy identification. An in-house prediction model based on artificial neural network (ANN) was implemented and evaluated, along with an alternative model from the literature, based on a support vector machine (SVM) classifier coupled with a least absolute shrinkage and selection operator (LASSO). External validation was performed on the second cohort to evaluate models' generalization ability. Additionally, the impact of the Combat harmonization method was investigated to compensate for multicenter datasets variabilities. A new training of the models based on harmonized features was performed on the first cohort, then tested separately on the harmonized and non-harmonized features of the second cohort. RESULTS: Preliminary results showed a good accuracy of the investigated models in distinguishing benign from malignant pulmonary nodules with both sets of radiomics features (i.e., non-harmonized and harmonized). The performance of the models, quantified in terms of Area Under the Curve (AUC), was > 0.89 in the training set and > 0.82 in the external validation set for all the investigated scenarios, outperforming the clinical standard (AUC of 0.76). Slightly higher performance was observed for the SVM-LASSO model than the ANN in the external dataset, although they did not result significantly different. For both harmonized and non-harmonized features, no statistical difference was found between Receiver operating characteristic (ROC) curves related to training and test set for both models. CONCLUSIONS: Although no significant improvements were observed when applying the Combat harmonization method, both in-house and literature-based models were able to classify lung nodules with good generalization to an independent dataset, thus showing their potential as tools for clinical decision-making in lung cancer screening.",2020,10.1002/mp.14308,diagnosis,True
Factors determining generalization in deep learning models for scoring COVID-CT images,"The COVID-19 pandemic has inspired unprecedented data collection and computer vision modelling efforts worldwide, focused on the diagnosis of COVID-19 from medical images. However, these models have found limited, if any, clinical application due in part to unproven generalization to data sets beyond their source training corpus. This study investigates the generalizability of deep learning models using publicly available COVID-19 Computed Tomography data through cross dataset validation. The predictive ability of these models for COVID-19 severity is assessed using an independent dataset that is stratified for COVID-19 lung involvement. Each inter-dataset study is performed using histogram equalization, and contrast limited adaptive histogram equalization with and without a learning Gabor filter. We show that under certain conditions, deep learning models can generalize well to an external dataset with F1 scores up to 86%. The best performing model shows predictive accuracy of between 75% and 96% for lung involvement scoring against an external expertly stratified dataset. From these results we identify key factors promoting deep learning generalization, being primarily the uniform acquisition of training images, and secondly diversity in CT slice position.",2021,10.3934/mbe.2021456,diagnosis,True
False positive reduction in pulmonary nodule classification using 3D texture and edge feature in CT images,"BACKGROUND: Pulmonary nodule detection can significantly influence the early diagnosis of lung cancer while is confused by false positives. OBJECTIVE: In this study, we focus on the false positive reduction and present a method for accurate and rapid detection of pulmonary nodule from suspective regions with 3D texture and edge feature. METHODS: This work mainly consists of four modules. Firstly, small pulmonary nodule candidates are preprocessed by a reconstruction approach for enhancing 3D image feature. Secondly, a texture feature descriptor is proposed, named cross-scale local binary patterns (CS-LBP), to extract spatial texture information. Thirdly, we design a 3D edge feature descriptor named orthogonal edge orientation histogram (ORT-EOH) to obtain spatial edge information. Finally, hierarchical support vector machines (H-SVMs) is used to classify suspective regions as either nodules or non-nodules with joint CS-LBP and ORT-EOH feature vector. RESULTS: For the solitary solid nodule, ground-glass opacity, juxta-vascular nodule and juxta-pleural nodule, average sensitivity, average specificity and average accuracy of our method are 95.69%, 96.95% and 96.04%, respectively. The elapsed time in training and testing stage are 321.76 s and 5.69 s. CONCLUSIONS: Our proposed method has the best performance compared with other state-of-the-art methods and is shown the improved precision of pulmonary nodule detection with computationaly low cost.",2021,10.3233/thc-181565,diagnosis,True
Fast and Accurate Detection of COVID-19 Along With 14 Other Chest Pathologies Using a Multi-Level Classification: Algorithm Development and Validation Study,"BACKGROUND: COVID-19 has spread very rapidly, and it is important to build a system that can detect it in order to help an overwhelmed health care system. Many research studies on chest diseases rely on the strengths of deep learning techniques. Although some of these studies used state-of-the-art techniques and were able to deliver promising results, these techniques are not very useful if they can detect only one type of disease without detecting the others. OBJECTIVE: The main objective of this study was to achieve a fast and more accurate diagnosis of COVID-19. This study proposes a diagnostic technique that classifies COVID-19 x-ray images from normal x-ray images and those specific to 14 other chest diseases. METHODS: In this paper, we propose a novel, multilevel pipeline, based on deep learning models, to detect COVID-19 along with other chest diseases based on x-ray images. This pipeline reduces the burden of a single network to classify a large number of classes. The deep learning models used in this study were pretrained on the ImageNet dataset, and transfer learning was used for fast training. The lungs and heart were segmented from the whole x-ray images and passed onto the first classifier that checks whether the x-ray is normal, COVID-19 affected, or characteristic of another chest disease. If it is neither a COVID-19 x-ray image nor a normal one, then the second classifier comes into action and classifies the image as one of the other 14 diseases. RESULTS: We show how our model uses state-of-the-art deep neural networks to achieve classification accuracy for COVID-19 along with 14 other chest diseases and normal cases based on x-ray images, which is competitive with currently used state-of-the-art models. Due to the lack of data in some classes such as COVID-19, we applied 10-fold cross-validation through the ResNet50 model. Our classification technique thus achieved an average training accuracy of 96.04% and test accuracy of 92.52% for the first level of classification (ie, 3 classes). For the second level of classification (ie, 14 classes), our technique achieved a maximum training accuracy of 88.52% and test accuracy of 66.634% by using ResNet50. We also found that when all the 16 classes were classified at once, the overall accuracy for COVID-19 detection decreased, which in the case of ResNet50 was 88.92% for training data and 71.905% for test data. CONCLUSIONS: Our proposed pipeline can detect COVID-19 with a higher accuracy along with detecting 14 other chest diseases based on x-ray images. This is achieved by dividing the classification task into multiple steps rather than classifying them collectively.",2021,10.2196/23693,diagnosis,False
Fast and fully-automated detection and segmentation of pulmonary nodules in thoracic CT scans using deep convolutional neural networks,"Deep learning techniques have been extensively used in computerized pulmonary nodule analysis in recent years. Many reported studies still utilized hybrid methods for diagnosis, in which convolutional neural networks (CNNs) are used only as one part of the pipeline, and the whole system still needs either traditional image processing modules or human intervention to obtain final results. In this paper, we introduced a fast and fully-automated end-to-end system that can efficiently segment precise lung nodule contours from raw thoracic CT scans. Our proposed system has four major modules: candidate nodule detection with Faster regional-CNN (R-CNN), candidate merging, false positive (FP) reduction with CNN, and nodule segmentation with customized fully convolutional neural network (FCN). The entire system has no human interaction or database specific design. The average runtime is about 16 s per scan on a standard workstation. The nodule detection accuracy is 91.4% and 94.6% with an average of 1 and 4 false positives (FPs) per scan. The average dice coefficient of nodule segmentation compared to the groundtruth is 0.793.",2019,10.1016/j.compmedimag.2019.02.003,diagnosis,True
Fast automated detection of COVID-19 from medical images using convolutional neural networks,"Coronavirus disease 2019 (COVID-19) is a global pandemic posing significant health risks. The diagnostic test sensitivity of COVID-19 is limited due to irregularities in specimen handling. We propose a deep learning framework that identifies COVID-19 from medical images as an auxiliary testing method to improve diagnostic sensitivity. We use pseudo-coloring methods and a platform for annotating X-ray and computed tomography images to train the convolutional neural network, which achieves a performance similar to that of experts and provides high scores for multiple statistical indices (F1 scores > 96.72% (0.9307, 0.9890) and specificity >99.33% (0.9792, 1.0000)). Heatmaps are used to visualize the salient features extracted by the neural network. The neural network-based regression provides strong correlations between the lesion areas in the images and five clinical indicators, resulting in high accuracy of the classification framework. The proposed method represents a potential computer-aided diagnosis method for COVID-19 in clinical practice.",2021,10.1038/s42003-020-01535-7,diagnosis,True
FBSED based automatic diagnosis of COVID-19 using X-ray and CT images,"This work introduces the Fourier-Bessel series expansion-based decomposition (FBSED) method, which is an implementation of the wavelet packet decomposition approach in the Fourier-Bessel series expansion domain. The proposed method has been used for the diagnosis of pneumonia caused by the 2019 novel coronavirus disease (COVID-19) using chest X-ray image (CXI) and chest computer tomography image (CCTI). The FBSED method is used to decompose CXI and CCTI into sub-band images (SBIs). The SBIs are then used to train various pre-trained convolutional neural network (CNN) models separately using a transfer learning approach. The combination of SBI and CNN is termed as one channel. Deep features from each channel are fused to get a feature vector. Different classifiers are used to classify pneumonia caused by COVID-19 from other viral and bacterial pneumonia and healthy subjects with the extracted feature vector. The different combinations of channels have also been analyzed to make the process computationally efficient. For CXI and CCTI databases, the best performance has been obtained with only one and four channels, respectively. The proposed model was evaluated using 5-fold and 10-fold cross-validation processes. The average accuracy for the CXI database was 100% for both 5-fold and 10-fold cross-validation processes, and for the CCTI database, it is 97.6% for the 5-fold cross-validation process. Therefore, the proposed method may be used by radiologists to rapidly diagnose patients with COVID-19.",2021,10.1016/j.compbiomed.2021.104454,diagnosis,True
Feature-shared adaptive-boost deep learning for invasiveness classification of pulmonary subsolid nodules in CT images,"PURPOSE: In clinical practice, invasiveness is an important reference indicator for differentiating the malignant degree of subsolid pulmonary nodules. These nodules can be classified as atypical adenomatous hyperplasia (AAH), adenocarcinoma in situ (AIS), minimally invasive adenocarcinoma (MIA), or invasive adenocarcinoma (IAC). The automatic determination of a nodule's invasiveness based on chest CT scans can guide treatment planning. However, it is challenging, owing to the insufficiency of training data and their interclass similarity and intraclass variation. To address these challenges, we propose a two-stage deep learning strategy for this task: prior-feature learning followed by adaptive-boost deep learning. METHODS: The adaptive-boost deep learning is proposed to train a strong classifier for invasiveness classification of subsolid nodules in chest CT images, using multiple 3D convolutional neural network (CNN)-based weak classifiers. Because ensembles of multiple deep 3D CNN models have a huge number of parameters and require large computing resources along with more training and testing time, the prior-feature learning is proposed to reduce the computations by sharing the CNN layers between all weak classifiers. Using this strategy, all weak classifiers can be integrated into a single network. RESULTS: Tenfold cross validation of binary classification was conducted on a total of 1357 nodules, including 765 noninvasive (AAH and AIS) and 592 invasive nodules (MIA and IAC). Ablation experimental results indicated that the proposed binary classifier achieved an accuracy of 73.4\% ± 1.4 with an AUC of 81.3 \% ± 2.2 . These results are superior compared to those achieved by three experienced chest imaging specialists who achieved an accuracy of 69.1\% , 69.3\% , and 67.9\% , respectively. About 200 additional nodules were also collected. These nodules covered 50 cases for each category (AAH, AIS, MIA, and IAC, respectively). Both binary and multiple classifications were performed on these data and the results demonstrated that the proposed method definitely achieves better performance than the performance achieved by nonensemble deep learning methods. CONCLUSIONS: It can be concluded that the proposed adaptive-boost deep learning can significantly improve the performance of invasiveness classification of pulmonary subsolid nodules in CT images, while the prior-feature learning can significantly reduce the total size of deep models. The promising results on clinical data show that the trained models can be used as an effective lung cancer screening tool in hospitals. Moreover, the proposed strategy can be easily extended to other similar classification tasks in 3D medical images.",2020,10.1002/mp.14068,diagnosis,True
Federated learning for predicting clinical outcomes in patients with COVID-19,"Federated learning (FL) is a method used for training artificial intelligence models with data from multiple sources while maintaining data anonymity, thus removing many barriers to data sharing. Here we used data from 20 institutes across the globe to train a FL model, called EXAM (electronic medical record (EMR) chest X-ray AI model), that predicts the future oxygen requirements of symptomatic patients with COVID-19 using inputs of vital signs, laboratory data and chest X-rays. EXAM achieved an average area under the curve (AUC) >0.92 for predicting outcomes at 24 and 72 h from the time of initial presentation to the emergency room, and it provided 16% improvement in average AUC measured across all participating sites and an average increase in generalizability of 38% when compared with models trained at a single site using that site's data. For prediction of mechanical ventilation treatment or death at 24 h at the largest independent test site, EXAM achieved a sensitivity of 0.950 and specificity of 0.882. In this study, FL facilitated rapid data science collaboration without data exchange and generated a model that generalized across heterogeneous, unharmonized datasets for prediction of clinical outcomes in patients with COVID-19, setting the stage for the broader use of FL in healthcare.",2021,10.1038/s41591-021-01506-3,prognosis,True
Federated Semi-Supervised Multi-Task Learning to Detect COVID-19 and Lungs Segmentation Marking Using Chest Radiography Images and Raspberry Pi Devices: An Internet of Medical Things Application,"Internet of Medical Things (IoMT) provides an excellent opportunity to investigate better automatic medical decision support tools with the effective integration of various medical equipment and associated data. This study explores two such medical decision-making tasks, namely COVID-19 detection and lung area segmentation detection, using chest radiography images. We also explore different cutting-edge machine learning techniques, such as federated learning, semi-supervised learning, transfer learning, and multi-task learning to explore the issue. To analyze the applicability of computationally less capable edge devices in the IoMT system, we report the results using Raspberry Pi devices as accuracy, precision, recall, Fscore for COVID-19 detection, and average dice score for lung segmentation detection tasks. We also publish the results obtained through server-centric simulation for comparison. The results show that Raspberry Pi-centric devices provide better performance in lung segmentation detection, and server-centric experiments provide better results in COVID-19 detection. We also discuss the IoMT application-centric settings, utilizing medical data and decision support systems, and posit that such a system could benefit all the stakeholders in the IoMT domain.",2021,10.3390/s21155025,diagnosis,False
FedSGDCOVID: Federated SGD COVID-19 Detection under Local Differential Privacy Using Chest X-ray Images and Symptom Information,"Coronavirus (COVID-19) has created an unprecedented global crisis because of its detrimental effect on the global economy and health. COVID-19 cases have been rapidly increasing, with no sign of stopping. As a result, test kits and accurate detection models are in short supply. Early identification of COVID-19 patients will help decrease the infection rate. Thus, developing an automatic algorithm that enables the early detection of COVID-19 is essential. Moreover, patient data are sensitive, and they must be protected to prevent malicious attackers from revealing information through model updates and reconstruction. In this study, we presented a higher privacy-preserving federated learning system for COVID-19 detection without sharing data among data owners. First, we constructed a federated learning system using chest X-ray images and symptom information. The purpose is to develop a decentralized model across multiple hospitals without sharing data. We found that adding the spatial pyramid pooling to a 2D convolutional neural network improves the accuracy of chest X-ray images. Second, we explored that the accuracy of federated learning for COVID-19 identification reduces significantly for non-independent and identically distributed (Non-IID) data. We then proposed a strategy to improve the model's accuracy on Non-IID data by increasing the total number of clients, parallelism (client-fraction), and computation per client. Finally, for our federated learning model, we applied a differential privacy stochastic gradient descent (DP-SGD) to improve the privacy of patient data. We also proposed a strategy to maintain the robustness of federated learning to ensure the security and accuracy of the model.",2022,10.3390/s22103728,diagnosis,False
FissureNet: A Deep Learning Approach For Pulmonary Fissure Detection in CT Images,"Pulmonary fissure detection in computed tomography (CT) is a critical component for automatic lobar segmentation. The majority of fissure detection methods use feature descriptors that are hand-crafted, low-level, and have local spatial extent. The design of such feature detectors is typically targeted toward normal fissure anatomy, yielding low sensitivity to weak, and abnormal fissures that are common in clinical data sets. Furthermore, local features commonly suffer from low specificity, as the complex textures in the lung can be indistinguishable from the fissure when the global context is not considered. We propose a supervised discriminative learning framework for simultaneous feature extraction and classification. The proposed framework, called FissureNet, is a coarse-to-fine cascade of two convolutional neural networks. The coarse-to-fine strategy alleviates the challenges associated with training a network to segment a thin structure that represents a small fraction of the image voxels. FissureNet was evaluated on a cohort of 3706 subjects with inspiration and expiration 3DCT scans from the COPDGene clinical trial and a cohort of 20 subjects with 4DCT scans from a lung cancer clinical trial. On both data sets, FissureNet showed superior performance compared with a deep learning approach using the U-Net architecture and a Hessian-based fissure detection method in terms of area under the precision-recall curve (PR-AUC). The overall PR-AUC for FissureNet, U-Net, and Hessian on the COPDGene (lung cancer) data set was 0.980 (0.966), 0.963 (0.937), and 0.158 (0.182), respectively. On a subset of 30 COPDGene scans, FissureNet was compared with a recently proposed advanced fissure detection method called derivative of sticks (DoS) and showed superior performance with a PR-AUC of 0.991 compared with 0.668 for DoS.",2019,10.1109/tmi.2018.2858202,diagnosis,True
From community-acquired pneumonia to COVID-19: a deep learning-based method for quantitative analysis of COVID-19 on thick-section CT scans,"OBJECTIVE: To develop a fully automated AI system to quantitatively assess the disease severity and disease progression of COVID-19 using thick-section chest CT images. METHODS: In this retrospective study, an AI system was developed to automatically segment and quantify the COVID-19-infected lung regions on thick-section chest CT images. Five hundred thirty-one CT scans from 204 COVID-19 patients were collected from one appointed COVID-19 hospital. The automatically segmented lung abnormalities were compared with manual segmentation of two experienced radiologists using the Dice coefficient on a randomly selected subset (30 CT scans). Two imaging biomarkers were automatically computed, i.e., the portion of infection (POI) and the average infection HU (iHU), to assess disease severity and disease progression. The assessments were compared with patient status of diagnosis reports and key phrases extracted from radiology reports using the area under the receiver operating characteristic curve (AUC) and Cohen's kappa, respectively. RESULTS: The dice coefficient between the segmentation of the AI system and two experienced radiologists for the COVID-19-infected lung abnormalities was 0.74 ± 0.28 and 0.76 ± 0.29, respectively, which were close to the inter-observer agreement (0.79 ± 0.25). The computed two imaging biomarkers can distinguish between the severe and non-severe stages with an AUC of 0.97 (p value < 0.001). Very good agreement (κ = 0.8220) between the AI system and the radiologists was achieved on evaluating the changes in infection volumes. CONCLUSIONS: A deep learning-based AI system built on the thick-section CT imaging can accurately quantify the COVID-19-associated lung abnormalities and assess the disease severity and its progressions. KEY POINTS: • A deep learning-based AI system was able to accurately segment the infected lung regions by COVID-19 using the thick-section CT scans (Dice coefficient ≥ 0.74). • The computed imaging biomarkers were able to distinguish between the non-severe and severe COVID-19 stages (area under the receiver operating characteristic curve 0.97). • The infection volume changes computed by the AI system were able to assess the COVID-19 progression (Cohen's kappa 0.8220).",2020,10.1007/s00330-020-07042-x,diagnosis,True
Fully automated unified prognosis of Covid-19 chest X-ray/CT scan images using Deep Covix-Net model,"SARS-COV2 (Covid-19) prevails in the form of multiple mutant variants causing pandemic situations around the world. Thus, medical diagnosis is not accurate. Although several clinical diagnostic methodologies have been introduced hitherto, chest X-ray and computed tomography (CT) imaging techniques complement the analytical methods (for instance, RT-PCR) to a certain extent. In this context, we demonstrate a novel framework by employing various image segmentation models to leverage the available image databases (9000 chest X-ray images and 6000 CT scan images). The proposed methodology is expected to assist in the prognosis of Covid-19-infected individuals through examination of chest X-rays and CT scans of images using the Deep Covix-Net model for identifying novel coronavirus-infected patients effectively and efficiently. The slice of the precision score is analysed in terms of performance metrics such as accuracy, the confusion matrix, and the receiver operating characteristic curve. The result leans on the database obtainable in the GitHub and Kaggle repository, conforming to their endorsed chest X-ray and CT images. The classification performances of various algorithms were examined for a test set with 1800 images. The proposed model achieved a 96.8% multiple-classification accuracy among Covid-19, normal, and pneumonia chest X-ray databases. Moreover, it attained a 97% accuracy among Covid-19 and normal CT scan images. Thus, the proposed mechanism achieves the rigorousness associated with the machine learning technique, providing rapid outcomes for both training and testing datasets.",2021,10.1016/j.compbiomed.2021.104729,prognosis,True
Fully automatic detection of lung nodules in CT images using a hybrid feature set,"PURPOSE: The aim of this study was to develop a novel technique for lung nodule detection using an optimized feature set. This feature set has been achieved after rigorous experimentation, which has helped in reducing the false positives significantly. METHOD: The proposed method starts with preprocessing, removing any present noise from input images, followed by lung segmentation using optimal thresholding. Then the image is enhanced using multiscale dot enhancement filtering prior to nodule detection and feature extraction. Finally, classification of lung nodules is achieved using Support Vector Machine (SVM) classifier. The feature set consists of intensity, shape (2D and 3D) and texture features, which have been selected to optimize the sensitivity and reduce false positives. In addition to SVM, some other supervised classifiers like K-Nearest-Neighbor (KNN), Decision Tree and Linear Discriminant Analysis (LDA) have also been used for performance comparison. The extracted features have also been compared class-wise to determine the most relevant features for lung nodule detection. The proposed system has been evaluated using 850 scans from Lung Image Database Consortium (LIDC) dataset and k-fold cross-validation scheme. RESULTS: The overall sensitivity has been improved compared to the previous methods and false positives per scan have been reduced significantly. The achieved sensitivities at detection and classification stages are 94.20% and 98.15%, respectively, with only 2.19 false positives per scan. CONCLUSIONS: It is very difficult to achieve high performance metrics using only a single feature class therefore hybrid approach in feature selection remains a better choice. Choosing right set of features can improve the overall accuracy of the system by improving the sensitivity and reducing false positives.",2017,10.1002/mp.12273,diagnosis,True
Fully automatic pipeline of convolutional neural networks and capsule networks to distinguish COVID-19 from community-acquired pneumonia via CT images,"BACKGROUND: Chest computed tomography (CT) is crucial in the diagnosis of coronavirus disease 2019 (COVID-19). However, the persistent pandemic and similar CT manifestations between COVID-19 and community-acquired pneumonia (CAP) raise methodological requirements. METHODS: A fully automatic pipeline of deep learning is proposed for distinguishing COVID-19 from CAP using CT images. Inspired by the diagnostic process of radiologists, the pipeline comprises four connected modules for lung segmentation, selection of slices with lesions, slice-level prediction, and patient-level prediction. The roles of the first and second modules and the effectiveness of the capsule network for slice-level prediction were investigated. A dataset of 326 CT scans was collected to train and test the pipeline. Another public dataset of 110 patients was used to evaluate the generalization capability. RESULTS: LinkNet exhibited the largest intersection over union (0.967) and Dice coefficient (0.983) for lung segmentation. For the selection of slices with lesions, the capsule network with the ResNet50 block achieved an accuracy of 92.5% and an area under the curve (AUC) of 0.933. The capsule network using the DenseNet121 block demonstrated better performance for slice-level prediction, with an accuracy of 97.1% and AUC of 0.992. For both datasets, the prediction accuracy of our pipeline was 100% at the patient level. CONCLUSIONS: The proposed fully automatic deep learning pipeline of deep learning can distinguish COVID-19 from CAP via CT images rapidly and accurately, thereby accelerating diagnosis and augmenting the performance of radiologists. This pipeline is convenient for use by radiologists and provides explainable predictions.",2022,10.1016/j.compbiomed.2021.105182,diagnosis,True
Fully convolutional network-based multi-output model for automatic segmentation of organs at risk in thorax,"PURPOSE: To propose a multi-output fully convolutional network (MOFCN) to segment bilateral lung, heart and spinal cord in the planning thoracic computed tomography (CT) slices automatically and simultaneously. METHODS: The MOFCN includes two components: one main backbone and three branches. The main backbone extracts the features about lung, heart and spinal cord. The extracted features are transferred to three branches which correspond to three organs respectively. The longest branch to segment spinal cord is nine layers, including input and output layers. The MOFCN was evaluated on 19,277 CT slices from 966 patients with cancer in the thorax. In these slices, the organs at risk (OARs) were delineated and validated by experienced radiation oncologists, and served as ground truth for training and evaluation. The data from 61 randomly chosen patients were used for training and validation. The remaining 905 patients' slices were used for testing. The metric used to evaluate the similarity between the auto-segmented organs and their ground truth was Dice. Besides, we compared the MOFCN with other published models. To assess the distinct output design and the impact of layer number and dilated convolution, we compared MOFCN with a multi-label learning model and its variants. By analyzing the not good performances, we suggested possible solutions. RESULTS: MOFCN achieved Dice of 0.95 ± 0.02 for lung, 0.91 ± 0.03 for heart and 0.87 ± 0.06 for spinal cord. Compared to other models, MOFCN could achieve a comparable accuracy with the least time cost. CONCLUSION: The results demonstrated the MOFCN's effectiveness. It uses less parameters to delineate three OARs simultaneously and automatically, and thus shows a relatively low requirement for hardware and has potential for broad application.",2021,10.1177/00368504211020161,treatment,True
Fully Integrated Quantitative Multiparametric Analysis of Non-Small Cell Lung Cancer at 3-T PET/MRI: Toward One-Stop-Shop Tumor Biological Characterization at the Supervoxel Level,"INTRODUCTION: The aim of this study was to study the feasibility of a fully integrated multiparametric imaging framework to characterize non-small cell lung cancer (NSCLC) at 3-T PET/MRI. PATIENTS AND METHODS: An 18F-FDG PET/MRI multiparametric imaging framework was developed and prospectively applied to 11 biopsy-proven NSCLC patients. For each tumor, 12 parametric maps were generated, including PET full kinetic modeling, apparent diffusion coefficient, T1/T2 relaxation times, and DCE full kinetic modeling. Gaussian mixture model-based clustering was applied at the whole data set level to define supervoxels of similar multidimensional PET/MRI behaviors. Taking the multidimensional voxel behaviors as input and the supervoxel class as output, machine learning procedure was finally trained and validated voxelwise to reveal the dominant PET/MRI characteristics of these supervoxels at the whole data set and individual tumor levels. RESULTS: The Gaussian mixture model-based clustering clustering applied at the whole data set level (17,316 voxels) found 3 main multidimensional behaviors underpinned by the 12 PET/MRI quantitative parameters. Four dominant PET/MRI parameters of clinical relevance (PET: k2, k3 and DCE: ve, vp) predicted the overall supervoxel behavior with 97% of accuracy (SD, 0.7; 10-fold cross-validation). At the individual tumor level, these dimensionality-reduced supervoxel maps showed mean discrepancy of 16.7% compared with the original ones. CONCLUSIONS: One-stop-shop PET/MRI multiparametric quantitative analysis of NSCLC is clinically feasible. Both PET and MRI parameters are useful to characterize the behavior of tumors at the supervoxel level. In the era of precision medicine, the full capabilities of PET/MRI would give further insight of the characterization of NSCLC behavior, opening new avenues toward image-based personalized medicine in this field.",2021,10.1097/rlu.0000000000003680,diagnosis,False
Fused feature signatures to probe tumour radiogenomics relationships,"Radiogenomics relationships (RRs) aims to identify statistically significant correlations between medical image features and molecular characteristics from analysing tissue samples. Previous radiogenomics studies mainly relied on a single category of image feature extraction techniques (ETs); these are (i) handcrafted ETs that encompass visual imaging characteristics, curated from knowledge of human experts and, (ii) deep ETs that quantify abstract-level imaging characteristics from large data. Prior studies therefore failed to leverage the complementary information that are accessible from fusing the ETs. In this study, we propose a fused feature signature (FF(Sig)): a selection of image features from handcrafted and deep ETs (e.g., transfer learning and fine-tuning of deep learning models). We evaluated the FF(Sig)'s ability to better represent RRs compared to individual ET approaches with two public datasets: the first dataset was used to build the FF(Sig) using 89 patients with non-small cell lung cancer (NSCLC) comprising of gene expression data and CT images of the thorax and the upper abdomen for each patient; the second NSCLC dataset comprising of 117 patients with CT images and RNA-Seq data and was used as the validation set. Our results show that our FF(Sig) encoded complementary imaging characteristics of tumours and identified more RRs with a broader range of genes that are related to important biological functions such as tumourigenesis. We suggest that the FF(Sig) has the potential to identify important RRs that may assist cancer diagnosis and treatment in the future.",2022,10.1038/s41598-022-06085-y,diagnosis,True
Fusion of 3D lung CT and serum biomarkers for diagnosis of multiple pathological types on pulmonary nodules,"BACKGROUND AND OBJECTIVE: Current researches on pulmonary nodules mainly focused on the binary-classification of benign and malignant pulmonary nodules. However, in clinical applications, it is not enough to judge whether pulmonary nodules are benign or malignant. In this paper, we proposed a fusion model based on the Lung Information Dataset Containing 3D CT Images and Serum Biomarkers (LIDCCISB) we constructed to accurately diagnose the types of pulmonary nodules in squamous cell carcinoma, adenocarcinoma, inflammation and other benign diseases. METHODS: Using single modal information of lung 3D CT images and single modal information of Lung Tumor Biomarkers (LTBs) in LIDCCISB, a Multi-resolution 3D Multi-classification deep learning model (Mr-Mc) and a Multi-Layer Perceptron machine learning model (MLP) were constructed for diagnosing multiple pathological types of pulmonary nodules, respectively. To comprehensively use the double modal information of CT images and LTBs, we used transfer learning to fuse Mr-Mc and MLP, and constructed a multimodal information fusion model that could classify multiple pathological types of benign and malignant pulmonary nodules. RESULTS: Experiments showed that the constructed Mr-Mc model can achieve an average accuracy of 0.805 and MLP model can achieve an average accuracy of 0.887. The fusion model was verified on a dataset containing 64 samples, and achieved an average accuracy of 0.906. CONCLUSIONS: This is the first study to simultaneously use CT images and LTBs to diagnose multiple pathological types of benign and malignant pulmonary nodules, and experiments showed that our research was more advanced and more suitable for practical clinical applications.",2021,10.1016/j.cmpb.2021.106381,diagnosis,True
Fusion of FDG-PET Image and Clinical Features for Prediction of Lung Metastasis in Soft Tissue Sarcomas,"Extracting massive features from images to quantify tumors provides a new insight to solve the problem that tumor heterogeneity is difficult to assess quantitatively. However, quantification of tumors by single-mode methods often has defects such as difficulty in features extraction and high computational complexity. The multimodal approach has shown effective application prospects in solving these problems. In this paper, we propose a feature fusion method based on positron emission tomography (PET) images and clinical information, which is used to obtain features for lung metastasis prediction of soft tissue sarcomas (STSs). Random forest method was adopted to select effective features by eliminating irrelevant or redundant features, and then they were used for the prediction of the lung metastasis combined with back propagation (BP) neural network. The results show that the prediction ability of the proposed model using fusion features is better than that of the model using an image or clinical feature alone. Furthermore, a good performance can be obtained using 3 standard uptake value (SUV) features of PET image and 7 clinical features, and its average accuracy, sensitivity, and specificity on all the sets can reach 92%, 91%, and 92%, respectively. Therefore, the fusing features have the potential to predict lung metastasis for STSs.",2020,10.1155/2020/8153295,prognosis,False
Gross Tumor Volume Segmentation for Stage III NSCLC Radiotherapy Using 3D ResSE-Unet,"INTRODUCTION: Radiotherapy is one of the most effective ways to treat lung cancer. Accurately delineating the gross target volume is a key step in the radiotherapy process. In current clinical practice, the target area is still delineated manually by radiologists, which is time-consuming and laborious. However, these problems can be better solved by deep learning-assisted automatic segmentation methods. METHODS: In this paper, a 3D CNN model named 3D ResSE-Unet is proposed for gross tumor volume segmentation for stage III NSCLC radiotherapy. This model is based on 3D Unet and combines residual connection and channel attention mechanisms. Three-dimensional convolution operation and encoding-decoding structure are used to mine three-dimensional spatial information of tumors from computed tomography data. Inspired by ResNet and SE-Net, residual connection and channel attention mechanisms are used to improve segmentation performance. A total of 214 patients with stage III NSCLC were collected selectively and 148 cases were randomly selected as the training set, 30 cases as the validation set, and 36 cases as the testing set. The segmentation performance of models was evaluated by the testing set. In addition, the segmentation results of different depths of 3D Unet were analyzed. And the performance of 3D ResSE-Unet was compared with 3D Unet, 3D Res-Unet, and 3D SE-Unet. RESULTS: Compared with other depths, 3D Unet with four downsampling depths is more suitable for our work. Compared with 3D Unet, 3D Res-Unet, and 3D SE-Unet, 3D ResSE-Unet can obtain superior results. Its dice similarity coefficient, 95th-percentile of Hausdorff distance, and average surface distance can reach 0.7367, 21.39mm, 4.962mm, respectively. And the average time cost of 3D ResSE-Unet to segment a patient is only about 10s. CONCLUSION: The method proposed in this study provides a new tool for GTV auto-segmentation and may be useful for lung cancer radiotherapy.",2022,10.1177/15330338221090847,treatment,True
High precision localization of pulmonary nodules on chest CT utilizing axial slice number labels,"BACKGROUND: Reidentification of prior nodules for temporal comparison is an important but time-consuming step in lung cancer screening. We develop and evaluate an automated nodule detector that utilizes the axial-slice number of nodules found in radiology reports to generate high precision nodule predictions. METHODS: 888 CTs from Lung Nodule Analysis were used to train a 2-dimensional (2D) object detection neural network. A pipeline of 2D object detection, 3D unsupervised clustering, false positive reduction, and axial-slice numbers were used to generate nodule candidates. 47 CTs from the National Lung Cancer Screening Trial (NLST) were used for model evaluation. RESULTS: Our nodule detector achieved a precision of 0.962 at a recall of 0.573 on the NLST test set for any nodule. When adjusting for unintended nodule predictions, we achieved a precision of 0.931 at a recall 0.561, which corresponds to 0.06 false positives per CT. Error analysis revealed better detection of nodules with soft tissue attenuation compared to ground glass and undeterminable attenuation. Nodule margins, size, location, and patient demographics did not differ between correct and incorrect predictions. CONCLUSIONS: Utilization of axial-slice numbers from radiology reports allowed for development of a lung nodule detector with a low false positive rate compared to prior feature-engineering and machine learning approaches. This high precision nodule detector can reduce time spent on reidentification of prior nodules during lung cancer screening and can rapidly develop new institutional datasets to explore novel applications of computer vision in lung cancer imaging.",2021,10.1186/s12880-021-00594-4,diagnosis,True
High-resolution CT image analysis based on 3D convolutional neural network can enhance the classification performance of radiologists in classifying pulmonary non-solid nodules,"OBJECTIVE: To investigate whether 3D convolutional neural network (CNN) is able to enhance the classification performance of radiologists in classifying pulmonary non-solid nodules (NSNs). MATERIALS AND METHODS: Data of patients with solitary NSNs and diagnosed as adenocarcinoma in situ (AIS), minimally invasive adenocarcinoma (MIA), or invasive adenocarcinoma (IAC) in pathological after surgical resection were analyzed retrospectively. Ultimately, 532 patients in our institution were included in the study: 427 cases (144 AIS, 167 MIA, 116 IAC) were assigned to training dataset and 105 cases (36 AIS, 41 MIA and 28 IAC) were assigned to validation dataset. For external validation, 177 patients (60 AIS, 69 MIA and 48 IAC) from another hospital were assigned to testing dataset. The clinical and morphological characteristics of NSNs were established as radiologists' model. The trained classification model based on 3D CNN was used to identify NSNs types automatically. The evaluation and comparison on classification performance of the two models and CNN + radiologists' model were performed via receiver operating curve (ROC) analysis and integrated discrimination improvement (IDI) index. The Akaike information criterion (AIC) was calculated to find the best-fit model. RESULTS: In external testing dataset, radiologists' model showed inferior classification performance than CNN model both in discriminating AIS from MIA-IAC and AIS-MIA from IAC (the area under the ROC curve (Az value), 0.693 vs 0.820, P = 0.011; 0.746 vs 0.833, P = 0.026, respectively). However, combining CNN significantly enhanced the classification performance of radiologists and exhibited higher Az values than CNN model alone (Az values, 0.893 vs 0.820, P < 0.001; 0.906 vs 0.833, P < 0.001, respectively). The IDI index further confirmed CNN's contribution to radiologists in classifying NSNs (IDI = 25.8 % (18.3-46.1 %), P < 0.001; IDI = 30.1 % (26.1-45.2 %), P < 0.001, respectively). The CNN + radiologists' model also provided the best fit over radiologists' model and CNN model alone (AIC value 63.3 % vs. 29.5 %, 49.5 %, P < 0.001; 69.2 % vs. 34.9 %, 53.6 %, P < 0.001, respectively). CONCLUSION: CNN successfully classified NSNs based on CT images and its classification performance were superior to radiologists' model. But the classification performance of radiologists can be significantly enhanced when combined with CNN in classifying NSNs.",2021,10.1016/j.ejrad.2021.109810,diagnosis,True
Highly accurate model for prediction of lung nodule malignancy with CT scans,"Computed tomography (CT) examinations are commonly used to predict lung nodule malignancy in patients, which are shown to improve noninvasive early diagnosis of lung cancer. It remains challenging for computational approaches to achieve performance comparable to experienced radiologists. Here we present NoduleX, a systematic approach to predict lung nodule malignancy from CT data, based on deep learning convolutional neural networks (CNN). For training and validation, we analyze >1000 lung nodules in images from the LIDC/IDRI cohort. All nodules were identified and classified by four experienced thoracic radiologists who participated in the LIDC project. NoduleX achieves high accuracy for nodule malignancy classification, with an AUC of ~0.99. This is commensurate with the analysis of the dataset by experienced radiologists. Our approach, NoduleX, provides an effective framework for highly accurate nodule malignancy prediction with the model trained on a large patient population. Our results are replicable with software available at http://bioinformatics.astate.edu/NoduleX .",2018,10.1038/s41598-018-27569-w,diagnosis,True
Histologic subtype classification of non-small cell lung cancer using PET/CT images,"PURPOSES: To evaluate the capability of PET/CT images for differentiating the histologic subtypes of non-small cell lung cancer (NSCLC) and to identify the optimal model from radiomics-based machine learning/deep learning algorithms. METHODS: In this study, 867 patients with adenocarcinoma (ADC) and 552 patients with squamous cell carcinoma (SCC) were retrospectively analysed. A stratified random sample of 283 patients (20%) was used as the testing set (173 ADC and 110 SCC); the remaining data were used as the training set. A total of 688 features were extracted from each outlined tumour region. Ten feature selection techniques, ten machine learning (ML) models and the VGG16 deep learning (DL) algorithm were evaluated to construct an optimal classification model for the differential diagnosis of ADC and SCC. Tenfold cross-validation and grid search technique were employed to evaluate and optimize the model hyperparameters on the training dataset. The area under the receiver operating characteristic curve (AUROC), accuracy, precision, sensitivity and specificity was used to evaluate the performance of the models on the test dataset. RESULTS: Fifty top-ranked subset features were selected by each feature selection technique for classification. The linear discriminant analysis (LDA) (AUROC, 0.863; accuracy, 0.794) and support vector machine (SVM) (AUROC, 0.863; accuracy, 0.792) classifiers, both of which coupled with the ℓ(2,1)NR feature selection method, achieved optimal performance. The random forest (RF) classifier (AUROC, 0.824; accuracy, 0.775) and ℓ(2,1)NR feature selection method (AUROC, 0.815; accuracy, 0.764) showed excellent average performance among the classifiers and feature selection methods employed in our study, respectively. Furthermore, the VGG16 DL algorithm (AUROC, 0.903; accuracy, 0.841) outperformed all conventional machine learning methods in combination with radiomics. CONCLUSION: Employing radiomic machine learning/deep learning algorithms could help radiologists to differentiate the histologic subtypes of NSCLC via PET/CT images.",2021,10.1007/s00259-020-04771-5,diagnosis,True
Histological Subtypes Classification of Lung Cancers on CT Images Using 3D Deep Learning and Radiomics,"RATIONALE AND OBJECTIVES: Histological subtypes of lung cancers are critical for clinical treatment decision. In this study, we attempt to use 3D deep learning and radiomics methods to automatically distinguish lung adenocarcinomas (ADC), squamous cell carcinomas (SCC), and small cell lung cancers (SCLC) respectively on Computed Tomography images, and then compare their performance. MATERIALS AND METHODS: 920 patients (mean age 61.2, range, 17-87; 340 Female and 580 Male) with lung cancer, including 554 patients with ADC, 175 patients with lung SCC and 191 patients with SCLC, were included in this retrospective study from January 2013 to August 2018. Histopathologic analysis was available for every patient. The classification models based on 3D deep learning (named the ProNet) and radiomics (named com_radNet) were designed to classify lung cancers into the three types mentioned above according to histopathologic results. The training, validation and testing cohorts counted 0.70, 0.15, and 0.15 of the whole datasets respectively. RESULTS: The ProNet model used to classify the three types of lung cancers achieved the F1-scores of 90.0%, 72.4%, 83.7% in ADC, SCC, and SCLC respectively, and the weighted average F1-score of 73.2%. For com_radNet, the F1-scores achieved 83.1%, 75.4%, 85.1% in ADC, SCC, and SCLC, and the weighted average F1-score was 72.2%. The area under the receiver operating characteristic curve of the ProNet model and com_radNet were 0.840 and 0.789, and the accuracy were 71.6% and 74.7% respectively. CONCLUSION: The ProNet and com_radNet models we developed can achieve high performance in distinguishing ADC, SCC, and SCLC and may be promising approaches for non-invasive predicting histological subtypes of lung cancers.",2021,10.1016/j.acra.2020.06.010,diagnosis,True
Homological radiomics analysis for prognostic prediction in lung cancer patients,"PURPOSE: This study explored a novel homological analysis method for prognostic prediction in lung cancer patients. MATERIALS AND METHODS: The potential of homology-based radiomic features (HFs) was investigated by comparing HFs to conventional wavelet-based radiomic features (WFs) and combined radiomic features consisting of HFs and WFs (HWFs), using training (n = 135) and validation (n = 70) datasets, and Kaplan-Meier analysis. A total of 13,824 HFs were derived through homology-based texture analysis using Betti numbers, which represent the topologically invariant morphological characteristics of lung cancer. The prognostic potential of HFs was evaluated using statistically significant differences (p-values, log-rank test) to compare the survival curves of high- and low-risk patients. Those patients were stratified into high- and low-risk groups using the medians of the radiomic scores of signatures constructed with an elastic-net-regularized Cox proportional hazard model. Furthermore, deep learning (DL) based on AlexNet was utilized to compare HFs by stratifying patients into the two groups using a network that was pre-trained with over one million natural images from an ImageNet database. RESULTS: For the training dataset, the p-values between the two survival curves were 6.7 × 10(-6) (HF), 5.9 × 10(-3) (WF), 7.4 × 10(-6) (HWF), and 1.1 × 10(-3) (DL). The p-values for the validation dataset were 3.4 × 10(-5) (HF), 6.7 × 10(-1) (WF), 1.7 × 10(-7) (HWF), and 1.2 × 10(-1) (DL). CONCLUSION: This study demonstrates the excellent potential of HFs for prognostic prediction in lung cancer patients.",2020,10.1016/j.ejmp.2019.11.026,prognosis,True
Homology-based radiomic features for prediction of the prognosis of lung cancer based on CT-based radiomics,"PURPOSE: Radiomics is a new technique that enables noninvasive prognostic prediction by extracting features from medical images. Homology is a concept used in many branches of algebra and topology that can quantify the contact degree. In the present study, we developed homology-based radiomic features to predict the prognosis of non-small-cell lung cancer (NSCLC) patients and then evaluated the accuracy of this prediction method. METHODS: Four datasets were used: two to provide training and test data and two for the selection of robust radiomic features. All the datasets were downloaded from The Cancer Imaging Archive (TCIA). In two-dimensional cases, the Betti numbers consist of two values: b(0) (zero-dimensional Betti number), which is the number of isolated components, and b(1) (one-dimensional Betti number), which is the number of one-dimensional or ""circular"" holes. For homology-based evaluation, computed tomography (CT) images must be converted to binarized images in which each pixel has two possible values: 0 or 1. All CT slices of the gross tumor volume were used for calculating the homology histogram. First, by changing the threshold of the CT value (range: -150 to 300 HU) for all its slices, we developed homology-based histograms for b(0) , b(1) , and b(1) /b(0) using binarized images. All histograms were then summed, and the summed histogram was normalized by the number of slices. 144 homology-based radiomic features were defined from the histogram. To compare the standard radiomic features, 107 radiomic features were calculated using the standard radiomics technique. To clarify the prognostic power, the relationship between the values of the homology-based radiomic features and overall survival was evaluated using LASSO Cox regression model and the Kaplan-Meier method. The retained features with nonzero coefficients calculated by the LASSO Cox regression model were used for fitting the regression model. Moreover, these features were then integrated into a radiomics signature. An individualized rad score was calculated from a linear combination of the selected features, which were weighted by their respective coefficients. RESULTS: When the patients in the training and test datasets were stratified into high-risk and low-risk groups according to the rad scores, the overall survival of the groups was significantly different. The C-index values for the homology-based features (rad score), standard features (rad score), and tumor size were 0.625, 0.603, and 0.607, respectively, for the training datasets and 0.689, 0.668, and 0.667 for the test datasets. This result showed that homology-based radiomic features had slightly higher prediction power than the standard radiomic features. CONCLUSIONS: Prediction performance using homology-based radiomic features had a comparable or slightly higher prediction power than standard radiomic features. These findings suggest that homology-based radiomic features may have great potential for improving the prognostic prediction accuracy of CT-based radiomics. In this result, it is noteworthy that there are some limitations.",2020,10.1002/mp.14104,prognosis,True
Human-recognizable CT image features of subsolid lung nodules associated with diagnosis and classification by convolutional neural networks,"OBJECTIVES: The interpretability of convolutional neural networks (CNNs) for classifying subsolid nodules (SSNs) is insufficient for clinicians. Our purpose was to develop CNN models to classify SSNs on CT images and to investigate image features associated with the CNN classification. METHODS: CT images containing SSNs with a diameter of ≤ 3 cm were retrospectively collected. We trained and validated CNNs by a 5-fold cross-validation method for classifying SSNs into three categories (benign and preinvasive lesions [PL], minimally invasive adenocarcinoma [MIA], and invasive adenocarcinoma [IA]) that were histologically confirmed or followed up for 6.4 years. The mechanism of CNNs on human-recognizable CT image features was investigated and visualized by gradient-weighted class activation map (Grad-CAM), separated activation channels and areas, and DeepDream algorithm. RESULTS: The accuracy was 93% for classifying 586 SSNs from 569 patients into three categories (346 benign and PL, 144 MIA, and 96 IA in 5-fold cross-validation). The Grad-CAM successfully located the entire region of image features that determined the final classification. Activated areas in the benign and PL group were primarily smooth margins (p < 0.001) and ground-glass components (p = 0.033), whereas in the IA group, the activated areas were mainly part-solid (p < 0.001) and solid components (p < 0.001), lobulated shapes (p < 0.001), and air bronchograms (p < 0.001). However, the activated areas for MIA were variable. The DeepDream algorithm showed the image features in a human-recognizable pattern that the CNN learned from a training dataset. CONCLUSION: This study provides medical evidence to interpret the mechanism of CNNs that helps support the clinical application of artificial intelligence. KEY POINTS: • CNN achieved high accuracy (93%) in classifying subsolid nodules on CT images into three categories: benign and preinvasive lesions, MIA, and IA. • The gradient-weighted class activation map (Grad-CAM) located the entire region of image features that determined the final classification, and the visualization of the separated activated areas was consistent with radiologists' expertise for diagnosing subsolid nodules. • DeepDream showed the image features that CNN learned from a training dataset in a human-recognizable pattern.",2021,10.1007/s00330-021-07901-1,diagnosis,True
Hybrid COVID-19 segmentation and recognition framework (HMB-HCF) using deep learning and genetic algorithms,"COVID-19 (Coronavirus) went through a rapid escalation until it became a pandemic disease. The normal and manual medical infection discovery may take few days and therefore computer science engineers can share in the development of the automatic diagnosis for fast detection of that disease. The study suggests a hybrid COVID-19 framework (named HMB-HCF) based on deep learning (DL), genetic algorithm (GA), weighted sum (WS), and majority voting principles in nine phases. Its segmentation phase suggests a lung segmentation algorithm using X-Ray images (named HMB-LSAXI) for extracting lungs. Its classification phase is built from a hybrid convolutional neural network (CNN) architecture using an abstractly-designed CNN (named HMB1-COVID19) and transfer learning (TL) pre-trained models (VGG16, VGG19, ResNet50, ResNet101, Xception, DenseNet121, DenseNet169, MobileNet, and MobileNetV2). The hybrid CNN architecture is used for learning, classification, and parameters optimization while GA is used to optimize the hyperparameters. This hybrid working mechanism is combined in an overall algorithm named HMB-DLGA. The study experiments implemented the WS approach to evaluate the models' performance using the loss, accuracy, F1-score, precision, recall, and area under curve (AUC) metrics with different pre-defined ratios. A collected, combined, and unified X-Ray dataset from 8 different public datasets was used alongside the regularization, dropout, and data augmentation techniques to limit the overall overfitting. The applied experiments reported state-of-the-art metrics. VGG16 reported 100% WS metric (i.e., 0.0097, 99.78%, 0.9984, 99.89%, 99.78%, and 0.9996 for the loss, accuracy, F1, precision, recall, and AUC respectively) concerning the highest WS. It also reported a 99.92% WS metric (i.e., 0.0099, 99.84%, 0.9984, 99.84%, 99.84%, and 0.9996 for the loss, accuracy, F1, precision, recall, and AUC respectively) concerning the last reported WS result. HMB-HCF was validated on 13 different public datasets to verify its generalization. The best-achieved metrics were compared with 13 related studies. These extensive experiments' target was the applicability verification and generalization.",2021,10.1016/j.artmed.2021.102156,diagnosis,False
Hybrid Deep-Learning and Machine-Learning Models for Predicting COVID-19,"The COVID-19 pandemic has had a significant impact on public life and health worldwide, putting the world's healthcare systems at risk. The first step in stopping this outbreak is to detect the infection in its early stages, which will relieve the risk, control the outbreak's spread, and restore full functionality to the world's healthcare systems. Currently, PCR is the most prevalent diagnosis tool for COVID-19. However, chest X-ray images may play an essential role in detecting this disease, as they are successful for many other viral pneumonia diseases. Unfortunately, there are common features between COVID-19 and other viral pneumonia, and hence manual differentiation between them seems to be a critical problem and needs the aid of artificial intelligence. This research employs deep- and transfer-learning techniques to develop accurate, general, and robust models for detecting COVID-19. The developed models utilize either convolutional neural networks or transfer-learning models or hybridize them with powerful machine-learning techniques to exploit their full potential. For experimentation, we applied the proposed models to two data sets: the COVID-19 Radiography Database from Kaggle and a local data set from Asir Hospital, Abha, Saudi Arabia. The proposed models achieved promising results in detecting COVID-19 cases and discriminating them from normal and other viral pneumonia with excellent accuracy. The hybrid models extracted features from the flatten layer or the first hidden layer of the neural network and then fed these features into a classification algorithm. This approach enhanced the results further to full accuracy for binary COVID-19 classification and 97.8% for multiclass classification.",2021,10.1155/2021/9996737,diagnosis,False
Hybrid detection of lung nodules on CT scan images,"PURPOSE: The diversity of lung nodules poses difficulty for the current computer-aided diagnostic (CAD) schemes for lung nodule detection on computed tomography (CT) scan images, especially in large-scale CT screening studies. We proposed a novel CAD scheme based on a hybrid method to address the challenges of detection in diverse lung nodules. METHODS: The hybrid method proposed in this paper integrates several existing and widely used algorithms in the field of nodule detection, including morphological operation, dot-enhancement based on Hessian matrix, fuzzy connectedness segmentation, local density maximum algorithm, geodesic distance map, and regression tree classification. All of the adopted algorithms were organized into tree structures with multi-nodes. Each node in the tree structure aimed to deal with one type of lung nodule. RESULTS: The method has been evaluated on 294 CT scans from the Lung Image Database Consortium (LIDC) dataset. The CT scans were randomly divided into two independent subsets: a training set (196 scans) and a test set (98 scans). In total, the 294 CT scans contained 631 lung nodules, which were annotated by at least two radiologists participating in the LIDC project. The sensitivity and false positive per scan for the training set were 87% and 2.61%. The sensitivity and false positive per scan for the testing set were 85.2% and 3.13%. CONCLUSIONS: The proposed hybrid method yielded high performance on the evaluation dataset and exhibits advantages over existing CAD schemes. We believe that the present method would be useful for a wide variety of CT imaging protocols used in both routine diagnosis and screening studies.",2015,10.1118/1.4927573,diagnosis,True
Hybrid ensemble model for differential diagnosis between COVID-19 and common viral pneumonia by chest X-ray radiograph,"BACKGROUND: Chest X-ray radiography (CXR) has been widely considered as an accessible, feasible, and convenient method to evaluate suspected patients' lung involvement during the COVID-19 pandemic. However, with the escalating number of suspected cases, traditional diagnosis via CXR fails to deliver results within a short period of time. Therefore, it is crucial to employ artificial intelligence (AI) to enhance CXRs for obtaining quick and accurate diagnoses. Previous studies have reported the feasibility of utilizing deep learning methods to screen for COVID-19 using CXR and CT results. However, these models only use a single deep learning network for chest radiograph detection; the accuracy of this approach required further improvement. METHODS: In this study, we propose a three-step hybrid ensemble model, including a feature extractor, a feature selector, and a classifier. First, a pre-trained AlexNet with an improved structure extracts the original image features. Then, the ReliefF algorithm is adopted to sort the extracted features, and a trial-and-error approach is used to select the n most important features to reduce the feature dimension. Finally, an SVM classifier provides classification results based on the n selected features. RESULTS: Compared to five existing models (InceptionV3: 97.916 ± 0.408%; SqueezeNet: 97.189 ± 0.526%; VGG19: 96.520 ± 1.220%; ResNet50: 97.476 ± 0.513%; ResNet101: 98.241 ± 0.209%), the proposed model demonstrated the best performance in terms of overall accuracy rate (98.642 ± 0.398%). Additionally, compared to the existing models, the proposed model demonstrates a considerable improvement in classification time efficiency (SqueezeNet: 6.602 ± 0.001s; InceptionV3: 12.376 ± 0.002s; ResNet50: 10.952 ± 0.001s; ResNet101: 18.040 ± 0.002s; VGG19: 16.632 ± 0.002s; proposed model: 5.917 ± 0.001s). CONCLUSION: The model proposed in this article is practical and effective, and can provide high-precision COVID-19 CXR detection. We demonstrated its suitability to aid medical professionals in distinguishing normal CXRs, viral pneumonia CXRs and COVID-19 CXRs efficiently on small sample sizes.",2021,10.1016/j.compbiomed.2021.104252,diagnosis,False
Hybrid-COVID: a novel hybrid 2D/3D CNN based on cross-domain adaptation approach for COVID-19 screening from chest X-ray images,"The novel Coronavirus disease (COVID-19), which first appeared at the end of December 2019, continues to spread rapidly in most countries of the world. Respiratory infections occur primarily in the majority of patients treated with COVID-19. In light of the growing number of COVID-19 cases, the need for diagnostic tools to identify COVID-19 infection at early stages is of vital importance. For decades, chest X-ray (CXR) technologies have proven their ability to accurately detect respiratory diseases. More recently, with the availability of COVID-19 CXR scans, deep learning algorithms have played a critical role in the healthcare arena by allowing radiologists to recognize COVID-19 patients from their CXR images. However, the majority of screening methods for COVID-19 reported in recent studies are based on 2D convolutional neural networks (CNNs). Although 3D CNNs are capable of capturing contextual information compared to their 2D counterparts, their use is limited due to their increased computational cost (i.e. requires much extra memory and much more computing power). In this study, a transfer learning-based hybrid 2D/3D CNN architecture for COVID-19 screening using CXRs has been developed. The proposed architecture consists of the incorporation of a pre-trained deep model (VGG16) and a shallow 3D CNN, combined with a depth-wise separable convolution layer and a spatial pyramid pooling module (SPP). Specifically, the depth-wise separable convolution helps to preserve the useful features while reducing the computational burden of the model. The SPP module is designed to extract multi-level representations from intermediate ones. Experimental results show that the proposed framework can achieve reasonable performances when evaluated on a collected dataset (3 classes to be predicted: COVID-19, Pneumonia, and Normal). Notably, it achieved a sensitivity of 98.33%, a specificity of 98.68% and an overall accuracy of 96.91.",2020,10.1007/s13246-020-00957-1,diagnosis,False
Hypergraph learning for identification of COVID-19 with CT imaging,"The coronavirus disease, named COVID-19, has become the largest global public health crisis since it started in early 2020. CT imaging has been used as a complementary tool to assist early screening, especially for the rapid identification of COVID-19 cases from community acquired pneumonia (CAP) cases. The main challenge in early screening is how to model the confusing cases in the COVID-19 and CAP groups, with very similar clinical manifestations and imaging features. To tackle this challenge, we propose an Uncertainty Vertex-weighted Hypergraph Learning (UVHL) method to identify COVID-19 from CAP using CT images. In particular, multiple types of features (including regional features and radiomics features) are first extracted from CT image for each case. Then, the relationship among different cases is formulated by a hypergraph structure, with each case represented as a vertex in the hypergraph. The uncertainty of each vertex is further computed with an uncertainty score measurement and used as a weight in the hypergraph. Finally, a learning process of the vertex-weighted hypergraph is used to predict whether a new testing case belongs to COVID-19 or not. Experiments on a large multi-center pneumonia dataset, consisting of 2148 COVID-19 cases and 1182 CAP cases from five hospitals, are conducted to evaluate the prediction accuracy of the proposed method. Results demonstrate the effectiveness and robustness of our proposed method on the identification of COVID-19 in comparison to state-of-the-art methods.",2021,10.1016/j.media.2020.101910,diagnosis,False
Identification of Benign and Malignant Lung Nodules in CT Images Based on Ensemble Learning Method,"BACKGROUND AND OBJECTIVE: Under the background of urgent need for computer-aided technology to provide physicians with objective decision support, aiming at reducing the false positive rate of nodule CT detection in pulmonary nodules detection and improving the accuracy of lung nodule recognition, this paper puts forward a method based on ensemble learning to distinguish between malignant and benign pulmonary nodules. METHODS: Firstly, trained on a public data set, a multi-layer feature fusion YOLOv3 network is used to detect lung nodules. Secondly, a CNN was trained to differentiate benign from malignant pulmonary nodules. Then, based on the idea of ensemble learning, the confidence probability of the above two models and the label of the training set are taken as data features to build a Logistic regression model. Finally, two test sets (public data set and private data set) were tested, and the confidence probability output by the two models was fused into the established logistic regression model to determine benign and malignant pulmonary nodules. RESULTS: The YOLOv3 network was trained to detect chest CT images of the test set. The number of pulmonary nodules detected in the public and private test sets was 356 and 314, respectively. The accuracy, sensitivity and specificity of the two test sets were 80.97%, 81.63%, 78.75% and 79.69%, 86.59%, 72.16%, respectively. With CNN training pulmonary nodules benign and malignant discriminant model analysis of two kinds of test set, the result of accuracy, sensitivity and specificity were 90.12%, 90.66%, 89.47% and 88.57%, 85.62%, 90.87%, respectively. Fused model based on YOLOv3 network and CNN is tested on two test sets, and the result of accuracy, sensitivity and specificity were 93.82%, 94.85%, 92.59% and 92.31%, 92.68%, 91.89%, respectively. CONCLUSION: The ensemble learning model is more effective than YOLOv3 network and CNN in removing false positives, and the accuracy of the ensemble. Learning model is higher than the other two networks in identifying pulmonary nodules.",2022,10.1007/s12539-021-00472-1,diagnosis,True
Identification of benign and malignant pulmonary nodules on chest CT using improved 3D U-Net deep learning framework,"PURPOSE: To accurately distinguish benign from malignant pulmonary nodules with CT based on partial structures of 3D U-Net integrated with Capsule Networks (CapNets) and provide a reference for the early diagnosis of lung cancer. METHOD: The dataset consisted of 1177 samples (benign/malignant: 414/763) from 997 patients provided by collaborating hospital. All nodules were biopsy or surgery proven, and pathologic results were regarded as the ""golden standard"". This study utilized partial U-Net to capture the low-level (edge, corner, etc.) information and CapNets to preserve high-level (semantic information) information of nodules. For CapNets, each capsule had a 4 × 4 matrix representing the pose and an activation probability representing the presence of an object. Furthermore, we chose accuracy (ACC), area under the curve (AUC), sensitivity (SE) and specificity (SP) to evaluate the generalization of the proposed architecture and compared its identification performance with 3D U-Net and experienced radiologists. RESULTS: The AUC of our architecture (0.84) was superior to that (0.81) of the original 3D U-Net (p = 0.04, DeLong's test). Moreover, ACC (84.5 %) and SE (92.9 %) of our model were clearly higher than radiologists' ACC (81.0 %) and SE (84.3 %) at the optimal operating point. However, SP (70 %) of our model was slightly lower than radiologists' SP (75 %), which might be the result of class imbalance with limited benign samples involved for algorithm training. CONCLUSIONS: Our architecture showed a high performance for identifying benign and malignant pulmonary nodules, indicating the improved model has a promising application in clinic.",2020,10.1016/j.ejrad.2020.109013,diagnosis,True
Identification of COVID-19 samples from chest X-Ray images using deep learning: A comparison of transfer learning approaches,"BACKGROUND: The novel coronavirus disease 2019 (COVID-19) constitutes a public health emergency globally. The number of infected people and deaths are proliferating every day, which is putting tremendous pressure on our social and healthcare system. Rapid detection of COVID-19 cases is a significant step to fight against this virus as well as release pressure off the healthcare system. OBJECTIVE: One of the critical factors behind the rapid spread of COVID-19 pandemic is a lengthy clinical testing time. The imaging tool, such as Chest X-ray (CXR), can speed up the identification process. Therefore, our objective is to develop an automated CAD system for the detection of COVID-19 samples from healthy and pneumonia cases using CXR images. METHODS: Due to the scarcity of the COVID-19 benchmark dataset, we have employed deep transfer learning techniques, where we examined 15 different pre-trained CNN models to find the most suitable one for this task. RESULTS: A total of 860 images (260 COVID-19 cases, 300 healthy and 300 pneumonia cases) have been employed to investigate the performance of the proposed algorithm, where 70% images of each class are accepted for training, 15% is used for validation, and rest is for testing. It is observed that the VGG19 obtains the highest classification accuracy of 89.3% with an average precision, recall, and F1 score of 0.90, 0.89, 0.90, respectively. CONCLUSION: This study demonstrates the effectiveness of deep transfer learning techniques for the identification of COVID-19 cases using CXR images.",2020,10.3233/xst-200715,diagnosis,False
Identification of Non-Small Cell Lung Cancer Sensitive to Systemic Cancer Therapies Using Radiomics,"PURPOSE: Using standard-of-care CT images obtained from patients with a diagnosis of non-small cell lung cancer (NSCLC), we defined radiomics signatures predicting the sensitivity of tumors to nivolumab, docetaxel, and gefitinib. EXPERIMENTAL DESIGN: Data were collected prospectively and analyzed retrospectively across multicenter clinical trials [nivolumab, n = 92, CheckMate017 (NCT01642004), CheckMate063 (NCT01721759); docetaxel, n = 50, CheckMate017; gefitinib, n = 46, (NCT00588445)]. Patients were randomized to training or validation cohorts using either a 4:1 ratio (nivolumab: 72T:20V) or a 2:1 ratio (docetaxel: 32T:18V; gefitinib: 31T:15V) to ensure an adequate sample size in the validation set. Radiomics signatures were derived from quantitative analysis of early tumor changes from baseline to first on-treatment assessment. For each patient, 1,160 radiomics features were extracted from the largest measurable lung lesion. Tumors were classified as treatment sensitive or insensitive; reference standard was median progression-free survival (NCT01642004, NCT01721759) or surgery (NCT00588445). Machine learning was implemented to select up to four features to develop a radiomics signature in the training datasets and applied to each patient in the validation datasets to classify treatment sensitivity. RESULTS: The radiomics signatures predicted treatment sensitivity in the validation dataset of each study group with AUC (95 confidence interval): nivolumab, 0.77 (0.55-1.00); docetaxel, 0.67 (0.37-0.96); and gefitinib, 0.82 (0.53-0.97). Using serial radiographic measurements, the magnitude of exponential increase in signature features deciphering tumor volume, invasion of tumor boundaries, or tumor spatial heterogeneity was associated with shorter overall survival. CONCLUSIONS: Radiomics signatures predicted tumor sensitivity to treatment in patients with NSCLC, offering an approach that could enhance clinical decision-making to continue systemic therapies and forecast overall survival.",2020,10.1158/1078-0432.Ccr-19-2942,prognosis,True
Identification of pathological subtypes of early lung adenocarcinoma based on artificial intelligence parameters and CT signs,"OBJECTIVE: To explore the value of quantitative parameters of artificial intelligence (AI) and computed tomography (CT) signs in identifying pathological subtypes of lung adenocarcinoma appearing as ground-glass nodules (GGNs). METHODS: CT images of 224 GGNs from 210 individuals were collected retrospectively and classified into atypical adenomatous hyperplasia (AAH)/adenocarcinoma in situ (AIS), minimally invasive adenocarcinoma (MIA), and invasive adenocarcinoma (IAC) groups. AI was used to identify GGNs and to obtain quantitative parameters, and CT signs were recognized manually. The mixed predictive model based on logistic multivariate regression was built and evaluated. RESULTS: Of the 224 GGNs, 55, 93, and 76 were AAH/AIS, MIA, and IAC, respectively. In terms of AI parameters, from AAH/AIS to MIA, and IAC, there was a gradual increase in two-dimensional mean diameter, three-dimensional mean diameter, mean CT value, maximum CT value, and volume of GGNs (all P<0.0001). Except for the CT signs of the location, and the tumor-lung interface, there were significant differences among the three groups in the density, shape, vacuolar signs, air bronchogram, lobulation, spiculation, pleural indentation, and vascular convergence signs (all P<0.05). The areas under the curve (AUC) of predictive model 1 for identifying the AAH/AIS and MIA and model 2 for identifying MIA and IAC were 0.779 and 0.918, respectively, which were greater than the quantitative parameters independently (all P<0.05). CONCLUSION: AI parameters are valuable for identifying subtypes of early lung adenocarcinoma and have improved diagnostic efficacy when combined with CT signs.",2022,10.1042/bsr20212416,diagnosis,True
Identification of pulmonary nodules via CT images with hierarchical fully convolutional networks,"Lung cancer is one of the most diagnosable forms of cancer worldwide. The early diagnoses of pulmonary nodules in computed tomography (CT) chest scans are crucial for potential patients. Recent researches have showed that the methods based on deep learning have made a significant progress for the medical diagnoses. However, the achievements on identification of pulmonary nodules are not yet satisfactory enough to be adopted in clinical practice. It is largely caused by either the existence of many false positives or the heavy time of processing. With the development of fully convolutional networks (FCNs), in this study, we proposed a new method of identifying the pulmonary nodules. The method segments the suspected nodules from their environments and then removes the false positives. Especially, it optimizes the network architecture for the identification of nodules rapidly and accurately. In order to remove the false positives, the suspected nodules are reduced using the 2D models. Furthermore, according to the significant differences between nodules and non-nodules in 3D shapes, the false positives are eliminated by integrating into the 3D models and classified via 3D CNNs. The experiments on 1000 patients indicate that our proposed method achieved 97.78% sensitivity rate for segmentation and 90.1% accuracy rate for detection. The maximum response time was less than 30 s and the average time was about 15 s. Graphical Abstract This paper has proposed a new method of identifying the pulmonary nodules. The method segments the suspected nodules from CT images and removes the false positives. As shown in the above, the proposed approach consists of three stages. In stage I, raw data are filtered and normalized. The clean normalized data are then segmented in stage II to extract the suspected nodular lesions through 2D FCNs. Stage III is to remove some false positives generated at stage II via 3D CNNs and outputs the final results. The experiments on 1000 patients indicate that our proposed method has achieved 97.78% sensitivity rate for segmentation and 90.1% accuracy rate for detection. The maximum response time was less than 30 s and the average time was about 15 s.",2019,10.1007/s11517-019-01976-1,diagnosis,True
Identifying COVID19 from Chest CT Images: A Deep Convolutional Neural Networks Based Approach,"Coronavirus Disease (COVID19) is a fast-spreading infectious disease that is currently causing a healthcare crisis around the world. Due to the current limitations of the reverse transcription-polymerase chain reaction (RT-PCR) based tests for detecting COVID19, recently radiology imaging based ideas have been proposed by various works. In this work, various Deep CNN based approaches are explored for detecting the presence of COVID19 from chest CT images. A decision fusion based approach is also proposed, which combines predictions from multiple individual models, to produce a final prediction. Experimental results show that the proposed decision fusion based approach is able to achieve above 86% results across all the performance metrics under consideration, with average AUROC and F1-Score being 0.883 and 0.867, respectively. The experimental observations suggest the potential applicability of such Deep CNN based approach in real diagnostic scenarios, which could be of very high utility in terms of achieving fast testing for COVID19.",2020,10.1155/2020/8843664,diagnosis,True
Identifying EGFR mutations in lung adenocarcinoma by noninvasive imaging using radiomics features and random forest modeling,"OBJECTIVES: The tyrosine kinase inhibitor (TKI)-sensitive mutations of the epidermal growth factor receptor (EGFR) gene is essential in the treatment of lung adenocarcinoma. To overcome the difficulty of EGFR gene test in situations where surgery and biopsy samples are too risky to obtain, we tried a noninvasive imaging method using radiomics features and random forest models. METHODS: Five hundred three lung adenocarcinoma patients who received surgery-based treatment were included in this study. The diagnosis and EGFR gene test were based on resections. TKI-sensitive mutations were found in 60.8% of the patients. CT scans before any invasive operation were gathered and analyzed to extract quantitative radiomics features and build random forest classifiers to identify EGFR mutants from wild types. Clinical features (sex and smoking history) were added to the image-based model. The model was trained on a set of 345 patients and validated on an independent test group (n = 158) using the area under the receiver operating characteristic curve (AUC), sensitivity, and specificity. RESULTS: The performance of the random forest model with 94 radiomics features reached an AUC of 0.802. Its AUC was further improved to 0.828 by adding sex and smoking history. The sensitivity and specificity are 60.6% and 85.1% at the best diagnostic decision point. CONCLUSION: Our results showed that radiomics could not only reflect the genetic differences among tumors but also have diagnostic value and the potential to be a diagnostic tool. KEY POINTS: • Radiomics provides a potential noninvasive method for the prediction of EGFR mutation status. • In situations where surgeries and biopsy are not available, CT image-based radiomics models could help to make treatment decisions. • The accuracy, sensitivity, and specificity still need to be improved before the image-based EGFR identifier could be used in clinics.",2019,10.1007/s00330-019-06024-y,prognosis,True
Automatic segmentation and quantification of epicardial adipose tissue from coronary computed tomography angiography,"Epicardial adipose tissue (EAT) is a visceral fat deposit, that's known for its association with factors, such as obesity, diabetes mellitus, age, and hypertension. Segmentation of the EAT in a fast and reproducible way is important for the interpretation of its role as an independent risk marker intricate. However, EAT has a variable distribution, and various diseases may affect the volume of the EAT, which can increase the complexity of the already time-consuming manual segmentation work. We propose a 3D deep attention U-Net method to automatically segment the EAT from coronary computed tomography angiography (CCTA). Five-fold cross-validation and hold-out experiments were used to evaluate the proposed method through a retrospective investigation of 200 patients. The automatically segmented EAT volume was compared with physician-approved clinical contours. Quantitative metrics used were the Dice similarity coefficient (DSC), sensitivity, specificity, Jaccard index (JAC), Hausdorff distance (HD), mean surface distance (MSD), residual mean square distance (RMSD), and the center of mass distance (CMD). For cross-validation, the median DSC, sensitivity, and specificity were 92.7%, 91.1%, and 95.1%, respectively, with JAC, HD, CMD, MSD, and RMSD are 82.9% ± 8.8%, 3.77 ± 1.86 mm, 1.98 ± 1.50 mm, 0.37 ± 0.24 mm, and 0.65 ± 0.37 mm, respectively. For the hold-out test, the accuracy of the proposed method remained high. We developed a novel deep learning-based approach for the automated segmentation of the EAT on CCTA images. We demonstrated the high accuracy of the proposed learning-based segmentation method through comparison with ground truth contour of 200 clinical patient cases using 8 quantitative metrics, Pearson correlation, and Bland-Altman analysis. Our automatic EAT segmentation results show the potential of the proposed method to be used in computer-aided diagnosis of coronary artery diseases (CADs) in clinical settings.",2020,10.1088/1361-6560/ab8077,prognosis,True
Deep learning segmentation and quantification method for assessing epicardial adipose tissue in CT calcium score scans,"Epicardial adipose tissue volume (EAT) has been linked to coronary artery disease and the risk of major adverse cardiac events. As manual quantification of EAT is time-consuming, requires specialized training, and is prone to human error, we developed a deep learning method (DeepFat) for the automatic assessment of EAT on non-contrast low-dose CT calcium score images. Our DeepFat intuitively segmented the tissue enclosed by the pericardial sac on axial slices, using two preprocessing steps. First, we applied a HU-attention-window with a window/level 350/40-HU to draw attention to the sac and reduce numerical errors. Second, we applied a novel look ahead slab-of-slices with bisection (""bisect"") in which we split the heart into halves and sequenced the lower half from bottom-to-middle and the upper half from top-to-middle, thereby presenting an always increasing curvature of the sac to the network. EAT volume was obtained by thresholding voxels within the sac in the fat window (- 190/- 30-HU). Compared to manual segmentation, our algorithm gave excellent results with volume Dice = 88.52% ± 3.3, slice Dice = 87.70% ± 7.5, EAT error = 0.5% ± 8.1, and R = 98.52% (p < 0.001). HU-attention-window and bisect improved Dice volume scores by 0.49% and 3.2% absolute, respectively. Variability between analysts was comparable to variability with DeepFat. Results compared favorably to those of previous publications.",2022,10.1038/s41598-022-06351-z,prognosis,True
Automatic quantification of epicardial adipose tissue volume,"PURPOSE: Epicardial fat is the adipose tissue between the serosal pericardial wall layer and the visceral layer. It is distributed mainly around the atrioventricular groove, atrial septum, ventricular septum and coronary arteries. Studies have shown that the density, thickness, volume and other characteristics of epicardial adipose tissue (EAT) are independently correlated with a variety of cardiovascular diseases. Given this association, the accurate determination of EAT volume is an essential aim of future research. Therefore, the purpose of this study was to establish a framework for fully automatic EAT segmentation and quantification in coronary computed tomography angiography (CCTA) scans. METHODS: A set of 103 scans are randomly selected from our medical center. An automatic pipeline has been developed to segment and quantify the volume of EAT. First, a multi-slice deep neural network is used to simultaneously segment the pericardium in multiple adjacent slices. Then a deformable model is employed to reduce false positive and negative regions in the segmented binary pericardial images. Finally, the pericardium mask is used to define the region of interest (ROI) and the threshold method is utilized to extract the pixels ranging from -175 Hounsfield units (HU) to -15 HU for the segmentation of EAT. RESULTS: The Dice indices of the pericardial segmentation using the proposed method with respect to the manual delineation results of two radiology experts were 97.1% ± 0.7% and 96.9% ± 0.6%, respectively. The inter-observer variability was also assessed, resulting in a Dice index of 97.0% ± 0.7%. For the EAT segmentation results, the Dice indices between the proposed method and the two radiology experts were 93.4% ± 1.5% and 93.3% ± 1.3%, respectively, and the same measurement between the experts themselves was 93.6% ± 1.9%. The Pearson's correlation coefficients between the EAT volumes computed from the results of the proposed method and the manual delineation by the two experts were 1.00 and 0.99 and the same coefficients between the experts was 0.99. CONCLUSIONS: This work describes the development of a fully automatic EAT segmentation and quantification method from CCTA scans and the results compare favorably with the assessments of two independent experts. The proposed method is also packaged with a graphical user interface which can be found at https://github.com/MountainAndMorning/EATSeg.",2021,10.1002/mp.15012,prognosis,True
Identifying epidermal growth factor receptor mutation status in patients with lung adenocarcinoma by three-dimensional convolutional neural networks,"OBJECTIVE: Genetic phenotype plays a central role in making treatment decisions of lung adenocarcinoma, especially the tyrosine-kinase-inhibitors-sensitive mutations of the epidermal growth factor receptor (EGFR) gene. We constructed three-dimensional convolutional neural networks (CNN) to analyze underlying patterns in CT images that could indicate that EGFR gene mutation status but are invisible to human eyes. METHODS: From 2012 to 2015, 503 Chinese patients with lung adenocarcinoma that had underwent surgery were included. Pathological types and EGFR mutation status were tested from surgical resections. EGFR mutations (exon 19 deletion or exon 21 L858R) were found in 215/345 (62.3%) and 91/158 (57.6%) patients in the training and independent validation set, respectively. CT images were taken before any invasive operation. The patients were randomly chosen to train the CNNs or validate the CNNs' performance. The performance was quantified using area under receiver operating characteristic curve (AUC), sensitivity, specificity, and accuracy. RESULTS: The CNNs showed an AUC of 0.776 (range: 0.702-0.849, p< 0.0001) in the independent validation set and a fusion model of CNNs and clinical features (sex and smoking history) showed an AUC of 0.838 (range: 0.778-0.899, p< 0.0001), accuracy of 77.2%, sensitivity of 75.8% and specificity of 79.1% at the best diagnostic decision point. CONCLUSION: The CNN exhibits potential ability to identify EGFR mutation status in patients with lung adenocarcinoma which might help make clinical decisions. ADVANCES IN KNOWLEDGE: The CNN showed some diagnostic power and its performance could be further improved by increasing the training set, optimizing the network structure and training strategy. Medical image based CNN has the potential to reflect spatial heterogeneity.",2018,10.1259/bjr.20180334,diagnosis,True
Identifying pulmonary nodules or masses on chest radiography using deep learning: external validation and strategies to improve clinical practice,"AIM: To test the diagnostic performance of a deep learning-based system for the detection of clinically significant pulmonary nodules/masses on chest radiographs. MATERIALS AND METHODS: Using a retrospective study of 100 patients (47 with clinically significant pulmonary nodules/masses and 53 control subjects without pulmonary nodules), two radiologists verified clinically significantly pulmonary nodules/masses according to chest computed tomography (CT) findings. A computer-aided diagnosis (CAD) software using a deep-learning approach was used to detect pulmonary nodules/masses to determine the diagnostic performance in four algorithms (heat map, abnormal probability, nodule probability, and mass probability). RESULTS: A total of 100 cases were included in the analysis. Among the four algorithms, mass algorithm could achieve a 76.6% sensitivity (36/47, 11 false negative) and 88.68% specificity (47/53, six false-positive) in the detection of pulmonary nodules/masses at the optimal probability score cut-off of 0.2884. Compared to the other three algorithms, mass probability algorithm had best predictive ability for pulmonary nodule/mass detection at the optimal probability score cut-off of 0.2884 (AUC(Mass): 0.916 versus AUC(Heat map): 0.682, p<0.001; AUC(Mass): 0.916 versus AUC(Abnormal): 0.810, p=0.002; AUC(Mass): 0.916 versus AUC(Nodule): 0.813, p=0.014). CONCLUSION: In conclusion, the deep-learning based computer-aided diagnosis system will likely play a vital role in the early detection and diagnosis of pulmonary nodules/masses on chest radiographs. In future applications, these algorithms could support triage workflow via double reading to improve sensitivity and specificity during the diagnostic process.",2020,10.1016/j.crad.2019.08.005,diagnosis,False
Identifying sarcopenia in advanced non-small cell lung cancer patients using skeletal muscle CT radiomics and machine learning,"BACKGROUND: Sarcopenia has been confirmed as a poor prognostic indicator of lung cancer. However, the lack of abdominal computed tomography (CT) hindered the application to assess the status of sarcopenia. The purpose of this study was to assess the ability of chest CT radiomics combined with machine learning classifiers to identify sarcopenia in advanced non-small cell lung cancer (NSCLC) patients. METHODS: This study retrospectively analyzed CT images of 99 patients with NSCLC. Skeletal muscle radiomics were extracted from a single axial slice of the chest CT scan at the 12th thoracic vertebrae level. In total, 854 radiomic and clinical features were obtained from each patient. Feature selection was conducted with FeatureSelector module, optimal key features were fed into the lightGBM classifier for model construction, and Bayesian optimization was adopted to tune hyperparameters. The model's performance was evaluated by specificity, sensitivity, accuracy, precision, F1-score, Matthew's correlation coefficient (MCC), Cohen's kappa coefficient (Kappa), and AUC. RESULTS: A total of 40 patients were found to have sarcopenia. Five optimal features were selected. In the base lightGBM model, the specificity, sensitivity, accuracy, precision, F1-score, AUC, MCC, Kappa of validation set were 0.889, 0.750, 0.833, 0.818, 0.783, 0.819, 0.649, 0.648, respectively. After Bayesian hyperparameter tuning, the optimized lightGBM model achieved better prediction performance, and the corresponding values were 0.944, 0.833, 0.900, 0.909, 0.870, 0.889, 0.791, 0.789, respectively. CONCLUSIONS: Chest CT-based radiomics has the potential to identify sarcopenia in NSCLC patients with the lightGBM classifier, and the optimal lightGBM model via Bayesian hyperparameter tuning demonstrated better performance. KEY POINTS: SIGNIFICANT FINDINGS OF THE STUDY: Our study demonstrates that chest CT-based radiomics combined with lightGBM classifier has the ability to identify sarcopenia in NSCLC patients. WHAT THIS STUDY ADDS: Skeletal muscle radiomics would be a potential biomarker for sarcopenia identity in NSCLC patients.",2020,10.1111/1759-7714.13598,diagnosis,True
IMAL-Net: Interpretable multi-task attention learning network for invasive lung adenocarcinoma screening in CT images,"PURPOSE: Feature maps created from deep convolutional neural networks (DCNNs) have been widely used for visual explanation of DCNN-based classification tasks. However, many clinical applications such as benign-malignant classification of lung nodules normally require quantitative and objective interpretability, rather than just visualization. In this paper, we propose a novel interpretable multi-task attention learning network named IMAL-Net for early invasive adenocarcinoma screening in chest computed tomography images, which takes advantage of segmentation prior to assist interpretable classification. METHODS: Two sub-ResNets are firstly integrated together via a prior-attention mechanism for simultaneous nodule segmentation and invasiveness classification. Then, numerous radiomic features from the segmentation results are concatenated with high-level semantic features from the classification subnetwork by FC layers to achieve superior performance. Meanwhile, an end-to-end feature selection mechanism (named FSM) is designed to quantify crucial radiomic features greatly affecting the prediction of each sample, and thus it can provide clinically applicable interpretability to the prediction result. RESULTS: Nodule samples from a total of 1626 patients were collected from two grade-A hospitals for large-scale verification. Five-fold cross validation demonstrated that the proposed IMAL-Net can achieve an AUC score of 93.8% ± 1.1% and a recall score of 93.8% ± 2.8% for identification of invasive lung adenocarcinoma. CONCLUSIONS: It can be concluded that fusing semantic features and radiomic features can achieve obvious improvements in the invasiveness classification task. Moreover, by learning more fine-grained semantic features and highlighting the most important radiomics features, the proposed attention and FSM mechanisms not only can further improve the performance but also can be used for both visual explanations and objective analysis of the classification results.",2021,10.1002/mp.15293,diagnosis,True
Impact of Confounding Thoracic Tubes and Pleural Dehiscence Extent on Artificial Intelligence Pneumothorax Detection in Chest Radiographs,"OBJECTIVES: We hypothesized that published performances of algorithms for artificial intelligence (AI) pneumothorax (PTX) detection in chest radiographs (CXRs) do not sufficiently consider the influence of PTX size and confounding effects caused by thoracic tubes (TTs). Therefore, we established a radiologically annotated benchmarking cohort (n = 6446) allowing for a detailed subgroup analysis. MATERIALS AND METHODS: We retrospectively identified 6434 supine CXRs, among them 1652 PTX-positive cases and 4782 PTX-negative cases. Supine CXRs were radiologically annotated for PTX size, PTX location, and inserted TTs. The diagnostic performances of 2 AI algorithms (""AI_CheXNet"" [Rajpurkar et al], ""AI_1.5"" [Guendel et al]), both trained on publicly available datasets with labels obtained from automatic report interpretation, were quantified. The algorithms' discriminative power for PTX detection was quantified by the area under the receiver operating characteristics (AUROC), and significance analysis was based on the corresponding 95% confidence interval. A detailed subgroup analysis was performed to quantify the influence of PTX size and the confounding effects caused by inserted TTs. RESULTS: Algorithm performance was quantified as follows: overall performance with AUROCs of 0.704 (AI_1.5) / 0.765 (AI_CheXNet) for unilateral PTXs, AUROCs of 0.666 (AI_1.5) / 0.722 (AI_CheXNet) for unilateral PTXs smaller than 1 cm, and AUROCs of 0.735 (AI_1.5) / 0.818 (AI_CheXNet) for unilateral PTXs larger than 2 cm. Subgroup analysis identified TTs to be strong confounders that significantly influence algorithm performance: Discriminative power is completely eliminated by analyzing PTX-positive cases without TTs referenced to control PTX-negative cases with inserted TTs. Contrarily, AUROCs increased up to 0.875 (AI_CheXNet) for large PTX-positive cases with inserted TTs referenced to control cases without TTs. CONCLUSIONS: Our detailed subgroup analysis demonstrated that the performance of established AI algorithms for PTX detection trained on public datasets strongly depends on PTX size and is significantly biased by confounding image features, such as inserted TTS. Our established, clinically relevant and radiologically annotated benchmarking cohort might be of great benefit for ongoing algorithm development.",2020,10.1097/rli.0000000000000707,diagnosis,False
Impact of dose reduction and iterative reconstruction algorithm on the detectability of pulmonary nodules by artificial intelligence,"PURPOSE: The purpose of this study was to assess whether the performances of an automated software for lung nodule detection with computed tomography (CT) are affected by radiation dose and the use of iterative reconstruction algorithm. MATERIALS AND METHODS: A chest phantom (Multipurpose Chest Phantom N1; Kyoto Kagaku Co. Ltd, Kyoto, Japan) with 15 pulmonary nodules was scanned with a total of five CT protocol settings with up to 20-fold dose reduction. All CT examinations were reconstructed with iterative reconstruction algorithms ADMIRE 3 and ADMIRE 5 and were then analyzed for the presence of pulmonary nodules with a fully automated computer aided detection software system (InferRead(TM) CT Lung, Infervision), which is based on deep neural networks. RESULTS: The sensitivity of fully automated pulmonary nodule detection for ground-glass nodules at standard dose CT was greater (70.0%; 14/20; 95% CI: 51.6-88.4%) than at 10-fold and 20-fold dose reduction (30.0%; 6/20; 95% CI: 0.0%-62.5%). There were less false positive findings when ADMIRE 5 reconstruction was used (4.0 ± 2.8 [SD]; range: 2-6) instead of ADMIRE 3 reconstruction (25.0 ± 15.6 [SD]; range: 14-36). There was no difference in the sensitivity of detection of solid and subsolid nodules between standard dose (100%; 95% CI: 100-100%) and 10- and 20-fold reduced dose CT (92.5%; 95% CI: 83.8-100.0%). Image noise was significantly greater with ADMIRE 3 (81 ± 2 [SD] [range: 79-84]; 104 ± 3 [SD] [range: 101-107]; 114 ± 5 [SD] [range: 110-119]; 193 ± 10 [SD] [range: 183-203]; 220 ± 16 [SD] [range: 210-238]) compared to ADMIRE 5 (44 ± 2 [SD] [range: 42-46]; 60 ± 2 [SD] [range: 57-61]; 66 ± 1 [SD] [range: 65-67]; 103 ± 4 [SD] [range: 98-106]; 110 ± 1 [SD] [range: 109-111]), respectively in each of the five CT protocols. CONCLUSION: This phantom study suggests that dose reduction and iterative reconstruction settings have an impact on detectability of pulmonary nodules by artificial intelligence software and we therefore encourage adaption of dose levels and reconstruction methods prior to widespread implementation of fully automatic nodule detection software for lung cancer screening purposes.",2022,10.1016/j.diii.2021.12.002,diagnosis,True
Impact of feature harmonization on radiogenomics analysis: Prediction of EGFR and KRAS mutations from non-small cell lung cancer PET/CT images,"OBJECTIVE: To investigate the impact of harmonization on the performance of CT, PET, and fused PET/CT radiomic features toward the prediction of mutations status, for epidermal growth factor receptor (EGFR) and Kirsten rat sarcoma viral oncogene (KRAS) genes in non-small cell lung cancer (NSCLC) patients. METHODS: Radiomic features were extracted from tumors delineated on CT, PET, and wavelet fused PET/CT images obtained from 136 histologically proven NSCLC patients. Univariate and multivariate predictive models were developed using radiomic features before and after ComBat harmonization to predict EGFR and KRAS mutation statuses. Multivariate models were built using minimum redundancy maximum relevance feature selection and random forest classifier. We utilized 70/30% splitting patient datasets for training/testing, respectively, and repeated the procedure 10 times. The area under the receiver operator characteristic curve (AUC), accuracy, sensitivity, and specificity were used to assess model performance. The performance of the models (univariate and multivariate), before and after ComBat harmonization was compared using statistical analyses. RESULTS: While the performance of most features in univariate modeling was significantly improved for EGFR prediction, most features did not show any significant difference in performance after harmonization in KRAS prediction. Average AUCs of all multivariate predictive models for both EGFR and KRAS were significantly improved (q-value < 0.05) following ComBat harmonization. The mean ranges of AUCs increased following harmonization from 0.87-0.90 to 0.92-0.94 for EGFR, and from 0.85-0.90 to 0.91-0.94 for KRAS. The highest performance was achieved by harmonized F_R0.66_W0.75 model with AUC of 0.94, and 0.93 for EGFR and KRAS, respectively. CONCLUSION: Our results demonstrated that regarding univariate modelling, while ComBat harmonization had generally a better impact on features for EGFR compared to KRAS status prediction, its effect is feature-dependent. Hence, no systematic effect was observed. Regarding the multivariate models, ComBat harmonization significantly improved the performance of all radiomics models toward more successful prediction of EGFR and KRAS mutation statuses in lung cancer patients. Thus, by eliminating the batch effect in multi-centric radiomic feature sets, harmonization is a promising tool for developing robust and reproducible radiomics using vast and variant datasets.",2022,10.1016/j.compbiomed.2022.105230,diagnosis,True
Impact of GAN-based lesion-focused medical image super-resolution on the robustness of radiomic features,"Robust machine learning models based on radiomic features might allow for accurate diagnosis, prognosis, and medical decision-making. Unfortunately, the lack of standardized radiomic feature extraction has hampered their clinical use. Since the radiomic features tend to be affected by low voxel statistics in regions of interest, increasing the sample size would improve their robustness in clinical studies. Therefore, we propose a Generative Adversarial Network (GAN)-based lesion-focused framework for Computed Tomography (CT) image Super-Resolution (SR); for the lesion (i.e., cancer) patch-focused training, we incorporate Spatial Pyramid Pooling (SPP) into GAN-Constrained by the Identical, Residual, and Cycle Learning Ensemble (GAN-CIRCLE). At [Formula: see text] SR, the proposed model achieved better perceptual quality with less blurring than the other considered state-of-the-art SR methods, while producing comparable results at [Formula: see text] SR. We also evaluated the robustness of our model's radiomic feature in terms of quantization on a different lung cancer CT dataset using Principal Component Analysis (PCA). Intriguingly, the most important radiomic features in our PCA-based analysis were the most robust features extracted on the GAN-super-resolved images. These achievements pave the way for the application of GAN-based image Super-Resolution techniques for studies of radiomics for robust biomarker discovery.",2021,10.1038/s41598-021-00898-z,prognosis,True
Impact of Lung Segmentation on the Diagnosis and Explanation of COVID-19 in Chest X-ray Images,"COVID-19 frequently provokes pneumonia, which can be diagnosed using imaging exams. Chest X-ray (CXR) is often useful because it is cheap, fast, widespread, and uses less radiation. Here, we demonstrate the impact of lung segmentation in COVID-19 identification using CXR images and evaluate which contents of the image influenced the most. Semantic segmentation was performed using a U-Net CNN architecture, and the classification using three CNN architectures (VGG, ResNet, and Inception). Explainable Artificial Intelligence techniques were employed to estimate the impact of segmentation. A three-classes database was composed: lung opacity (pneumonia), COVID-19, and normal. We assessed the impact of creating a CXR image database from different sources, and the COVID-19 generalization from one source to another. The segmentation achieved a Jaccard distance of 0.034 and a Dice coefficient of 0.982. The classification using segmented images achieved an F1-Score of 0.88 for the multi-class setup, and 0.83 for COVID-19 identification. In the cross-dataset scenario, we obtained an F1-Score of 0.74 and an area under the ROC curve of 0.9 for COVID-19 identification using segmented images. Experiments support the conclusion that even after segmentation, there is a strong bias introduced by underlying factors from different sources.",2021,10.3390/s21217116,diagnosis,False
Impact of pixel-based machine-learning techniques on automated frameworks for delineation of gross tumor volume regions for stereotactic body radiation therapy,"The aim of this study was to investigate the impact of pixel-based machine learning (ML) techniques, i.e., fuzzy-c-means clustering method (FCM), and the artificial neural network (ANN) and support vector machine (SVM), on an automated framework for delineation of gross tumor volume (GTV) regions of lung cancer for stereotactic body radiation therapy. The morphological and metabolic features for GTV regions, which were determined based on the knowledge of radiation oncologists, were fed on a pixel-by-pixel basis into the respective FCM, ANN, and SVM ML techniques. Then, the ML techniques were incorporated into the automated delineation framework of GTVs followed by an optimum contour selection (OCS) method, which we proposed in a previous study. The three-ML-based frameworks were evaluated for 16 lung cancer cases (six solid, four ground glass opacity (GGO), six part-solid GGO) with the datasets of planning computed tomography (CT) and (18)F-fluorodeoxyglucose (FDG) positron emission tomography (PET)/CT images using the three-dimensional Dice similarity coefficient (DSC). DSC denotes the degree of region similarity between the GTVs contoured by radiation oncologists and those estimated using the automated framework. The FCM-based framework achieved the highest DSCs of 0.79±0.06, whereas DSCs of the ANN-based and SVM-based frameworks were 0.76±0.14 and 0.73±0.14, respectively. The FCM-based framework provided the highest segmentation accuracy and precision without a learning process (lowest calculation cost). Therefore, the FCM-based framework can be useful for delineation of tumor regions in practical treatment planning.",2017,10.1016/j.ejmp.2017.08.012,treatment,True
Impact of Vessel Suppressed-CT on Diagnostic Accuracy in Detection of Pulmonary Metastasis and Reading Time,"RATIONALE AND OBJECTIVES: To assess if vessel suppression (VS) improves nodule detection rate, interreader agreement, and reduces reading time in oncologic chest computed tomography (CT). MATERIAL AND METHODS: One-hundred consecutive oncologic patients (65 male; median age 60y) who underwent contrast-enhanced chest CT were retrospectively included. For all exams, additional VS series (ClearRead CT, Riverrain Technologies, Miamisburg) were reconstructed. Two groups of three radiologists each with matched experience were defined. Each group evaluated the SD-CT as well as VS-CT. Each reader marked the presence, size, and position of pulmonary nodules and documented reading time. In addition, for the VS-CT the presence of false positive nodules had to be stated. Cohen's Kappa (k) was used to calculate the interreader-agreement between groups. Reading time was compared using paired t test. RESULTS: Nodule detection rate was significantly higher in VS-CT compared to the SD-CT (+21%; p <0.001). Interreader-agreement was higher in the VS-CT (k = 0.431, moderate agreement) compared to SD-CT (k = 0.209, fair agreement). Almost all VS-CT series had false positive findings (97-99 out of 100). Average reading time was significantly shorter in the VS-CT compared to the SD-CT (154 ± 134vs. 194 ± 126; 21%, p<0.001). CONCLUSIONS: Vessel suppression increases nodule detection rate, improves interreader agreement, and reduces reading time in chest CT of oncologic patients. Due to false positive results a consensus reading with the SD-CT is essential.",2021,10.1016/j.acra.2020.01.014,diagnosis,True
Implementation of a Deep Learning-Based Computer-Aided Detection System for the Interpretation of Chest Radiographs in Patients Suspected for COVID-19,"OBJECTIVE: To describe the experience of implementing a deep learning-based computer-aided detection (CAD) system for the interpretation of chest X-ray radiographs (CXR) of suspected coronavirus disease (COVID-19) patients and investigate the diagnostic performance of CXR interpretation with CAD assistance. MATERIALS AND METHODS: In this single-center retrospective study, initial CXR of patients with suspected or confirmed COVID-19 were investigated. A commercialized deep learning-based CAD system that can identify various abnormalities on CXR was implemented for the interpretation of CXR in daily practice. The diagnostic performance of radiologists with CAD assistance were evaluated based on two different reference standards: 1) real-time reverse transcriptase-polymerase chain reaction (rRT-PCR) results for COVID-19 and 2) pulmonary abnormality suggesting pneumonia on chest CT. The turnaround times (TATs) of radiology reports for CXR and rRT-PCR results were also evaluated. RESULTS: Among 332 patients (male:female, 173:159; mean age, 57 years) with available rRT-PCR results, 16 patients (4.8%) were diagnosed with COVID-19. Using CXR, radiologists with CAD assistance identified rRT-PCR positive COVID-19 patients with sensitivity and specificity of 68.8% and 66.7%, respectively. Among 119 patients (male:female, 75:44; mean age, 69 years) with available chest CTs, radiologists assisted by CAD reported pneumonia on CXR with a sensitivity of 81.5% and a specificity of 72.3%. The TATs of CXR reports were significantly shorter than those of rRT-PCR results (median 51 vs. 507 minutes; p < 0.001). CONCLUSION: Radiologists with CAD assistance could identify patients with rRT-PCR-positive COVID-19 or pneumonia on CXR with a reasonably acceptable performance. In patients suspected with COVID-19, CXR had much faster TATs than rRT-PCRs.",2020,10.3348/kjr.2020.0536,diagnosis,False
Implementation of an Artificial Intelligence-Based Double Read System in Capturing Pulmonary Nodule Discrepancy in CT Studies,"Studies show that up to 80% of all radiology errors are due to errors in perception. Early detection is critical for good outcomes in patients with primary lung cancer and lung metastasis. However, pulmonary nodules can be easily missed due to their small size. We prospectively applied a machine vision algorithm to CT studies containing lung parenchyma to detect pulmonary nodules, as well as a natural language processing algorithm to the text of the report to identify documentation of pulmonary nodules. Apparent discrepancies in perception - instances where a pulmonary nodule was not reported - were flagged for a secondary review by a radiologist. Four thousand and nine hundred studies were prospectively processed, of which 450 cases with potential discrepancies were detected. Preliminary manual analysis was performed to analyze the base error rate and to optimize thresholds for the machine vision and natural language processing algorithms, and 104 cases were flagged for final review. Of these 104 cases, 50 cases contained undocumented pulmonary nodules. Among these cases, 7 cases were classified as likely to be significant, where report addendums were done and the clinicians notified. We have successfully implemented an automated double read system to detect pulmonary nodule discrepancies, with minimal disruption to the radiology workflow and while keeping personal health information on-premises. This successful implementation demonstrates the viability of using an automated and secure radiology double-read system to improve patient safety in radiology workflows, either at a health system or an independent radiology practice.",2021,10.1067/j.cpradiol.2020.07.006,diagnosis,True
Implementation of convolutional neural network approach for COVID-19 disease detection,"In this paper, two novel, powerful, and robust convolutional neural network (CNN) architectures are designed and proposed for two different classification tasks using publicly available data sets. The first architecture is able to decide whether a given chest X-ray image of a patient contains COVID-19 or not with 98.92% average accuracy. The second CNN architecture is able to divide a given chest X-ray image of a patient into three classes (COVID-19 versus normal versus pneumonia) with 98.27% average accuracy. The hyperparameters of both CNN models are automatically determined using Grid Search. Experimental results on large clinical data sets show the effectiveness of the proposed architectures and demonstrate that the proposed algorithms can overcome the disadvantages mentioned above. Moreover, the proposed CNN models are fully automatic in terms of not requiring the extraction of diseased tissue, which is a great improvement of available automatic methods in the literature. To the best of the author's knowledge, this study is the first study to detect COVID-19 disease from given chest X-ray images, using CNN, whose hyperparameters are automatically determined by the Grid Search. Another important contribution of this study is that it is the first CNN-based COVID-19 chest X-ray image classification study that uses the largest possible clinical data set. A total of 1,524 COVID-19, 1,527 pneumonia, and 1524 normal X-ray images are collected. It is aimed to collect the largest number of COVID-19 X-ray images that exist in the literature until the writing of this research paper.",2020,10.1152/physiolgenomics.00084.2020,diagnosis,False
Improved digital chest tomosynthesis image quality by use of a projection-based dual-energy virtual monochromatic convolutional neural network with super resolution,"We developed a novel dual-energy (DE) virtual monochromatic (VM) very-deep super-resolution (VDSR) method with an unsharp masking reconstruction algorithm (DE-VM-VDSR) that uses projection data to improve the nodule contrast and reduce ripple artifacts during chest digital tomosynthesis (DT). For estimating the residual errors from high-resolution and multiscale VM images from the projection space, the DE-VM-VDSR algorithm employs a training network (mini-batch stochastic gradient-descent algorithm with momentum) and a hybrid super-resolution (SR) image [simultaneous algebraic reconstruction technique (SART) total-variation (TV) first-iterative shrinkage-thresholding algorithm (FISTA); SART-TV-FISTA] that involves subjective reconstruction with bilateral filtering (BF) [DE-VM-VDSR with BF]. DE-DT imaging was accomplished by pulsed X-ray exposures rapidly switched between low (60 kV, 37 projection) and high (120 kV, 37 projection) tube-potential kVp by employing a 40° swing angle. This was followed by comparison of images obtained employing the conventional polychromatic filtered backprojection (FBP), SART, SART-TV-FISTA, and DE-VM-SART-TV-FISTA algorithms. The improvements in contrast, ripple artifacts, and resolution were compared using the signal-difference-to-noise ratio (SDNR), Gumbel distribution of the largest variations, radial modulation transfer function (radial MTF) for a chest phantom with simulated ground-glass opacity (GGO) nodules, and noise power spectrum (NPS) for uniform water phantom. The novel DE-VM-VDSR with BF improved the overall performance in terms of SDNR (DE-VM-VDSR with BF: 0.1603, without BF: 0.1517; FBP: 0.0521; SART: 0.0645; SART-TV-FISTA: 0.0984; and DE-VM-SART-TV-FISTA: 0.1004), obtained a Gumbel distribution that yielded good images showing the type of simulated GGO nodules used in the chest phantom, and reduced the ripple artifacts. The NPS of DE-VM-VDSR with BF showed the lowest noise characteristics in the high-frequency region (~0.8 cycles/mm). The DE-VM-VDSR without BF yielded an improved resolution relative to that of the conventional reconstruction algorithms for radial MTF analysis (0.2-0.3 cycles/mm). Finally, based on the overall image quality, DE-VM-VDSR with BF improved the contrast and reduced the high-frequency ripple artifacts and noise.",2020,10.1371/journal.pone.0244745,diagnosis,False
Improved lung nodule diagnosis accuracy using lung CT images with uncertain class,"BACKGROUND AND OBJECTIVE: Among all malignant tumors, lung cancer ranks in the top in mortality rate. Pulmonary nodule is the early manifestation of lung cancer, and plays an important role in its discovery, diagnosis and treatment. The technology of medical imaging has encountered a rapid development in recent years, thus the amount of pulmonary nodules can be discovered are on the raise, which means even tiny or minor changes in lung can be recorded by the CT images. This paper proposes a pulmonary nodule computer aided diagnosis (CAD) based on semi-supervised extreme learning machine(SS-ELM). METHODS: First, the feature model based on the pulmonary nodules regions of lung CT images is established. After that, the same feature data sets have been put into ELM, support vector machine (SVM) methods, probabilistic neural network (PNN) and multilayer perceptron (MLP) so as to compare the performance of the methods. ELM turned out to have better performance in training time and testing accuracy compared with SVM, PNN and MLP. Then, we propose a pulmonary nodules computer aided diagnosis algorithm based on semi-supervised ELM (SS-ELM), which enables both certain class feature sets with labels and unlabeled feature sets to be input for training and computer aided diagnosing. This algorithm has provided a solution for the using of uncertain class data and improve the testing accuracy of benign and malignant diagnosis. RESULTS: 1018 sets of thoracic CT images from the Lung Database Consortium and Image Database Resource Initiative (LIDC-IDRI) have been used in experiment in order to test the effectiveness of the algorithm. Compared with ELM, the pulmonary nodules CAD based on SS-ELM has better testing accuracy performance. CONCLUSIONS: We have proposed a pulmonary nodule CAD system based on SS-ELM, which achieving better generalization performance at faster learning speed and higher testing accuracy than ELM, SVM, PNN and MLP. The SS-ELM based pulmonary nodules CAD has been proposed to solve the problem of uncertain class data using.",2018,10.1016/j.cmpb.2018.05.028,diagnosis,True
Improving Accuracy of Lung Nodule Classification Using Deep Learning with Focal Loss,"Early detection and classification of pulmonary nodules using computer-aided diagnosis (CAD) systems is useful in reducing mortality rates of lung cancer. In this paper, we propose a new deep learning method to improve classification accuracy of pulmonary nodules in computed tomography (CT) scans. Our method uses a novel 15-layer 2D deep convolutional neural network architecture for automatic feature extraction and classification of pulmonary candidates as nodule or nonnodule. Focal loss function is then applied to the training process to boost classification accuracy of the model. We evaluated our method on the LIDC/IDRI dataset extracted by the LUNA16 challenge. The experiments showed that our deep learning method with focal loss is a high-quality classifier with an accuracy of 97.2%, sensitivity of 96.0%, and specificity of 97.3%.",2019,10.1155/2019/5156416,diagnosis,True
Improving clinical disease subtyping and future events prediction through a chest CT-based deep learning approach,"PURPOSE: To develop and evaluate a deep learning (DL) approach to extract rich information from high-resolution computed tomography (HRCT) of patients with chronic obstructive pulmonary disease (COPD). METHODS: We develop a DL-based model to learn a compact representation of a subject, which is predictive of COPD physiologic severity and other outcomes. Our DL model learned: (a) to extract informative regional image features from HRCT; (b) to adaptively weight these features and form an aggregate patient representation; and finally, (c) to predict several COPD outcomes. The adaptive weights correspond to the regional lung contribution to the disease. We evaluate the model on 10 300 participants from the COPDGene cohort. RESULTS: Our model was strongly predictive of spirometric obstruction ( r2 = 0.67) and grouped 65.4% of subjects correctly and 89.1% within one stage of their GOLD severity stage. Our model achieved an accuracy of 41.7% and 52.8% in stratifying the population-based on centrilobular (5-grade) and paraseptal (3-grade) emphysema severity score, respectively. For predicting future exacerbation, combining subjects' representations from our model with their past exacerbation histories achieved an accuracy of 80.8% (area under the ROC curve of 0.73). For all-cause mortality, in Cox regression analysis, we outperformed the BODE index improving the concordance metric (ours: 0.61 vs BODE: 0.56). CONCLUSIONS: Our model independently predicted spirometric obstruction, emphysema severity, exacerbation risk, and mortality from CT imaging alone. This method has potential applicability in both research and clinical practice.",2021,10.1002/mp.14673,prognosis,True
Improving Early Identification of Significant Weight Loss Using Clinical Decision Support System in Lung Cancer Radiation Therapy,"PURPOSE: Early identification of patients who may be at high risk of significant weight loss (SWL) is important for timely clinical intervention in lung cancer radiotherapy (RT). A clinical decision support system (CDSS) for SWL prediction was implemented within the routine clinical workflow and assessed on a prospective cohort of patients. MATERIALS AND METHODS: CDSS incorporated a machine learning prediction model on the basis of radiomics and dosiomics image features and was connected to a web-based dashboard for streamlined patient enrollment, feature extraction, SWL prediction, and physicians' evaluation processes. Patients with lung cancer (N = 37) treated with definitive RT without prior RT were prospectively enrolled in the study. Radiomics and dosiomics features were extracted from CT and 3D dose volume, and SWL probability (≥ 0.5 considered as SWL) was predicted. Two physicians predicted whether the patient would have SWL before and after reviewing the CDSS prediction. The physician's prediction performance without and with CDSS and prediction changes before and after using CDSS were compared. RESULTS: CDSS showed significantly better prediction accuracy than physicians (0.73 v 0.54) with higher specificity (0.81 v 0.50) but with lower sensitivity (0.55 v 0.64). Physicians changed their original prediction after reviewing CDSS prediction for four cases (three correctly and one incorrectly), for all of which CDSS prediction was correct. Physicians' prediction was improved with CDSS in accuracy (0.54-0.59), sensitivity (0.64-0.73), specificity (0.50-0.54), positive predictive value (0.35-0.40), and negative predictive value (0.76-0.82). CONCLUSION: Machine learning-based CDSS showed the potential to improve SWL prediction in lung cancer RT. More investigation on a larger patient cohort is needed to properly interpret CDSS prediction performance and its benefit in clinical decision making.",2021,10.1200/cci.20.00189,diagnosis,True
Improving the performance of CNN to predict the likelihood of COVID-19 using chest X-ray images with preprocessing algorithms,"OBJECTIVE: This study aims to develop and test a new computer-aided diagnosis (CAD) scheme of chest X-ray images to detect coronavirus (COVID-19) infected pneumonia. METHOD: CAD scheme first applies two image preprocessing steps to remove the majority of diaphragm regions, process the original image using a histogram equalization algorithm, and a bilateral low-pass filter. Then, the original image and two filtered images are used to form a pseudo color image. This image is fed into three input channels of a transfer learning-based convolutional neural network (CNN) model to classify chest X-ray images into 3 classes of COVID-19 infected pneumonia, other community-acquired no-COVID-19 infected pneumonia, and normal (non-pneumonia) cases. To build and test the CNN model, a publicly available dataset involving 8474 chest X-ray images is used, which includes 415, 5179 and 2,880 cases in three classes, respectively. Dataset is randomly divided into 3 subsets namely, training, validation, and testing with respect to the same frequency of cases in each class to train and test the CNN model. RESULTS: The CNN-based CAD scheme yields an overall accuracy of 94.5 % (2404/2544) with a 95 % confidence interval of [0.93,0.96] in classifying 3 classes. CAD also yields 98.4 % sensitivity (124/126) and 98.0 % specificity (2371/2418) in classifying cases with and without COVID-19 infection. However, without using two preprocessing steps, CAD yields a lower classification accuracy of 88.0 % (2239/2544). CONCLUSION: This study demonstrates that adding two image preprocessing steps and generating a pseudo color image plays an important role in developing a deep learning CAD scheme of chest X-ray images to improve accuracy in detecting COVID-19 infected pneumonia.",2020,10.1016/j.ijmedinf.2020.104284,diagnosis,False
Improving the Subtype Classification of Non-small Cell Lung Cancer by Elastic Deformation Based Machine Learning,"Non-invasive image-based machine learning models have been used to classify subtypes of non-small cell lung cancer (NSCLC). However, the classification performance is limited by the dataset size, because insufficient data cannot fully represent the characteristics of the tumor lesions. In this work, a data augmentation method named elastic deformation is proposed to artificially enlarge the image dataset of NSCLC patients with two subtypes (squamous cell carcinoma and large cell carcinoma) of 3158 images. Elastic deformation effectively expanded the dataset by generating new images, in which tumor lesions go through elastic shape transformation. To evaluate the proposed method, two classification models were trained on the original and augmented dataset, respectively. Using augmented dataset for training significantly increased classification metrics including area under the curve (AUC) values of receiver operating characteristics (ROC) curves, accuracy, sensitivity, specificity, and f(1)-score, thus improved the NSCLC subtype classification performance. These results suggest that elastic deformation could be an effective data augmentation method for NSCLC tumor lesion images, and building classification models with the help of elastic deformation has the potential to serve for clinical lung cancer diagnosis and treatment design.",2021,10.1007/s10278-021-00455-0,diagnosis,True
INASNET: Automatic identification of coronavirus disease (COVID-19) based on chest X-ray using deep neural network,"Testing is one of the important methodologies used by various countries in order to fight against COVID-19 infection. The infection is considered as one of the deadliest ones although the mortality rate is not very high. COVID-19 infection is being caused by SARS-CoV2 which is termed as severe acute respiratory syndrome coronavirus 2 virus. To prevent the community, transfer among the masses, testing plays an important role. Efficient and quicker testing techniques helps in identification of infected person which makes it easier for to isolate the patient. Deep learning methods have proved their presence and effectiveness in medical image analysis and in the identification of some of the diseases like pneumonia. Authors have been proposed a deep learning mechanism and system to identify the COVID-19 infected patient on analyzing the X-ray images. Symptoms in the COVID-19 infection is well similar to the symptoms occurring in the influenza and pneumonia. The proposed model Inception Nasnet (INASNET) is being able to separate out and classify the X-ray images in the corresponding normal, COVID-19 infected or pneumonia infected classes. This testing method will be a boom for the doctors and for the state as it is a way cheaper method as compared to the other testing kits used by the healthcare workers for the diagnosis of the disease. Continuous analysis by convolutional neural network and regular evaluation will result in better accuracy and helps in eliminating the false-negative results. INASNET is based on the combined platform of InceptionNet and Neural network architecture search which will result in having higher and faster predictions. Regular testing, faster results, economically viable testing using X-ray images will help the front line workers to make a win over COVID-19.",2022,10.1016/j.isatra.2022.02.033,diagnosis,True
Incorporating automatically learned pulmonary nodule attributes into a convolutional neural network to improve accuracy of benign-malignant nodule classification,"Existing deep-learning-based pulmonary nodule classification models usually use images and benign-malignant labels as inputs for training. Image attributes of the nodules, as human-nameable high-level semantic labels, are rarely used to build a convolutional neural network (CNN). In this paper, a new method is proposed to combine the advantages of two classifications, which are pulmonary nodule benign-malignant classification and pulmonary nodule image attributes classification, into a deep learning network to improve the accuracy of pulmonary nodule classification. For this purpose, a unique 3D CNN is built to learn image attribute and benign-malignant classification simultaneously. A novel loss function is designed to balance the influence of two different kinds of classifications. The CNN is trained by a publicly available lung image database consortium (LIDC) dataset and is tested by a cross-validation method to predict the risk of a pulmonary nodule being malignant. This proposed method generates the accuracy of 91.47%, which is better than many existing models. Experimental findings show that if the CNN is built properly, the nodule attributes classification and benign-malignant classification can benefit from each other. By using nodule attribute learning as a control factor in a deep learning scheme, the accuracy of pulmonary nodule classification can be significantly improved by using a deep learning scheme.",2018,10.1088/1361-6560/aaf09f,diagnosis,True
Industry 4.0 technologies and their applications in fighting COVID-19 pandemic using deep learning techniques,"The disease known as COVID-19 has turned into a pandemic and spread all over the world. The fourth industrial revolution known as Industry 4.0 includes digitization, the Internet of Things, and artificial intelligence. Industry 4.0 has the potential to fulfil customized requirements during the COVID-19 emergency crises. The development of a prediction framework can help health authorities to react appropriately and rapidly. Clinical imaging like X-rays and computed tomography (CT) can play a significant part in the early diagnosis of COVID-19 patients that will help with appropriate treatment. The X-ray images could help in developing an automated system for the rapid identification of COVID-19 patients. This study makes use of a deep convolutional neural network (CNN) to extract significant features and discriminate X-ray images of infected patients from non-infected ones. Multiple image processing techniques are used to extract a region of interest (ROI) from the entire X-ray image. The ImageDataGenerator class is used to overcome the small dataset size and generate ten thousand augmented images. The performance of the proposed approach has been compared with state-of-the-art VGG16, AlexNet, and InceptionV3 models. Results demonstrate that the proposed CNN model outperforms other baseline models with high accuracy values: 97.68% for two classes, 89.85% for three classes, and 84.76% for four classes. This system allows COVID-19 patients to be processed by an automated screening system with minimal human contact.",2022,10.1016/j.compbiomed.2022.105418,diagnosis,False
Influence of CT effective dose and convolution kernel on the detection of pulmonary nodules in different artificial intelligence software systems: A phantom study,"PURPOSE: To investigate the effective dose (E) and convolution kernel's effects on the detection of pulmonary nodules in different artificial intelligence (AI) software systems. METHODS: Simulated nodules of various sizes and densities in the Lungman phantom were CT scanned at different levels of E (3 - 5, 1 - 3, 0.5 - 1, and <0.5 mSv) and were reconstructed with different kernels (B30f, B60f, and B80f). The number of nodules and corresponding volumes in different images were detected by four AI software systems (A, B, C, and D). Sensitivity, false positives (FPs), false negatives (FNs), and relative volume error (RVE) were calculated and compared to the aspects of the E and convolution kernel. RESULTS: System B had the highest median sensitivity (100 %). The median FPs of systems B (1) and D (1) was lower than A (11.5) and C (5). System D had the smallest RVE (13.12 %). When the E was <0.5 mSv, system D's sensitivity decreased, while the FPs and FNs of systems A and B increased significantly (P < 0.05). When the kernel was changed from B80f to B30f, the FPs of system A decreased, while that of system C increased, and the RVE of systems A, B, and C increased (P < 0.05). CONCLUSION: AI software systems B and D have high detection efficiency under normal or low dose conditions and show better stability. However, the detection efficiency of systems A and C would be affected by the E or convolution kernel, but the E would not affect the volume measurement of four systems.",2020,10.1016/j.ejrad.2020.108928,diagnosis,True
Initial chest radiographs and artificial intelligence (AI) predict clinical outcomes in COVID-19 patients: analysis of 697 Italian patients,"OBJECTIVE: To evaluate whether the initial chest X-ray (CXR) severity assessed by an AI system may have prognostic utility in patients with COVID-19. METHODS: This retrospective single-center study included adult patients presenting to the emergency department (ED) between February 25 and April 9, 2020, with SARS-CoV-2 infection confirmed on real-time reverse transcriptase polymerase chain reaction (RT-PCR). Initial CXRs obtained on ED presentation were evaluated by a deep learning artificial intelligence (AI) system and compared with the Radiographic Assessment of Lung Edema (RALE) score, calculated by two experienced radiologists. Death and critical COVID-19 (admission to intensive care unit (ICU) or deaths occurring before ICU admission) were identified as clinical outcomes. Independent predictors of adverse outcomes were evaluated by multivariate analyses. RESULTS: Six hundred ninety-seven 697 patients were included in the study: 465 males (66.7%), median age of 62 years (IQR 52-75). Multivariate analyses adjusting for demographics and comorbidities showed that an AI system-based score ≥ 30 on the initial CXR was an independent predictor both for mortality (HR 2.60 (95% CI 1.69 - 3.99; p < 0.001)) and critical COVID-19 (HR 3.40 (95% CI 2.35-4.94; p < 0.001)). Other independent predictors were RALE score, older age, male sex, coronary artery disease, COPD, and neurodegenerative disease. CONCLUSION: AI- and radiologist-assessed disease severity scores on CXRs obtained on ED presentation were independent and comparable predictors of adverse outcomes in patients with COVID-19. TRIAL REGISTRATION: ClinicalTrials.gov NCT04318366 ( https://clinicaltrials.gov/ct2/show/NCT04318366 ). KEY POINTS: • AI system-based score ≥ 30 and a RALE score ≥ 12 at CXRs performed at ED presentation are independent and comparable predictors of death and/or ICU admission in COVID-19 patients. • Other independent predictors are older age, male sex, coronary artery disease, COPD, and neurodegenerative disease. • The comparable performance of the AI system in relation to a radiologist-assessed score in predicting adverse outcomes may represent a game-changer in resource-constrained settings.",2021,10.1007/s00330-020-07269-8,diagnosis,False
Integrated Clinical and CT Based Artificial Intelligence Nomogram for Predicting Severity and Need for Ventilator Support in COVID-19 Patients: A Multi-Site Study,"Almost 25% of COVID-19 patients end up in ICU needing critical mechanical ventilation support. There is currently no validated objective way to predict which patients will end up needing ventilator support, when the disease is mild and not progressed. N = 869 patients from two sites (D(1): N = 822, D(2): N = 47) with baseline clinical characteristics and chest CT scans were considered for this study. The entire dataset was randomly divided into 70% training, D(1)(train) (N = 606) and 30% test-set (D(test): D(1)(test) (N = 216) + D(2) (N = 47)). An expert radiologist delineated ground-glass-opacities (GGOs) and consolidation regions on a subset of D(1)(train), (D(1)(train_sub), N = 88). These regions were automatically segmented and used along with their corresponding CT volumes to train an imaging AI predictor (AIP) on D(1)(train) to predict the need of mechanical ventilators for COVID-19 patients. Finally, top five prognostic clinical factors selected using univariate analysis were integrated with AIP to construct an integrated clinical and AI imaging nomogram (ClAIN). Univariate analysis identified lactate dehydrogenase, prothrombin time, aspartate aminotransferase, %lymphocytes, albumin as top five prognostic clinical features. AIP yielded an AUC of 0.81 on D(test) and was independently prognostic irrespective of other clinical parameters on multivariable analysis (p<0.001). ClAIN improved the performance over AIP yielding an AUC of 0.84 (p = 0.04) on D(test). ClAIN outperformed AIP in predicting which COVID-19 patients ended up needing a ventilator. Our results across multiple sites suggest that ClAIN could help identify COVID-19 with severe disease more precisely and likely to end up on a life-saving mechanical ventilation.",2021,10.1109/jbhi.2021.3103389,prognosis,True
Integrating Domain Knowledge Into Deep Networks for Lung Ultrasound With Applications to COVID-19,"Lung ultrasound (LUS) is a cheap, safe and non-invasive imaging modality that can be performed at patient bed-side. However, to date LUS is not widely adopted due to lack of trained personnel required for interpreting the acquired LUS frames. In this work we propose a framework for training deep artificial neural networks for interpreting LUS, which may promote broader use of LUS. When using LUS to evaluate a patient's condition, both anatomical phenomena (e.g., the pleural line, presence of consolidations), as well as sonographic artifacts (such as A- and B-lines) are of importance. In our framework, we integrate domain knowledge into deep neural networks by inputting anatomical features and LUS artifacts in the form of additional channels containing pleural and vertical artifacts masks along with the raw LUS frames. By explicitly supplying this domain knowledge, standard off-the-shelf neural networks can be rapidly and efficiently finetuned to accomplish various tasks on LUS data, such as frame classification or semantic segmentation. Our framework allows for a unified treatment of LUS frames captured by either convex or linear probes. We evaluated our proposed framework on the task of COVID-19 severity assessment using the ICLUS dataset. In particular, we finetuned simple image classification models to predict per-frame COVID-19 severity score. We also trained a semantic segmentation model to predict per-pixel COVID-19 severity annotations. Using the combined raw LUS frames and the detected lines for both tasks, our off-the-shelf models performed better than complicated models specifically designed for these tasks, exemplifying the efficacy of our framework.",2022,10.1109/tmi.2021.3117246,diagnosis,False
Integrating Multiomics Information in Deep Learning Architectures for Joint Actuarial Outcome Prediction in Non-Small Cell Lung Cancer Patients After Radiation Therapy,"PURPOSE: Novel actuarial deep learning neural network (ADNN) architectures are proposed for joint prediction of radiation therapy outcomes-radiation pneumonitis (RP) and local control (LC)-in stage III non-small cell lung cancer (NSCLC) patients. Unlike normal tissue complication probability/tumor control probability models that use dosimetric information solely, our proposed models consider complex interactions among multiomics information including positron emission tomography (PET) radiomics, cytokines, and miRNAs. Additional time-to-event information is also used in the actuarial prediction. METHODS AND MATERIALS: Three architectures were investigated: ADNN-DVH considered dosimetric information only; ADNN-com integrated multiomics information; and ADNN-com-joint combined RP2 (RP grade ≥2) and LC prediction. In these architectures, differential dose-volume histograms (DVHs) were fed into 1D convolutional neural networks (CNN) for extracting reduced representations. Variational encoders were used to learn representations of imaging and biological data. Reduced representations were fed into Surv-Nets to predict time-to-event probabilities for RP2 and LC independently and jointly by incorporating time information into designated loss functions. RESULTS: Models were evaluated on 117 retrospective patients and were independently tested on 25 newly accrued patients prospectively. A multi-institutional RTOG0617 data set of 327 patients was used for external validation. ADNN-DVH yielded cross-validated c-indexes (95% confidence intervals) of 0.660 (0.630-0.690) for RP2 prediction and 0.727 (0.700-0.753) for LC prediction, outperforming a generalized Lyman model for RP2 (0.613 [0.583-0.643]) and a generalized log-logistic model for LC (0.569 [0.545-0.594]). The independent internal test and external validation yielded similar results. ADNN-com achieved an even better performance than ADNN-DVH on both cross-validation and independent internal test. Furthermore, ADNN-com-joint, which yielded performance similar to ADNN-com, realized joint prediction with c-indexes of 0.705 (0.676-0.734) for RP2 and 0.740 (0.714-0.765) for LC and achieved an area under a free-response receiving operator characteristic curve (AU-FROC) of 0.729 (0.697-0.773) for the joint prediction of RP2 and LC. CONCLUSION: Novel deep learning architectures that integrate multiomics information outperformed traditional normal tissue complication probability/tumor control probability models in actuarial prediction of RP2 and LC.",2021,10.1016/j.ijrobp.2021.01.042,prognosis,False
"Integration of CNN, CBMIR, and Visualization Techniques for Diagnosis and Quantification of Covid-19 Disease","Diagnosis techniques based on medical image modalities have higher sensitivities compared to conventional RT-PCT tests. We propose two methods for diagnosing COVID-19 disease using X-ray images and differentiating it from viral pneumonia. The diagnosis section is based on deep neural networks, and the discriminating uses an image retrieval approach. Both units were trained by healthy, pneumonia, and COVID-19 images. In COVID-19 patients, the maximum intensity projection of the lung CT is visualized to a physician, and the CT Involvement Score is calculated. The performance of the CNN and image retrieval algorithms were improved by transfer learning and hashing functions. We achieved an accuracy of 97% and an overall prec@10 of 87%, respectively, concerning the CNN and the retrieval methods.",2021,10.1109/jbhi.2021.3067333,diagnosis,False
Integration of convolutional neural networks for pulmonary nodule malignancy assessment in a lung cancer classification pipeline,"The early identification of malignant pulmonary nodules is critical for a better lung cancer prognosis and less invasive chemo or radio therapies. Nodule malignancy assessment done by radiologists is extremely useful for planning a preventive intervention but is, unfortunately, a complex, time-consuming and error-prone task. This explains the lack of large datasets containing radiologists malignancy characterization of nodules; METHODS: In this article, we propose to assess nodule malignancy through 3D convolutional neural networks and to integrate it in an automated end-to-end existing pipeline of lung cancer detection. For training and testing purposes we used independent subsets of the LIDC dataset; RESULTS: Adding the probabilities of nodules malignity in a baseline lung cancer pipeline improved its F1-weighted score by 14.7%, whereas integrating the malignancy model itself using transfer learning outperformed the baseline prediction by 11.8% of F1-weighted score; CONCLUSIONS: Despite the limited size of the lung cancer datasets, integrating predictive models of nodule malignancy improves prediction of lung cancer.",2020,10.1016/j.cmpb.2019.105172,diagnosis,True
Integration of Deep Learning Radiomics and Counts of Circulating Tumor Cells Improves Prediction of Outcomes of Early Stage NSCLC Patients Treated With Stereotactic Body Radiation Therapy,"PURPOSE: We develop a deep learning (DL) radiomics model and integrate it with circulating tumor cell (CTC) counts as a clinically useful prognostic marker for predicting recurrence outcomes of early-stage (ES) non-small cell lung cancer (NSCLC) patients treated with stereotactic body radiation therapy (SBRT). METHODS AND MATERIALS: A cohort of 421 NSCLC patients was used to train a DL model for gleaning informative imaging features from computed tomography (CT) data. The learned imaging features were optimized on a cohort of 98 ES-NSCLC patients treated with SBRT for predicting individual patient recurrence risks by building DL models on CT data and clinical measures. These DL models were validated on the third cohort of 60 ES-NSCLC patients treated with SBRT to predict recurrent risks and stratify patients into subgroups with distinct outcomes in conjunction with CTC counts. RESULTS: The DL model obtained a concordance-index of 0.880 (95% confidence interval, 0.879-0.881). Patient subgroups with low and high DL risk scores had significantly different recurrence outcomes (P = 3.5e-04). The integration of DL risk scores and CTC measures identified 4 subgroups of patients with significantly different risks of recurrence (χ(2) = 20.11, P = 1.6e-04). Patients with positive CTC measures were associated with increased risks of recurrence that were significantly different from patients with negative CTC measures (P = 0.0447). CONCLUSIONS: In this first-ever study integrating DL radiomics models and CTC counts, our results suggested that this integration improves patient stratification compared with either imagining data or CTC measures alone in predicting recurrence outcomes for patients treated with SBRT for ES-NSCLC.",2022,10.1016/j.ijrobp.2021.11.006,prognosis,True
Integrative analysis for COVID-19 patient outcome prediction,"While image analysis of chest computed tomography (CT) for COVID-19 diagnosis has been intensively studied, little work has been performed for image-based patient outcome prediction. Management of high-risk patients with early intervention is a key to lower the fatality rate of COVID-19 pneumonia, as a majority of patients recover naturally. Therefore, an accurate prediction of disease progression with baseline imaging at the time of the initial presentation can help in patient management. In lieu of only size and volume information of pulmonary abnormalities and features through deep learning based image segmentation, here we combine radiomics of lung opacities and non-imaging features from demographic data, vital signs, and laboratory findings to predict need for intensive care unit (ICU) admission. To our knowledge, this is the first study that uses holistic information of a patient including both imaging and non-imaging data for outcome prediction. The proposed methods were thoroughly evaluated on datasets separately collected from three hospitals, one in the United States, one in Iran, and another in Italy, with a total 295 patients with reverse transcription polymerase chain reaction (RT-PCR) assay positive COVID-19 pneumonia. Our experimental results demonstrate that adding non-imaging features can significantly improve the performance of prediction to achieve AUC up to 0.884 and sensitivity as high as 96.1%, which can be valuable to provide clinical decision support in managing COVID-19 patients. Our methods may also be applied to other lung diseases including but not limited to community acquired pneumonia. The source code of our work is available at https://github.com/DIAL-RPI/COVID19-ICUPrediction.",2021,10.1016/j.media.2020.101844,prognosis,True
Integrative Predictive Models of Computed Tomography Texture Parameters and Hematological Parameters for Lymph Node Metastasis in Lung Adenocarcinomas,"OBJECTIVES: The aims of the study were to integrate characteristics of computed tomography (CT), texture, and hematological parameters and to establish predictive models for lymph node (LN) metastasis in lung adenocarcinoma. METHODS: A total of 207 lung adenocarcinoma cases with confirmed postoperative pathology and preoperative CT scans between February 2017 and April 2019 were included in this retrospective study. All patients were divided into training and 2 validation cohorts chronologically in the ratio of 3:1:1. The χ2 test or Fisher exact test were used for categorical variables. The Shapiro-Wilk test and Mann-Whitney U test were used for continuous variables. Logistic regression and machine learning algorithm models based on CT characteristics, texture, and hematological parameters were used to predict LN metastasis. The performance of the multivariate models was evaluated using a receiver operating characteristic curve; prediction performance was evaluated in the validation cohorts. Decision curve analysis confirmed its clinical utility. RESULTS: Logistic regression analysis demonstrated that pleural thickening (P = 0.013), percentile 25th (P = 0.033), entropy gray-level co-occurrence matrix 10 (P = 0.019), red blood cell distribution width (P = 0.012), and lymphocyte-to-monocyte ratio (P = 0.049) were independent risk factors associated with LN metastasis. The area under the curve of the predictive model established using the previously mentioned 5 independent risk factors was 0.929 in the receiver operating characteristic analysis. The highest area under the curve was obtained in the training cohort (0.777 using Naive Bayes algorithm). CONCLUSIONS: Integrative predictive models of CT characteristics, texture, and hematological parameters could predict LN metastasis in lung adenocarcinomas. These findings may provide a reference for clinical decision making.",2022,10.1097/rct.0000000000001264,diagnosis,True
Interpretative computer-aided lung cancer diagnosis: From radiology analysis to malignancy evaluation,"BACKGROUND AND OBJECTIVE: Computer-aided diagnosis (CAD) systems promote accurate diagnosis and reduce the burden of radiologists. A CAD system for lung cancer diagnosis includes nodule candidate detection and nodule malignancy evaluation. Recently, deep learning-based pulmonary nodule detection has reached satisfactory performance ready for clinical application. However, deep learning-based nodule malignancy evaluation depends on heuristic inference from low-dose computed tomography (LDCT) volume to malignant probability, and lacks clinical cognition. METHODS: In this paper, we propose a joint radiology analysis and malignancy evaluation network called R2MNet to evaluate pulmonary nodule malignancy via the analysis of radiological characteristics. Radiological features are extracted as channel descriptor to highlight specific regions of the input volume that are critical for nodule malignancy evaluation. In addition, for model explanations, we propose channel-dependent activation mapping (CDAM) to visualize features and shed light on the decision process of deep neural networks (DNNs). RESULTS: Experimental results on the lung image database consortium image collection (LIDC-IDRI) dataset demonstrate that the proposed method achieved an area under curve (AUC) of 96.27% and 97.52% on nodule radiology analysis and nodule malignancy evaluation, respectively. In addition, explanations of CDAM features proved that the shape and density of nodule regions are two critical factors that influence a nodule to be inferred as malignant. This process conforms to the diagnosis cognition of experienced radiologists. CONCLUSION: The network inference process conforms to the diagnostic procedure of radiologists and increases the confidence of evaluation results by incorporating radiology analysis with nodule malignancy evaluation. Besides, model interpretation with CDAM features shed light on the focus regions of DNNs during the estimation of nodule malignancy probabilities.",2021,10.1016/j.cmpb.2021.106363,diagnosis,True
Intra-tumoural heterogeneity characterization through texture and colour analysis for differentiation of non-small cell lung carcinoma subtypes,"Radiomics has shown potential in disease diagnosis, but its feasibility for non-small cell lung carcinoma (NSCLC) subtype classification is unclear. This study aims to explore the diagnosis value of texture and colour features from positron emission tomography computed tomography (PET-CT) images in differentiation of NSCLC subtypes: adenocarcinoma (ADC) and squamous cell carcinoma (SqCC). Two patient cohorts were retrospectively collected into a dataset of 341 (18)F-labeled 2-deoxy-2fluoro-d-glucose ([(18)F] FDG) PET-CT images of NSCLC tumours (125 ADC, 174 SqCC, and 42 cases with unknown subtype). Quantification of texture and colour features was performed using freehand regions of interest. The relation between extracted features and commonly used parameters such as age, gender, tumour size, and standard uptake value (SUVmax) was explored. To classify NSCLC subtypes, support vector machine algorithm was applied on these features and the classification performance was evaluated by receiver operating characteristic curve analysis. There was a significant difference between ADC and SqCC subtypes in texture and colour features (P < 0.05); this showed that imaging features were significantly correlated to both SUVmax and tumour diameter (P < 0.05). When evaluating classification performance, features combining texture and colour showed an AUC of 0.89 (95% CI, 0.78-1.00), colour features showed an AUC of 0.85 (95% CI, 0.71-0.99), and texture features showed an AUC of 0.68 (95% CI, 0.48-0.88). DeLong's test showed that AUC was higher for features combining texture and colour than that for texture features only (P = 0.010), but not significantly different from that for colour features only (P = 0.328). HSV colour features showed a similar performance to RGB colour features (P = 0.473). The colour features are promising in the refinement of NSCLC subtype differentiation, and features combining texture and colour of PET-CT images could result in better classification performance.",2018,10.1088/1361-6560/aad648,diagnosis,False
Introducing the GEV Activation Function for Highly Unbalanced Data to Develop COVID-19 Diagnostic Models,"Fast and accurate diagnosis is essential for the efficient and effective control of the COVID-19 pandemic that is currently disrupting the whole world. Despite the prevalence of the COVID-19 outbreak, relatively few diagnostic images are openly available to develop automatic diagnosis algorithms. Traditional deep learning methods often struggle when data is highly unbalanced with many cases in one class and only a few cases in another; new methods must be developed to overcome this challenge. We propose a novel activation function based on the generalized extreme value (GEV) distribution from extreme value theory, which improves performance over the traditional sigmoid activation function when one class significantly outweighs the other. We demonstrate the proposed activation function on a publicly available dataset and externally validate on a dataset consisting of 1,909 healthy chest X-rays and 84 COVID-19 X-rays. The proposed method achieves an improved area under the receiver operating characteristic (DeLong's p-value < 0.05) compared to the sigmoid activation. Our method is also demonstrated on a dataset of healthy and pneumonia vs. COVID-19 X-rays and a set of computerized tomography images, achieving improved sensitivity. The proposed GEV activation function significantly improves upon the previously used sigmoid activation for binary classification. This new paradigm is expected to play a significant role in the fight against COVID-19 and other diseases, with relatively few training cases available.",2020,10.1109/jbhi.2020.3012383,diagnosis,False
Investigating phenotypes of pulmonary COVID-19 recovery: A longitudinal observational prospective multicenter trial,"BACKGROUND: The optimal procedures to prevent, identify, monitor, and treat long-term pulmonary sequelae of COVID-19 are elusive. Here, we characterized the kinetics of respiratory and symptom recovery following COVID-19. METHODS: We conducted a longitudinal, multicenter observational study in ambulatory and hospitalized COVID-19 patients recruited in early 2020 (n = 145). Pulmonary computed tomography (CT) and lung function (LF) readouts, symptom prevalence, and clinical and laboratory parameters were collected during acute COVID-19 and at 60, 100, and 180 days follow-up visits. Recovery kinetics and risk factors were investigated by logistic regression. Classification of clinical features and participants was accomplished by unsupervised and semi-supervised multiparameter clustering and machine learning. RESULTS: At the 6-month follow-up, 49% of participants reported persistent symptoms. The frequency of structural lung CT abnormalities ranged from 18% in the mild outpatient cases to 76% in the intensive care unit (ICU) convalescents. Prevalence of impaired LF ranged from 14% in the mild outpatient cases to 50% in the ICU survivors. Incomplete radiological lung recovery was associated with increased anti-S1/S2 antibody titer, IL-6, and CRP levels at the early follow-up. We demonstrated that the risk of perturbed pulmonary recovery could be robustly estimated at early follow-up by clustering and machine learning classifiers employing solely non-CT and non-LF parameters. CONCLUSIONS: The severity of acute COVID-19 and protracted systemic inflammation is strongly linked to persistent structural and functional lung abnormality. Automated screening of multiparameter health record data may assist in the prediction of incomplete pulmonary recovery and optimize COVID-19 follow-up management. FUNDING: The State of Tyrol (GZ 71934), Boehringer Ingelheim/Investigator initiated study (IIS 1199-0424). CLINICAL TRIAL NUMBER: ClinicalTrials.gov: NCT04416100.",2022,10.7554/eLife.72500,prognosis,True
Investigating training-test data splitting strategies for automated segmentation and scoring of COVID-19 lung ultrasound images,"Ultrasound in point-of-care lung assessment is becoming increasingly relevant. This is further reinforced in the context of the COVID-19 pandemic, where rapid decisions on the lung state must be made for staging and monitoring purposes. The lung structural changes due to severe COVID-19 modify the way ultrasound propagates in the parenchyma. This is reflected by changes in the appearance of the lung ultrasound images. In abnormal lungs, vertical artifacts known as B-lines appear and can evolve into white lung patterns in the more severe cases. Currently, these artifacts are assessed by trained physicians, and the diagnosis is qualitative and operator dependent. In this article, an automatic segmentation method using a convolutional neural network is proposed to automatically stage the progression of the disease. 1863 B-mode images from 203 videos obtained from 14 asymptomatic individual,14 confirmed COVID-19 cases, and 4 suspected COVID-19 cases were used. Signs of lung damage, such as the presence and extent of B-lines and white lung areas, are manually segmented and scored from zero to three (most severe). These manually scored images are considered as ground truth. Different test-training strategies are evaluated in this study. The results shed light on the efficient approaches and common challenges associated with automatic segmentation methods.",2021,10.1121/10.0007272,diagnosis,False
Investigation of pulmonary nodule classification using multi-scale residual network enhanced with 3DGAN-synthesized volumes,"It is often difficult to distinguish between benign and malignant pulmonary nodules using only image diagnosis. A biopsy is performed when malignancy is suspected based on CT examination. However, biopsies are highly invasive, and patients with benign nodules may undergo unnecessary procedures. In this study, we performed automated classification of pulmonary nodules using a three-dimensional convolutional neural network (3DCNN). In addition, to increase the number of training data, we utilized generative adversarial networks (GANs), a deep learning technique used as a data augmentation method. In this approach, three-dimensional regions of different sizes centered on pulmonary nodules were extracted from CT images, and a large number of pseudo-pulmonary nodules were synthesized using 3DGAN. The 3DCNN has a multi-scale structure in which multiple nodules in each region are inputted and integrated into the final layer. During the training of multi-scale 3DCNN, pre-training was first performed using 3DGAN-synthesized nodules, and the pulmonary nodules were then comprehensively classified by fine-tuning the pre-trained model using real nodules. Using an evaluation process that involved 60 confirmed cases of pathological diagnosis based on biopsies, the sensitivity was determined to be 90.9% and specificity was 74.1%. The classification accuracy was improved compared to the case of training with only real nodules without pre-training. The 2DCNN results of our previous study were slightly better than the 3DCNN results. However, it was shown that even though 3DCNN is difficult to train with limited data such as in the case of medical images, classification accuracy can be improved by GAN.",2020,10.1007/s12194-020-00564-5,diagnosis,True
IoT with cloud based lung cancer diagnosis model using optimal support vector machine,"In the last decade, exponential growth of Internet of Things (IoT) and cloud computing takes the healthcare services to the next level. At the same time, lung cancer is identified as a dangerous disease which increases the global mortality rate annually. Presently, support vector machine (SVM) is the effective image classification tool especially in medical imaging. Feature selection and parameter optimization are the effective ways to improve the results of SVM and are conventionally resolved individually. This paper presents an optimal SVM for lung image classification where the parameters of SVM are optimized and feature selection takes place by modified grey wolf optimization algorithm combined with genetic algorithm (GWO-GA). The experimentation part takes place on three dimensions: test for parameter optimization, feature selection, and optimal SVM. For assessing the performance of the presented approach, a benchmark image database is employed which comprises of 50 low-dosage and stored lung CT images. The presented method exhibits its superior results on all the applied test images under several aspects. In addition, it achieves average classification accuracy of 93.54 which is significantly higher than the compared methods.",2020,10.1007/s10729-019-09489-x,diagnosis,True
Issues associated with deploying CNN transfer learning to detect COVID-19 from chest X-rays,"Covid-19 first occurred in Wuhan, China in December 2019. Subsequently, the virus spread throughout the world and as of June 2020 the total number of confirmed cases are above 4.7 million with over 315,000 deaths. Machine learning algorithms built on radiography images can be used as a decision support mechanism to aid radiologists to speed up the diagnostic process. The aim of this work is to conduct a critical analysis to investigate the applicability of convolutional neural networks (CNNs) for the purpose of COVID-19 detection in chest X-ray images and highlight the issues of using CNN directly on the whole image. To accomplish this task, we use 12-off-the-shelf CNN architectures in transfer learning mode on 3 publicly available chest X-ray databases together with proposing a shallow CNN architecture in which we train it from scratch. Chest X-ray images are fed into CNN models without any preprocessing to replicate researches used chest X-rays in this manner. Then a qualitative investigation performed to inspect the decisions made by CNNs using a technique known as class activation maps (CAM). Using CAMs, one can map the activations contributed to the decision of CNNs back to the original image to visualize the most discriminating region(s) on the input image. We conclude that CNN decisions should not be taken into consideration, despite their high classification accuracy, until clinicians can visually inspect and approve the region(s) of the input image used by CNNs that lead to its prediction.",2020,10.1007/s13246-020-00934-8,diagnosis,False
JCS: An Explainable COVID-19 Diagnosis System by Joint Classification and Segmentation,"Recently, the coronavirus disease 2019 (COVID-19) has caused a pandemic disease in over 200 countries, influencing billions of humans. To control the infection, identifying and separating the infected people is the most crucial step. The main diagnostic tool is the Reverse Transcription Polymerase Chain Reaction (RT-PCR) test. Still, the sensitivity of the RT-PCR test is not high enough to effectively prevent the pandemic. The chest CT scan test provides a valuable complementary tool to the RT-PCR test, and it can identify the patients in the early-stage with high sensitivity. However, the chest CT scan test is usually time-consuming, requiring about 21.5 minutes per case. This paper develops a novel Joint Classification and Segmentation (JCS) system to perform real-time and explainable COVID- 19 chest CT diagnosis. To train our JCS system, we construct a large scale COVID- 19 Classification and Segmentation (COVID-CS) dataset, with 144,167 chest CT images of 400 COVID- 19 patients and 350 uninfected cases. 3,855 chest CT images of 200 patients are annotated with fine-grained pixel-level labels of opacifications, which are increased attenuation of the lung parenchyma. We also have annotated lesion counts, opacification areas, and locations and thus benefit various diagnosis aspects. Extensive experiments demonstrate that the proposed JCS diagnosis system is very efficient for COVID-19 classification and segmentation. It obtains an average sensitivity of 95.0% and a specificity of 93.0% on the classification test set, and 78.5% Dice score on the segmentation test set of our COVID-CS dataset. The COVID-CS dataset and code are available at https://github.com/yuhuan-wu/JCS.",2021,10.1109/tip.2021.3058783,diagnosis,True
Joint Learning of 3D Lesion Segmentation and Classification for Explainable COVID-19 Diagnosis,"Given the outbreak of COVID-19 pandemic and the shortage of medical resource, extensive deep learning models have been proposed for automatic COVID-19 diagnosis, based on 3D computed tomography (CT) scans. However, the existing models independently process the 3D lesion segmentation and disease classification, ignoring the inherent correlation between these two tasks. In this paper, we propose a joint deep learning model of 3D lesion segmentation and classification for diagnosing COVID-19, called DeepSC-COVID, as the first attempt in this direction. Specifically, we establish a large-scale CT database containing 1,805 3D CT scans with fine-grained lesion annotations, and reveal 4 findings about lesion difference between COVID-19 and community acquired pneumonia (CAP). Inspired by our findings, DeepSC-COVID is designed with 3 subnets: a cross-task feature subnet for feature extraction, a 3D lesion subnet for lesion segmentation, and a classification subnet for disease diagnosis. Besides, the task-aware loss is proposed for learning the task interaction across the 3D lesion and classification subnets. Different from all existing models for COVID-19 diagnosis, our model is interpretable with fine-grained 3D lesion distribution. Finally, extensive experimental results show that the joint learning framework in our model significantly improves the performance of 3D lesion segmentation and disease classification in both efficiency and efficacy.",2021,10.1109/tmi.2021.3079709,diagnosis,True
Knowledge-Based Analysis for Mortality Prediction From CT Images,"Low-Dose CT (LDCT) can significantly improve the accuracy of lung cancer diagnosis and thus reduce cancer deaths compared to chest X-ray. The lung cancer risk population is also at high risk of other deadly diseases, for instance, cardiovascular diseases. Therefore, predicting the all-cause mortality risks of this population is of great importance. This paper introduces a knowledge-based analytical method using deep convolutional neural network (CNN) for all-cause mortality prediction. The underlying approach combines structural image features extracted from CNNs, based on LDCT volume at different scales, and clinical knowledge obtained from quantitative measurements, to predict the mortality risk of lung cancer screening subjects. The proposed method is referred as Knowledge-based Analysis of Mortality Prediction Network (KAMP-Net). It constitutes a collaborative framework that utilizes both imaging features and anatomical information, instead of completely relying on automatic feature extraction. Our work demonstrates the feasibility of incorporating quantitative clinical measurements to assist CNNs in all-cause mortality prediction from chest LDCT images. The results of this study confirm that radiologist defined features can complement CNNs in performance improvement. The experiments demonstrate that KAMP-Net can achieve a superior performance when compared to other methods.",2020,10.1109/jbhi.2019.2946066,prognosis,True
Knowledge-based Collaborative Deep Learning for Benign-Malignant Lung Nodule Classification on Chest CT,"The accurate identification of malignant lung nodules on chest CT is critical for the early detection of lung cancer, which also offers patients the best chance of cure. Deep learning methods have recently been successfully introduced to computer vision problems, although substantial challenges remain in the detection of malignant nodules due to the lack of large training data sets. In this paper, we propose a multi-view knowledge-based collaborative (MV-KBC) deep model to separate malignant from benign nodules using limited chest CT data. Our model learns 3-D lung nodule characteristics by decomposing a 3-D nodule into nine fixed views. For each view, we construct a knowledge-based collaborative (KBC) submodel, where three types of image patches are designed to fine-tune three pre-trained ResNet-50 networks that characterize the nodules' overall appearance, voxel, and shape heterogeneity, respectively. We jointly use the nine KBC submodels to classify lung nodules with an adaptive weighting scheme learned during the error back propagation, which enables the MV-KBC model to be trained in an end-to-end manner. The penalty loss function is used for better reduction of the false negative rate with a minimal effect on the overall performance of the MV-KBC model. We tested our method on the benchmark LIDC-IDRI data set and compared it to the five state-of-the-art classification approaches. Our results show that the MV-KBC model achieved an accuracy of 91.60% for lung nodule classification with an AUC of 95.70%. These results are markedly superior to the state-of-the-art approaches.",2019,10.1109/tmi.2018.2876510,diagnosis,True
Label Co-Occurrence Learning With Graph Convolutional Networks for Multi-Label Chest X-Ray Image Classification,"Existing multi-label medical image learning tasks generally contain rich relationship information among pathologies such as label co-occurrence and interdependency, which is of great importance for assisting in clinical diagnosis and can be represented as the graph-structured data. However, most state-of-the-art works only focus on regression from the input to the binary labels, failing to make full use of such valuable graph-structured information due to the complexity of graph data. In this paper, we propose a novel label co-occurrence learning framework based on Graph Convolution Networks (GCNs) to explicitly explore the dependencies between pathologies for the multi-label chest X-ray (CXR) image classification task, which we term the ""CheXGCN"". Specifically, the proposed CheXGCN consists of two modules, i.e., the image feature embedding (IFE) module and label co-occurrence learning (LCL) module. Thanks to the LCL model, the relationship between pathologies is generalized into a set of classifier scores by introducing the word embedding of pathologies and multi-layer graph information propagation. During end-to-end training, it can be flexibly integrated into the IFE module and then adaptively recalibrate multi-label outputs with these scores. Extensive experiments on the ChestX-Ray14 and CheXpert datasets have demonstrated the effectiveness of CheXGCN as compared with the state-of-the-art baselines.",2020,10.1109/jbhi.2020.2967084,diagnosis,False
Latent traits of lung tissue patterns in former smokers derived by dual channel deep learning in computed tomography images,"Chronic obstructive pulmonary disease (COPD) is a heterogeneous disease and the traditional variables extracted from computed tomography (CT) images may not be sufficient to describe all the topological features of lung tissues in COPD patients. We employed an unsupervised three-dimensional (3D) convolutional autoencoder (CAE)-feature constructor (FC) deep learning network to learn from CT data and derive tissue pattern-clusters jointly. We then applied exploratory factor analysis (EFA) to discover the unobserved latent traits (factors) among pattern-clusters. CT images at total lung capacity (TLC) and residual volume (RV) of 541 former smokers and 59 healthy non-smokers from the cohort of the SubPopulations and Intermediate Outcome Measures in the COPD Study (SPIROMICS) were analyzed. TLC and RV images were registered to calculate the Jacobian (determinant) values for all the voxels in TLC images. 3D Regions of interest (ROIs) with two data channels of CT intensity and Jacobian value were randomly extracted from training images and were fed to the 3D CAE-FC model. 80 pattern-clusters and 7 factors were identified. Factor scores computed for individual subjects were able to predict spirometry-measured pulmonary functions. Two factors which correlated with various emphysema subtypes, parametric response mapping (PRM) metrics, airway variants, and airway tree to lung volume ratio were discriminants of patients across all severity stages. Our findings suggest the potential of developing factor-based surrogate markers for new COPD phenotypes.",2021,10.1038/s41598-021-84547-5,prognosis,True
LBP-based information assisted intelligent system for COVID-19 identification,"A real-time COVID-19 detection system is an utmost requirement of the present situation. This article presents a chest X-ray image-based automated COVID-19 detection system which can be employed with the RT-PCR test to improve the diagnosis rate. In the proposed approach, the textural features are extracted from the chest X-ray images and local binary pattern (LBP) based images. Further, the image-based and LBP image-based features are jointly investigated. Thereafter, highly discriminatory features are provided to the classifier for developing an automated model for COVID-19 identification. The performance of the proposed approach is investigated over 2905 chest X-ray images of normal, pneumonia, and COVID-19 infected persons on various class combinations to analyze the robustness. The developed method achieves 97.97% accuracy (acc) and 99.88% sensitivity (sen) for classifying COVID-19 X-ray images against pneumonia infected and normal person's X-ray images. It attains 98.91% acc and 99.33% sen for COVID-19 X-ray against the normal X-ray classification. This method can be employed to assist the radiologists during mass screening for fast, accurate, and contact-free COVID-19 diagnosis.",2021,10.1016/j.compbiomed.2021.104453,diagnosis,False
Learning from imbalanced COVID-19 chest X-ray (CXR) medical imaging data,"The trendy task of digital medical image analysis has been continually evolving. It has been an area of prominent and growing importance from both research and deployment perspectives. Nonetheless, it is necessary to realize that the use of algorithms, methodology, as well as the source of medical image data, must be strictly scrutinized. As the COVID-19 pandemic has been gripping much of the world recently, there has been much efforts gone into developing affordable testing for the masses, and it has been shown that the established and widely available chest X-rays (CXR) images may be used as a screening criteria for assistive diagnosis purpose. Thanks to the dedicated work by various individuals and organizations, publicly available CXR of COVID-19 subjects are available for analytic usage. We have also provided a publicly available CXR dataset on the Kaggle platform. As a case study, this paper presents a systematic approach to learn from a typically imbalanced set of CXR images, which consists of a limited number of publicly available COVID-19 images. Our results show that we are able to outperform the top finishers in a related Kaggle multi-class CXR challenge. The proposed methodology should be able to help guide medical personnel in obtaining a robust diagnosis model to discern COVID-19 from other conditions confidently.",2022,10.1016/j.ymeth.2021.06.002,diagnosis,False
Learning to Quantify Emphysema Extent: What Labels Do We Need?,"Accurate assessment of pulmonary emphysema is crucial to assess disease severity and subtype, to monitor disease progression, and to predict lung cancer risk. However, visual assessment is time-consuming and subject to substantial inter-rater variability while standard densitometry approaches to quantify emphysema remain inferior to visual scoring. We explore if machine learning methods that learn from a large dataset of visually assessed CT scans can provide accurate estimates of emphysema extent and if methods that learn from emphysema extent scoring can outperform algorithms that learn only from emphysema presence scoring. Four Multiple Instance Learning classifiers, trained on emphysema presence labels, and five Learning with Label Proportions classifiers, trained on emphysema extent labels, are compared. Performance is evaluated on 600 low-dose CT scans from the Danish Lung Cancer Screening Trial and we find that learning from emphysema presence labels, which are much easier to obtain, gives equally good performance to learning from emphysema extent labels. The best performing Multiple Instance Learning and Learning with Label Proportions classifiers, achieve intra-class correlation coefficients around 0.90 and average overall agreement with raters of 78% and 79% compared to an inter-rater agreement of 83%.",2020,10.1109/jbhi.2019.2932145,diagnosis,True
Learning-to-augment strategy using noisy and denoised data: Improving generalizability of deep CNN for the detection of COVID-19 in X-ray images,"Chest X-ray images are used in deep convolutional neural networks for the detection of COVID-19, the greatest human challenge of the 21st century. Robustness to noise and improvement of generalization are the major challenges in designing these networks. In this paper, we introduce a strategy for data augmentation using the determination of the type and value of noise density to improve the robustness and generalization of deep CNNs for COVID-19 detection. Firstly, we present a learning-to-augment approach that generates new noisy variants of the original image data with optimized noise density. We apply a Bayesian optimization technique to control and choose the optimal noise type and its parameters. Secondly, we propose a novel data augmentation strategy, based on denoised X-ray images, that uses the distance between denoised and original pixels to generate new data. We develop an autoencoder model to create new data using denoised images corrupted by the Gaussian and impulse noise. A database of chest X-ray images, containing COVID-19 positive, healthy, and non-COVID pneumonia cases, is used to fine-tune the pre-trained networks (AlexNet, ShuffleNet, ResNet18, and GoogleNet). The proposed method performs better results compared to the state-of-the-art learning to augment strategies in terms of sensitivity (0.808), specificity (0.915), and F-Measure (0.737). The source code of the proposed method is available at https://github.com/mohamadmomeny/Learning-to-augment-strategy.",2021,10.1016/j.compbiomed.2021.104704,diagnosis,False
Lightweight deep learning models for detecting COVID-19 from chest X-ray images,"Deep learning methods have already enjoyed an unprecedented success in medical imaging problems. Similar success has been evidenced when it comes to the detection of COVID-19 from medical images, therefore deep learning approaches are considered good candidates for detecting this disease, in collaboration with radiologists and/or physicians. In this paper, we propose a new approach to detect COVID-19 via exploiting a conditional generative adversarial network to generate synthetic images for augmenting the limited amount of data available. Additionally, we propose two deep learning models following a lightweight architecture, commensurating with the overall amount of data available. Our experiments focused on both binary classification for COVID-19 vs Normal cases and multi-classification that includes a third class for bacterial pneumonia. Our models achieved a competitive performance compared to other studies in literature and also a ResNet8 model. Our best performing binary model achieved 98.7% accuracy, 100% sensitivity and 98.3% specificity, while our three-class model achieved 98.3% accuracy, 99.3% sensitivity and 98.1% specificity. Moreover, via adopting a testing protocol proposed in literature, our models proved to be more robust and reliable in COVID-19 detection than a baseline ResNet8, making them good candidates for detecting COVID-19 from posteroanterior chest X-ray images.",2021,10.1016/j.compbiomed.2020.104181,diagnosis,False
Lobar Emphysema Distribution Is Associated With 5-Year Radiological Disease Progression,"BACKGROUND: Emphysema has considerable variability in its regional distribution. Craniocaudal emphysema distribution is an important predictor of the response to lung volume reduction. However, there is little consensus regarding how to define upper lobe-predominant and lower lobe-predominant emphysema subtypes. Consequently, the clinical and genetic associations with these subtypes are poorly characterized. METHODS: We sought to identify subgroups characterized by upper-lobe or lower-lobe emphysema predominance and comparable amounts of total emphysema by analyzing data from 9,210 smokers without alpha-1-antitrypsin deficiency in the Genetic Epidemiology of COPD (COPDGene) cohort. CT densitometric emphysema was measured in each lung lobe. Random forest clustering was applied to lobar emphysema variables after regressing out the effects of total emphysema. Clusters were tested for association with clinical and imaging outcomes at baseline and at 5-year follow-up. Their associations with genetic variants were also compared. RESULTS: Three clusters were identified: minimal emphysema (n = 1,312), upper lobe-predominant emphysema (n = 905), and lower lobe-predominant emphysema (n = 796). Despite a similar amount of total emphysema, the lower-lobe group had more severe airflow obstruction at baseline and higher rates of metabolic syndrome compared with subjects with upper-lobe predominance. The group with upper-lobe predominance had greater 5-year progression of emphysema, gas trapping, and dyspnea. Differential associations with known COPD genetic risk variants were noted. CONCLUSIONS: Subgroups of smokers defined by upper-lobe or lower-lobe emphysema predominance exhibit different functional and radiological disease progression rates, and the upper-lobe predominant subtype shows evidence of association with known COPD genetic risk variants. These subgroups may be useful in the development of personalized treatments for COPD.",2018,10.1016/j.chest.2017.09.022,prognosis,True
Localized thin-section CT with radiomics feature extraction and machine learning to classify early-detected pulmonary nodules from lung cancer screening,"Lung cancer screening aims to detect small pulmonary nodules and decrease the mortality rate of those affected. However, studies from large-scale clinical trials of lung cancer screening have shown that the false-positive rate is high and positive predictive value is low. To address these problems, a technical approach is greatly needed for accurate malignancy differentiation among these early-detected nodules. We studied the clinical feasibility of an additional protocol of localized thin-section CT for further assessment on recalled patients from lung cancer screening tests. Our approach of localized thin-section CT was integrated with radiomics features extraction and machine learning classification which was supervised by pathological diagnosis. Localized thin-section CT images of 122 nodules were retrospectively reviewed and 374 radiomics features were extracted. In this study, 48 nodules were benign and 74 malignant. There were nine patients with multiple nodules and four with synchronous multiple malignant nodules. Different machine learning classifiers with a stratified ten-fold cross-validation were used and repeated 100 times to evaluate classification accuracy. Of the image features extracted from the thin-section CT images, 238 (64%) were useful in differentiating between benign and malignant nodules. These useful features include CT density (p = 0.002 518), sigma (p = 0.002 781), uniformity (p = 0.032 41), and entropy (p = 0.006 685). The highest classification accuracy was 79% by the logistic classifier. The performance metrics of this logistic classification model was 0.80 for the positive predictive value, 0.36 for the false-positive rate, and 0.80 for the area under the receiver operating characteristic curve. Our approach of direct risk classification supervised by the pathological diagnosis with localized thin-section CT and radiomics feature extraction may support clinical physicians in determining truly malignant nodules and therefore reduce problems in lung cancer screening.",2018,10.1088/1361-6560/aaafab,diagnosis,True
Long-term follow-up of persistent pulmonary pure ground-glass nodules with deep learning-assisted nodule segmentation,"OBJECTIVE: To investigate the natural history of persistent pulmonary pure ground-glass nodules (pGGNs) with deep learning-assisted nodule segmentation. METHODS: Between January 2007 and October 2018, 110 pGGNs from 110 patients with 573 follow-up CT scans were included in this retrospective study. pGGN automatic segmentation was performed on initial and all follow-up CT scans using the Dr. Wise system based on convolution neural networks. Subsequently, pGGN diameter, density, volume, mass, volume doubling time (VDT), and mass doubling time (MDT) were calculated automatically. Enrolled pGGNs were categorized into growth, 52 (47.3%), and non-growth, 58 (52.7%), groups according to volume growth. Kaplan-Meier analyses with the log-rank test and Cox proportional hazards regression analysis were conducted to analyze the cumulative percentages of pGGN growth and identify risk factors for growth. RESULTS: The mean follow-up period of the enrolled pGGNs was 48.7 ± 23.8 months. The median VDT of the 52 pGGNs having grown was 1448 (range, 339-8640) days, and their median MDT was 1332 (range, 290-38,912) days. The 12-month, 24.7-month, and 60.8-month cumulative percentages of pGGN growth were 10%, 25.5%, and 51.1%, respectively, and they significantly differed among the initial diameter, volume, and mass subgroups (all p < 0.001). The growth pattern of pGGNs may conform to the exponential model. Lobulated sign (p = 0.044), initial mean diameter (p < 0.001), volume (p = 0.003), and mass (p = 0.023) predicted pGGN growth. CONCLUSIONS: Persistent pGGNs showed an indolent course. Deep learning can assist in accurately elucidating the natural history of pGGNs. pGGNs with lobulated sign and larger initial diameter, volume, and mass are more likely to grow. KEY POINTS: • The pure ground-glass nodule (pGGN) segmentation accuracy of the Dr. Wise system based on convolution neural networks (CNNs) was 96.5% (573/594). • The median volume doubling time (VDT) of 52 pure ground-glass nodules (pGGNs) having grown was 1448 days (range, 339-8640 days), and their median mass doubling time (MDT) was 1332 days (range, 290-38,912 days). The mean time to growth in volume was 854 ± 675 days (range, 116-2856 days). • The 12-month, 24.7-month, and 60.8-month cumulative percentages of pGGN growth were 10%, 25.5%, and 51.1%, respectively, and they significantly differed among the initial diameter, volume, and mass subgroups (all p values < 0.001). The growth pattern of pure ground-glass nodules may conform to exponential model.",2020,10.1007/s00330-019-06344-z,diagnosis,True
Lung and Pancreatic Tumor Characterization in the Deep Learning Era: Novel Supervised and Unsupervised Learning Approaches,"Risk stratification (characterization) of tumors from radiology images can be more accurate and faster with computer-aided diagnosis (CAD) tools. Tumor characterization through such tools can also enable non-invasive cancer staging, prognosis, and foster personalized treatment planning as a part of precision medicine. In this papet, we propose both supervised and unsupervised machine learning strategies to improve tumor characterization. Our first approach is based on supervised learning for which we demonstrate significant gains with deep learning algorithms, particularly by utilizing a 3D convolutional neural network and transfer learning. Motivated by the radiologists' interpretations of the scans, we then show how to incorporate task-dependent feature representations into a CAD system via a graph-regularized sparse multi-task learning framework. In the second approach, we explore an unsupervised learning algorithm to address the limited availability of labeled training data, a common problem in medical imaging applications. Inspired by learning from label proportion approaches in computer vision, we propose to use proportion-support vector machine for characterizing tumors. We also seek the answer to the fundamental question about the goodness of ""deep features"" for unsupervised tumor classification. We evaluate our proposed supervised and unsupervised learning algorithms on two different tumor diagnosis challenges: lung and pancreas with 1018 CT and 171 MRI scans, respectively, and obtain the state-of-the-art sensitivity and specificity results in both problems.",2019,10.1109/tmi.2019.2894349,diagnosis,True
Lung Cancer and Granuloma Identification Using a Deep Learning Model to Extract 3-Dimensional Radiomics Features in CT Imaging,"BACKGROUND: We aimed to evaluate a deep learning (DL) model combining perinodular and intranodular radiomics features and clinical features for preoperative differentiation of solitary granuloma nodules (GNs) from solid lung cancer nodules in patients with spiculation, lobulation, or pleural indentation on CT. PATIENTS AND METHODS: We retrospectively recruited 915 patients with solitary solid pulmonary nodules and suspicious signs of malignancy. Data including clinical characteristics and subjective CT findings were obtained. A 3-dimensional U-Net-based DL model was used for tumor segmentation and extraction of 3-dimensional radiomics features. We used the Maximum Relevance and Minimum Redundancy (mRMR) algorithm and the eXtreme Gradient Boosting (XGBoost) algorithm to select the intranodular, perinodular, and gross nodular radiomics features. We propose a medical image DL (IDL) model, a clinical image DL (CIDL) model, a radiomics DL (RDL) model, and a clinical image radiomics DL (CIRDL) model to preoperatively differentiate GNs from solid lung cancer. Five-fold cross-validation was used to select and evaluate the models. The prediction performance of the models was evaluated using receiver operating characteristic and calibration curves. RESULTS: The CIRDL model achieved the best performance in differentiating between GNs and solid lung cancer (area under the curve [AUC] = 0.9069), which was significantly higher compared with the IDL (AUC = 0.8322), CIDL (AUC = 0.8652), intra-RDL (AUC = 0.8583), peri-RDL (AUC = 0.8259), and gross-RDL (AUC = 0.8705) models. CONCLUSION: The proposed CIRDL model is a noninvasive diagnostic tool to differentiate between granuloma nodules and solid lung cancer nodules and reduce the need for invasive diagnostic and surgical procedures.",2021,10.1016/j.cllc.2021.02.004,diagnosis,True
Lung cancer classification using neural networks for CT images,"Early detection of cancer is the most promising way to enhance a patient's chance for survival. This paper presents a computer aided classification method in computed tomography (CT) images of lungs developed using artificial neural network. The entire lung is segmented from the CT images and the parameters are calculated from the segmented image. The statistical parameters like mean, standard deviation, skewness, kurtosis, fifth central moment and sixth central moment are used for classification. The classification process is done by feed forward and feed forward back propagation neural networks. Compared to feed forward networks the feed forward back propagation network gives better classification. The parameter skewness gives the maximum classification accuracy. Among the already available thirteen training functions of back propagation neural network, the Traingdx function gives the maximum classification accuracy of 91.1%. Two new training functions are proposed in this paper. The results show that the proposed training function 1 gives an accuracy of 93.3%, specificity of 100% and sensitivity of 91.4% and a mean square error of 0.998. The proposed training function 2 gives a classification accuracy of 93.3% and minimum mean square error of 0.0942.",2014,10.1016/j.cmpb.2013.10.011,diagnosis,True
Lung Cancer Detection Using Fuzzy Auto-Seed Cluster Means Morphological Segmentation and SVM Classifier,"An effective fuzzy auto-seed cluster means morphological algorithm developed in this work to segment the lung nodules from the consecutive slices of Computer Tomography (CT) images to detect the lung cancer. The initial cluster values were chosen automatically by averaging the minimum and maximum pixel values in each row of an image. The area and eccentricity features were used to eliminate the line like structure and very tiny clusters less than 3 mm in size. The change in centroid analysis was carried out to eliminate the blood vessels. The tissue clusters whose centroid varies much in consecutive slices must be blood vessels. After eliminating the blood vessels, the co-occurrence matrix based texture features contrast, homogeneity and auto correlation were computed on the remaining nodules from the consecutive CT slices to discriminate the calcifications. The extracted centroid shift and texture features were used as the inputs to the Support Vector Machine (SVM) kernel classifier in order to classify the real malignant nodules. This work was carried out on 56 malignant (cancerous) cases and 50 normal cases (with lung infections), which had a total of 56 malignant nodules and 745 benign nodules. Out of these, 60 % of subjects (34 cancerous & 30 non-cancerous) were used for training. The remaining 40 % subjects (22 cancerous & 20 non-cancerous) were used for testing. This work produced a good sensitivity, specificity and accuracy of 100 %, 93 % and 94 %, respectively. The False Positive (FP) per patient was calculated as 0.38.",2016,10.1007/s10916-016-0539-9,diagnosis,True
Lung Cancer Detection using Probabilistic Neural Network with modified Crow-Search Algorithm,"Objective: Lung cancer is a type of malignancy that occurs most commonly among men and the third most common type of malignancy among women. The timely recognition of lung cancer is necessary for decreasing the effect of death rate worldwide. Since the symptoms of lung cancer are identified only at an advanced stage, it is essential to predict the disease at its earlier stage using any medical imaging techniques. This work aims to propose a classification methodology for lung cancer automatically at the initial stage. Methods: The work adopts computed tomography (CT) imaging modality of lungs for the examination and probabilistic neural network (PNN) for the classification task. After pre-processing of the input lung images, feature extraction for the work is carried out based on the Gray-Level Co-Occurrence Matrix (GLCM) and chaotic crow search algorithm (CCSA) based feature selection is proposed. Results: Specificity, Sensitivity, Positive and Negative Predictive Values, Accuracy are the computation metrics used. The results indicate that the CCSA based feature selection effectively provides an accuracy of 90%. Conclusion: The strategy for the selection of appropriate extracted features is employed to improve the efficiency of classification and the work shows that the PNN with CCSA based feature selection gives an improved classification than without using CCSA for feature selection.",2019,10.31557/apjcp.2019.20.7.2159,diagnosis,True
Lung Cancer Diagnosis Based on an ANN Optimized by Improved TEO Algorithm,"A quarter of all cancer deaths are due to lung cancer. Studies show that early diagnosis and treatment of this disease are the most effective way to increase patient life expectancy. In this paper, automatic and optimized computer-aided detection is proposed for lung cancer. The method first applies a preprocessing step for normalizing and denoising the input images. Afterward, Kapur entropy maximization is performed along with mathematical morphology to lung area segmentation. Afterward, 19 GLCM features are extracted from the segmented images for the final evaluations. The higher priority images are then selected for decreasing the system complexity. The feature selection is based on a new optimization design, called Improved Thermal Exchange Optimization (ITEO), which is designed to improve the accuracy and convergence abilities. The images are finally classified into healthy or cancerous cases based on an optimized artificial neural network by ITEO. Simulation is compared with some well-known approaches and the results showed the superiority of the suggested method. The results showed that the proposed method with 92.27% accuracy provides the highest value among the compared methods.",2021,10.1155/2021/6078524,diagnosis,True
Lung cancer histology classification from CT images based on radiomics and deep learning models,"Adenocarcinoma (AC) and squamous cell carcinoma (SCC) are frequent reported cases of non-small cell lung cancer (NSCLC), responsible for a large fraction of cancer deaths worldwide. In this study, we aim to investigate the potential of NSCLC histology classification into AC and SCC by applying different feature extraction and classification techniques on pre-treatment CT images. The employed image dataset (102 patients) was taken from the publicly available cancer imaging archive collection (TCIA). We investigated four different families of techniques: (a) radiomics with two classifiers (kNN and SVM), (b) four state-of-the-art convolutional neural networks (CNNs) with transfer learning and fine tuning (Alexnet, ResNet101, Inceptionv3 and InceptionResnetv2), (c) a CNN combined with a long short-term memory (LSTM) network to fuse information about the spatial coherency of tumor's CT slices, and (d) combinatorial models (LSTM + CNN + radiomics). In addition, the CT images were independently evaluated by two expert radiologists. Our results showed that the best CNN was Inception (accuracy = 0.67, auc = 0.74). LSTM + Inception yielded superior performance than all other methods (accuracy = 0.74, auc = 0.78). Moreover, LSTM + Inception outperformed experts by 7-25% (p < 0.05). The proposed methodology does not require detailed segmentation of the tumor region and it may be used in conjunction with radiological findings to improve clinical decision-making. Lung cancer histology classification from CT images based on CNN + LSTM.",2021,10.1007/s11517-020-02302-w,diagnosis,True
Lung cancer prediction by Deep Learning to identify benign lung nodules,"INTRODUCTION: Deep Learning has been proposed as promising tool to classify malignant nodules. Our aim was to retrospectively validate our Lung Cancer Prediction Convolutional Neural Network (LCP-CNN), which was trained on US screening data, on an independent dataset of indeterminate nodules in an European multicentre trial, to rule out benign nodules maintaining a high lung cancer sensitivity. METHODS: The LCP-CNN has been trained to generate a malignancy score for each nodule using CT data from the U.S. National Lung Screening Trial (NLST), and validated on CT scans containing 2106 nodules (205 lung cancers) detected in patients from from the Early Lung Cancer Diagnosis Using Artificial Intelligence and Big Data (LUCINDA) study, recruited from three tertiary referral centers in the UK, Germany and Netherlands. We pre-defined a benign nodule rule-out test, to identify benign nodules whilst maintaining a high sensitivity, by calculating thresholds on the malignancy score that achieve at least 99 % sensitivity on the NLST data. Overall performance per validation site was evaluated using Area-Under-the-ROC-Curve analysis (AUC). RESULTS: The overall AUC across the European centers was 94.5 % (95 %CI 92.6-96.1). With a high sensitivity of 99.0 %, malignancy could be ruled out in 22.1 % of the nodules, enabling 18.5 % of the patients to avoid follow-up scans. The two false-negative results both represented small typical carcinoids. CONCLUSION: The LCP-CNN, trained on participants with lung nodules from the US NLST dataset, showed excellent performance on identification of benign lung nodules in a multi-center external dataset, ruling out malignancy with high accuracy in about one fifth of the patients with 5-15 mm nodules.",2021,10.1016/j.lungcan.2021.01.027,diagnosis,True
Lung detection and severity prediction of pneumonia patients based on COVID-19 DET-PRE network,"BACKGROUND: The sudden outbreak of COVID-19 pneumonia has brought a heavy disaster to individuals globally. Facing this new virus, the clinicians have no automatic tools to assess the severity of pneumonia patients. METHODS: In the current work, a COVID-19 DET-PRE network with two pipelines was proposed. Firstly, the lungs in X-rays were detected and segmented through the improved YOLOv3 Dense network to remove redundant features. Then, the VGG16 classifier was pre-trained on the source domain, and the severity of the disease was predicted on the target domain by means of transfer learning. RESULTS: The experiment results demonstrated that the COVID-19 DET-PRE network can effectively detect the lungs from X-rays and accurately predict the severity of the disease. The mean average precisions (mAPs) of lung detection in patients with mild and severe illness were 0.976 and 0.983 respectively. Moreover, the accuracy of severity prediction of COVID-19 pneumonia can reach 86.1%. CONCLUSIONS: The proposed neural network has high accuracy, which is suitable for the clinical diagnosis of COVID-19 pneumonia.",2022,10.1080/17434440.2022.2014319,prognosis,False
Lung Disease Classification in CXR Images Using Hybrid Inception-ResNet-v2 Model and Edge Computing,"Chest X-ray (CXR) imaging is one of the most widely used and economical tests to diagnose a wide range of diseases. However, even for expert radiologists, it is a challenge to accurately diagnose diseases from CXR samples. Furthermore, there remains an acute shortage of trained radiologists worldwide. In the present study, a range of machine learning (ML), deep learning (DL), and transfer learning (TL) approaches have been evaluated to classify diseases in an openly available CXR image dataset. A combination of the synthetic minority over-sampling technique (SMOTE) and weighted class balancing is used to alleviate the effects of class imbalance. A hybrid Inception-ResNet-v2 transfer learning model coupled with data augmentation and image enhancement gives the best accuracy. The model is deployed in an edge environment using Amazon IoT Core to automate the task of disease detection in CXR images with three categories, namely pneumonia, COVID-19, and normal. Comparative analysis has been given in various metrics such as precision, recall, accuracy, AUC-ROC score, etc. The proposed technique gives an average accuracy of 98.66%. The accuracies of other TL models, namely SqueezeNet, VGG19, ResNet50, and MobileNetV2 are 97.33%, 91.66%, 90.33%, and 76.00%, respectively. Further, a DL model, trained from scratch, gives an accuracy of 92.43%. Two feature-based ML classification techniques, namely support vector machine with local binary pattern (SVM + LBP) and decision tree with histogram of oriented gradients (DT + HOG) yield an accuracy of 87.98% and 86.87%, respectively.",2022,10.1155/2022/9036457,diagnosis,False
Lung Lesion Detection in CT Scan Images Using the Fuzzy Local Information Cluster Means (FLICM) Automatic Segmentation Algorithm and Back Propagation Network Classification,"Lung cancer is a frequently lethal disease often causing death of human beings at an early age because of uncontrolled cell growth in the lung tissues. The diagnostic methods available are less than effective for detection of cancer. Therefore an automatic lesion segmentation method with computed tomography (CT) scans has been developed. However it is very difficult to perform automatic identification and segmentation of lung tumours with good accuracy because of the existence of variation in lesions. This paper describes the application of a robust lesion detection and segmentation technique to segment every individual cell from pathological images to extract the essential features. The proposed technique based on the FLICM (Fuzzy Local Information Cluster Means) algorithm used for segmentation, with reduced false positives in detecting lung cancers. The back propagation network used to classify cancer cells is based on computer aided diagnosis (CAD).",2017,10.22034/apjcp.2017.18.12.3395,diagnosis,True
Lung Lesion Localization of COVID-19 From Chest CT Image: A Novel Weakly Supervised Learning Method,"Chest computed tomography (CT) image data is necessary for early diagnosis, treatment, and prognosis of Coronavirus Disease 2019 (COVID-19). Artificial intelligence has been tried to help clinicians in improving the diagnostic accuracy and working efficiency of CT. Whereas, existing supervised approaches on CT image of COVID-19 pneumonia require voxel-based annotations for training, which take a lot of time and effort. This paper proposed a weakly-supervised method for COVID-19 lesion localization based on generative adversarial network (GAN) with image-level labels only. We first introduced a GAN-based framework to generate normal-looking CT slices from CT slices with COVID-19 lesions. We then developed a novel feature match strategy to improve the reality of generated images by guiding the generator to capture the complex texture of chest CT images. Finally, the localization map of lesions can be easily obtained by subtracting the output image from its corresponding input image. By adding a classifier branch to the GAN-based framework to classify localization maps, we can further develop a diagnosis system with improved classification accuracy. Three CT datasets from hospitals of Sao Paulo, Italian Society of Medical and Interventional Radiology, and China Medical University about COVID-19 were collected in this article for evaluation. Our weakly supervised learning method obtained AUC of 0.883, dice coefficient of 0.575, accuracy of 0.884, sensitivity of 0.647, specificity of 0.929, and F1-score of 0.640, which exceeded other widely used weakly supervised object localization methods by a significant margin. We also compared the proposed method with fully supervised learning methods in COVID-19 lesion segmentation task, the proposed weakly supervised method still leads to a competitive result with dice coefficient of 0.575. Furthermore, we also analyzed the association between illness severity and visual score, we found that the common severity cohort had the largest sample size as well as the highest visual score which suggests our method can help rapid diagnosis of COVID-19 patients, especially in massive common severity cohort. In conclusion, we proposed this novel method can serve as an accurate and efficient tool to alleviate the bottleneck of expert annotation cost and advance the progress of computer-aided COVID-19 diagnosis.",2021,10.1109/jbhi.2021.3067465,diagnosis,True
"Lung Nodule Classification Using Biomarkers, Volumetric Radiomics, and 3D CNNs","We present a hybrid algorithm to estimate lung nodule malignancy that combines imaging biomarkers from Radiologist's annotation with image classification of CT scans. Our algorithm employs a 3D Convolutional Neural Network (CNN) as well as a Random Forest in order to combine CT imagery with biomarker annotation and volumetric radiomic features. We analyze and compare the performance of the algorithm using only imagery, only biomarkers, combined imagery + biomarkers, combined imagery + volumetric radiomic features, and finally the combination of imagery + biomarkers + volumetric features in order to classify the suspicion level of nodule malignancy. The National Cancer Institute (NCI) Lung Image Database Consortium (LIDC) IDRI dataset is used to train and evaluate the classification task. We show that the incorporation of semi-supervised learning by means of K-Nearest-Neighbors (KNN) can increase the available training sample size of the LIDC-IDRI, thereby further improving the accuracy of malignancy estimation of most of the models tested although there is no significant improvement with the use of KNN semi-supervised learning if image classification with CNNs and volumetric features is combined with descriptive biomarkers. Unexpectedly, we also show that a model using image biomarkers alone is more accurate than one that combines biomarkers with volumetric radiomics, 3D CNNs, and semi-supervised learning. We discuss the possibility that this result may be influenced by cognitive bias in LIDC-IDRI because malignancy estimates were recorded by the same radiologist panel as biomarkers, as well as future work to incorporate pathology information over a subset of study participants.",2021,10.1007/s10278-020-00417-y,diagnosis,True
Lung nodule classification using deep feature fusion in chest radiography,"Lung nodules are small, round, or oval-shaped masses of tissue in the lung region. Early diagnosis and treatment of lung nodules can significantly improve the quality of patients' lives. Because of their small size and the interlaced nature of chest anatomy, detection of lung nodules using different medical imaging techniques becomes challenging. Recently, several methods for computer aided diagnosis (CAD) were proposed to improve the detection of lung nodules with good performances. However, the current methods are unable to achieve high sensitivity and high specificity. In this paper, we propose using deep feature fusion from the non-medical training and hand-crafted features to reduce the false positive results. Based on our experimentation of the public dataset, our results show that, the deep fusion feature can achieve promising results in terms of sensitivity and specificity (69.3% and 96.2%) at 1.19 false positive per image, which is better than the single hand-crafted features (62% and 95.4%) at 1.45 false positive per image. As it stands, fusion features that were used to classify our candidate nodules have resulted in a more promising outcome as compared to the single features from deep learning features and the hand-crafted features. This will improve the current CAD method based on the use of deep feature fusion to more effectively diagnose the presence of lung nodules.",2017,10.1016/j.compmedimag.2016.11.004,diagnosis,False
Lung nodule classification using deep Local-Global networks,"PURPOSE: Lung nodules have very diverse shapes and sizes, which makes classifying them as benign/malignant a challenging problem. In this paper, we propose a novel method to predict the malignancy of nodules that have the capability to analyze the shape and size of a nodule using a global feature extractor, as well as the density and structure of the nodule using a local feature extractor. METHODS: We propose to use Residual Blocks with a 3 × 3 kernel size for local feature extraction and Non-Local Blocks to extract the global features. The Non-Local Block has the ability to extract global features without using a huge number of parameters. The key idea behind the Non-Local Block is to apply matrix multiplications between features on the same feature maps. RESULTS: We trained and validated the proposed method on the LIDC-IDRI dataset which contains 1018 computed tomography scans. We followed a rigorous procedure for experimental setup, namely tenfold cross-validation, and ignored the nodules that had been annotated by < 3 radiologists. The proposed method achieved state-of-the-art results with AUC = 95.62%, while significantly outperforming other baseline methods. CONCLUSIONS: Our proposed deep Local-Global network has the capability to accurately extract both local and global features. Our new method outperforms state-of-the-art architecture including Densenet and Resnet with transfer learning.",2019,10.1007/s11548-019-01981-7,diagnosis,True
Lung nodule classification with multilevel patch-based context analysis,"In this paper, we propose a novel classification method for the four types of lung nodules, i.e., well-circumscribed, vascularized, juxta-pleural, and pleural-tail, in low dose computed tomography scans. The proposed method is based on contextual analysis by combining the lung nodule and surrounding anatomical structures, and has three main stages: an adaptive patch-based division is used to construct concentric multilevel partition; then, a new feature set is designed to incorporate intensity, texture, and gradient information for image patch feature description, and then a contextual latent semantic analysis-based classifier is designed to calculate the probabilistic estimations for the relevant images. Our proposed method was evaluated on a publicly available dataset and clearly demonstrated promising classification performance.",2014,10.1109/tbme.2013.2295593,diagnosis,True
Lung Nodule Detectability of Artificial Intelligence-assisted CT Image Reading in Lung Cancer Screening,"BACKGROUND: Artificial Intelligence (AI)-based automatic lung nodule detection system improves the detection rate of nodules. It is important to evaluate the clinical value of the AI system by comparing AI-assisted nodule detection with actual radiology reports. OBJECTIVE: To compare the detection rate of lung nodules between the actual radiology reports and AI-assisted reading in lung cancer CT screening. METHODS: Participants in chest CT screening from November to December 2019 were retrospectively included. In the real-world radiologist observation, 14 residents and 15 radiologists participated in finalizing radiology reports. In AI-assisted reading, one resident and one radiologist reevaluated all subjects with the assistance of an AI system to locate and measure the detected lung nodules. A reading panel determined the type and number of detected lung nodules between these two methods. RESULTS: In 860 participants (57±7 years), the reading panel confirmed 250 patients with >1 solid nodule, while radiologists observed 131, lower than 247 by AI-assisted reading (p<0.001). The panel confirmed 111 patients with >1 non-solid nodule, whereas radiologist observation identified 28, lower than 110 by AI-assisted reading (p<0.001). The accuracy and sensitivity of radiologist observation for solid nodules were 86.2% and 52.4%, lower than 99.1% and 98.8% by AI-assisted reading, respectively. These metrics were 90.4% and 25.2% for non-solid nodules, lower than 98.8% and 99.1% by AI-assisted reading, respectively. CONCLUSION: Comparing with the actual radiology reports, AI-assisted reading greatly improves the accuracy and sensitivity of nodule detection in chest CT, which benefits lung nodule detection, especially for non-solid nodules.",2022,10.2174/1573405617666210806125953,diagnosis,True
Lung Nodule Detection based on Ensemble of Hand Crafted and Deep Features,"Lung cancer is considered as a deadliest disease worldwide due to which 1.76 million deaths occurred in the year 2018. Keeping in view its dreadful effect on humans, cancer detection at a premature stage is a more significant requirement to reduce the probability of mortality rate. This manuscript depicts an approach of finding lung nodule at an initial stage that comprises of three major phases: (1) lung nodule segmentation using Otsu threshold followed by morphological operation; (2) extraction of geometrical, texture and deep learning features for selecting optimal features; (3) The optimal features are fused serially for classification of lung nodule into two categories that is malignant and benign. The lung image database consortium image database resource initiative (LIDC-IDRI) is used for experimentation. The experimental outcomes show better performance of presented approach as compared with the existing methods.",2019,10.1007/s10916-019-1455-6,diagnosis,True
Lung Nodule Detection based on Faster R-CNN Framework,"BACKGROUND: Lung cancer is a worldwide high-risk disease, and lung nodules are the main manifestation of early lung cancer. Automatic detection of lung nodules reduces the workload of radiologists, the rate of misdiagnosis and missed diagnosis. For this purpose, we propose a Faster R-CNN algorithm for the detection of these lung nodules. METHOD: Faster R-CNN algorithm can detect lung nodules, and the training set is used to prove the feasibility of this technique. In theory, parameter optimization can improve network structure, as well as detection accuracy. RESULT: Through experiments, the best parameters are that the basic learning rate is 0.001, step size is 70,000, attenuation coefficient is 0.1, the value of Dropout is 0.5, and the value of Batch Size is 64. Compared with other networks for detecting lung nodules, the optimized and improved algorithm proposed in this paper generally improves detection accuracy by more than 20% when compared with the other traditional algorithms. CONCLUSION: Our experimental results have proved that the method of detecting lung nodules based on Faster R-CNN algorithm has good accuracy and therefore, presents potential clinical value in lung disease diagnosis. This method can further assist radiologists, and also for researchers in the design and development of the detection system for lung nodules.",2021,10.1016/j.cmpb.2020.105866,diagnosis,True
Lung nodule detection in chest X-rays using synthetic ground-truth data comparing CNN-based diagnosis to human performance,"We present a method to generate synthetic thorax radiographs with realistic nodules from CT scans, and a perfect ground truth knowledge. We evaluated the detection performance of nine radiologists and two convolutional neural networks in a reader study. Nodules were artificially inserted into the lung of a CT volume and synthetic radiographs were obtained by forward-projecting the volume. Hence, our framework allowed for a detailed evaluation of CAD systems' and radiologists' performance due to the availability of accurate ground-truth labels for nodules from synthetic data. Radiographs for network training (U-Net and RetinaNet) were generated from 855 CT scans of a public dataset. For the reader study, 201 radiographs were generated from 21 nodule-free CT scans with altering nodule positions, sizes and nodule counts of inserted nodules. Average true positive detections by nine radiologists were 248.8 nodules, 51.7 false positive predicted nodules and 121.2 false negative predicted nodules. The best performing CAD system achieved 268 true positives, 66 false positives and 102 false negatives. Corresponding weighted alternative free response operating characteristic figure-of-merits (wAFROC FOM) for the radiologists range from 0.54 to 0.87 compared to a value of 0.81 (CI 0.75-0.87) for the best performing CNN. The CNN did not perform significantly better against the combined average of the 9 readers (p = 0.49). Paramediastinal nodules accounted for most false positive and false negative detections by readers, which can be explained by the presence of more tissue in this area.",2021,10.1038/s41598-021-94750-z,diagnosis,False
Lung Nodule Detection in CT Images Using a Raw Patch-Based Convolutional Neural Network,"Remarkable progress has been made in image classification and segmentation, due to the recent study of deep convolutional neural networks (CNNs). To solve the similar problem of diagnostic lung nodule detection in low-dose computed tomography (CT) scans, we propose a new Computer-Aided Detection (CAD) system using CNNs and CT image segmentation techniques. Unlike former studies focusing on the classification of malignant nodule types or relying on prior image processing, in this work, we put raw CT image patches directly in CNNs to reduce the complexity of the system. Specifically, we split each CT image into several patches, which are divided into 6 types consisting of 3 nodule types and 3 non-nodule types. We compare the performance of ResNet with different CNNs architectures on CT images from a publicly available dataset named the Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI). Results show that our best model reaches a high detection sensitivity of 92.8% with 8 false positives per scan (FPs/scan). Compared with related work, our work obtains a state-of-the-art effect.",2019,10.1007/s10278-019-00221-3,diagnosis,True
Lung Nodule Detection using Convolutional Neural Networks with Transfer Learning on CT Images,"AIM AND OBJECTIVE: Lung nodule detection is critical in improving the five-year survival rate and reducing mortality for patients with lung cancer. Numerous methods based on Convolutional Neural Networks (CNNs) have been proposed for lung nodule detection in Computed Tomography (CT) images. With the collaborative development of computer hardware technology, the detection accuracy and efficiency can still be improved. MATERIALS AND METHODS: In this study, an automatic lung nodule detection method using CNNs with transfer learning is presented. We first compared three of the state-of-the-art convolutional neural network (CNN) models, namely, VGG16, VGG19 and ResNet50, to determine the most suitable model for lung nodule detection. We then utilized two different training strategies, namely, freezing layers and fine-tuning, to illustrate the effectiveness of transfer learning. Furthermore, the hyper-parameters of the CNN model such as optimizer, batch size and epoch were optimized. RESULTS: Evaluated on the Lung Nodule Analysis 2016 (LUNA16) challenge, promising results with an accuracy of 96.86%, a precision of 91.10%, a sensitivity of 90.78%, a specificity of 98.13%, and an AUC of 99.37% were achieved. CONCLUSION: Compared with other works, state-of-the-art specificity is obtained, which demonstrates that the proposed method is effective and applicable to lung nodule detection.",2021,10.2174/1386207323666200714002459,diagnosis,True
Lung nodule segmentation using Salp Shuffled Shepherd Optimization Algorithm-based Generative Adversarial Network,"Lung nodule segmentation is an exciting area of research for the effective detection of lung cancer. One of the significant challenges in detecting lung cancer is Accuracy, which is affected due to the visual deviations and heterogeneity in the lung nodules. Hence, to improve the segmentation process's Accuracy, a Salp Shuffled Shepherd Optimization Algorithm-based Generative Adversarial Network (SSSOA-based GAN) model is developed in this research for lung nodule segmentation. The SSSOA is the hybrid optimization algorithm developed by integrating the Salp Swarm Algorithm (SSA) and shuffled shepherd optimization algorithm (SSOA). The artefacts in the input Computed Tomography (CT) image are removed by performing pre-processing with the help of a Gaussian filter. The pre-processed image is subjected to lung lobe segmentation, which is done with the help of deep joint segmentation for segmenting the appropriate regions. The lung nodule segmentation is performed using the GAN. The GAN is trained using the SSSOA to effectively segment the lung nodule from the lung lobe image. The metrics, such as Dice Coefficient, Accuracy, and Jaccard Similarity, are used to evaluate the performance. The developed SSSOA-based GAN method obtained a maximum Accuracy of 0.9387, a maximum Dice Coefficient of 0.7986, and a maximum Jaccard Similarity of 0.8026, respectively, compared with the existing lung nodule segmentation method.",2021,10.1016/j.compbiomed.2021.104811,diagnosis,True
Lung tumor segmentation in 4D CT images using motion convolutional neural networks,"PURPOSE: Manual delineation on all breathing phases of lung cancer 4D CT image datasets can be challenging, exhaustive, and prone to subjective errors because of both the large number of images in the datasets and variations in the spatial location of tumors secondary to respiratory motion. The purpose of this work is to present a new deep learning-based framework for fast and accurate segmentation of lung tumors on 4D CT image sets. METHODS: The proposed DL framework leverages motion region convolutional neural network (R-CNN). Through integration of global and local motion estimation network architectures, the network can learn both major and minor changes caused by tumor motion. Our network design first extracts tumor motion information by feeding 4D CT images with consecutive phases into an integrated backbone network architecture, locating volume-of-interest (VOIs) via a regional proposal network and removing irrelevant information via a regional convolutional neural network. Extracted motion information is then advanced into the subsequent global and local motion head network architecture to predict corresponding deformation vector fields (DVFs) and further adjust tumor VOIs. Binary masks of tumors are then segmented within adjusted VOIs via a mask head. A self-attention strategy is incorporated in the mask head network to remove any noisy features that might impact segmentation performance. We performed two sets of experiments. In the first experiment, a five-fold cross-validation on 20 4D CT datasets, each consisting of 10 breathing phases (i.e., 200 3D image volumes in total). The network performance was also evaluated on an additional unseen 200 3D images volumes from 20 hold-out 4D CT datasets. In the second experiment, we trained another model with 40 patients' 4D CT datasets from experiment 1 and evaluated on additional unseen nine patients' 4D CT datasets. The Dice similarity coefficient (DSC), center of mass distance (CMD), 95th percentile Hausdorff distance (HD(95) ), mean surface distance (MSD), and volume difference (VD) between the manual and segmented tumor contour were computed to evaluate tumor detection and segmentation accuracy. The performance of our method was quantitatively evaluated against four different methods (VoxelMorph, U-Net, network without global and local networks, and network without attention gate strategy) across all evaluation metrics through a paired t-test. RESULTS: The proposed fully automated DL method yielded good overall agreement with the ground truth for contoured tumor volume and segmentation accuracy. Our model yielded significantly better values of evaluation metrics (p < 0.05) than all four competing methods in both experiments. On hold-out datasets of experiment 1 and 2, our method yielded DSC of 0.86 and 0.90 compared to 0.82 and 0.87, 0.75 and 0.83, 081 and 0.89, and 0.81 and 0.89 yielded by VoxelMorph, U-Net, network without global and local networks, and networks without attention gate strategy. Tumor VD between ground truth and our method was the smallest with the value of 0.50 compared to 0.99, 1.01, 0.92, and 0.93 for between ground truth and VoxelMorph, U-Net, network without global and local networks, and networks without attention gate strategy, respectively. CONCLUSIONS: Our proposed DL framework of tumor segmentation on lung cancer 4D CT datasets demonstrates a significant promise for fully automated delineation. The promising results of this work provide impetus for its integration into the 4D CT treatment planning workflow to improve the accuracy and efficiency of lung radiotherapy.",2021,10.1002/mp.15204,diagnosis,True
LungNet: A hybrid deep-CNN model for lung cancer diagnosis using CT and wearable sensor-based medical IoT data,"Lung cancer, also known as pulmonary cancer, is one of the deadliest cancers, but yet curable if detected at the early stage. At present, the ambiguous features of the lung cancer nodule make the computer-aided automatic diagnosis a challenging task. To alleviate this, we present LungNet, a novel hybrid deep-convolutional neural network-based model, trained with CT scan and wearable sensor-based medical IoT (MIoT) data. LungNet consists of a unique 22-layers Convolutional Neural Network (CNN), which combines latent features that are learned from CT scan images and MIoT data to enhance the diagnostic accuracy of the system. Operated from a centralized server, the network has been trained with a balanced dataset having 525,000 images that can classify lung cancer into five classes with high accuracy (96.81%) and low false positive rate (3.35%), outperforming similar CNN-based classifiers. Moreover, it classifies the stage-1 and stage-2 lung cancers into 1A, 1B, 2A and 2B sub-classes with 91.6% accuracy and false positive rate of 7.25%. High predictive capability accompanied with sub-stage classification renders LungNet as a promising prospect in developing CNN-based automatic lung cancer diagnosis systems.",2021,10.1016/j.compbiomed.2021.104961,diagnosis,True
Lungs nodule detection framework from computed tomography images using support vector machine,"The emergence of cloud infrastructure has the potential to provide significant benefits in a variety of areas in the medical imaging field. The driving force behind the extensive use of cloud infrastructure for medical image processing is the exponential increase in the size of computed tomography (CT) and magnetic resonance imaging (MRI) data. The size of a single CT/MRI image has increased manifold since the inception of these imagery techniques. This demand for the introduction of effective and efficient frameworks for extracting relevant and most suitable information (features) from these sizeable images. As early detection of lungs cancer can significantly increase the chances of survival of a lung scanner patient, an effective and efficient nodule detection system can play a vital role. In this article, we have proposed a novel classification framework for lungs nodule classification with less false positive rates (FPRs), high accuracy, sensitivity rate, less computationally expensive and uses a small set of features while preserving edge and texture information. The proposed framework comprises multiple phases that include image contrast enhancement, segmentation, feature extraction, followed by an employment of these features for training and testing of a selected classifier. Image preprocessing and feature selection being the primary steps-playing their vital role in achieving improved classification accuracy. We have empirically tested the efficacy of our technique by utilizing the well-known Lungs Image Consortium Database dataset. The results prove that the technique is highly effective for reducing FPRs with an impressive sensitivity rate of 97.45%.",2019,10.1002/jemt.23275,diagnosis,True
M (3)Lung-Sys: A Deep Learning System for Multi-Class Lung Pneumonia Screening From CT Imaging,"To counter the outbreak of COVID-19, the accurate diagnosis of suspected cases plays a crucial role in timely quarantine, medical treatment, and preventing the spread of the pandemic. Considering the limited training cases and resources (e.g, time and budget), we propose a Multi-task Multi-slice Deep Learning System (M (3)Lung-Sys) for multi-class lung pneumonia screening from CT imaging, which only consists of two 2D CNN networks, i.e., slice- and patient-level classification networks. The former aims to seek the feature representations from abundant CT slices instead of limited CT volumes, and for the overall pneumonia screening, the latter one could recover the temporal information by feature refinement and aggregation between different slices. In addition to distinguish COVID-19 from Healthy, H1N1, and CAP cases, our M (3)Lung-Sys also be able to locate the areas of relevant lesions, without any pixel-level annotation. To further demonstrate the effectiveness of our model, we conduct extensive experiments on a chest CT imaging dataset with a total of 734 patients (251 healthy people, 245 COVID-19 patients, 105 H1N1 patients, and 133 CAP patients). The quantitative results with plenty of metrics indicate the superiority of our proposed model on both slice- and patient-level classification tasks. More importantly, the generated lesion location maps make our system interpretable and more valuable to clinicians.",2020,10.1109/jbhi.2020.3030853,diagnosis,True
Machine Learning Algorithms Utilizing Functional Respiratory Imaging May Predict COPD Exacerbations,"RATIONALE AND OBJECTIVES: Acute chronic obstructive pulmonary disease exacerbations (AECOPD) have a significant negative impact on the quality of life and accelerate progression of the disease. Functional respiratory imaging (FRI) has the potential to better characterize this disease. The purpose of this study was to identify FRI parameters specific to AECOPD and assess their ability to predict future AECOPD, by use of machine learning algorithms, enabling a better understanding and quantification of disease manifestation and progression. MATERIALS AND METHODS: A multicenter cohort of 62 patients with COPD was analyzed. FRI obtained from baseline high resolution CT data (unenhanced and volume gated), clinical, and pulmonary function test were analyzed and incorporated into machine learning algorithms. RESULTS: A total of 11 baseline FRI parameters could significantly distinguish ( p < 0.05) the development of AECOPD from a stable period. In contrast, no baseline clinical or pulmonary function test parameters allowed significant classification. Furthermore, using Support Vector Machines, an accuracy of 80.65% and positive predictive value of 82.35% could be obtained by combining baseline FRI features such as total specific image-based airway volume and total specific image-based airway resistance, measured at functional residual capacity. Patients who developed an AECOPD, showed significantly smaller airway volumes and (hence) significantly higher airway resistances at baseline. CONCLUSION: This study indicates that FRI is a sensitive tool (PPV 82.35%) for predicting future AECOPD on a patient specific level in contrast to classical clinical parameters.",2019,10.1016/j.acra.2018.10.022,prognosis,True
"Machine learning applied on chest x-ray can aid in the diagnosis of COVID-19: a first experience from Lombardy, Italy","BACKGROUND: We aimed to train and test a deep learning classifier to support the diagnosis of coronavirus disease 2019 (COVID-19) using chest x-ray (CXR) on a cohort of subjects from two hospitals in Lombardy, Italy. METHODS: We used for training and validation an ensemble of ten convolutional neural networks (CNNs) with mainly bedside CXRs of 250 COVID-19 and 250 non-COVID-19 subjects from two hospitals (Centres 1 and 2). We then tested such system on bedside CXRs of an independent group of 110 patients (74 COVID-19, 36 non-COVID-19) from one of the two hospitals. A retrospective reading was performed by two radiologists in the absence of any clinical information, with the aim to differentiate COVID-19 from non-COVID-19 patients. Real-time polymerase chain reaction served as the reference standard. RESULTS: At 10-fold cross-validation, our deep learning model classified COVID-19 and non-COVID-19 patients with 0.78 sensitivity (95% confidence interval [CI] 0.74-0.81), 0.82 specificity (95% CI 0.78-0.85), and 0.89 area under the curve (AUC) (95% CI 0.86-0.91). For the independent dataset, deep learning showed 0.80 sensitivity (95% CI 0.72-0.86) (59/74), 0.81 specificity (29/36) (95% CI 0.73-0.87), and 0.81 AUC (95% CI 0.73-0.87). Radiologists' reading obtained 0.63 sensitivity (95% CI 0.52-0.74) and 0.78 specificity (95% CI 0.61-0.90) in Centre 1 and 0.64 sensitivity (95% CI 0.52-0.74) and 0.86 specificity (95% CI 0.71-0.95) in Centre 2. CONCLUSIONS: This preliminary experience based on ten CNNs trained on a limited training dataset shows an interesting potential of deep learning for COVID-19 diagnosis. Such tool is in training with new CXRs to further increase its performance.",2021,10.1186/s41747-020-00203-z,diagnosis,False
Machine learning applied to near-infrared spectra for clinical pleural effusion classification,"Lung cancer patients with malignant pleural effusions (MPE) have a particular poor prognosis. It is crucial to distinguish MPE from benign pleural effusion (BPE). The present study aims to develop a rapid, convenient and economical diagnostic method based on FTIR near-infrared spectroscopy (NIRS) combined with machine learning strategy for clinical pleural effusion classification. NIRS spectra were recorded for 47 MPE samples and 35 BPE samples. The sample data were randomly divided into train set (n = 62) and test set (n = 20). Partial least squares, random forest, support vector machine (SVM), and gradient boosting machine models were trained, and subsequent predictive performance were predicted on the test set. Besides the whole spectra used in modeling, selected features using SVM recursive feature elimination algorithm were also investigated in modeling. Among those models, NIRS combined with SVM showed the best predictive performance (accuracy: 1.0, kappa: 1.0, and AUC(ROC): 1.0). SVM with the top 50 feature wavenumbers also displayed a high predictive performance (accuracy: 0.95, kappa: 0.89, AUC(ROC): 0.99). Our study revealed that the combination of NIRS and machine learning is an innovative, rapid, and convenient method for clinical pleural effusion classification, and worth further evaluation.",2021,10.1038/s41598-021-87736-4,diagnosis,True
Machine learning approach for distinguishing malignant and benign lung nodules utilizing standardized perinodular parenchymal features from CT,"PURPOSE: Computed tomography (CT) is an effective method for detecting and characterizing lung nodules in vivo. With the growing use of chest CT, the detection frequency of lung nodules is increasing. Noninvasive methods to distinguish malignant from benign nodules have the potential to decrease the clinical burden, risk, and cost involved in follow-up procedures on the large number of false-positive lesions detected. This study examined the benefit of including perinodular parenchymal features in machine learning (ML) tools for pulmonary nodule assessment. METHODS: Lung nodule cases with pathology confirmed diagnosis (74 malignant, 289 benign) were used to extract quantitative imaging characteristics from computed tomography scans of the nodule and perinodular parenchyma tissue. A ML tool development pipeline was employed using k-medoids clustering and information theory to determine efficient predictor sets for different amounts of parenchyma inclusion and build an artificial neural network classifier. The resulting ML tool was validated using an independent cohort (50 malignant, 50 benign). RESULTS: The inclusion of parenchymal imaging features improved the performance of the ML tool over exclusively nodular features (P < 0.01). The best performing ML tool included features derived from nodule diameter-based surrounding parenchyma tissue quartile bands. We demonstrate similar high-performance values on the independent validation cohort (AUC-ROC = 0.965). A comparison using the independent validation cohort with the Fleischner pulmonary nodule follow-up guidelines demonstrated a theoretical reduction in recommended follow-up imaging and procedures. CONCLUSIONS: Radiomic features extracted from the parenchyma surrounding lung nodules contain valid signals with spatial relevance for the task of lung cancer risk classification. Through standardization of feature extraction regions from the parenchyma, ML tool validation performance of 100% sensitivity and 96% specificity was achieved.",2019,10.1002/mp.13592,diagnosis,True
Machine learning automatically detects COVID-19 using chest CTs in a large multicenter cohort,"OBJECTIVES: To investigate machine learning classifiers and interpretable models using chest CT for detection of COVID-19 and differentiation from other pneumonias, interstitial lung disease (ILD) and normal CTs. METHODS: Our retrospective multi-institutional study obtained 2446 chest CTs from 16 institutions (including 1161 COVID-19 patients). Training/validation/testing cohorts included 1011/50/100 COVID-19, 388/16/33 ILD, 189/16/33 other pneumonias, and 559/17/34 normal (no pathologies) CTs. A metric-based approach for the classification of COVID-19 used interpretable features, relying on logistic regression and random forests. A deep learning-based classifier differentiated COVID-19 via 3D features extracted directly from CT attenuation and probability distribution of airspace opacities. RESULTS: Most discriminative features of COVID-19 are the percentage of airspace opacity and peripheral and basal predominant opacities, concordant with the typical characterization of COVID-19 in the literature. Unsupervised hierarchical clustering compares feature distribution across COVID-19 and control cohorts. The metrics-based classifier achieved AUC = 0.83, sensitivity = 0.74, and specificity = 0.79 versus respectively 0.93, 0.90, and 0.83 for the DL-based classifier. Most of ambiguity comes from non-COVID-19 pneumonia with manifestations that overlap with COVID-19, as well as mild COVID-19 cases. Non-COVID-19 classification performance is 91% for ILD, 64% for other pneumonias, and 94% for no pathologies, which demonstrates the robustness of our method against different compositions of control groups. CONCLUSIONS: Our new method accurately discriminates COVID-19 from other types of pneumonia, ILD, and CTs with no pathologies, using quantitative imaging features derived from chest CT, while balancing interpretability of results and classification performance and, therefore, may be useful to facilitate diagnosis of COVID-19. KEY POINTS: • Unsupervised clustering reveals the key tomographic features including percent airspace opacity and peripheral and basal opacities most typical of COVID-19 relative to control groups. • COVID-19-positive CTs were compared with COVID-19-negative chest CTs (including a balanced distribution of non-COVID-19 pneumonia, ILD, and no pathologies). Classification accuracies for COVID-19, pneumonia, ILD, and CT scans with no pathologies are respectively 90%, 64%, 91%, and 94%. • Our deep learning (DL)-based classification method demonstrates an AUC of 0.93 (sensitivity 90%, specificity 83%). Machine learning methods applied to quantitative chest CT metrics can therefore improve diagnostic accuracy in suspected COVID-19, particularly in resource-constrained environments.",2021,10.1007/s00330-021-07937-3,diagnosis,True
Machine learning based on clinical characteristics and chest CT quantitative measurements for prediction of adverse clinical outcomes in hospitalized patients with COVID-19,"OBJECTIVES: To develop and validate a machine learning model for the prediction of adverse outcomes in hospitalized patients with COVID-19. METHODS: We included 424 patients with non-severe COVID-19 on admission from January 17, 2020, to February 17, 2020, in the primary cohort of this retrospective multicenter study. The extent of lung involvement was quantified on chest CT images by a deep learning-based framework. The composite endpoint was the occurrence of severe or critical COVID-19 or death during hospitalization. The optimal machine learning classifier and feature subset were selected for model construction. The performance was further tested in an external validation cohort consisting of 98 patients. RESULTS: There was no significant difference in the prevalence of adverse outcomes (8.7% vs. 8.2%, p = 0.858) between the primary and validation cohorts. The machine learning method extreme gradient boosting (XGBoost) and optimal feature subset including lactic dehydrogenase (LDH), presence of comorbidity, CT lesion ratio (lesion%), and hypersensitive cardiac troponin I (hs-cTnI) were selected for model construction. The XGBoost classifier based on the optimal feature subset performed well for the prediction of developing adverse outcomes in the primary and validation cohorts, with AUCs of 0.959 (95% confidence interval [CI]: 0.936-0.976) and 0.953 (95% CI: 0.891-0.986), respectively. Furthermore, the XGBoost classifier also showed clinical usefulness. CONCLUSIONS: We presented a machine learning model that could be effectively used as a predictor of adverse outcomes in hospitalized patients with COVID-19, opening up the possibility for patient stratification and treatment allocation. KEY POINTS: • Developing an individually prognostic model for COVID-19 has the potential to allow efficient allocation of medical resources. • We proposed a deep learning-based framework for accurate lung involvement quantification on chest CT images. • Machine learning based on clinical and CT variables can facilitate the prediction of adverse outcomes of COVID-19.",2021,10.1007/s00330-021-07957-z,prognosis,True
Machine learning based on clinico-biological features integrated (18)F-FDG PET/CT radiomics for distinguishing squamous cell carcinoma from adenocarcinoma of lung,"PURPOSE: To develop and validate a clinico-biological features and (18)F-fluorodeoxyglucose (FDG) positron emission tomography/computed tomography (PET/CT) radiomic-based nomogram via machine learning for the pretherapy prediction of discriminating between adenocarcinoma (ADC) and squamous cell carcinoma (SCC) in non-small cell lung cancer (NSCLC). METHODS: A total of 315 NSCLC patients confirmed by postoperative pathology between January 2017 and June 2019 were retrospectively analyzed and randomly divided into the training (n = 220) and validation (n = 95) sets. Preoperative clinical factors, serum tumor markers, and PET, and CT radiomic features were analyzed. Prediction models were developed using the least absolute shrinkage and selection operator (LASSO) regression analysis. The performance of the models was evaluated and compared by the area under receiver-operator characteristic (ROC) curve (AUC) and DeLong test. The clinical utility of the models was determined via decision curve analysis (DCA). Then, a nomogram was developed based on the model with the best predictive efficiency and clinical utility and was validated using the calibration plots. RESULTS: In total, 122 SCC and 193 ADC patients were enrolled in this study. Four independent prediction models were separately developed to differentiate SCC from ADC using clinical factors-tumor markers, PET radiomics, CT radiomics, and their combination. The DeLong test and DCA showed that the Combined Model, consisting of 2 clinical factors, 2 tumor markers, 7 PET radiomics, and 3 CT radiomic parameters, held the highest predictive efficiency and clinical utility in predicting the NSCLC subtypes compared with the use of these parameters alone in both the training and validation sets (AUCs (95% CIs) = 0.932 (0.900-0.964), 0.901 (0.840-0.957), respectively) (p < 0.05). A quantitative nomogram was subsequently constructed using the independently risk factors from the Combined Model. The calibration curves indicated a good consistency between the actual observations and nomogram predictions. CONCLUSION: This study presents an integrated clinico-biologico-radiological nomogram that can be accurately and noninvasively used for the individualized differentiation SCC from ADC in NSCLC, thereby assisting in clinical decision making for precision treatment.",2021,10.1007/s00259-020-05065-6,diagnosis,True
Machine Learning methods for Quantitative Radiomic Biomarkers,"Radiomics extracts and mines large number of medical imaging features quantifying tumor phenotypic characteristics. Highly accurate and reliable machine-learning approaches can drive the success of radiomic applications in clinical care. In this radiomic study, fourteen feature selection methods and twelve classification methods were examined in terms of their performance and stability for predicting overall survival. A total of 440 radiomic features were extracted from pre-treatment computed tomography (CT) images of 464 lung cancer patients. To ensure the unbiased evaluation of different machine-learning methods, publicly available implementations along with reported parameter configurations were used. Furthermore, we used two independent radiomic cohorts for training (n = 310 patients) and validation (n = 154 patients). We identified that Wilcoxon test based feature selection method WLCX (stability = 0.84 ± 0.05, AUC = 0.65 ± 0.02) and a classification method random forest RF (RSD = 3.52%, AUC = 0.66 ± 0.03) had highest prognostic performance with high stability against data perturbation. Our variability analysis indicated that the choice of classification method is the most dominant source of performance variation (34.21% of total variance). Identification of optimal machine-learning methods for radiomic applications is a crucial step towards stable and clinically relevant radiomic biomarkers, providing a non-invasive way of quantifying and monitoring tumor-phenotypic characteristics in clinical practice.",2015,10.1038/srep13087,diagnosis,True
Machine learning predictive model for severe COVID-19,"To develop a modified predictive model for severe COVID-19 in people infected with Sars-Cov-2. We developed the predictive model for severe patients of COVID-19 based on the clinical date from the Tumor Center of Union Hospital affiliated with Tongji Medical College, China. A total of 151 cases from Jan. 26 to Mar. 20, 2020, were included. Then we followed 5 steps to predict and evaluate the model: data preprocessing, data splitting, feature selection, model building, prevention of overfitting, and Evaluation, and combined with artificial neural network algorithms. We processed the results in the 5 steps. In feature selection, ALB showed a strong negative correlation (r = 0.771, P < 0.001) whereas GLB (r = 0.661, P < 0.001) and BUN (r = 0.714, P < 0.001) showed a strong positive correlation with severity of COVID-19. TensorFlow was subsequently applied to develop a neural network model. The model achieved good prediction performance, with an area under the curve value of 0.953(0.889-0.982). Our results showed its outstanding performance in prediction. GLB and BUN may be two risk factors for severe COVID-19. Our findings could be of great benefit in the future treatment of patients with COVID-19 and will help to improve the quality of care in the long term. This model has great significance to rationalize early clinical interventions and improve the cure rate.",2021,10.1016/j.meegid.2021.104737,prognosis,False
Machine Learning Radiomics Model for Early Identification of Small-Cell Lung Cancer on Computed Tomography Scans,"PURPOSE: Small-cell lung cancer (SCLC) is the deadliest form of lung cancer, partly because of its short doubling time. Delays in imaging identification and diagnosis of nodules create a risk for stage migration. The purpose of our study was to determine if a machine learning radiomics model can detect SCLC on computed tomography (CT) among all nodules at least 1 cm in size. MATERIALS AND METHODS: Computed tomography scans from a single institution were selected and resampled to 1 × 1 × 1 mm. Studies were divided into SCLC and other scans comprising benign, adenocarcinoma, and squamous cell carcinoma that were segregated into group A (noncontrast scans) and group B (contrast-enhanced scans). Four machine learning classification models, support vector classifier, random forest (RF), XGBoost, and logistic regression, were used to generate radiomic models using 59 quantitative first-order and texture Imaging Biomarker Standardization Initiative compliant PyRadiomics features, which were found to be robust between two segmenters with minimum Redundancy Maximum Relevance feature selection within each leave-one-out-cross-validation to avoid overfitting. The performance was evaluated using a receiver operating characteristic curve. A final model was created using the RF classifier and aggregate minimum Redundancy Maximum Relevance to determine feature importance. RESULTS: A total of 103 studies were included in the analysis. The area under the receiver operating characteristic curve for RF, support vector classifier, XGBoost, and logistic regression was 0.81, 0.77, 0.84, and 0.84 in group A, and 0.88, 0.87, 0.85, and 0.81 in group B, respectively. Nine radiomic features in group A and 14 radiomic features in group B were predictive of SCLC. Six radiomic features overlapped between groups A and B. CONCLUSION: A machine learning radiomics model may help differentiate SCLC from other lung lesions.",2021,10.1200/cci.21.00021,diagnosis,True
Machine learning to distinguish lymphangioleiomyomatosis from other diffuse cystic lung diseases,"Patients with lymphangioleiomyomatosis (LAM) frequently experience delays in diagnosis, owing partly to the delayed characterization of imaging findings. This project aimed to develop a machine learning model to distinguish LAM from other diffuse cystic lung diseases (DCLDs). Computed tomography scans from patients with confirmed DCLDs were acquired from registry datasets and a recurrent convolutional neural network was trained for their classification. The final model provided sensitivity and specificity of 85% and 92%, respectively, for LAM, similar to the historical metrics of 88% and 97%, respectively, by experts. The proof-of-concept work holds promise as a clinically useful tool to assist in recognizing LAM.",2022,10.1016/j.resinv.2022.01.001,diagnosis,True
Machine learning-based CT radiomics model distinguishes COVID-19 from non-COVID-19 pneumonia,"BACKGROUND: To develop a machine learning-based CT radiomics model is critical for the accurate diagnosis of the rapid spreading coronavirus disease 2019 (COVID-19). METHODS: In this retrospective study, a total of 326 chest CT exams from 134 patients (63 confirmed COVID-19 patients and 71 non-COVID-19 patients) were collected from January 20 to February 8, 2020. A semi-automatic segmentation procedure was used to delineate the volume of interest (VOI), and radiomic features were extracted. The Support Vector Machine (SVM) model was built on the combination of 4 groups of features, including radiomic features, traditional radiological features, quantifying features, and clinical features. By repeating cross-validation procedure, the performance on the time-independent testing cohort was evaluated by the area under the receiver operating characteristic curve (AUC), accuracy, sensitivity, and specificity. RESULTS: For the SVM model built on the combination of 4 groups of features (integrated model), the per-exam AUC was 0.925 (95% CI 0.856 to 0.994) for differentiating COVID-19 on the testing cohort, and the sensitivity and specificity were 0.816 (95% CI 0.651 to 0.917) and 0.923 (95% CI 0.621 to 0.996), respectively. As for the SVM models built on radiomic features, radiological features, quantifying features, and clinical features, individually, the AUC on the testing cohort reached 0.765, 0.818, 0.607, and 0.739, respectively, significantly lower than the integrated model, except for the radiomic model. CONCLUSION: The machine learning-based CT radiomics models may accurately classify COVID-19, helping clinicians and radiologists to identify COVID-19 positive cases.",2021,10.1186/s12879-021-06614-6,diagnosis,True
Machine learning-based diagnostic method of pre-therapeutic (18)F-FDG PET/CT for evaluating mediastinal lymph nodes in non-small cell lung cancer,"OBJECTIVES: We aimed to find the best machine learning (ML) model using (18)F-fluorodeoxyglucose (FDG) positron emission tomography/computed tomography (PET/CT) for evaluating metastatic mediastinal lymph nodes (MedLNs) in non-small cell lung cancer, and compare the diagnostic results with those of nuclear medicine physicians. METHODS: A total of 1329 MedLNs were reviewed. Boosted decision tree, logistic regression, support vector machine, neural network, and decision forest models were compared. The diagnostic performance of the best ML model was compared with that of physicians. The ML method was divided into ML with quantitative variables only (MLq) and adding clinical information (MLc). We performed an analysis based on the (18)F-FDG-avidity of the MedLNs. RESULTS: The boosted decision tree model obtained higher sensitivity and negative predictive values but lower specificity and positive predictive values than the physicians. There was no significant difference between the accuracy of the physicians and MLq (79.8% vs. 76.8%, p = 0.067). The accuracy of MLc was significantly higher than that of the physicians (81.0% vs. 76.8%, p = 0.009). In MedLNs with low (18)F-FDG-avidity, ML had significantly higher accuracy than the physicians (70.0% vs. 63.3%, p = 0.018). CONCLUSION: Although there was no significant difference in accuracy between the MLq and physicians, the diagnostic performance of MLc was better than that of MLq or of the physicians. The ML method appeared to be useful for evaluating low metabolic MedLNs. Therefore, adding clinical information to the quantitative variables from (18)F-FDG PET/CT can improve the diagnostic results of ML. KEY POINTS: • Machine learning using two-class boosted decision tree model revealed the highest value of area under curve, and it showed higher sensitivity and negative predictive values but lower specificity and positive predictive values than nuclear medicine physicians. • The diagnostic results from machine learning method after adding clinical information to the quantitative variables improved accuracy significantly than nuclear medicine physicians. • Machine learning could improve the diagnostic significance of metastatic mediastinal lymph nodes, especially in mediastinal lymph nodes with low 18F-FDG-avidity.",2021,10.1007/s00330-020-07523-z,diagnosis,True
Machine Learning-Based Prediction of COVID-19 Severity and Progression to Critical Illness Using CT Imaging and Clinical Data,"OBJECTIVE: To develop a machine learning (ML) pipeline based on radiomics to predict Coronavirus Disease 2019 (COVID-19) severity and the future deterioration to critical illness using CT and clinical variables. MATERIALS AND METHODS: Clinical data were collected from 981 patients from a multi-institutional international cohort with real-time polymerase chain reaction-confirmed COVID-19. Radiomics features were extracted from chest CT of the patients. The data of the cohort were randomly divided into training, validation, and test sets using a 7:1:2 ratio. A ML pipeline consisting of a model to predict severity and time-to-event model to predict progression to critical illness were trained on radiomics features and clinical variables. The receiver operating characteristic area under the curve (ROC-AUC), concordance index (C-index), and time-dependent ROC-AUC were calculated to determine model performance, which was compared with consensus CT severity scores obtained by visual interpretation by radiologists. RESULTS: Among 981 patients with confirmed COVID-19, 274 patients developed critical illness. Radiomics features and clinical variables resulted in the best performance for the prediction of disease severity with a highest test ROC-AUC of 0.76 compared with 0.70 (0.76 vs. 0.70, p = 0.023) for visual CT severity score and clinical variables. The progression prediction model achieved a test C-index of 0.868 when it was based on the combination of CT radiomics and clinical variables compared with 0.767 when based on CT radiomics features alone (p < 0.001), 0.847 when based on clinical variables alone (p = 0.110), and 0.860 when based on the combination of visual CT severity scores and clinical variables (p = 0.549). Furthermore, the model based on the combination of CT radiomics and clinical variables achieved time-dependent ROC-AUCs of 0.897, 0.933, and 0.927 for the prediction of progression risks at 3, 5 and 7 days, respectively. CONCLUSION: CT radiomics features combined with clinical variables were predictive of COVID-19 severity and progression to critical illness with fairly high accuracy.",2021,10.3348/kjr.2020.1104,prognosis,True
Machine learning-based prognostic modeling using clinical data and quantitative radiomic features from chest CT images in COVID-19 patients,"OBJECTIVE: To develop prognostic models for survival (alive or deceased status) prediction of COVID-19 patients using clinical data (demographics and history, laboratory tests, visual scoring by radiologists) and lung/lesion radiomic features extracted from chest CT images. METHODS: Overall, 152 patients were enrolled in this study protocol. These were divided into 106 training/validation and 46 test datasets (untouched during training), respectively. Radiomic features were extracted from the segmented lungs and infectious lesions separately from chest CT images. Clinical data, including patients' history and demographics, laboratory tests and radiological scores were also collected. Univariate analysis was first performed (q-value reported after false discovery rate (FDR) correction) to determine the most predictive features among all imaging and clinical data. Prognostic modeling of survival was performed using radiomic features and clinical data, separately or in combination. Maximum relevance minimum redundancy (MRMR) and XGBoost were used for feature selection and classification. The receiver operating characteristic (ROC) curve and the area under the ROC curve (AUC), sensitivity, specificity, and accuracy were used to assess the prognostic performance of the models on the test datasets. RESULTS: For clinical data, cancer comorbidity (q-value < 0.01), consciousness level (q-value < 0.05) and radiological score involved zone (q-value < 0.02) were found to have high correlated features with outcome. Oxygen saturation (AUC = 0.73, q-value < 0.01) and Blood Urea Nitrogen (AUC = 0.72, q-value = 0.72) were identified as high clinical features. For lung radiomic features, SAHGLE (AUC = 0.70) and HGLZE (AUC = 0.67) from GLSZM were identified as most prognostic features. Amongst lesion radiomic features, RLNU from GLRLM (AUC = 0.73), HGLZE from GLSZM (AUC = 0.73) had the highest performance. In multivariate analysis, combining lung, lesion and clinical features was determined to provide the most accurate prognostic model (AUC = 0.95 ± 0.029 (95%CI: 0.95-0.96), accuracy = 0.88 ± 0.046 (95% CI: 0.88-0.89), sensitivity = 0.88 ± 0.066 (95% CI = 0.87-0.9) and specificity = 0.89 ± 0.07 (95% CI = 0.87-0.9)). CONCLUSION: Combination of radiomic features and clinical data can effectively predict outcome in COVID-19 patients. The developed model has significant potential for improved management of COVID-19 patients.",2021,10.1016/j.compbiomed.2021.104304,prognosis,True
Machine Learning-Based Radiomics for Prediction of Epidermal Growth Factor Receptor Mutations in Lung Adenocarcinoma,"Identifying an epidermal growth factor receptor (EGFR) mutation is important because EGFR tyrosine kinase inhibitors are the first-line treatment of choice for patients with EGFR mutation-positive lung adenocarcinomas (LUAC). This study is aimed at developing and validating a radiomics-based machine learning (ML) approach to identify EGFR mutations in patients with LUAC. We retrospectively collected data from 201 patients with positive EGFR mutation LUAC (140 in the training cohort and 61 in the validation cohort). We extracted 1316 radiomics features from preprocessed CT images and selected 14 radiomics features and 1 clinical feature which were most relevant to mutations through filter method. Subsequently, we built models using 7 ML approaches and established the receiver operating characteristic (ROC) curve to assess the discriminating performance of these models. In terms of predicting EGFR mutation, the model derived from radiomics features and combined models (radiomics features and relevant clinical factors) had an AUC of 0.79 (95% confidence interval (CI): 0.77-0.82), 0.86 (0.87-0.88), respectively. Our study offers a radiomics-based ML model using filter methods to detect the EGFR mutation in patients with LUAC. This convenient and low-cost method may be of help to noninvasively identify patients before obtaining tumor sample for molecule testing.",2022,10.1155/2022/2056837,diagnosis,True
Machine Learning-Based Radiomics Signatures for EGFR and KRAS Mutations Prediction in Non-Small-Cell Lung Cancer,"Early identification of epidermal growth factor receptor (EGFR) and Kirsten rat sarcoma viral oncogene homolog (KRAS) mutations is crucial for selecting a therapeutic strategy for patients with non-small-cell lung cancer (NSCLC). We proposed a machine learning-based model for feature selection and prediction of EGFR and KRAS mutations in patients with NSCLC by including the least number of the most semantic radiomics features. We included a cohort of 161 patients from 211 patients with NSCLC from The Cancer Imaging Archive (TCIA) and analyzed 161 low-dose computed tomography (LDCT) images for detecting EGFR and KRAS mutations. A total of 851 radiomics features, which were classified into 9 categories, were obtained through manual segmentation and radiomics feature extraction from LDCT. We evaluated our models using a validation set consisting of 18 patients derived from the same TCIA dataset. The results showed that the genetic algorithm plus XGBoost classifier exhibited the most favorable performance, with an accuracy of 0.836 and 0.86 for detecting EGFR and KRAS mutations, respectively. We demonstrated that a noninvasive machine learning-based model including the least number of the most semantic radiomics signatures could robustly predict EGFR and KRAS mutations in patients with NSCLC.",2021,10.3390/ijms22179254,diagnosis,True
Machine learning-based radiomics strategy for prediction of cell proliferation in non-small cell lung cancer,"PURPOSE: To explore the feasibility and performance of machine learning-based radiomics classifier to predict the cell proliferation(Ki-67)in non-small cell lung cancer (NSCLC). METHODS: 245 histopathological confirmed NSCLC patients who underwent CT scans were retrospectively included. The Ki-67 proliferation index (Ki-67 PI) were measured within 2 weeks after CT scans. A lesion volume of interest (VOI) was manually delineated and radiomics features were extracted by MaZda software from CT images. A random forest feature selection algorithm (RFFS) was used to reduce features. Six kinds of machine learning methods were used to establish radiomics classifiers, subjective imaging feature classifiers and combined classifiers, respectively. The performance of these classifiers was evaluated by the receiver operating characteristic curve (ROC) and compared with Delong test. RESULTS: 103 radiomics features were extracted and 20 optimal features were selected using RFFS. Among the radiomics classifiers established by six machine learning methods, random forest-based radiomics classifier achieved the best performance (AUC = 0.776) in predicting the Ki-67 expression level with sensitivity and specificity of 0.726 and 0.661, which was better than that of subjective imaging classifiers (AUC = 0.625, P < 0.05). However, the combined classifiers did not improve the predictive performance (AUC = 0.780, P > 0.05), with sensitivity and specificity of 0.752 and 0.633. CONCLUSIONS: The machine learning-based CT radiomics classifier in NSCLC can facilitate the prediction of the expression level of Ki-67 and provide a novel non-invasive strategy for assessing the cell proliferation.",2019,10.1016/j.ejrad.2019.06.025,prognosis,True
Malignant-benign classification of pulmonary nodules based on random forest aided by clustering analysis,"To help the radiologists better differentiate the benign from malignant pulmonary nodules on CT images, a novel classification scheme was proposed to improve the performance of benign and malignant classifier of pulmonary nodules. First, the pulmonary nodules were segmented with the references to the results from four radiologists. Then, some basic features of the segmented nodules such as the shape, gray and texture are given by calculation. Finally, malignant-benign classification of pulmonary nodules is performed by using random forest (RF) with the aid of clustering analysis. The data with a set of 952 nodules have been collected from lung image database consortium (LIDC). The effect of proposed classification scheme was verified by three experiments, in which the variant composite rank of malignancy were got from four radiologists (experiment 1: rank of malignancy '1', '2' as benign and '4', '5' as malignant; experiment 2: rank of malignancy '1', '2', '3' as benign and '4', '5' as malignant; experiment 3: rank of malignancy '1', '2' as benign and '3', '4', '5' as malignant) and the corresponding ([Formula: see text]) (area under the receiver operating characteristic curve) are 0.9702, 0.9190 and 0.8662, respectively. It can be drawn that the method in this work can greatly improve the accuracy of the classification of benign and malignant pulmonary nodules based on CT images.",2019,10.1088/1361-6560/aafab0,diagnosis,True
MAMA Net: Multi-Scale Attention Memory Autoencoder Network for Anomaly Detection,"Anomaly detection refers to the identification of cases that do not conform to the expected pattern, which takes a key role in diverse research areas and application domains. Most of existing methods can be summarized as anomaly object detection-based and reconstruction error-based techniques. However, due to the bottleneck of defining encompasses of real-world high-diversity outliers and inaccessible inference process, individually, most of them have not derived groundbreaking progress. To deal with those imperfectness, and motivated by memory-based decision-making and visual attention mechanism as a filter to select environmental information in human vision perceptual system, in this paper, we propose a Multi-scale Attention Memory with hash addressing Autoencoder network (MAMA Net) for anomaly detection. First, to overcome a battery of problems result from the restricted stationary receptive field of convolution operator, we coin the multi-scale global spatial attention block which can be straightforwardly plugged into any networks as sampling, upsampling and downsampling function. On account of its efficient features representation ability, networks can achieve competitive results with only several level blocks. Second, it's observed that traditional autoencoder can only learn an ambiguous model that also reconstructs anomalies ""well"" due to lack of constraints in training and inference process. To mitigate this challenge, we design a hash addressing memory module that proves abnormalities to produce higher reconstruction error for classification. In addition, we couple the mean square error (MSE) with Wasserstein loss to improve the encoding data distribution. Experiments on various datasets, including two different COVID-19 datasets and one brain MRI (RIDER) dataset prove the robustness and excellent generalization of the proposed MAMA Net.",2021,10.1109/tmi.2020.3045295,diagnosis,False
Managing tumor changes during radiotherapy using a deep learning model,"PURPOSE: We propose a treatment planning framework that accounts for weekly lung tumor shrinkage using cone beam computed tomography (CBCT) images with a deep learning-based model. METHODS: Sixteen patients with non-small-cell lung cancer (NSCLC) were selected with one planning CT and six weekly CBCTs each. A deep learning-based model was applied to predict the weekly deformation of the primary tumor based on the spatial and temporal features extracted from previous weekly CBCTs. Starting from Week 3, the tumor contour at Week N was predicted by the model based on the input from all the previous weeks (1, 2 … N - 1), and was evaluated against the manually contoured tumor using Dice coefficient (DSC), precision, average surface distance (ASD), and Hausdorff distance (HD). Information about the predicted tumor was then entered into the treatment planning system and the plan was re-optimized every week. The objectives were to maximize the dose coverage in the target region while minimizing the toxicity to the surrounding healthy tissue. Dosimetric evaluation of the target and organs at risk (heart, lung, esophagus, and spinal cord) was performed on four cases, comparing between a conventional plan (ignoring tumor shrinkage) and the shrinkage-based plan. RESULTS: he primary tumor volumes decreased on average by 38% ± 26% during six weeks of treatment. DSCs and ASD between the predicted tumor and the actual tumor for Weeks 3, 4, 5, 6 were 0.81, 0.82, 0.79, 0.78 and 1.49, 1.59, 1.92, 2.12 mm, respectively, which were significantly superior to the score of 0.70, 0.68, 0.66, 0.63 and 2.81, 3.22, 3.69, 3.63 mm between the rigidly transferred tumors ignoring shrinkage and the actual tumor. While target coverage metrics were maintained for the re-optimized plans, lung mean dose dropped by 2.85, 0.46, 2.39, and 1.48 Gy for four sample cases when compared to the original plan. Doses in other organs such as esophagus were also reduced for some cases. CONCLUSION: We developed a deep learning-based model for tumor shrinkage prediction. This model used CBCTs and contours from previous weeks as input and produced reasonable tumor contours with a high prediction accuracy (DSC, precision, HD, and ASD). The proposed framework maintained target coverage while reducing dose in the lungs and esophagus.",2021,10.1002/mp.14925,treatment,True
Marginal radiomics features as imaging biomarkers for pathological invasion in lung adenocarcinoma,"OBJECTIVES: Lung adenocarcinomas which manifest as ground-glass nodules (GGNs) have different degrees of pathological invasion and differentiating among them is critical for treatment. Our goal was to evaluate the addition of marginal features to a baseline radiomics model on computed tomography (CT) images to predict the degree of pathologic invasiveness. METHODS: We identified 236 patients from two cohorts (training, n = 189; validation, n = 47) who underwent surgery for GGNs. All GGNs were pathologically confirmed as adenocarcinoma in situ (AIS), minimally invasive adenocarcinoma (MIA), or invasive adenocarcinoma (IA). The regions of interest were semi-automatically annotated and 40 radiomics features were computed. We selected features using L1-norm regularization to build the baseline radiomics model. Additional marginal features were developed using the cumulative distribution function (CDF) of intratumoral intensities. An improved model was built combining the baseline model with CDF features. Three classifiers were tested for both models. RESULTS: The baseline radiomics model included five features and resulted in an average area under the curve (AUC) of 0.8419 (training) and 0.9142 (validation) for the three classifiers. The second model, with the additional marginal features, resulted in AUCs of 0.8560 (training) and 0.9581 (validation). All three classifiers performed better with the added features. The support vector machine showed the most performance improvement (AUC improvement = 0.0790) and the best performance was achieved by the logistic classifier (validation AUC = 0.9825). CONCLUSION: Our novel marginal features, when combined with a baseline radiomics model, can help differentiate IA from AIS and MIA on preoperative CT scans. KEY POINTS: • Our novel marginal features could improve the existing radiomics model to predict the degree of pathologic invasiveness in lung adenocarcinoma.",2020,10.1007/s00330-019-06581-2,prognosis,True
MD-NDNet: a multi-dimensional convolutional neural network for false-positive reduction in pulmonary nodule detection,"Pulmonary nodule false-positive reduction is of great significance for automated nodule detection in clinical diagnosis of low-dose computed tomography (LDCT) lung cancer screening. Due to individual intra-nodule variations and visual similarities between true nodules and false positives as soft tissues in LDCT images, the current clinical practices remain subject to shortcomings of potential high-risk and time-consumption issues. In this paper, we propose a multi-dimensional nodule detection network (MD-NDNet) for automatic nodule false-positive reduction using deep convolutional neural network (DCNNs). The underlying method collaboratively integrates multi-dimensional nodule information to complementarily and comprehensively extract nodule inter-plane volumetric correlation features using three-dimensional CNNs (3D CNNs) and spatial nodule correlation features from sagittal, coronal, and axial planes using two-dimensional CNNs (2D CNNs) with attention module. To incorporate different sizes and shapes of nodule candidates, a multi-scale ensemble strategy is employed for probability aggregation with weights. The proposed method is evaluated on the LUNA16 challenge dataset in ISBI 2016 with ten-fold cross-validation. Experiment results show that the proposed framework achieves classification performance with a CPM score of 0.9008. All of these indicate that our method enables an efficient, accurate and reliable pulmonary nodule detection for clinical diagnosis.",2020,10.1088/1361-6560/aba87c,diagnosis,True
Mediastinal lymph node detection and station mapping on chest CT using spatial priors and random forest,"PURPOSE: To develop an automated system for mediastinal lymph node detection and station mapping for chest CT. METHODS: The contextual organs, trachea, lungs, and spine are first automatically identified to locate the region of interest (ROI) (mediastinum). The authors employ shape features derived from Hessian analysis, local object scale, and circular transformation that are computed per voxel in the ROI. Eight more anatomical structures are simultaneously segmented by multiatlas label fusion. Spatial priors are defined as the relative multidimensional distance vectors corresponding to each structure. Intensity, shape, and spatial prior features are integrated and parsed by a random forest classifier for lymph node detection. The detected candidates are then segmented by the following curve evolution process. Texture features are computed on the segmented lymph nodes and a support vector machine committee is used for final classification. For lymph node station labeling, based on the segmentation results of the above anatomical structures, the textual definitions of mediastinal lymph node map according to the International Association for the Study of Lung Cancer are converted into patient-specific color-coded CT image, where the lymph node station can be automatically assigned for each detected node. RESULTS: The chest CT volumes from 70 patients with 316 enlarged mediastinal lymph nodes are used for validation. For lymph node detection, their system achieves 88% sensitivity at eight false positives per patient. For lymph node station labeling, 84.5% of lymph nodes are correctly assigned to their stations. CONCLUSIONS: Multiple-channel shape, intensity, and spatial prior features aggregated by a random forest classifier improve mediastinal lymph node detection on chest CT. Using the location information of segmented anatomic structures from the multiatlas formulation enables accurate identification of lymph node stations.",2016,10.1118/1.4954009,diagnosis,True
MHA-CoroCapsule: Multi-Head Attention Routing-Based Capsule Network for COVID-19 Chest X-Ray Image Classification,"The outbreak of COVID-19 threatens the lives and property safety of countless people and brings a tremendous pressure to health care systems worldwide. The principal challenge in the fight against this disease is the lack of efficient detection methods. AI-assisted diagnosis based on deep learning can detect COVID-19 cases for chest X-ray images automatically, and also improve the accuracy and efficiency of doctors' diagnosis. However, large scale annotation of chest X-ray images is difficult because of limited resources and heavy burden on the medical system. To meet the challenge, we propose a capsule network model with multi-head attention routing algorithm, called MHA-CoroCapsule, to provide fast and accurate diagnostics for COVID-19 diseases from chest X-ray images. The MHA-CoroCapsule consists of convolutional layers, two capsule layers, and a non-iterative, parameterized multi-head attention routing algorithm is used to quantify the relationship between the two capsule layers. The experiments are performed on a combined dataset constituted by two publicly available datasets including normal, non-COVID pneumonia and COVID-19 images. The model achieves the accuracy of 97.28%, recall of 97.36%, and precision of 97.38% even with a limited number of samples. The experimental results demonstrate that, contrary to the transfer learning and deep feature extraction approaches, the proposed MHA-CoroCapsule has an encouraging performance with fewer trainable parameters and does not require pretraining and plenty of training samples.",2022,10.1109/tmi.2021.3134270,diagnosis,False
Microscopic handcrafted features selection from computed tomography scans for early stage lungs cancer diagnosis using hybrid classifiers,"Lung's cancer is the leading cause of cancer-related deaths worldwide. Recently cancer mortality rate and incidence increased exponentially. Many patients with lung cancer are diagnosed late, so the survival rate is shallow. Machine learning approaches have been widely used to increase the effectiveness of cancer detection at an early stage. Even while these methods are efficient in detecting specific forms of cancer, there is no known technique that could be used universally and consistently to identify new malignancies. As a result, cancer diagnosis via machine learning algorithms is still fresh area of research. Computed tomography (CT) images are frequently employed for early cancer detection and diagnosis because they contain significant information. In this research, an automated lung cancer detection and classification framework is proposed which consists of preprocessing, three patches local binary pattern feature encoding, local binary pattern, histogram of oriented gradients features are extracted and fused. The fast learning network (FLN) is a novel machine-learning technique that is fast to train and economical in terms of processing resources. However, the FLN's internal power parameters (weight and basis) are randomly initialized, resulting it an unstable algorithm. Therefore, to enhance accuracy, FLN is hybrid with K-nearest neighbors to classify texture and appearance-based features of lung chest CT scans from Kaggle dataset into cancerous and non-cancerous images. The proposed model performance is evaluated using accuracy, sensitivity, specificity on the Kaggle benchmark dataset that is found comparable in state of the art using simple machine learning strategies. RESEARCH HIGHLIGHTS: Fast learning network and K-nearest neighbor hybrid classifier proposed first time for lung cancer classification using handcrafted features including three patches local binary pattern, local binary pattern, and histogram of oriented gradients. Promising results obtained from novel simple combination.",2022,10.1002/jemt.24075,diagnosis,True
Microscopic segmentation and classification of COVID-19 infection with ensemble convolutional neural network,"The detection of biological RNA from sputum has a comparatively poor positive rate in the initial/early stages of discovering COVID-19, as per the World Health Organization. It has a different morphological structure as compared to healthy images, manifested by computer tomography (CT). COVID-19 diagnosis at an early stage can aid in the timely cure of patients, lowering the mortality rate. In this reported research, three-phase model is proposed for COVID-19 detection. In Phase I, noise is removed from CT images using a denoise convolutional neural network (DnCNN). In the Phase II, the actual lesion region is segmented from the enhanced CT images by using deeplabv3 and ResNet-18. In Phase III, segmented images are passed to the stack sparse autoencoder (SSAE) deep learning model having two stack auto-encoders (SAE) with the selected hidden layers. The designed SSAE model is based on both SAE and softmax layers for COVID19 classification. The proposed method is evaluated on actual patient data of Pakistan Ordinance Factories and other public benchmark data sets with different scanners/mediums. The proposed method achieved global segmentation accuracy of 0.96 and 0.97 for classification.",2022,10.1002/jemt.23913,diagnosis,True
Mini-COVIDNet: Efficient Lightweight Deep Neural Network for Ultrasound Based Point-of-Care Detection of COVID-19,"Lung ultrasound (US) imaging has the potential to be an effective point-of-care test for detection of COVID-19, due to its ease of operation with minimal personal protection equipment along with easy disinfection. The current state-of-the-art deep learning models for detection of COVID-19 are heavy models that may not be easy to deploy in commonly utilized mobile platforms in point-of-care testing. In this work, we develop a lightweight mobile friendly efficient deep learning model for detection of COVID-19 using lung US images. Three different classes including COVID-19, pneumonia, and healthy were included in this task. The developed network, named as Mini-COVIDNet, was bench-marked with other lightweight neural network models along with state-of-the-art heavy model. It was shown that the proposed network can achieve the highest accuracy of 83.2% and requires a training time of only 24 min. The proposed Mini-COVIDNet has 4.39 times less number of parameters in the network compared to its next best performing network and requires a memory of only 51.29 MB, making the point-of-care detection of COVID-19 using lung US imaging plausible on a mobile platform. Deployment of these lightweight networks on embedded platforms shows that the proposed Mini-COVIDNet is highly versatile and provides optimal performance in terms of being accurate as well as having latency in the same order as other lightweight networks. The developed lightweight models are available at https://github.com/navchetan-awasthi/Mini-COVIDNet.",2021,10.1109/tuffc.2021.3068190,diagnosis,False
Mining whole-lung information by artificial intelligence for predicting EGFR genotype and targeted therapy response in lung cancer: a multicohort study,"BACKGROUND: Epidermal growth factor receptor (EGFR) genotype is crucial for treatment decision making in lung cancer, but it can be affected by tumour heterogeneity and invasive biopsy during gene sequencing. Importantly, not all patients with an EGFR mutation have good prognosis with EGFR-tyrosine kinase inhibitors (TKIs), indicating the necessity of stratifying for EGFR-mutant genotype. In this study, we proposed a fully automated artificial intelligence system (FAIS) that mines whole-lung information from CT images to predict EGFR genotype and prognosis with EGFR-TKI treatment. METHODS: We included 18 232 patients with lung cancer with CT imaging and EGFR gene sequencing from nine cohorts in China and the USA, including a prospective cohort in an Asian population (n=891) and The Cancer Imaging Archive cohort in a White population. These cohorts were divided into thick CT group and thin CT group. The FAIS was built for predicting EGFR genotype and progression-free survival of patients receiving EGFR-TKIs, and it was evaluated by area under the curve (AUC) and Kaplan-Meier analysis. We further built two tumour-based deep learning models as comparison with the FAIS, and we explored the value of combining FAIS and clinical factors (the FAIS-C model). Additionally, we included 891 patients with 56-panel next-generation sequencing and 87 patients with RNA sequencing data to explore the biological mechanisms of FAIS. FINDINGS: FAIS achieved AUCs ranging from 0·748 to 0·813 in the six retrospective and prospective testing cohorts, outperforming the commonly used tumour-based deep learning model. Genotype predicted by the FAIS-C model was significantly associated with prognosis to EGFR-TKIs treatment (log-rank p<0·05), an important complement to gene sequencing. Moreover, we found 29 prognostic deep learning features in FAIS that were able to identify patients with an EGFR mutation at high risk of TKI resistance. These features showed strong associations with multiple genotypes (p<0·05, t test or Wilcoxon test) and gene pathways linked to drug resistance and cancer progression mechanisms. INTERPRETATION: FAIS provides a non-invasive method to detect EGFR genotype and identify patients with an EGFR mutation at high risk of TKI resistance. The superior performance of FAIS over tumour-based deep learning methods suggests that genotype and prognostic information could be obtained from the whole lung instead of only tumour tissues. FUNDING: National Natural Science Foundation of China.",2022,10.1016/s2589-7500(22)00024-3,diagnosis,True
Mix Contrast for COVID-19 Mild-to-Critical Prediction,"OBJECTIVE: In a few patients with mild COVID-19, there is a possibility of the infection becoming severe or critical in the future. This work aims to identify high-risk patients who have a high probability of changing from mild to critical COVID-19 (only account for 5% of cases). METHODS: Using traditional convolutional neural networks for classification may not be suitable to identify this 5% of high risk patients from an entire dataset due to the highly imbalanced label distribution. To address this problem, we propose a Mix Contrast model, which matches original features with mixed features for contrastive learning. Three modules are proposed for training the model: 1) a cumulative learning strategy for synthesizing the mixed feature; 2) a commutative feature combination module for learning the commutative law of feature concatenation; 3) a united pairwise loss assigning adaptive weights for sample pairs with different class anchors based on their current optimization status. RESULTS: We collect a multi-center computed tomography dataset including 918 confirmed COVID-19 patients from four hospitals and evaluate the proposed method on both the COVID-19 mild-to-critical prediction and COVID-19 diagnosis tasks. For mild-to-critical prediction, the experimental results show a recall of 0.80 and a specificity of 0.815. For diagnosis, the model shows comparable results with deep neural networks using a large dataset. Our method demonstrates improvements when the amount of training data is small or imbalanced. SIGNIFICANCE: Identifying mild-to-critical COVID-19 patients is important for early prevention and personalized treatment planning.",2021,10.1109/tbme.2021.3085576,prognosis,True
Mix-and-Interpolate: A Training Strategy to Deal With Source-Biased Medical Data,"Till March 31st, 2021, the coronavirus disease 2019 (COVID-19) had reportedly infected more than 127 million people and caused over 2.5 million deaths worldwide. Timely diagnosis of COVID-19 is crucial for management of individual patients as well as containment of the highly contagious disease. Having realized the clinical value of non-contrast chest computed tomography (CT) for diagnosis of COVID-19, deep learning (DL) based automated methods have been proposed to aid the radiologists in reading the huge quantities of CT exams as a result of the pandemic. In this work, we address an overlooked problem for training deep convolutional neural networks for COVID-19 classification using real-world multi-source data, namely, the data source bias problem. The data source bias problem refers to the situation in which certain sources of data comprise only a single class of data, and training with such source-biased data may make the DL models learn to distinguish data sources instead of COVID-19. To overcome this problem, we propose MIx-aNd-Interpolate (MINI), a conceptually simple, easy-to-implement, efficient yet effective training strategy. The proposed MINI approach generates volumes of the absent class by combining the samples collected from different hospitals, which enlarges the sample space of the original source-biased dataset. Experimental results on a large collection of real patient data (1,221 COVID-19 and 1,520 negative CT images, and the latter consisting of 786 community acquired pneumonia and 734 non-pneumonia) from eight hospitals and health institutions show that: 1) MINI can improve COVID-19 classification performance upon the baseline (which does not deal with the source bias), and 2) MINI is superior to competing methods in terms of the extent of improvement.",2022,10.1109/jbhi.2021.3119325,diagnosis,True
Modality alignment contrastive learning for severity assessment of COVID-19 from lung ultrasound and clinical information,"The outbreak of COVID-19 around the world has caused great pressure to the health care system, and many efforts have been devoted to artificial intelligence (AI)-based analysis of CT and chest X-ray images to help alleviate the shortage of radiologists and improve the diagnosis efficiency. However, only a few works focus on AI-based lung ultrasound (LUS) analysis in spite of its significant role in COVID-19. In this work, we aim to propose a novel method for severity assessment of COVID-19 patients from LUS and clinical information. Great challenges exist regarding the heterogeneous data, multi-modality information, and highly nonlinear mapping. To overcome these challenges, we first propose a dual-level supervised multiple instance learning module (DSA-MIL) to effectively combine the zone-level representations into patient-level representations. Then a novel modality alignment contrastive learning module (MA-CLR) is presented to combine representations of the two modalities, LUS and clinical information, by matching the two spaces while keeping the discriminative features. To train the nonlinear mapping, a staged representation transfer (SRT) strategy is introduced to maximumly leverage the semantic and discriminative information from the training data. We trained the model with LUS data of 233 patients, and validated it with 80 patients. Our method can effectively combine the two modalities and achieve accuracy of 75.0% for 4-level patient severity assessment, and 87.5% for the binary severe/non-severe identification. Besides, our method also provides interpretation of the severity assessment by grading each of the lung zone (with accuracy of 85.28%) and identifying the pathological patterns of each lung zone. Our method has a great potential in real clinical practice for COVID-19 patients, especially for pregnant women and children, in aspects of progress monitoring, prognosis stratification, and patient management.",2021,10.1016/j.media.2021.101975,diagnosis,True
Models of Artificial Intelligence-Assisted Diagnosis of Lung Cancer Pathology Based on Deep Learning Algorithms,"In this article, in order to explore the application of a diagnosis system for lung cancer, we use an auxiliary diagnostic system to predict and diagnose the good and evil attributes of chest CT pulmonary nodules. This research improves the new diagnosis method based on the convolutional neural network (CNN) and the recurrent neural network (RNN) and combines the dual effects of the two algorithms to process the classification of benign and malignant nodules. By collecting H-E-stained pathological slices of 652 patients' lung lesions from two hospitals between January 2018 and January 2019, the output results of the improved 3D U-net system and the consistent results of two-person reading were compared. This article analyzes the sensitivity, specificity, positive flammability rate, and negative flammability rate of different lung nodule detection methods. In addition, the artificial intelligence system's and the radiologist's judgment results of benign and malignant pulmonary nodules are used to draw ROC curves for further analysis. The improved model has an accuracy rate of 92.3% for predicting malignant lung nodules and an accuracy rate of 82.8% for benign lung nodules. The new diagnostic method using the convolutional neural network and the recurrent neural network can be very effective for improving the accuracy of predicting lung cancer diagnosis. It can play a very effective role in the disease prediction of lung cancer patients, thereby improving the treatment effect.",2022,10.1155/2022/3972298,diagnosis,True
MR-Forest: A Deep Decision Framework for False Positive Reduction in Pulmonary Nodule Detection,"With the development of deep learning methods such as convolutional neural network (CNN), the accuracy of automated pulmonary nodule detection has been greatly improved. However, the high computational and storage costs of the large-scale network have been a potential concern for the future widespread clinical application. In this paper, an alternative Multi-ringed (MR)-Forest framework, against the resource-consuming neural networks (NN)-based architectures, has been proposed for false positive reduction in pulmonary nodule detection, which consists of three steps. First, a novel multi-ringed scanning method is used to extract the order ring facets (ORFs) from the surface voxels of the volumetric nodule models; Second, Mesh-LBP and mapping deformation are employed to estimate the texture and shape features. By sliding and resampling the multi-ringed ORFs, feature volumes with different lengths are generated. Finally, the outputs of multi-level are cascaded to predict the candidate class. On 1034 scans merging the dataset from the Affiliated Hospital of Liaoning University of Traditional Chinese Medicine (AH-LUTCM) and the LUNA16 Challenge dataset, our framework performs enough competitiveness than state-of-the-art in false positive reduction task (CPM score of 0.865). Experimental results demonstrate that MR-Forest is a successful solution to satisfy both resource-consuming and effectiveness for automated pulmonary nodule detection. The proposed MR-forest is a general architecture for 3D target detection, it can be easily extended in many other medical imaging analysis tasks, where the growth trend of the targeting object is approximated as a spheroidal expansion.",2020,10.1109/jbhi.2019.2947506,diagnosis,True
MRI Image Segmentation Model with Support Vector Machine Algorithm in Diagnosis of Solitary Pulmonary Nodule,"This study focused on the application value of MRI images processed by a Support Vector Machine (SVM) algorithm-based model in diagnosis of benign and malignant solitary pulmonary nodule (SPN). The SVM algorithm was constrained by a self-paced regularization item and gradient value to establish the MRI image segmentation model (SVM-L) for lung. Its performance was compared factoring into the Dice index (DI), sensitivity (SE), specificity (SP), and Mean Square Error (MSE). 28 SPN patients who underwent the parallel MRI examination were selected as research subjects and were divided into the benign group (11 patients) and malignant group (17 patients) according to different plans for diagnosis and treatment. The apparent diffusion coefficient (ADC) at different b values was analyzed, and the steepest slope (SS) and washout ratio (WR) values in the two groups were calculated. The result showed that the MSE, DI, SE, SP values, and operation time of the SVM-L model were (0.41 ± 0.02), (0.84 ± 0.13), (0.89 ± 0.04), (0.993 ± 0.004), and (30.69 ± 2.60)s, respectively, apparently superior to those of the other algorithms, but there were no statistic differences (P > 0.05) in the WR value between the two groups of patients. The SS values of the time-signal curve in the benign and malignant groups were (2.52 ± 0.69) %/s and (3.34 ± 00.41) %/s, respectively. Obviously, the SS value of the benign group was significantly lower than that of the malignant group (P < 0.01). The ADC value with different b values in the benign group was significantly lower than that of the malignant group (P < 0.01). It suggested that the SVM-L model significantly improved the quality of lung MRI images and increased the accuracy to differentiate benign and malignant SPN, providing reference for the diagnosis and treatment of SPN patients.",2021,10.1155/2021/9668836,diagnosis,False
MS-ResNet: disease-specific survival prediction using longitudinal CT images and clinical data,"PURPOSE: Medical imaging data of lung cancer in different stages contain a large amount of time information related to its evolution (emergence, development, or extinction). We try to explore the evolution process of lung images in time dimension to improve the prediction of lung cancer survival by using longitudinal CT images and clinical data jointly. METHODS: In this paper, we propose an innovative multi-branch spatiotemporal residual network (MS-ResNet) for disease-specific survival (DSS) prediction by integrating the longitudinal computed tomography (CT) images at different times and clinical data. Specifically, we first extract the deep features from the multi-period CT images by an improved residual network. Then, the feature selection algorithm is used to select the most relevant feature subset from the clinical data. Finally, we integrate the deep features and feature subsets to take full advantage of the complementarity between the two types of data to generate the final prediction results. RESULTS: The experimental results demonstrate that our MS-ResNet model is superior to other methods, achieving a promising 86.78% accuracy in the classification of short-survivor, med-survivor, and long-survivor. CONCLUSION: In computer-aided prognostic analysis of cancer, the time dimension features of the course of disease and the integration of patient clinical data and CT data can effectively improve the prediction accuracy.",2022,10.1007/s11548-022-02625-z,prognosis,True
MSCS-DeepLN: Evaluating lung nodule malignancy using multi-scale cost-sensitive neural networks,"The accurate identification of malignant lung nodules using computed tomography (CT) screening images is vital for the early detection of lung cancer. It also offers patients the best chance of cure, because non-invasive CT imaging has the ability to capture intra-tumoral heterogeneity. Deep learning methods have obtained promising results for the malignancy identification problem; however, two substantial challenges still remain. First, small datasets cannot insufficiently train the model and tend to overfit it. Second, category imbalance in the data is a problem. In this paper, we propose a method called MSCS-DeepLN that evaluates lung nodule malignancy and simultaneously solves these two problems. Three light models are trained and combined to evaluate the malignancy of a lung nodule. Three-dimensional convolutional neural networks (CNNs) are employed as the backbone of each light model to extract the lung nodule features from CT images and preserve lung nodule spatial heterogeneity. Multi-scale input cropped from CT images enables the sub-networks to learn the multi-level contextual features and preserve diverse. To tackle the imbalance problem, our proposed method employs an AUC approximation as the penalty term. During training, the error in this penalty term is generated from each major and minor class pair, so that negatives and positives can contribute equally to updating this model. Based on these methods, we obtain state-of-the-art results on the LIDC-IDRI dataset. Furthermore, we constructed a new dataset collected from a grade-A tertiary hospital and annotated using biopsy-based cytological analysis to verify the performance of our method in clinical practice.",2020,10.1016/j.media.2020.101772,diagnosis,True
MTU-COVNet: A hybrid methodology for diagnosing the COVID-19 pneumonia with optimized features from multi-net,"PURPOSE: The aim of this study was to establish and evaluate a fully automatic deep learning system for the diagnosis of COVID-19 using thoracic computed tomography (CT). MATERIALS AND METHODS: In this retrospective study, a novel hybrid model (MTU-COVNet) was developed to extract visual features from volumetric thoracic CT scans for the detection of COVID-19. The collected dataset consisted of 3210 CT scans from 953 patients. Of the total 3210 scans in the final dataset, 1327 (41%) were obtained from the COVID-19 group, 929 (29%) from the CAP group, and 954 (30%) from the Normal CT group. Diagnostic performance was assessed with the area under the receiver operating characteristic (ROC) curve, sensitivity, and specificity. RESULTS: The proposed approach with the optimized features from concatenated layers reached an overall accuracy of 97.7% for the CT-MTU dataset. The rest of the total performance metrics, such as; specificity, sensitivity, precision, F1 score, and Matthew Correlation Coefficient were 98.8%, 97.6%, 97.8%, 97.7%, and 96.5%, respectively. This model showed high diagnostic performance in detecting COVID-19 pneumonia (specificity: 98.0% and sensitivity: 98.2%) and CAP (specificity: 99.1% and sensitivity: 97.1%). The areas under the ROC curves for COVID-19 and CAP were 0.997 and 0.996, respectively. CONCLUSION: A deep learning-based AI system built on the CT imaging can detect COVID-19 pneumonia with high diagnostic efficiency and distinguish it from CAP and normal CT. AI applications can have beneficial effects in the fight against COVID-19.",2022,10.1016/j.clinimag.2021.09.007,diagnosis,True
Multi-classifier-based identification of COVID-19 from chest computed tomography using generalizable and interpretable radiomics features,"PURPOSE: To investigate the efficacy of radiomics in diagnosing patients with coronavirus disease (COVID-19) and other types of viral pneumonia with clinical symptoms and CT signs similar to those of COVID-19. METHODS: Between 18 January 2020 and 20 May 2020, 110 SARS-CoV-2 positive and 108 SARS-CoV-2 negative patients were retrospectively recruited from three hospitals based on the inclusion criteria. Manual segmentation of pneumonia lesions on CT scans was performed by four radiologists. The latest version of Pyradiomics was used for feature extraction. Four classifiers (linear classifier, k-nearest neighbour, least absolute shrinkage and selection operator [LASSO], and random forest) were used to differentiate SARS-CoV-2 positive and SARS-CoV-2 negative patients. Comparison of the performance of the classifiers and radiologists was evaluated by ROC curve and Kappa score. RESULTS: We manually segmented 16,053 CT slices, comprising 32,625 pneumonia lesions, from the CT scans of all patients. Using Pyradiomics, 120 radiomic features were extracted from each image. The key radiomic features screened by different classifiers varied and lead to significant differences in classification accuracy. The LASSO achieved the best performance (sensitivity: 72.2%, specificity: 75.1%, and AUC: 0.81) on the external validation dataset and attained excellent agreement (Kappa score: 0.89) with radiologists (average sensitivity: 75.6%, specificity: 78.2%, and AUC: 0.81). All classifiers indicated that ""Original_Firstorder_RootMeanSquared"" and ""Original_Firstorder_Uniformity"" were significant features for this task. CONCLUSIONS: We identified radiomic features that were significantly associated with the classification of COVID-19 pneumonia using multiple classifiers. The quantifiable interpretation of the differences in features between the two groups extends our understanding of CT imaging characteristics of COVID-19 pneumonia.",2021,10.1016/j.ejrad.2021.109552,diagnosis,True
Multi-Dimension and Multi-Feature Hybrid Learning Network for Classifying the Sub Pathological Type of Lung Nodules through LDCT,"In order to develop appropriate treatment and rehabilitation plans with regard to different subpathological types (PILs and IAs) of lung nodules, it is important to diagnose them through low-dose spiral computed tomography (LDCT) during routine screening before surgery. Based on the characteristics of different subpathological lung nodules expressed from LDCT images, we propose a multi-dimension and multi-feature hybrid learning neural network in this paper. Our network consists of a 2D network part and a 3D network part. The feature vectors extracted from the 2D network and 3D network are further learned by XGBoost. Through this formation, the network can better integrate the feature information from the 2D and 3D networks. The main learning block of the network is a residual block combined with attention mechanism. This learning block enables the network to learn better from multiple features and pay more attention to the key feature map among all the feature maps in different channels. We conduct experiments on our dataset collected from a cooperating hospital. The results show that the accuracy, sensitivity and specificity of our network are 83%, 86%, 80%, respectively It is feasible to use this network to classify the subpathological type of lung nodule through routine screening.",2021,10.3390/s21082734,diagnosis,True
Multi-energy level fusion for nodal metastasis classification of primary lung tumor on dual energy CT using deep learning,"Lymph node metastasis also called nodal metastasis (Nmet), is a clinically primary task for physicians. The survival and recurrence of lung cancer are related to the Nmet staging from Tumor-Node-Metastasis (TNM) reports. Furthermore, preoperative Nmet prediction is still a challenge for the patient in managing the surgical plan and making treatment decisions. We proposed a multi-energy level fusion model with a principal feature enhancement (PFE) block incorporating radiologist and computer science knowledge for Nmet prediction. The proposed model is custom-designed by gemstone spectral imaging (GSI) with different energy levels on dual-energy computer tomography (CT) from a primary tumor of lung cancer. In the experiment, we take three different energy level fusion datasets: lower energy level fusion (40, 50, 60, 70 keV), higher energy level fusion (110, 120, 130, 140 keV), and average energy level fusion (40, 70, 100, 140 keV). The proposed model is trained by lower energy level fusion that is 93% accurate and the value of Kappa is 86%. When we used the lower energy level images to train the fusion model, there has been a significant difference to other energy level fusion models. Hence, we apply 5-fold cross-validation, which is used to validate the performance result of the multi-keV model with different fusion datasets of energy level images in the pathology report. The cross-validation result also demonstrates that the model with the lower energy level dataset is more robust and suitable in predicting the Nmet of the primary tumor. The lower energy level shows more information of tumor angiogenesis or heterogeneity provided the proposed fusion model with a PFE block and channel attention blocks to predict Nmet from primary tumors.",2022,10.1016/j.compbiomed.2021.105185,diagnosis,True
Multi-institutional dose-segmented dosiomic analysis for predicting radiation pneumonitis after lung stereotactic body radiation therapy,"PURPOSE: To predict radiation pneumonitis (RP) grade 2 or worse after lung stereotactic body radiation therapy (SBRT) using dose-based radiomic (dosiomic) features. METHODS: This multi-institutional study included 247 early-stage nonsmall cell lung cancer patients who underwent SBRT with a prescribed dose of 48-70 Gy at an isocenter between June 2009 and March 2016. Ten dose-volume indices (DVIs) were used, including the mean lung dose, internal target volume size, and percentage of entire lung excluding the internal target volume receiving greater than x Gy (x = 5, 10, 15, 20, 25, 30, 35, and 40). A total of 6,808 dose-segmented dosiomic features, such as shape, first order, and texture features, were extracted from the dose distribution. Patients were randomly partitioned into two groups: model training (70%) and test datasets (30%) over 100 times. Dosiomic features were converted to z-scores (standardized values) with a mean of zero and a standard deviation (SD) of one to put different variables on the same scale. The feature dimension was reduced using the following methods: interfeature correlation based on Spearman's correlation coefficients and feature importance based on a light gradient boosting machine (LightGBM) feature selection function. Three different models were developed using LightGBM as follows: (a) a model with ten DVIs (DVI model), (b) a model with the selected dosiomic features (dosiomic model), and (c) a model with ten DVIs and selected dosiomic features (hybrid model). Suitable hyperparameters were determined by searching the largest average area under the curve (AUC) value in the receiver operating characteristic curve (ROC-AUC) via stratified fivefold cross-validation. Each of the final three models with the closest the ROC-AUC value to the average ROC-AUC value was applied to the test datasets. The classification performance was evaluated by calculating the ROC-AUC, AUC in the precision-recall curve (PR-AUC), accuracy, precision, recall, and f1-score. The entire process was repeated 100 times with randomization, and 100 individual models were developed for each of the three models. Then the mean value and SD for the 100 random iterations were calculated for each performance metric. RESULTS: Thirty-seven (15.0%) patients developed RP after SBRT. The ROC-AUC and PR-AUC values in the DVI, dosiomic, and hybrid models were 0.660 ± 0.054 and 0.272 ± 0.052, 0.837 ± 0.054 and 0.510 ± 0.115, and 0.846 ± 0.049 and 0.531 ± 0.116, respectively. For each performance metric, the dosiomic and hybrid models outperformed the DVI models (P < 0.05). Texture-based dosiomic feature was confirmed as an effective indicator for predicting RP. CONCLUSIONS: Our dose-segmented dosiomic approach improved the prediction of the incidence of RP after SBRT.",2021,10.1002/mp.14769,prognosis,False
Multi-level 3D Densenets for False-positive Reduction in Lung Nodule Detection Based on Chest Computed Tomography,"OBJECTIVE: False-positive nodule reduction is a crucial part of a computer-aided detection (CADe) system, which assists radiologists in accurate lung nodule detection. In this research, a novel scheme using multi-level 3D DenseNet framework is proposed to implement false-positive nodule reduction task. METHODS: Multi-level 3D DenseNet models were extended to differentiate lung nodules from falsepositive nodules. First, different models were fed with 3D cubes with different sizes for encoding multi-level contextual information to meet the challenges of the large variations of lung nodules. In addition, image rotation and flipping were utilized to upsample positive samples which consisted of a positive sample set. Furthermore, the 3D DenseNets were designed to keep low-level information of nodules, as densely connected structures in DenseNet can reuse features of lung nodules and then boost feature propagation. Finally, the optimal weighted linear combination of all model scores obtained the best classification result in this research. RESULTS: The proposed method was evaluated with LUNA16 dataset which contained 888 thin-slice CT scans. The performance was validated via 10-fold cross-validation. Both the Free-response Receiver Operating Characteristic (FROC) curve and the Competition Performance Metric (CPM) score show that the proposed scheme can achieve a satisfactory detection performance in the falsepositive reduction track of the LUNA16 challenge. CONCLUSION: The result shows that the proposed scheme can be significant for false-positive nodule reduction task.",2020,10.2174/1573405615666191113122840,diagnosis,True
Multi-Level Cross Residual Network for Lung Nodule Classification,"Computer-aided algorithm plays an important role in disease diagnosis through medical images. As one of the major cancers, lung cancer is commonly detected by computer tomography. To increase the survival rate of lung cancer patients, an early-stage diagnosis is necessary. In this paper, we propose a new structure, multi-level cross residual convolutional neural network (ML-xResNet), to classify the different types of lung nodule malignancies. ML-xResNet is constructed by three-level parallel ResNets with different convolution kernel sizes to extract multi-scale features of the inputs. Moreover, the residuals are connected not only with the current level but also with other levels in a crossover manner. To illustrate the performance of ML-xResNet, we apply the model to process ternary classification (benign, indeterminate, and malignant lung nodules) and binary classification (benign and malignant lung nodules) of lung nodules, respectively. Based on the experiment results, the proposed ML-xResNet achieves the best results of 85.88% accuracy for ternary classification and 92.19% accuracy for binary classification, without any additional handcrafted preprocessing algorithm.",2020,10.3390/s20102837,diagnosis,True
Multi-model Ensemble Learning Architecture Based on 3D CNN for Lung Nodule Malignancy Suspiciousness Classification,"Classification of benign and malignant in lung nodules using chest CT images is a key step in the diagnosis of early-stage lung cancer, as well as an effective way to improve the patients' survival rate. However, due to the diversity of lung nodules and the visual similarity of lung nodules to their surrounding tissues, it is difficult to construct a robust classification model with conventional deep learning-based diagnostic methods. To address this problem, we propose a multi-model ensemble learning architecture based on 3D convolutional neural network (MMEL-3DCNN). This approach incorporates three key ideas: (1) Constructed multi-model network architecture can be well adapted to the heterogeneity of lung nodules. (2) The input that concatenated of the intensity image corresponding to the nodule mask, the original image, and the enhanced image corresponding to which can help training model to extract advanced feature with more discriminative capacity. (3) Select the corresponding model to different nodule size dynamically for prediction, which can improve the generalization ability of the model effectively. In addition, ensemble learning is applied in this paper to further improve the robustness of the nodule classification model. The proposed method has been experimentally verified on the public dataset, LIDC-IDRI. The experimental results show that the proposed MMEL-3DCNN architecture can obtain satisfactory classification results.",2020,10.1007/s10278-020-00372-8,diagnosis,True
Multi-Radiologist User Study for Artificial Intelligence-Guided Grading of COVID-19 Lung Disease Severity on Chest Radiographs,"RATIONALE AND OBJECTIVES: Radiographic findings of COVID-19 pneumonia can be used for patient risk stratification; however, radiologist reporting of disease severity is inconsistent on chest radiographs (CXRs). We aimed to see if an artificial intelligence (AI) system could help improve radiologist interrater agreement. MATERIALS AND METHODS: We performed a retrospective multi-radiologist user study to evaluate the impact of an AI system, the PXS score model, on the grading of categorical COVID-19 lung disease severity on 154 chest radiographs into four ordinal grades (normal/minimal, mild, moderate, and severe). Four radiologists (two thoracic and two emergency radiologists) independently interpreted 154 CXRs from 154 unique patients with COVID-19 hospitalized at a large academic center, before and after using the AI system (median washout time interval was 16 days). Three different thoracic radiologists assessed the same 154 CXRs using an updated version of the AI system trained on more imaging data. Radiologist interrater agreement was evaluated using Cohen and Fleiss kappa where appropriate. The lung disease severity categories were associated with clinical outcomes using a previously published outcomes dataset using Fisher's exact test and Chi-square test for trend. RESULTS: Use of the AI system improved radiologist interrater agreement (Fleiss κ = 0.40 to 0.66, before and after use of the system). The Fleiss κ for three radiologists using the updated AI system was 0.74. Severity categories were significantly associated with subsequent intubation or death within 3 days. CONCLUSION: An AI system used at the time of CXR study interpretation can improve the interrater agreement of radiologists.",2021,10.1016/j.acra.2021.01.016,diagnosis,False
Multi-resolution classification of exhaled aerosol images to detect obstructive lung diseases in small airways,"Exhaled aerosol patterns have been used to detect obstructive respiratory diseases in the upper airways. Signals from small airway diseases are weak and may not manifest themselves in the exhaled aerosol patterns. Therefore, it will be more challenging to detect abnormalities in small airways. The objective of this study is to develop a simulation-based classification model that can accurately classify small airway diseases. The model performance was evaluated in five obstructed models that are located in lung bifurcations G7-9. The exhaled aerosol images were quantified using local fractal dimensions at different sampling resolutions (n × n). The datasets were classified using both the random forest (RF) and support vector machine (SVM) algorithms. Results show that RF performs slightly and persistently better than SVM. The sampling resolution of 12 × 12 gave the optimal classification for both algorithms. Based on the lung models with predefined obstructive levels, the optimal classification accuracy is 87.0% for 5-class classification, and is 92.5% for 4-class classification by regrouping the mislabeled samples. The proposed model with multi-resolution fractal feature extraction and RF algorithm appears to be sensitive enough to accurately distinguish airway abnormalities in small airways beyond G7 with healthy bronchiole diameter <4 mm. This aerosol-based breath test is promising to develop into an alternative or supplemental tool to the low-dose CT scanning for lung cancer screening.",2017,10.1016/j.compbiomed.2017.05.019,diagnosis,False
Multi-resolution convolutional networks for chest X-ray radiograph based lung nodule detection,"Lung cancer is the leading cause of cancer death worldwide. Early detection of lung cancer is helpful to provide the best possible clinical treatment for patients. Due to the limited number of radiologist and the huge number of chest x-ray radiographs (CXR) available for observation, a computer-aided detection scheme should be developed to assist radiologists in decision-making. While deep learning showed state-of-the-art performance in several computer vision applications, it has not been used for lung nodule detection on CXR. In this paper, a deep learning-based lung nodule detection method was proposed. We employed patch-based multi-resolution convolutional networks to extract the features and employed four different fusion methods for classification. The proposed method shows much better performance and is much more robust than those previously reported researches. For publicly available Japanese Society of Radiological Technology (JSRT) database, more than 99% of lung nodules can be detected when the false positives per image (FPs/image) was 0.2. The FAUC and R-CPM of the proposed method were 0.982 and 0.987, respectively. The proposed approach has the potential of applications in clinical practice.",2020,10.1016/j.artmed.2019.101744,diagnosis,False
Multi-scale gradual integration CNN for false positive reduction in pulmonary nodule detection,"Lung cancer is a global and dangerous disease, and its early detection is crucial for reducing the risks of mortality. In this regard, it has been of great interest in developing a computer-aided system for pulmonary nodules detection as early as possible on thoracic CT scans. In general, a nodule detection system involves two steps: (i) candidate nodule detection at a high sensitivity, which captures many false positives and (ii) false positive reduction from candidates. However, due to the high variation of nodule morphological characteristics and the possibility of mistaking them for neighboring organs, candidate nodule detection remains a challenge. In this study, we propose a novel Multi-scale Gradual Integration Convolutional Neural Network (MGI-CNN), designed with three main strategies: (1) to use multi-scale inputs with different levels of contextual information, (2) to use abstract information inherent in different input scales with gradual integration, and (3) to learn multi-stream feature integration in an end-to-end manner. To verify the efficacy of the proposed network, we conducted exhaustive experiments on the LUNA16 challenge datasets by comparing the performance of the proposed method with state-of-the-art methods in the literature. On two candidate subsets of the LUNA16 dataset, i.e., V1 and V2, our method achieved an average CPM of 0.908 (V1) and 0.942 (V2), outperforming comparable methods by a large margin. Our MGI-CNN is implemented in Python using TensorFlow and the source code is available from https://github.com/ku-milab/MGICNN.",2019,10.1016/j.neunet.2019.03.003,diagnosis,True
Multi-task deep learning based CT imaging analysis for COVID-19 pneumonia: Classification and segmentation,"This paper presents an automatic classification segmentation tool for helping screening COVID-19 pneumonia using chest CT imaging. The segmented lesions can help to assess the severity of pneumonia and follow-up the patients. In this work, we propose a new multitask deep learning model to jointly identify COVID-19 patient and segment COVID-19 lesion from chest CT images. Three learning tasks: segmentation, classification and reconstruction are jointly performed with different datasets. Our motivation is on the one hand to leverage useful information contained in multiple related tasks to improve both segmentation and classification performances, and on the other hand to deal with the problems of small data because each task can have a relatively small dataset. Our architecture is composed of a common encoder for disentangled feature representation with three tasks, and two decoders and a multi-layer perceptron for reconstruction, segmentation and classification respectively. The proposed model is evaluated and compared with other image segmentation techniques using a dataset of 1369 patients including 449 patients with COVID-19, 425 normal ones, 98 with lung cancer and 397 of different kinds of pathology. The obtained results show very encouraging performance of our method with a dice coefficient higher than 0.88 for the segmentation and an area under the ROC curve higher than 97% for the classification.",2020,10.1016/j.compbiomed.2020.104037,diagnosis,True
Multi-Task Deep Model With Margin Ranking Loss for Lung Nodule Analysis,"Lung cancer is the leading cause of cancer deaths worldwide and early diagnosis of lung nodule is of great importance for therapeutic treatment and saving lives. Automated lung nodule analysis requires both accurate lung nodule benign-malignant classification and attribute score regression. However, this is quite challenging due to the considerable difficulty of lung nodule heterogeneity modeling and the limited discrimination capability on ambiguous cases. To solve these challenges, we propose a Multi-Task deep model with Margin Ranking loss (referred as MTMR-Net) for automated lung nodule analysis. Compared to existing methods which consider these two tasks separately, the relatedness between lung nodule classification and attribute score regression is explicitly explored in a cause-and-effect manner within our multi-task deep model, which can contribute to the performance gains of both tasks. The results of different tasks can be yielded simultaneously for assisting the radiologists in diagnosis interpretation. Furthermore, a Siamese network with a margin ranking loss is elaborately designed to enhance the discrimination capability on ambiguous nodule cases. To further explore the internal relationship between two tasks and validate the effectiveness of the proposed model, we use the recursive feature elimination method to iteratively rank the most malignancy-related features. We validate the efficacy of our method MTMR-Net on the public benchmark LIDC-IDRI dataset. Extensive experiments show that the diagnosis results with internal relationship explicitly explored in our model has met some similar patterns in clinical usage and also demonstrate that our approach can achieve competitive classification performance and more accurate scoring on attributes over the state-of-the-arts. Codes are publicly available at: https://github.com/CaptainWilliam/MTMR-NET.",2020,10.1109/tmi.2019.2934577,diagnosis,True
Multi-view radiomics and dosiomics analysis with machine learning for predicting acute-phase weight loss in lung cancer patients treated with radiotherapy,"We propose a multi-view data analysis approach using radiomics and dosiomics (R&D) texture features for predicting acute-phase weight loss (WL) in lung cancer radiotherapy. Baseline weight of 388 patients who underwent intensity modulated radiation therapy (IMRT) was measured between one month prior to and one week after the start of IMRT. Weight change between one week and two months after the commencement of IMRT was analyzed, and dichotomized at 5% WL. Each patient had a planning CT and contours of gross tumor volume (GTV) and esophagus (ESO). A total of 355 features including clinical parameter (CP), GTV and ESO (GTV&ESO) dose-volume histogram (DVH), GTV radiomics, and GTV&ESO dosiomics features were extracted. R&D features were categorized as first- (L1), second- (L2), higher-order (L3) statistics, and three combined groups, L1 + L2, L2 + L3 and L1 + L2 + L3. Multi-view texture analysis was performed to identify optimal R&D input features. In the training set (194 earlier patients), feature selection was performed using Boruta algorithm followed by collinearity removal based on variance inflation factor. Machine-learning models were developed using Laplacian kernel support vector machine (lpSVM), deep neural network (DNN) and their averaged ensemble classifiers. Prediction performance was tested on an independent test set (194 more recent patients), and compared among seven different input conditions: CP-only, DVH-only, R&D-only, DVH + CP, R&D + CP, R&D + DVH and R&D + DVH + CP. Combined GTV L1 + L2 + L3 radiomics and GTV&ESO L3 dosiomics were identified as optimal input features, which achieved the best performance with an ensemble classifier (AUC = 0.710), having statistically significantly higher predictability compared with DVH and/or CP features (p < 0.05). When this performance was compared to that with full R&D-only features which reflect traditional single-view data, there was a statistically significant difference (p < 0.05). Using optimized multi-view R&D input features is beneficial for predicting early WL in lung cancer radiotherapy, leading to improved performance compared to using conventional DVH and/or CP features.",2020,10.1088/1361-6560/ab8531,prognosis,False
Multicenter analysis and a rapid screening model to predict early novel coronavirus pneumonia using a random forest algorithm,"Early determination of coronavirus disease 2019 (COVID-19) pneumonia from numerous suspected cases is critical for the early isolation and treatment of patients.The purpose of the study was to develop and validate a rapid screening model to predict early COVID-19 pneumonia from suspected cases using a random forest algorithm in China.A total of 914 initially suspected COVID-19 pneumonia in multiple centers were prospectively included. The computer-assisted embedding method was used to screen the variables. The random forest algorithm was adopted to build a rapid screening model based on the training set. The screening model was evaluated by the confusion matrix and receiver operating characteristic (ROC) analysis in the validation.The rapid screening model was set up based on 4 epidemiological features, 3 clinical manifestations, decreased white blood cell count and lymphocytes, and imaging changes on chest X-ray or computed tomography. The area under the ROC curve was 0.956, and the model had a sensitivity of 83.82% and a specificity of 89.57%. The confusion matrix revealed that the prospective screening model had an accuracy of 87.0% for predicting early COVID-19 pneumonia.Here, we developed and validated a rapid screening model that could predict early COVID-19 pneumonia with high sensitivity and specificity. The use of this model to screen for COVID-19 pneumonia have epidemiological and clinical significance.",2021,10.1097/md.0000000000026279,diagnosis,False
Multicenter Assessment of CT Pneumonia Analysis Prototype for Predicting Disease Severity and Patient Outcome,"To perform a multicenter assessment of the CT Pneumonia Analysis prototype for predicting disease severity and patient outcome in COVID-19 pneumonia both without and with integration of clinical information. Our IRB-approved observational study included consecutive 241 adult patients (> 18 years; 105 females; 136 males) with RT-PCR-positive COVID-19 pneumonia who underwent non-contrast chest CT at one of the two tertiary care hospitals (site A: Massachusetts General Hospital, USA; site B: Firoozgar Hospital Iran). We recorded patient age, gender, comorbid conditions, laboratory values, intensive care unit (ICU) admission, mechanical ventilation, and final outcome (recovery or death). Two thoracic radiologists reviewed all chest CTs to record type, extent of pulmonary opacities based on the percentage of lobe involved, and severity of respiratory motion artifacts. Thin-section CT images were processed with the prototype (Siemens Healthineers) to obtain quantitative features including lung volumes, volume and percentage of all-type and high-attenuation opacities (≥ -200 HU), and mean HU and standard deviation of opacities within a given lung region. These values are estimated for the total combined lung volume, and separately for each lung and each lung lobe. Multivariable analyses of variance (MANOVA) and multiple logistic regression were performed for data analyses. About 26% of chest CTs (62/241) had moderate to severe motion artifacts. There were no significant differences in the AUCs of quantitative features for predicting disease severity with and without motion artifacts (AUC 0.94-0.97) as well as for predicting patient outcome (AUC 0.7-0.77) (p > 0.5). Combination of the volume of all-attenuation opacities and the percentage of high-attenuation opacities (AUC 0.76-0.82, 95% confidence interval (CI) 0.73-0.82) had higher AUC for predicting ICU admission than the subjective severity scores (AUC 0.69-0.77, 95% CI 0.69-0.81). Despite a high frequency of motion artifacts, quantitative features of pulmonary opacities from chest CT can help differentiate patients with favorable and adverse outcomes.",2021,10.1007/s10278-021-00430-9,prognosis,True
Multiclass Classification of Chest X-Ray Images for the Prediction of COVID-19 Using Capsule Network,"It is critical to establish a reliable method for detecting people infected with COVID-19 since the pandemic has numerous harmful consequences worldwide. If the patient is infected with COVID-19, a chest X-ray can be used to determine this. In this work, an X-ray showing a COVID-19 infection is classified by the capsule neural network model we trained to recognise. 6310 chest X-ray pictures were used to train the models, separated into three categories: normal, pneumonia, and COVID-19. This work is considered an improved deep learning model for the classification of COVID-19 disease through X-ray images. Viewpoint invariance, fewer parameters, and better generalisation are some of the advantages of CapsNet compared with the classic convolutional neural network (CNN) models. The proposed model has achieved an accuracy greater than 95% during the model's training, which is better than the other state-of-the-art algorithms. Furthermore, to aid in detecting COVID-19 in a chest X-ray, the model could provide extra information.",2022,10.1155/2022/6185013,diagnosis,False
Multiclass Convolution Neural Network for Classification of COVID-19 CT Images,"In the late December of 2019, a novel coronavirus was discovered in Wuhan, China. In March 2020, WHO announced this epidemic had become a global pandemic and that the novel coronavirus may be mild to most people. However, some people may experience a severe illness that results in hospitalization or maybe death. COVID-19 classification remains challenging due to the ambiguity and similarity with other known respiratory diseases such as SARS, MERS, and other viral pneumonia. The typical symptoms of COVID-19 are fever, cough, chills, shortness of breath, loss of smell and taste, headache, sore throat, chest pains, confusion, and diarrhoea. This research paper suggests the concept of transfer learning using the deterministic algorithm in all binary classification models and evaluates the performance of various CNN architectures. The datasets of 746 CT images of COVID-19 and non-COVID-19 were divided for training, validation, and testing. Various augmentation techniques were applied to increase the number of datasets except for testing images. The images were then pretrained using CNN to obtain a binary class. ResNeXt101 and ResNet152 have the best F1 score of 0.978 and 0.938, whereas GoogleNet has an F1 score of 0.762. ResNeXt101 and ResNet152 have an accuracy of 97.81% and 93.80%. ResNeXt101, DenseNet201, and ResNet152 have 95.71%, 93.81%, and 90% sensitivity, whereas ResNeXt101, ResNet101, and ResNet152 have 100%, 99.58%, and 98.33 specificity, respectively.",2022,10.1155/2022/9167707,diagnosis,True
Multilevel Deep-Aggregated Boosted Network to Recognize COVID-19 Infection from Large-Scale Heterogeneous Radiographic Data,"In the present epidemic of the coronavirus disease 2019 (COVID-19), radiological imaging modalities, such as X-ray and computed tomography (CT), have been identified as effective diagnostic tools. However, the subjective assessment of radiographic examination is a time-consuming task and demands expert radiologists. Recent advancements in artificial intelligence have enhanced the diagnostic power of computer-aided diagnosis (CAD) tools and assisted medical specialists in making efficient diagnostic decisions. In this work, we propose an optimal multilevel deep-aggregated boosted network to recognize COVID-19 infection from heterogeneous radiographic data, including X-ray and CT images. Our method leverages multilevel deep-aggregated features and multistage training via a mutually beneficial approach to maximize the overall CAD performance. To improve the interpretation of CAD predictions, these multilevel deep features are visualized as additional outputs that can assist radiologists in validating the CAD results. A total of six publicly available datasets were fused to build a single large-scale heterogeneous radiographic collection that was used to analyze the performance of the proposed technique and other baseline methods. To preserve generality of our method, we selected different patient data for training, validation, and testing, and consequently, the data of same patient were not included in training, validation, and testing subsets. In addition, fivefold cross-validation was performed in all the experiments for a fair evaluation. Our method exhibits promising performance values of 95.38%, 95.57%, 92.53%, 98.14%, 93.16%, and 98.55% in terms of average accuracy, F-measure, specificity, sensitivity, precision, and area under the curve, respectively and outperforms various state-of-the-art methods.",2021,10.1109/jbhi.2021.3072076,diagnosis,True
Multiplanar analysis for pulmonary nodule classification in CT images using deep convolutional neural network and generative adversarial networks,"PURPOSE: Early detection and treatment of lung cancer holds great importance. However, pulmonary-nodule classification using CT images alone is difficult to realize. To address this concern, a method for pulmonary-nodule classification based on a deep convolutional neural network (DCNN) and generative adversarial networks (GAN) has previously been proposed by the authors. In that method, the said classification was performed exclusively using axial cross sections of pulmonary nodules. During actual medical-examination procedures, however, a comprehensive judgment can only be made via observation of various pulmonary-nodule cross sections. In the present study, a comprehensive analysis was performed by extending the application of the previously proposed DCNN- and GAN-based automatic classification method to multiple cross sections of pulmonary nodules. METHODS: Using the proposed method, CT images of 60 cases with confirmed pathological diagnosis by biopsy are analyzed. Firstly, multiplanar images of the pulmonary nodule are generated. Classification training was performed for three DCNNs. A certain pretraining was initially performed using GAN-generated nodule images. This was followed by fine-tuning of each pretrained DCNN using original nodule images provided as input. RESULTS: As a result of the evaluation, the specificity was 77.8% and the sensitivity was 93.9%. Additionally, the specificity was observed to have improved by 11.1% without any reduction in the sensitivity, compared to our previous report. CONCLUSION: This study reports development of a comprehensive analysis method to classify pulmonary nodules at multiple sections using GAN and DCNN. The effectiveness of the proposed discrimination method based on use of multiplanar images has been demonstrated to be improved compared to that realized in a previous study reported by the authors. In addition, the possibility of enhancing classification accuracy via application of GAN-generated images, instead of data augmentation, for pretraining even for medical datasets that contain relatively few images has also been demonstrated.",2020,10.1007/s11548-019-02092-z,diagnosis,True
Multiscale Attention Guided Network for COVID-19 Diagnosis Using Chest X-Ray Images,"Coronavirus disease 2019 (COVID-19) is one of the most destructive pandemic after millennium, forcing the world to tackle a health crisis. Automated lung infections classification using chest X-ray (CXR) images could strengthen diagnostic capability when handling COVID-19. However, classifying COVID-19 from pneumonia cases using CXR image is a difficult task because of shared spatial characteristics, high feature variation and contrast diversity between cases. Moreover, massive data collection is impractical for a newly emerged disease, which limited the performance of data thirsty deep learning models. To address these challenges, Multiscale Attention Guided deep network with Soft Distance regularization (MAG-SD) is proposed to automatically classify COVID-19 from pneumonia CXR images. In MAG-SD, MA-Net is used to produce prediction vector and attention from multiscale feature maps. To improve the robustness of trained model and relieve the shortage of training data, attention guided augmentations along with a soft distance regularization are posed, which aims at generating meaningful augmentations and reduce noise. Our multiscale attention model achieves better classification performance on our pneumonia CXR image dataset. Plentiful experiments are proposed for MAG-SD which demonstrates its unique advantage in pneumonia classification over cutting-edge models. The code is available at https://github.com/JasonLeeGHub/MAG-SD.",2021,10.1109/jbhi.2021.3058293,diagnosis,False
Multiscale CNN with compound fusions for false positive reduction in lung nodule detection,"Pulmonary lung nodules are often benign at the early stage but they could easily become malignant and metastasize to other locations in later stages. Morphological characteristics of these nodule instances vary largely in terms of their size, shape, and texture. There are also other co-existing lung anatomical structures such as lung walls and blood vessels surrounding these nodules resulting in complex contextual information. As a result, their early diagnosis to enable decisive intervention using Computer-Aided Diagnosis (CAD) systems face serious challenges, especially at low false positive rates. In this paper, we propose a new Convolutional Neural Network (CNN) architecture called Multiscale CNN with Compound Fusions (MCNN-CF) for this purpose which uses multiscale 3D patches as inputs and performs a fusion of intermediate features at two different depths of the network in two diverse fashions. The network is trained by a new iterative training procedure adapted to circumvent the class imbalance problem and obtained a Competitive Performance Metric (CPM) score of 0.948 when tested on the LUNA16 dataset. Experimental results illustrate the robustness of the proposed system which has increased the confidence of the prediction probabilities in the detection of the most variety of nodules.",2021,10.1016/j.artmed.2021.102017,diagnosis,True
Multiscale Mask R-CNN-Based Lung Tumor Detection Using PET Imaging,"Positron emission tomography (PET) imaging serves as one of the most competent methods for the diagnosis of various malignancies, such as lung tumor. However, with an elevation in the utilization of PET scan, radiologists are overburdened considerably. Consequently, a new approach of ""computer-aided diagnosis"" is being contemplated to curtail the heavy workloads. In this article, we propose a multiscale Mask Region-Based Convolutional Neural Network (Mask R-CNN)-based method that uses PET imaging for the detection of lung tumor. First, we produced 3 models of Mask R-CNN for lung tumor candidate detection. These 3 models were generated by fine-tuning the Mask R-CNN using certain training data that consisted of images from 3 different scales. Each of the training data set included 594 slices with lung tumor. These 3 models of Mask R-CNN models were then integrated using weighted voting strategy to diminish the false-positive outcomes. A total of 134 PET slices were employed as test set in this experiment. The precision, recall, and F score values of our proposed method were 0.90, 1, and 0.95, respectively. Experimental results exhibited strong conviction about the effectiveness of this method in detecting lung tumors, along with the capability of identifying a healthy chest pattern and reducing incorrect identification of tumors to a large extent.",2019,10.1177/1536012119863531,diagnosis,True
Natural history of pathologically confirmed pulmonary subsolid nodules with deep learning-assisted nodule segmentation,"OBJECTIVE: To explore the natural history of pulmonary subsolid nodules (SSNs) with different pathological types by deep learning-assisted nodule segmentation. METHODS: Between June 2012 and June 2019, 95 resected SSNs with preoperative long-term follow-up were enrolled in this retrospective study. SSN detection and segmentation were performed on preoperative follow-up CTs using the deep learning-based Dr. Wise system. SSNs were categorized into invasive adenocarcinoma (IAC, n = 47) and non-IAC (n = 48) groups; according to the interval change during the preoperative follow-up, SSNs were divided into growth (n = 68), nongrowth (n = 22), and new emergence (n = 5) groups. We analyzed the cumulative percentages and pattern of SSN growth and identified significant factors for IAC diagnosis and SSN growth. RESULTS: The mean preoperative follow-up was 42.1 ± 17.0 months. More SSNs showed growth or new emergence in the IAC than in the non-IAC group (89.4% vs. 64.6%, p = 0.009). Volume doubling time was non-significantly shorter for IACs than for non-IACs (1436.0 ± 1188.2 vs. 2087.5 ± 1799.7 days, p = 0.077). Median mass doubling time was significantly shorter for IACs than for non-IACs (821.7 vs. 1944.1 days, p = 0.001). Lobulated sign (p = 0.002) and SSN mass (p = 0.004) were significant factors for differentiating IACs. IACs showed significantly higher cumulative growth percentages than non-IACs in the first 70 months of follow-up. The growth pattern of SSNs may conform to the exponential model. The initial volume (p = 0.042) was a predictor for SSN growth. CONCLUSIONS: IACs appearing as SSNs showed an indolent course. The mean growth rate was larger for IACs than for non-IACs. SSNs with larger initial volume are more likely to grow. KEY POINTS: • Invasive adenocarcinomas (IACs) appearing as subsolid nodules (SSNs), with a mean volume doubling time (VDT) of 1436.0 ± 1188.2 days and median mass doubling time (MDT) of 821.7 days, showed an indolent course. • The VDT was shorter for IACs than for non-IACs (1436.0 ± 1188.2 vs. 2087.5 ± 1799.7 days), but the difference was not significant (p = 0.077). The median MDT was significantly shorter for IACs than for non-IACs (821.7 vs. 1944.1 days, p = 0.001). • SSNs with lobulated sign and larger mass (> 390.5 mg) may very likely be IACs. SSNs with larger initial volume are more likely to grow.",2021,10.1007/s00330-020-07450-z,treatment,True
Neural networks for nodal staging of non-small cell lung cancer with FDG PET and CT: importance of combining uptake values and sizes of nodes and primary tumor,"PURPOSE: To evaluate the effect of adding lymph node size to three previously explored artificial neural network (ANN) input parameters (primary tumor maximum standardized uptake value or tumor uptake, tumor size, and nodal uptake at N1, N2, and N3 stations) in the structure of the ANN. The goal was to allow the resulting ANN structure to relate lymph node uptake for size to primary tumor uptake for size in the determination of the status of nodes as human readers do. MATERIALS AND METHODS: This prospective study was approved by the institutional review board, and informed consent was obtained from all participants. The authors developed a back-propagation ANN with one hidden layer and eight processing units. The data set used to train the network included node and tumor size and uptake from 133 patients with non-small cell lung cancer with surgically proved N status. Statistical analysis was performed with the paired t test. RESULTS: The ANN correctly predicted the N stage in 99.2% of cases, compared with 72.4% for the expert reader (P < .001). In categorization of N0 and N1 versus N2 and N3 disease, the ANN performed with 99.2% accuracy versus 92.2% for the expert reader (P < .001). CONCLUSION: The ANN is 99.2% accurate in predicting surgical-pathologic nodal status with use of four fluorine 18 fluorodeoxyglucose (FDG) positron emission tomography (PET)/computed tomography (CT)-derived parameters. Malignant and benign inflammatory lymph nodes have overlapping appearances at FDG PET/CT but can be differentiated by ANNs when the crucial input of node size is used.",2014,10.1148/radiol.13122427,diagnosis,True
New machine learning method for image-based diagnosis of COVID-19,"COVID-19 is a worldwide epidemic, as announced by the World Health Organization (WHO) in March 2020. Machine learning (ML) methods can play vital roles in identifying COVID-19 patients by visually analyzing their chest x-ray images. In this paper, a new ML-method proposed to classify the chest x-ray images into two classes, COVID-19 patient or non-COVID-19 person. The features extracted from the chest x-ray images using new Fractional Multichannel Exponent Moments (FrMEMs). A parallel multi-core computational framework utilized to accelerate the computational process. Then, a modified Manta-Ray Foraging Optimization based on differential evolution used to select the most significant features. The proposed method evaluated using two COVID-19 x-ray datasets. The proposed method achieved accuracy rates of 96.09% and 98.09% for the first and second datasets, respectively.",2020,10.1371/journal.pone.0235187,diagnosis,False
Next-Generation Radiogenomics Sequencing for Prediction of EGFR and KRAS Mutation Status in NSCLC Patients Using Multimodal Imaging and Machine Learning Algorithms,"PURPOSE: Considerable progress has been made in the assessment and management of non-small cell lung cancer (NSCLC) patients based on mutation status in the epidermal growth factor receptor (EGFR) and Kirsten rat sarcoma viral oncogene (KRAS). At the same time, NSCLC management through KRAS and EGFR mutation profiling faces challenges. In the present work, we aimed to evaluate a comprehensive radiomics framework that enabled prediction of EGFR and KRAS mutation status in NSCLC patients based on radiomic features from low-dose computed tomography (CT), contrast-enhanced diagnostic quality CT (CTD), and positron emission tomography (PET) imaging modalities and use of machine learning algorithms. METHODS: Our study involved NSCLC patients including 150 PET, low-dose CT, and CTD images. Radiomic features from original and preprocessed (including 64 bin discretizing, Laplacian-of-Gaussian (LOG), and Wavelet) images were extracted. Conventional clinically used standard uptake value (SUV) parameters and metabolic tumor volume (MTV) were also obtained from PET images. Highly correlated features were pre-eliminated, and false discovery rate (FDR) correction was performed with the resulting q-values reported for univariate analysis. Six feature selection methods and 12 classifiers were then used for multivariate prediction of gene mutation status (provided by polymerase chain reaction (PCR)) in patients. We performed 10-fold cross-validation for model tuning to improve robustness, and our developed models were assessed on an independent validation set with 68 patients (common in all three imaging modalities). The average area under the receiver operator characteristic curve (AUC) was utilized for performance evaluation. RESULTS: The best predictive power for conventional PET parameters was achieved by SUV(peak) (AUC 0.69, p value = 0.0002) and MTV (AUC 0.55, p value = 0.0011) for EGFR and KRAS, respectively. Univariate analysis of extracted radiomics features improved AUC performance to 0.75 (q-value 0.003, Short-Run Emphasis feature of GLRLM from LOG preprocessed image of PET with sigma value 1.5) and 0.71 (q-value 0.00005, Large Dependence Low Gray-Level Emphasis feature of GLDM in LOG preprocessed image of CTD with sigma value 5) for EGFR and KRAS, respectively. Furthermore, multivariate machine learning-based AUC performances were significantly improved to 0.82 for EGFR (LOG preprocessed image of PET with sigma 3 with variance threshold (VT) feature selector and stochastic gradient descent (SGD) classifier (q-value = 4.86E-05) and 0.83 for KRAS (LOG preprocessed image of CT with sigma 3.5 with select model (SM) feature selector and SGD classifier (q-value = 2.81E-09). CONCLUSION: Our work demonstrated that non-invasive and reliable radiomics analysis can be successfully used to predict EGFR and KRAS mutation status in NSCLC patients. We demonstrated that radiomic features extracted from different image-feature sets could be used for EGFR and KRAS mutation status prediction in NSCLC patients and showed improved predictive power relative to conventional image-derived metrics.",2020,10.1007/s11307-020-01487-8,diagnosis,True
No Surprises: Training Robust Lung Nodule Detection for Low-Dose CT Scans by Augmenting With Adversarial Attacks,"Detecting malignant pulmonary nodules at an early stage can allow medical interventions which may increase the survival rate of lung cancer patients. Using computer vision techniques to detect nodules can improve the sensitivity and the speed of interpreting chest CT for lung cancer screening. Many studies have used CNNs to detect nodule candidates. Though such approaches have been shown to outperform the conventional image processing based methods regarding the detection accuracy, CNNs are also known to be limited to generalize on under-represented samples in the training set and prone to imperceptible noise perturbations. Such limitations can not be easily addressed by scaling up the dataset or the models. In this work, we propose to add adversarial synthetic nodules and adversarial attack samples to the training data to improve the generalization and the robustness of the lung nodule detection systems. To generate hard examples of nodules from a differentiable nodule synthesizer, we use projected gradient descent (PGD) to search the latent code within a bounded neighbourhood that would generate nodules to decrease the detector response. To make the network more robust to unanticipated noise perturbations, we use PGD to search for noise patterns that can trigger the network to give over-confident mistakes. By evaluating on two different benchmark datasets containing consensus annotations from three radiologists, we show that the proposed techniques can improve the detection performance on real CT data. To understand the limitations of both the conventional networks and the proposed augmented networks, we also perform stress-tests on the false positive reduction networks by feeding different types of artificially produced patches. We show that the augmented networks are more robust to both under-represented nodules as well as resistant to noise perturbations.",2021,10.1109/tmi.2020.3026261,diagnosis,True
Non-invasive classification of non-small cell lung cancer: a comparison between random forest models utilising radiomic and semantic features,"OBJECTIVE: Non-invasive distinction between squamous cell carcinoma and adenocarcinoma subtypes of non-small-cell lung cancer (NSCLC) may be beneficial to patients unfit for invasive diagnostic procedures or when tissue is insufficient for diagnosis. The purpose of our study was to compare the performance of random forest algorithms utilizing CT radiomics and/or semantic features in classifying NSCLC. METHODS: Two thoracic radiologists scored 11 semantic features on CT scans of 106 patients with NSCLC. A set of 115 radiomics features was extracted from the CT scans. Random forest models were developed from semantic (RM-sem), radiomics (RM-rad), and all features combined (RM-all). External validation of models was performed using an independent test data set (n = 100) of CT scans. Model performance was measured with out-of-bag error and area under curve (AUC), and compared using receiver-operating characteristics curve analysis on the test data set. RESULTS: The median (interquartile-range) error rates of the models were: RF-sem 24.5 % (22.6 - 37.5 %), RF-rad 35.8 % (34.9 - 38.7 %), and RM-all 37.7 % (37.7 - 37.7). On training data, both RF-rad and RF-all gave perfect discrimination (AUC = 1), which was significantly higher than that achieved by RF-sem (AUC = 0.78; p < 0.0001). On test data, however, RM-sem model (AUC = 0.82) out-performed RM-rad and RM-all (AUC = 0.5 and AUC = 0.56; p < 0.0001), neither of which was significantly different from random guess ( p = 0.9 and 0.6 respectively). CONCLUSION: Non-invasive classification of NSCLC can be done accurately using random forest classification models based on well-known CT-derived descriptive features. However, radiomics-based classification models performed poorly in this scenario when tested on independent data and should be used with caution, due to their possible lack of generalizability to new data. ADVANCES IN KNOWLEDGE: Our study describes novel CT-derived random forest models based on radiologist-interpretation of CT scans (semantic features) that can assist NSCLC classification when histopathology is equivocal or when histopathological sampling is not possible. It also shows that random forest models based on semantic features may be more useful than those built from computational radiomic features.",2019,10.1259/bjr.20190159,diagnosis,True
Non-invasive decision support for NSCLC treatment using PET/CT radiomics,"Two major treatment strategies employed in non-small cell lung cancer, NSCLC, are tyrosine kinase inhibitors, TKIs, and immune checkpoint inhibitors, ICIs. The choice of strategy is based on heterogeneous biomarkers that can dynamically change during therapy. Thus, there is a compelling need to identify comprehensive biomarkers that can be used longitudinally to help guide therapy choice. Herein, we report a (18)F-FDG-PET/CT-based deep learning model, which demonstrates high accuracy in EGFR mutation status prediction across patient cohorts from different institutions. A deep learning score (EGFR-DLS) was significantly and positively associated with longer progression free survival (PFS) in patients treated with EGFR-TKIs, while EGFR-DLS is significantly and negatively associated with higher durable clinical benefit, reduced hyperprogression, and longer PFS among patients treated with ICIs. Thus, the EGFR-DLS provides a non-invasive method for precise quantification of EGFR mutation status in NSCLC patients, which is promising to identify NSCLC patients sensitive to EGFR-TKI or ICI-treatments.",2020,10.1038/s41467-020-19116-x,diagnosis,True
Non-Invasive Measurement Using Deep Learning Algorithm Based on Multi-Source Features Fusion to Predict PD-L1 Expression and Survival in NSCLC,"BACKGROUND: Programmed death-ligand 1 (PD-L1) assessment of lung cancer in immunohistochemical assays was only approved diagnostic biomarker for immunotherapy. But the tumor proportion score (TPS) of PD-L1 was challenging owing to invasive sampling and intertumoral heterogeneity. There was a strong demand for the development of an artificial intelligence (AI) system to measure PD-L1 expression signature (ES) non-invasively. METHODS: We developed an AI system using deep learning (DL), radiomics and combination models based on computed tomography (CT) images of 1,135 non-small cell lung cancer (NSCLC) patients with PD-L1 status. The deep learning feature was obtained through a 3D ResNet as the feature map extractor and the specialized classifier was constructed for the prediction and evaluation tasks. Then, a Cox proportional-hazards model combined with clinical factors and PD-L1 ES was utilized to evaluate prognosis in survival cohort. RESULTS: The combination model achieved a robust high-performance with area under the receiver operating characteristic curves (AUCs) of 0.950 (95% CI, 0.938-0.960), 0.934 (95% CI, 0.906-0.964), and 0.946 (95% CI, 0.933-0.958), for predicting PD-L1ES <1%, 1-49%, and ≥50% in validation cohort, respectively. Additionally, when combination model was trained on multi-source features the performance of overall survival evaluation (C-index: 0.89) could be superior compared to these of the clinical model alone (C-index: 0.86). CONCLUSION: A non-invasive measurement using deep learning was proposed to access PD-L1 expression and survival outcomes of NSCLC. This study also indicated that deep learning model combined with clinical characteristics improved prediction capabilities, which would assist physicians in making rapid decision on clinical treatment options.",2022,10.3389/fimmu.2022.828560,prognosis,True
Non-small cell lung cancer: quantitative phenotypic analysis of CT images as a potential marker of prognosis,"This was a retrospective study to investigate the predictive and prognostic ability of quantitative computed tomography phenotypic features in patients with non-small cell lung cancer (NSCLC). 661 patients with pathological confirmed as NSCLC were enrolled between 2007 and 2014. 592 phenotypic descriptors was automatically extracted on the pre-therapy CT images. Firstly, support vector machine (SVM) was used to evaluate the predictive value of each feature for pathology and TNM clinical stage. Secondly, Cox proportional hazards model was used to evaluate the prognostic value of these imaging signatures selected by SVM which subjected to a primary cohort of 138 patients, and an external independent validation of 61 patients. The results indicated that predictive accuracy for histopathology, N staging, and overall clinical stage was 75.16%, 79.40% and 80.33%, respectively. Besides, Cox models indicated the signatures selected by SVM: ""correlation of co-occurrence after wavelet transform"" was significantly associated with overall survival in the two datasets (hazard ratio [HR]: 1.65, 95% confidence interval [CI]: 1.41-2.75, p = 0.010; and HR: 2.74, 95%CI: 1.10-6.85, p = 0.027, respectively). Our study indicates that the phenotypic features might provide some insight in metastatic potential or aggressiveness for NSCLC, which potentially offer clinical value in directing personalized therapeutic regimen selection for NSCLC.",2016,10.1038/srep38282,diagnosis,True
Novel ensemble of optimized CNN and dynamic selection techniques for accurate Covid-19 screening using chest CT images,"The world is significantly affected by infectious coronavirus disease (covid-19). Timely prognosis and treatment are important to control the spread of this infection. Unreliable screening systems and limited number of clinical facilities are the major hurdles in controlling the spread of covid-19. Nowadays, many automated detection systems based on deep learning techniques using computed tomography (CT) images have been proposed to detect covid-19. However, these systems have the following drawbacks: (i) limited data problem poses a major hindrance to train the deep neural network model to provide accurate diagnosis, (ii) random choice of hyperparameters of Convolutional Neural Network (CNN) significantly affects the classification performance, since the hyperparameters have to be application dependent and, (iii) the generalization ability using CNN classification is usually not validated. To address the aforementioned issues, we propose two models: (i) based on a transfer learning approach, and (ii) using novel strategy to optimize the CNN hyperparameters using Whale optimization-based BAT algorithm + AdaBoost classifier built using dynamic ensemble selection techniques. According to our second method depending on the characteristics of test sample, the classifier is chosen, thereby reducing the risk of overfitting and simultaneously produced promising results. Our proposed methodologies are developed using 746 CT images. Our method obtained a sensitivity, specificity, accuracy, F-1 score, and precision of 0.98, 0.97, 0.98, 0.98, and 0.98, respectively with five-fold cross-validation strategy. Our developed prototype is ready to be tested with huge chest CT images database before its real-world application.",2021,10.1016/j.compbiomed.2021.104835,diagnosis,True
Novel loss functions for ensemble-based medical image classification,"Medical images commonly exhibit multiple abnormalities. Predicting them requires multi-class classifiers whose training and desired reliable performance can be affected by a combination of factors, such as, dataset size, data source, distribution, and the loss function used to train deep neural networks. Currently, the cross-entropy loss remains the de-facto loss function for training deep learning classifiers. This loss function, however, asserts equal learning from all classes, leading to a bias toward the majority class. Although the choice of the loss function impacts model performance, to the best of our knowledge, we observed that no literature exists that performs a comprehensive analysis and selection of an appropriate loss function toward the classification task under study. In this work, we benchmark various state-of-the-art loss functions, critically analyze model performance, and propose improved loss functions for a multi-class classification task. We select a pediatric chest X-ray (CXR) dataset that includes images with no abnormality (normal), and those exhibiting manifestations consistent with bacterial and viral pneumonia. We construct prediction-level and model-level ensembles to improve classification performance. Our results show that compared to the individual models and the state-of-the-art literature, the weighted averaging of the predictions for top-3 and top-5 model-level ensembles delivered significantly superior classification performance (p < 0.05) in terms of MCC (0.9068, 95% confidence interval (0.8839, 0.9297)) metric. Finally, we performed localization studies to interpret model behavior and confirm that the individual models and ensembles learned task-specific features and highlighted disease-specific regions of interest. The code is available at https://github.com/sivaramakrishnan-rajaraman/multiloss_ensemble_models.",2021,10.1371/journal.pone.0261307,diagnosis,False
"Novel, non-invasive imaging approach to identify patients with advanced non-small cell lung cancer at risk of hyperprogressive disease with immune checkpoint blockade","PURPOSE: Hyperprogression is an atypical response pattern to immune checkpoint inhibition that has been described within non-small cell lung cancer (NSCLC). The paradoxical acceleration of tumor growth after immunotherapy has been associated with significantly shortened survival, and currently, there are no clinically validated biomarkers to identify patients at risk of hyperprogression. EXPERIMENTAL DESIGN: A total of 109 patients with advanced NSCLC who underwent monotherapy with Programmed cell death protein-1 (PD1)/Programmed death-ligand-1 (PD-L1) inhibitors were included in the study. Using RECIST measurements, we divided the patients into responders (n=50) (complete/partial response or stable disease) and non-responders (n=59) (progressive disease). Tumor growth kinetics were used to further identify hyperprogressors (HPs, n=19) among non-responders. Patients were randomized into a training set (D(1)=30) and a test set (D(2)=79) with the essential caveat that HPs were evenly distributed among the two sets. A total of 198 radiomic textural patterns from within and around the target nodules and features relating to tortuosity of the nodule associated vasculature were extracted from the pretreatment CT scans. RESULTS: The random forest classifier using the top features associated with hyperprogression was able to distinguish between HP and other radiographical response patterns with an area under receiver operating curve of 0.85±0.06 in the training set (D(1)=30) and 0.96 in the validation set (D(2)=79). These features included one peritumoral texture feature from 5 to 10 mm outside the tumor and two nodule vessel-related tortuosity features. Kaplan-Meier survival curves showed a clear stratification between classifier predicted HPs versus non-HPs for overall survival (D(2): HR=2.66, 95% CI 1.27 to 5.55; p=0.009). CONCLUSIONS: Our study suggests that image-based radiomics markers extracted from baseline CTs of advanced NSCLC treated with PD-1/PD-L1 inhibitors may help identify patients at risk of hyperprogressions.",2020,10.1136/jitc-2020-001343,treatment,True
NSCR-Based DenseNet for Lung Tumor Recognition Using Chest CT Image,"Nonnegative sparse representation has become a popular methodology in medical analysis and diagnosis in recent years. In order to resolve network degradation, higher dimensionality in feature extraction, data redundancy, and other issues faced when medical images parameters are trained using convolutional neural networks. Lung tumors in chest CT image based on nonnegative, sparse, and collaborative representation classification of DenseNet (DenseNet-NSCR) are proposed by this paper: firstly, initialization parameters of pretrained DenseNet model using transfer learning; secondly, training DenseNet using CT images to extract feature vectors for the full connectivity layer; thirdly, a nonnegative, sparse, and collaborative representation (NSCR) is used to represent the feature vector and solve the coding coefficient matrix; fourthly, the residual similarity is used for classification. The experimental results show that the DenseNet-NSCR classification is better than the other models, and the various evaluation indexes such as specificity and sensitivity are also high, and the method has better robustness and generalization ability through comparison experiment using AlexNet, GoogleNet, and DenseNet-201 models.",2020,10.1155/2020/6636321,diagnosis,True
Objective evaluation of deep uncertainty predictions for COVID-19 detection,"Deep neural networks (DNNs) have been widely applied for detecting COVID-19 in medical images. Existing studies mainly apply transfer learning and other data representation strategies to generate accurate point estimates. The generalization power of these networks is always questionable due to being developed using small datasets and failing to report their predictive confidence. Quantifying uncertainties associated with DNN predictions is a prerequisite for their trusted deployment in medical settings. Here we apply and evaluate three uncertainty quantification techniques for COVID-19 detection using chest X-Ray (CXR) images. The novel concept of uncertainty confusion matrix is proposed and new performance metrics for the objective evaluation of uncertainty estimates are introduced. Through comprehensive experiments, it is shown that networks pertained on CXR images outperform networks pretrained on natural image datasets such as ImageNet. Qualitatively and quantitatively evaluations also reveal that the predictive uncertainty estimates are statistically higher for erroneous predictions than correct predictions. Accordingly, uncertainty quantification methods are capable of flagging risky predictions with high uncertainty estimates. We also observe that ensemble methods more reliably capture uncertainties during the inference. DNN-based solutions for COVID-19 detection have been mainly proposed without any principled mechanism for risk mitigation. Previous studies have mainly focused on on generating single-valued predictions using pretrained DNNs. In this paper, we comprehensively apply and comparatively evaluate three uncertainty quantification techniques for COVID-19 detection using chest X-Ray images. The novel concept of uncertainty confusion matrix is proposed and new performance metrics for the objective evaluation of uncertainty estimates are introduced for the first time. Using these new uncertainty performance metrics, we quantitatively demonstrate when we could trust DNN predictions for COVID-19 detection from chest X-rays. It is important to note the proposed novel uncertainty evaluation metrics are generic and could be applied for evaluation of probabilistic forecasts in all classification problems.",2022,10.1038/s41598-022-05052-x,diagnosis,False
On the robustness of deep learning-based lung-nodule classification for CT images with respect to image noise,"Robustness is an important aspect when evaluating a method of medical image analysis. In this study, we investigated the robustness of a deep learning (DL)-based lung-nodule classification model for CT images with respect to noise perturbations. A deep neural network (DNN) was established to classify 3D CT images of lung nodules into malignant or benign groups. The established DNN was able to predict malignancy rate of lung nodules based on CT images, achieving the area under the curve of 0.91 for the testing dataset in a tenfold cross validation as compared to radiologists' prediction. We then evaluated its robustness against noise perturbations. We added to the input CT images noise signals generated randomly or via an optimization scheme using a realistic noise model based on a noise power spectrum for a given mAs level, and monitored the DNN's output. The results showed that the CT noise was able to affect the prediction results of the established DNN model. With random noise perturbations at 100 mAs, DNN's predictions for 11.2% of training data and 17.4% of testing data were successfully altered by at least once. The percentage increased to 23.4% and 34.3%, respectively, for optimization-based perturbations. We further evaluated robustness of models with different architectures, parameters, number of output labels, etc, and robustness concern was found in these models to different degrees. To improve model robustness, we empirically proposed an adaptive training scheme. It fine-tuned the DNN model by including perturbations in the training dataset that successfully altered the DNN's perturbations. The adaptive scheme was repeatedly performed to gradually improve DNN's robustness. The numbers of perturbations at 100 mAs affecting DNN's predictions were reduced to 10.8% for training and 21.1% for testing by the adaptive training scheme after two iterations. Our study illustrated that robustness may potentially be a concern for an exemplary DL-based lung-nodule classification model for CT images, indicating the needs for evaluating and ensuring model robustness when developing similar models. The proposed adaptive training scheme may be able to improve model robustness.",2020,10.1088/1361-6560/abc812,diagnosis,True
On the Use of Deep Learning for Imaging-Based COVID-19 Detection Using Chest X-rays,"The global COVID-19 pandemic that started in 2019 and created major disruptions around the world demonstrated the imperative need for quick, inexpensive, accessible and reliable diagnostic methods that would allow the detection of infected individuals with minimal resources. Radiography, and more specifically, chest radiography, is a relatively inexpensive medical imaging modality that can potentially offer a solution for the diagnosis of COVID-19 cases. In this work, we examined eleven deep convolutional neural network architectures for the task of classifying chest X-ray images as belonging to healthy individuals, individuals with COVID-19 or individuals with viral pneumonia. All the examined networks are established architectures that have been proven to be efficient in image classification tasks, and we evaluated three different adjustments to modify the architectures for the task at hand by expanding them with additional layers. The proposed approaches were evaluated for all the examined architectures on a dataset with real chest X-ray images, reaching the highest classification accuracy of 98.04% and the highest F1-score of 98.22% for the best-performing setting.",2021,10.3390/s21175702,diagnosis,False
One deep learning local-global model based on CT imaging to differentiate between nodular cryptococcosis and lung cancer which are hard to be diagnosed,"OBJECTIVES: We aim to evaluate a deep learning (DL) model and radiomic model for preoperative differentiation of nodular cryptococcosis from solitary lung cancer in patients with malignant features on CT images. MATERIALS AND METHODS: We retrospectively recruited 319 patients with solitary pulmonary nodules and suspicious signs of malignancy from three hospitals. All lung nodules were resected, and one by one radiologic-pathologic correlation was performed. A three-dimensional DL model was used for tumor segmentation and extraction of three-dimensional radiomic features. We used the Max-Relevance and Min-Redundancy algorithm and the eXtreme Gradient Boosting algorithm to select the nodular radiomics features. We proposed a DL local-global model, a DL local model and radiomic model to preoperatively differentiate nodular cryptococcosis from solitary lung cancer. The DL local-global model includes information of both nodules and the whole lung, while the DL local model only includes information of solitary lung nodules. Five-fold cross-validation was used to select and validate these models. The prediction performance of the model was evaluated using receiver operating characteristic curve (ROC) and calibration curve. A new loss function was applied in our deep learning framework to optimize the area under the ROC curve (AUC) directly. RESULTS: 295 patients were enrolled and they were non-symptomatic, with negative tumor markers and fungus markers in blood tests. These patients have not been diagnosed by the combination of CT imaging, laboratory results and clinical data. The lung volume was slightly larger in patients with lung cancers than that in patients with cryptococcosis (3552.8 ± 1184.6 ml vs 3491.9 ± 1017.8 ml). The DL local-global model achieved the best performance in differentiating between nodular cryptococcosis and lung cancer (area under the curve [AUC] = 0.88), which was higher than that of the DL local model (AUC = 0.84) and radiomic (AUC = 0.79) model. CONCLUSION: The DL local-global model is a non-invasive diagnostic tool to differentiate between nodular cryptococcosis and lung cancer nodules which are hard to be diagnosed by the combination of CT imaging, laboratory results and clinical data, and overtreatment may be avoided.",2021,10.1016/j.compmedimag.2021.102009,diagnosis,True
One-stage pulmonary nodule detection using 3-D DCNN with feature fusion and attention mechanism in CT image,"BACKGROUND AND OBJECTIVE: Lung cancer is the most common cause of cancer-related death in the world. Low-dose computed tomography (LDCT) is a widely used modality in lung cancer detection. The nodule is an abnormal tissue and may evolve into lung cancer. Hence, it is crucial to detect nodules in the early detection stage. However, reviewing the LDCT scans to observe suspicious nodules is a time-consuming task. Recently, designing a computer-aided detection (CADe) system with convolutional neural network (CNN) architecture has been proven that it is helpful for radiologists. Hence, in this study, a 3-D YOLO-based CADe system, 3-D OSAF-YOLOv3, is proposed for nodule detection in LDCT images. METHODS: The proposed CADe system consists of data preprocessing, nodule detection, and non-maximum suppression algorithm (NMS). At first, the data preprocessing including the background elimination, the spacing normalization, and the volume of interest (VOI) extraction, are conducted to remove the non-lung region, normalize the image spacing, and divide LDCT image into numerous VOIs. Then, the VOIs are fed into the 3-D OSAF-YOLOv3 model, to detect the suspicious nodules. The proposed model is constructed by integrating the 3-D YOLOv3 with the one-shot aggregation module (OSA), the receptive field block (RFB), and the feature fusion scheme (FFS). Finally, the NMS algorithm is performed to eliminate the duplicated detection generated by the model. RESULTS: In this study, the LUNA-16 dataset composed 1186 nodules from 888 LDCT scans and the competition performance metric (CPM) are used to evaluate our CADe system. In the experiment results, the proposed system can achieve a sensitivities rate of 0.962 with the false positive rate of 8 and complete a CPM value of 0.905. Moreover, according to the ablation study results, the employment of OSA module, RFB, and FFS could improve the detection performance actually. Furthermore, compared to other start-of-the-art (SOTA) models, our detection system could also achieve the higher performance. CONCLUSIONS: In this study, a YOLO-based CADe system for nodule detection in CT image system integrating additional modules and scheme is proposed for nodule detection in LDCT. The result indicates that the proposed the modification can significantly improve detection performance.",2022,10.1016/j.cmpb.2022.106786,diagnosis,True
Open resource of clinical data from patients with pneumonia for the prediction of COVID-19 outcomes via deep learning,"Data from patients with coronavirus disease 2019 (COVID-19) are essential for guiding clinical decision making, for furthering the understanding of this viral disease, and for diagnostic modelling. Here, we describe an open resource containing data from 1,521 patients with pneumonia (including COVID-19 pneumonia) consisting of chest computed tomography (CT) images, 130 clinical features (from a range of biochemical and cellular analyses of blood and urine samples) and laboratory-confirmed severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) clinical status. We show the utility of the database for prediction of COVID-19 morbidity and mortality outcomes using a deep learning algorithm trained with data from 1,170 patients and 19,685 manually labelled CT slices. In an independent validation cohort of 351 patients, the algorithm discriminated between negative, mild and severe cases with areas under the receiver operating characteristic curve of 0.944, 0.860 and 0.884, respectively. The open database may have further uses in the diagnosis and management of patients with COVID-19.",2020,10.1038/s41551-020-00633-5,prognosis,True
Optical coherence tomography for identification of malignant pulmonary nodules based on random forest machine learning algorithm,"OBJECTIVE: To explore the feasibility of using random forest (RF) machine learning algorithm in assessing normal and malignant peripheral pulmonary nodules based on in vivo endobronchial optical coherence tomography (EB-OCT). METHODS: A total of 31 patients with pulmonary nodules were admitted to Department of Respiratory Medicine, Zhongda Hospital, Southeast University, and underwent chest CT, EB-OCT and biopsy. Attenuation coefficient and up to 56 different image features were extracted from A-line and B-scan of 1703 EB-OCT images. Attenuation coefficient and 29 image features with significant p-values were used to analyze the differences between normal and malignant samples. A RF classifier was trained using 70% images as training set, while 30% images were included in the testing set. The accuracy of the automated classification was validated by clinically proven pathological results. RESULTS: Attenuation coefficient and 29 image features were found to present different properties with significant p-values between normal and malignant EB-OCT images. The RF algorithm successfully classified the malignant pulmonary nodules with sensitivity, specificity, and accuracy of 90.41%, 77.87% and 83.51% respectively. CONCLUSION: It is clinically practical to distinguish the nature of pulmonary nodules by integrating EB-OCT imaging with automated machine learning algorithm. Diagnosis of malignant pulmonary nodules by analyzing quantitative features from EB-OCT images could be a potentially powerful way for early detection of lung cancer.",2021,10.1371/journal.pone.0260600,diagnosis,True
Optimal Deep-Learning-Enabled Intelligent Decision Support System for SARS-CoV-2 Classification,"Intelligent decision support systems (IDSS) for complex healthcare applications aim to examine a large quantity of complex healthcare data to assist doctors, researchers, pathologists, and other healthcare professionals. A decision support system (DSS) is an intelligent system that provides improved assistance in various stages of health-related disease diagnosis. At the same time, the SARS-CoV-2 infection that causes COVID-19 disease has spread globally from the beginning of 2020. Several research works reported that the imaging pattern based on computed tomography (CT) can be utilized to detect SARS-CoV-2. Earlier identification and detection of the diseases is essential to offer adequate treatment and avoid the severity of the disease. With this motivation, this study develops an efficient deep-learning-based fusion model with swarm intelligence (EDLFM-SI) for SARS-CoV-2 identification. The proposed EDLFM-SI technique aims to detect and classify the SARS-CoV-2 infection or not. Also, the EDLFM-SI technique comprises various processes, namely, data augmentation, preprocessing, feature extraction, and classification. Moreover, a fusion of capsule network (CapsNet) and MobileNet based feature extractors are employed. Besides, a water strider algorithm (WSA) is applied to fine-tune the hyperparameters involved in the DL models. Finally, a cascaded neural network (CNN) classifier is applied for detecting the existence of SARS-CoV-2. In order to showcase the improved performance of the EDLFM-SI technique, a wide range of simulations take place on the COVID-19 CT data set and the SARS-CoV-2 CT scan data set. The simulation outcomes highlighted the supremacy of the EDLFM-SI technique over the recent approaches.",2022,10.1155/2022/4130674,diagnosis,True
Optimal Diagnosis of COVID-19 Based on Convolutional Neural Network and Red Fox Optimization Algorithm,"SARS-CoV-2 is a specific type of Coronavirus that was firstly reported in China in December 2019 and is the causative agent of coronavirus disease 2019 (COVID-19). In March 2020, this disease spread to different parts of the world causing a global pandemic. Although this disease is still increasing exponentially day by day, early diagnosis of this disease is very important to reduce the death rate and to reduce the prevalence of this pandemic. Since there are sometimes human errors by physicians in the diagnosis of this disease, using computer-aided diagnostic systems can be helpful to get more accurate results. In this paper, chest X-ray images have been examined using a new pipeline machine vision-based system to provide more accurate results. In the proposed method, after preprocessing the input X-ray images, the region of interest has been segmented. Then, a combined gray-level cooccurrence matrix (GLCM) and Discrete Wavelet Transform (DWT) features have been extracted from the processed images. Finally, an improved version of Convolutional Neural Network (CNN) based on the Red Fox Optimization algorithm is employed for the classification of the images based on the features. The proposed method is validated by performing to three datasets and its results are compared with some state-of-the-art methods. The final results show that the suggested method has proper efficiency toward the others for the diagnosis of COVID-19.",2021,10.1155/2021/4454507,diagnosis,False
Optimal matrix size of chest radiographs for computer-aided detection on lung nodule or mass with deep learning,"OBJECTIVES: To investigate the optimal input matrix size for deep learning-based computer-aided detection (CAD) of nodules and masses on chest radiographs. METHODS: We retrospectively collected 2088 abnormal (nodule/mass) and 352 normal chest radiographs from two institutions. Three thoracic radiologists drew 2758 abnormalities regions. A total of 1736 abnormal chest radiographs were used for training and tuning convolutional neural networks (CNNs). The remaining 352 abnormal and 352 normal chest radiographs were used as a test set. Two CNNs (Mask R-CNN and RetinaNet) were selected to validate the effects of the squared different matrix size of chest radiograph (256, 448, 896, 1344, and 1792). For comparison, figure of merit (FOM) of jackknife free-response receiver operating curve and sensitivity were obtained. RESULTS: In Mask R-CNN, matrix size 896 and 1344 achieved significantly higher FOM (0.869 and 0.856, respectively) for detecting abnormalities than 256, 448, and 1792 (0.667-0.820) (p < 0.05). In RetinaNet, matrix size 896 was significantly higher FOM (0.906) than others (0.329-0.832) (p < 0.05). For sensitivity of abnormalities, there was a tendency to increase sensitivity when lesion size increases. For small nodules (< 10 mm), the sensitivities were 0.418 and 0.409, whereas the sensitivities were 0.937 and 0.956 for masses. Matrix size 896 and 1344 in Mask R-CNN and matrix size 896 in RetinaNet showed significantly higher sensitivity than others (p < 0.05). CONCLUSIONS: Matrix size 896 had the highest performance for various sizes of abnormalities using different CNNs. The optimal matrix size of chest radiograph could improve CAD performance without additional training data. KEY POINTS: • Input matrix size significantly affected the performance of a deep learning-based CAD for detection of nodules or masses on chest radiographs. • The matrix size 896 showed the best performance in two different CNN detection models. • The optimal matrix size of chest radiographs could enhance CAD performance without additional training data.",2020,10.1007/s00330-020-06892-9,diagnosis,False
Optimised genetic algorithm-extreme learning machine approach for automatic COVID-19 detection,"The coronavirus disease (COVID-19), is an ongoing global pandemic caused by severe acute respiratory syndrome. Chest Computed Tomography (CT) is an effective method for detecting lung illnesses, including COVID-19. However, the CT scan is expensive and time-consuming. Therefore, this work focus on detecting COVID-19 using chest X-ray images because it is widely available, faster, and cheaper than CT scan. Many machine learning approaches such as Deep Learning, Neural Network, and Support Vector Machine; have used X-ray for detecting the COVID-19. Although the performance of those approaches is acceptable in terms of accuracy, however, they require high computational time and more memory space. Therefore, this work employs an Optimised Genetic Algorithm-Extreme Learning Machine (OGA-ELM) with three selection criteria (i.e., random, K-tournament, and roulette wheel) to detect COVID-19 using X-ray images. The most crucial strength factors of the Extreme Learning Machine (ELM) are: (i) high capability of the ELM in avoiding overfitting; (ii) its usability on binary and multi-type classifiers; and (iii) ELM could work as a kernel-based support vector machine with a structure of a neural network. These advantages make the ELM efficient in achieving an excellent learning performance. ELMs have successfully been applied in many domains, including medical domains such as breast cancer detection, pathological brain detection, and ductal carcinoma in situ detection, but not yet tested on detecting COVID-19. Hence, this work aims to identify the effectiveness of employing OGA-ELM in detecting COVID-19 using chest X-ray images. In order to reduce the dimensionality of a histogram oriented gradient features, we use principal component analysis. The performance of OGA-ELM is evaluated on a benchmark dataset containing 188 chest X-ray images with two classes: a healthy and a COVID-19 infected. The experimental result shows that the OGA-ELM achieves 100.00% accuracy with fast computation time. This demonstrates that OGA-ELM is an efficient method for COVID-19 detecting using chest X-ray images.",2020,10.1371/journal.pone.0242899,diagnosis,True
Optimizing the radiomics-machine-learning model based on non-contrast enhanced CT for the simplified risk categorization of thymic epithelial tumors: A large cohort retrospective study,"PURPOSE: This study aimed to establish and compare the radiomics machine learning (ML) models based on non-contrast enhanced computed tomography (NECT) and clinical features for predicting the simplified risk categorization of thymic epithelial tumors (TETs). EXPERIMENTAL DESIGN: A total of 509 patients with pathologically confirmed TETs from January 2009 to May 2018 were retrospectively enrolled, consisting of 238 low-risk thymoma (LRT), 232 high-risk thymoma (HRT), and 39 thymic carcinoma (TC), and were divided into training (n = 433) and testing cohorts (n = 76) according to the admission time. Volumes of interest (VOIs) covering the whole tumor were manually segmented on preoperative NECT images. A total of 1218 radiomic features were extracted from the VOIs, and 4 clinical variables were collected from the hospital database. Fourteen ML models, along with varied feature selection strategies, were used to establish triple-classification models using the radiomic features (radiomic models), while clinical-radiomic models were built after combining with the clinical variables. The diagnostic accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (AUC) of radiologist assessment, the radiomic and clinical-radiomic models were evaluated on the testing cohort. RESULTS: The Support Vector Machine (SVM) clinical-radiomic model demonstrated the highest AUC of 0.841 (95% CI 0.820 to 0.861) on the cross-validation result and reached an AUC of 0.844 (95% CI 0.793 to 0.894) in the testing cohort. For the one-vs-rest question of LRT vs HRT + TC, the sensitivity, specificity, and accuracy reached 80.00%, 63.41%, and 71.05%, respectively. For HRT vs LRT + TC, they reached 60.53%, 78.95%, and 69.74%. For TC vs LRT + HRT they reached 33.33%, 98.63%, and 96.05%, respectively. Compared with the radiomic models, superior diagnostic efficacy was demonstrated for most clinical-radiomics models, and the AUC of the Bernoulli Naive Bayes model was significantly improved. Radiologist2's assessment achieved a higher AUC of 0.813 (95% CI: 0.756-0.8761) than other radiologists, which was slightly lower than the SVM clinical-radiomic model. Combined with other evaluation indicators, SVM, as the best ML model, demonstrated the potential of predicting the simplified risk categorization of TETs with superior predictive performance to that of radiologists' assessment. CONCLUSION: Most of the ML models are promising in predicting the simplified TETs risk categorization with superior efficacy to that of radiologists' assessment, especially the SVM models, demonstrated the integration of ML with NECT may be valuable in aiding the diagnosis and treatment planning.",2022,10.1016/j.lungcan.2022.03.007,diagnosis,True
Overall Survival Prognostic Modelling of Non-small Cell Lung Cancer Patients Using Positron Emission Tomography/Computed Tomography Harmonised Radiomics Features: The Quest for the Optimal Machine Learning Algorithm,"AIMS: Despite the promising results achieved by radiomics prognostic models for various clinical applications, multiple challenges still need to be addressed. The two main limitations of radiomics prognostic models include information limitation owing to single imaging modalities and the selection of optimum machine learning and feature selection methods for the considered modality and clinical outcome. In this work, we applied several feature selection and machine learning methods to single-modality positron emission tomography (PET) and computed tomography (CT) and multimodality PET/CT fusion to identify the best combinations for different radiomics modalities towards overall survival prediction in non-small cell lung cancer patients. MATERIALS AND METHODS: A PET/CT dataset from The Cancer Imaging Archive, including subjects from two independent institutions (87 and 95 patients), was used in this study. Each cohort was used once as training and once as a test, followed by averaging of the results. ComBat harmonisation was used to address the centre effect. In our proposed radiomics framework, apart from single-modality PET and CT models, multimodality radiomics models were developed using multilevel (feature and image levels) fusion. Two different methods were considered for the feature-level strategy, including concatenating PET and CT features into a single feature set and alternatively averaging them. For image-level fusion, we used three different fusion methods, namely wavelet fusion, guided filtering-based fusion and latent low-rank representation fusion. In the proposed prognostic modelling framework, combinations of four feature selection and seven machine learning methods were applied to all radiomics modalities (two single and five multimodalities), machine learning hyper-parameters were optimised and finally the models were evaluated in the test cohort with 1000 repetitions via bootstrapping. Feature selection and machine learning methods were selected as popular techniques in the literature, supported by open source software in the public domain and their ability to cope with continuous time-to-event survival data. Multifactor ANOVA was used to carry out variability analysis and the proportion of total variance explained by radiomics modality, feature selection and machine learning methods was calculated by a bias-corrected effect size estimate known as ω(2). RESULTS: Optimum feature selection and machine learning methods differed owing to the applied radiomics modality. However, minimum depth (MD) as feature selection and Lasso and Elastic-Net regularized generalized linear model (glmnet) as machine learning method had the highest average results. Results from the ANOVA test indicated that the variability that each factor (radiomics modality, feature selection and machine learning methods) introduces to the performance of models is case specific, i.e. variances differ regarding different radiomics modalities and fusion strategies. Overall, the greatest proportion of variance was explained by machine learning, except for models in feature-level fusion strategy. CONCLUSION: The identification of optimal feature selection and machine learning methods is a crucial step in developing sound and accurate radiomics risk models. Furthermore, optimum methods are case specific, differing due to the radiomics modality and fusion strategy used.",2022,10.1016/j.clon.2021.11.014,prognosis,True
Parameter tuning in machine learning based on radiomics biomarkers of lung cancer,"BACKGROUND: Lung cancer is one of the most common cancers, and early diagnosis and intervention can improve cancer cure rate. OBJECTIVE: To improve predictive performance of radiomics features for lung cancer by tuning the machine learning model parameters. METHODS: Using a dataset involving 263 cases (125 benign and 138 malignant) acquired from our hospital, each classifier model is trained and tested using 237 and 26 cases, respectively. We initially extract 867 radiomics features of CT images for model development and then test 10 feature selections and 7 models to determine the best method. We further tune the parameter of the final model to reach the best performance. The adjusted final model is then validated using 224 cases acquired from Lung Image Database Consortium (LIDC) dataset (64 benign and 160 malignant) with the same set of selected radiomics features. RESULTS: During model development, the feature selection via concave minimization method show the best performance of area under ROC curve (AUC = 0.765), followed by l0-norm regularization (AUC = 0.741) and Fisher discrimination criterion (AUC = 0.734). Support vector machine (SVM) and random forest (RF) are the top two machine learning algorithms showing the best performance (AUC = 0.765 and 0.734, respectively), using by the default parameter. After parameter tuning, SVM with linear kernel achieves the best performance (AUC = 0.837), whereas the best tuned RF with the number of trees is 510 and yields a slightly lower performance (AUC = 0.775) in 26 test samples data. During model validation, the SVM and RF models yield AUC = 0.78 and 0.77, respectively. CONCLUSION: Appropriate quantitative radiomics features and accurate parameters can improve the model's performance to predict lung cancer.",2022,10.3233/xst-211096,diagnosis,True
Performance and educational training of radiographers in lung nodule or mass detection: Retrospective comparison with different deep learning algorithms,"The aim of this investigation was to compare the diagnostic performance of radiographers and deep learning algorithms in pulmonary nodule/mass detection on chest radiograph.A test set of 100 chest radiographs containing 53 cases with no pathology (normal) and 47 abnormal cases (pulmonary nodules/masses) independently interpreted by 6 trained radiographers and deep learning algorithems in a random order. The diagnostic performances of both deep learning algorithms and trained radiographers for pulmonary nodules/masses detection were compared.QUIBIM Chest X-ray Classifier, a deep learning through mass algorithm that performs superiorly to practicing radiographers in the detection of pulmonary nodules/masses (AUCMass: 0.916 vs AUCTrained radiographer: 0.778, P < .001). In addition, heat-map algorithm could automatically detect and localize pulmonary nodules/masses in chest radiographs with high specificity.In conclusion, the deep-learning based computer-aided diagnosis system through 4 algorithms could potentially assist trained radiographers by increasing the confidence and access to chest radiograph interpretation in the age of digital age with the growing demand of medical imaging usage and radiologist burnout.",2021,10.1097/md.0000000000026270,diagnosis,False
Performance and reading time of lung nodule identification on multidetector CT with or without an artificial intelligence-powered computer-aided detection system,"AIM: To compare the performance and reading time of different readers using automatic artificial intelligence (AI)-powered computer-aided detection (CAD) to detect lung nodules in different reading modes. MATERIALS AND METHODS: One hundred and fifty multidetector computed tomography (CT) datasets containing 340 nodules ≤10 mm in diameter were collected retrospectively. A CAD with vessel-suppressed function was used to interpret the images. Three junior and three senior readers were assigned to read (1) CT images without CAD, (2) second-read using CAD in which CAD was applied only after initial unassisted assessment, and (3) a concurrent read with CAD in which CAD was applied at the start of assessment. Diagnostic performances and reading times were compared using analysis of variance. RESULTS: For all readers, the mean sensitivity improved from 64% (95% confidence interval [CI]: 62%, 66%) for the without-CAD mode to 82% (95% CI: 80%, 84%) for the second-reading mode and to 80% (95% CI: 79%, 82%) for the concurrent-reading mode (p<0.001). There was no significant difference between the two modes in terms of the mean sensitivity, specificity, and area under the receiver operating characteristic curve (AUC) for both junior and senior readers and all readers (p>0.05). The reading time of all readers was significantly shorter for the concurrent-reading mode (124 ± 25 seconds) compared to without CAD (156 ± 34 seconds; p<0.001) and the second-reading mode (197 ± 46 seconds; p<0.001). CONCLUSION: In CAD for lung nodules at CT, the second-reading mode and concurrent-reading mode may improve detection performance for all readers in both screening and clinical routine practice. Concurrent use of CAD is more efficient for both junior and senior readers.",2021,10.1016/j.crad.2021.04.006,diagnosis,True
Performance change with the number of training data: A case study on the binary classification of COVID-19 chest X-ray by using convolutional neural networks,"One of the features of artificial intelligence/machine learning-based medical devices resides in their ability to learn from real-world data. However, obtaining a large number of training data in the early phase is difficult, and the device performance may change after their first introduction into the market. To introduce the safety and effectiveness of these devices into the market in a timely manner, an appropriate post-market performance change plan must be established at the timing of the premarket approval. In this work, we evaluate the performance change with the variation of the number of training data. Two publicly available datasets are used: one consisting of 4000 images for COVID-19 and another comprising 4000 images for Normal. The dataset was split into 7000 images for training and validation, also 1000 images for test. Furthermore, the training and validation data were selected as different 16 datasets. Two different convolutional neural networks, namely AlexNet and ResNet34, with and without a fine-tuning method were used to classify two image types. The area under the curve, sensitivity, and specificity were evaluated for each dataset. Our result shows that all performances were rapidly improved as the number of training data was increased and reached an equilibrium state. AlexNet outperformed ResNet34 when the number of images was small. The difference tended to decrease as the number of training data increased, and the fine-tuning method improved all performances. In conclusion, the appropriate model and method should be selected considering the intended performance and available number of data.",2022,10.1016/j.compbiomed.2022.105251,diagnosis,False
"Performance evaluation of a deep learning image reconstruction (DLIR) algorithm in ""double low"" chest CTA in children: a feasibility study","BACKGROUND: Chest CT angiography (CTA) is a convenient clinical examination for children with an increasing need to reduce both radiation and contrast medium doses. Iterative Reconstruction algorithms are often used to reduce image noise but encounter limitations under low radiation dose and conventional 100 kVp tube voltage may not provide adequate enhancement under low contrast dose. PURPOSE: To evaluate the performance of a deep learning image reconstruction (DLIR) algorithm in conjunction with lower tube voltage in chest CTA in children under reduced radiation and contrast medium (CM) dose. MATERIALS AND METHODS: 46 Children (age 5.9 ± 4.2 years) in the study group underwent chest CTA with 70 kVp and CM dose of 0.8-1.2 ml/kg. Images were reconstructed at 0.625 mm using a high setting DLIR (DLIR-H). The control group consisted of 46 age-matching children scanned with 100 kVp, CM dose of 1.3-1.8 ml/kg and images reconstructed with 50% and 100% adaptive statistical iterative reconstruction-V. Two radiologists evaluated images subjectively for overall image noise, vessel contrast and vessel margin clarity separately on a 5-point scale (5, excellent and 1, not acceptable). CT value and image noise of aorta and erector spinae muscle were measured. RESULTS: Compared to the control group, the study group reduced the dose-length-product by 11.2% (p = 0.01) and CM dose by 24% (p < 0.001), improved the enhancement in aorta (416.5 ± 113.1HU vs. 342.0 ± 57.6HU, p < 0.001) and reduced noise (15.1 ± 3.5HU vs. 18.6 ± 4.4HU, p < 0.001). The DLIR-H images provided acceptable scores on all 3 aspects of the qualitative evaluation. CONCLUSION: ""Double low"" chest CTA in children using 70 kVp and DLIR provides high image quality with reduced noise and improved vessel enhancement for diagnosis while further reduces radiation and CM dose.",2021,10.1007/s11547-021-01384-2,diagnosis,True
Performance Evaluation of a Deep Learning System for Differential Diagnosis of Lung Cancer With Conventional CT and FDG PET/CT Using Transfer Learning and Metadata,"PURPOSE: We aimed to evaluate the performance of a deep learning system for differential diagnosis of lung cancer with conventional CT and FDG PET/CT using transfer learning (TL) and metadata. METHODS: A total of 359 patients with a lung mass or nodule who underwent noncontrast chest CT and FDG PET/CT prior to treatment were enrolled retrospectively. All pulmonary lesions were classified by pathology (257 malignant, 102 benign). Deep learning classification models based on ResNet-18 were developed using the pretrained weights obtained from ImageNet data set. We propose a deep TL model for differential diagnosis of lung cancer using CT imaging data and metadata with SUVmax and lesion size derived from PET/CT. The area under the receiver operating characteristic curve (AUC) of the deep learning model was measured as a performance metric and verified by 5-fold cross-validation. RESULTS: The performance metrics of the conventional CT model were generally better than those of the CT of PET/CT model. Introducing metadata with SUVmax and lesion size derived from PET/CT into baseline CT models improved the diagnostic performance of the CT of PET/CT model (AUC = 0.837 vs 0.762) and the conventional CT model (AUC = 0.877 vs 0.817). CONCLUSIONS: Deep TL models with CT imaging data provide good diagnostic performance for lung cancer, and the conventional CT model showed overall better performance than the CT of PET/CT model. Metadata information derived from PET/CT can improve the performance of deep learning systems.",2021,10.1097/rlu.0000000000003661,diagnosis,True
Performance improvement of mediastinal lymph node severity detection using GAN and Inception network,"BACKGROUND AND OBJECTIVE: In lung cancer, the determination of mediastinal lymph node (MLN) status as benign or malignant influence treatment planning and survival rate. Invasive pathological tests for the classification of MLNs into benign and malignant have various shortcomings like painfulness, the risk associated with anesthesia, and depends to a large extent on skillset and preferences of the surgeon performing the test. Hence, computer-aided system for MLNs severity detection has been explored widely by the researchers. Very recently, in our earlier concluded work on non-invasive method for MLNs differential diagnosis in computed tomography (CT) images, combination of different data augmentation approaches and state-of-art fully convolutional network (FCN) were implemented to enhance the performance of malignancy detection. However, the performance of FCN network were highly depended on the selection of appropriate data augmentation approach and control of their hyperparameters. Moreover, a standard practice to get hierarchical features in convolutional neural network (CNN) models requires deeper stacking of layers. This leads to an increase in number of trainable parameters which prone to overfitting of the network. METHODS: In view of the above mention limitations, in this paper, authors have proposed an approach that includes: 1) Generative Adversarial Network (GAN) for data augmentation, and 2) Inception network for malignancy detection. Unlike conventional data augmentation strategy, GAN based augmentation approach generates data that correlates to original data distribution. In the case of Inception based model, it uses multiple size kernels with factorized convolution for hierarchical feature extraction. This helps to a significant reduction in trainable parameters and the problem of overfitting. RESULTS: In this paper, experiments with different GAN approaches, as well as with different Inception architectures, are conducted to evaluate and justify the selection of appropriate GAN and Inception architecture, respectively for MLNs severity detection. The proposed approach achieves superior results with an average accuracy, sensitivity, specificity, and area under curve of 94.95%, 93.65%, 96.67%, and 95%, respectively. CONCLUSION: The obtained results validate the usefulness of GANs for data augmentation in the differential diagnosis of benign and malignant MLNs. The proposed Inception network based classifier for malignancy detection shows promising results compared to all investigated methods presented in various literature.",2020,10.1016/j.cmpb.2020.105478,diagnosis,True
Performance of a computer aided diagnosis system for SARS-CoV-2 pneumonia based on ultrasound images,"PURPOSE: In this study we aimed to leverage deep learning to develop a computer aided diagnosis (CAD) system toward helping radiologists in the diagnosis of SARS-CoV-2 virus syndrome on Lung ultrasonography (LUS). METHOD: A CAD system is developed based on a transfer learning of a residual network (ResNet) to extract features on LUS and help radiologists to distinguish SARS-CoV-2 virus syndrome from healthy and non-SARS-CoV-2 pneumonia. A publicly available LUS dataset for SARS-CoV-2 virus syndrome consisting of 3909 images has been employed. Six radiologists with different experiences participated in the experiment. A comprehensive LUS data set was constructed and employed to train and verify the proposed method. Several metrics such as accuracy, recall, precision, and F1-score, are used to evaluate the performance of the proposed CAD approach. The performances of the radiologists with and without the help of CAD are also evaluated quantitively. The p-values of the t-test shows that with the help of the CAD system, both junior and senior radiologists significantly improve their diagnosis performance on both balanced and unbalanced datasets. RESULTS: Experimental results indicate the proposed CAD approach and the machine features from it can significantly improve the radiologists' performance in the SARS-CoV-2 virus syndrome diagnosis. With the help of the proposed CAD system, the junior and senior radiologists achieved F1-score values of 91.33% and 95.79% on balanced dataset and 94.20% and 96.43% on unbalanced dataset. The proposed approach is verified on an independent test dataset and reports promising performance. CONCLUSIONS: The proposed CAD system reports promising performance in facilitating radiologists' diagnosis SARS-CoV-2 virus syndrome and might assist the development of a fast, accessible screening method for pulmonary diseases.",2022,10.1016/j.ejrad.2021.110066,diagnosis,False
Performance of a Deep Learning Algorithm Compared with Radiologic Interpretation for Lung Cancer Detection on Chest Radiographs in a Health Screening Population,"Background The performance of a deep learning algorithm for lung cancer detection on chest radiographs in a health screening population is unknown. Purpose To validate a commercially available deep learning algorithm for lung cancer detection on chest radiographs in a health screening population. Materials and Methods Out-of-sample testing of a deep learning algorithm was retrospectively performed using chest radiographs from individuals undergoing a comprehensive medical check-up between July 2008 and December 2008 (validation test). To evaluate the algorithm performance for visible lung cancer detection, the area under the receiver operating characteristic curve (AUC) and diagnostic measures, including sensitivity and false-positive rate (FPR), were calculated. The algorithm performance was compared with that of radiologists using the McNemar test and the Moskowitz method. Additionally, the deep learning algorithm was applied to a screening cohort undergoing chest radiography between January 2008 and December 2012, and its performances were calculated. Results In a validation test comprising 10 285 radiographs from 10 202 individuals (mean age, 54 years ± 11 [standard deviation]; 5857 men) with 10 radiographs of visible lung cancers, the algorithm's AUC was 0.99 (95% confidence interval: 0.97, 1), and it showed comparable sensitivity (90% [nine of 10 radiographs]) to that of the radiologists (60% [six of 10 radiographs]; P = .25) with a higher FPR (3.1% [319 of 10 275 radiographs] vs 0.3% [26 of 10 275 radiographs]; P < .001). In the screening cohort of 100 525 chest radiographs from 50 070 individuals (mean age, 53 years ± 11; 28 090 men) with 47 radiographs of visible lung cancers, the algorithm's AUC was 0.97 (95% confidence interval: 0.95, 0.99), and its sensitivity and FPR were 83% (39 of 47 radiographs) and 3% (2999 of 100 478 radiographs), respectively. Conclusion A deep learning algorithm detected lung cancers on chest radiographs with a performance comparable to that of radiologists, which will be helpful for radiologists in healthy populations with a low prevalence of lung cancer. © RSNA, 2020 Online supplemental material is available for this article. See also the editorial by Armato in this issue.",2020,10.1148/radiol.2020201240,diagnosis,False
Performance of a deep learning-based lung nodule detection system as an alternative reader in a Chinese lung cancer screening program,"OBJECTIVE: To evaluate the performance of a deep learning-based computer-aided detection (DL-CAD) system in a Chinese low-dose CT (LDCT) lung cancer screening program. MATERIALS AND METHODS: One-hundred-and-eighty individuals with a lung nodule on their baseline LDCT lung cancer screening scan were randomly mixed with screenees without nodules in a 1:1 ratio (total: 360 individuals). All scans were assessed by double reading and subsequently processed by an academic DL-CAD system. The findings of double reading and the DL-CAD system were then evaluated by two senior radiologists to derive the reference standard. The detection performance was evaluated by the Free Response Operating Characteristic curve, sensitivity and false-positive (FP) rate. The senior radiologists categorized nodules according to nodule diameter, type (solid, part-solid, non-solid) and Lung-RADS. RESULTS: The reference standard consisted of 262 nodules ≥ 4 mm in 196 individuals; 359 findings were considered false positives. The DL-CAD system achieved a sensitivity of 90.1% with 1.0 FP/scan for detection of lung nodules regardless of size or type, whereas double reading had a sensitivity of 76.0% with 0.04 FP/scan (P = 0.001). The sensitivity for detection of nodules ≥ 4 - ≤ 6 mm was significantly higher with DL-CAD than with double reading (86.3% vs. 58.9% respectively; P = 0.001). Sixty-three nodules were only identified by the DL-CAD system, and 27 nodules only found by double reading. The DL-CAD system reached similar performance compared to double reading in Lung-RADS 3 (94.3% vs. 90.0%, P = 0.549) and Lung-RADS 4 nodules (100.0% vs. 97.0%, P = 1.000), but showed a higher sensitivity in Lung-RADS 2 (86.2% vs. 65.4%, P < 0.001). CONCLUSIONS: The DL-CAD system can accurately detect pulmonary nodules on LDCT, with an acceptable false-positive rate of 1 nodule per scan and has higher detection performance than double reading. This DL-CAD system may assist radiologists in nodule detection in LDCT lung cancer screening.",2022,10.1016/j.ejrad.2021.110068,diagnosis,True
Performance of Deep Learning Model in Detecting Operable Lung Cancer With Chest Radiographs,"PURPOSE: The aim of this study was to evaluate the diagnostic performance of a trained deep convolutional neural network (DCNN) model for detecting operable lung cancer with chest radiographs (CXRs). MATERIALS AND METHODS: The institutional review board approved this study. A deep learning model (DLM) based on DCNN was trained with 17,211 CXRs (5700 CT-confirmed lung nodules in 3500 CXRs and 13,711 normal CXRs), finally augmented to 600,000 images. For validation, a trained DLM was tested with 1483 CXRs with surgically resected lung cancer, marked and scored by 2 radiologists. Furthermore, diagnostic performances of DLM and 6 human observers were compared with 500 cases (200 visible T1 lung cancer on CXR and 300 normal CXRs) and analyzed using free-response receiver-operating characteristics curve (FROC) analysis. RESULTS: The overall detection rate of DLM for resected lung cancers (27.2±14.6 mm) was a sensitivity of 76.8% (1139/1483) with a false positive per image (FPPI) of 0.3 and area under the FROC curve (AUC) of 0.732. In the comparison with human readers, DLM demonstrated a sensitivity of 86.5% at 0.1 FPPI and a sensitivity of 92% at 0.3 FPPI with AUC of 0.899 at an FPPI range of 0.03 to 0.44 for detecting visible T1 lung cancers, which were superior to the average of 6 human readers [mean sensitivity; 78% (range, 71.6% to 82.6%) at an FPPI of 0.1% and 85% (range, 80.2% to 89.2%) at an FPPI of 0.3, AUC of 0.819 (range, 0.754 to 0.862) at an FPPI of 0.03 to 0.44). CONCLUSIONS: A DLM has high diagnostic performance in detecting operable lung cancer with CXR, demonstrating a potential of playing a pivotal role for lung cancer screening.",2019,10.1097/rti.0000000000000388,treatment,False
Perinodular and Intranodular Radiomic Features on Lung CT Images Distinguish Adenocarcinomas from Granulomas,"Purpose To evaluate ability of radiomic (computer-extracted imaging) features to distinguish non-small cell lung cancer adenocarcinomas from granulomas at noncontrast CT. Materials and Methods For this retrospective study, screening or standard diagnostic noncontrast CT images were collected for 290 patients (mean age, 68 years; range, 18-92 years; 125 men [mean age, 67 years; range, 18-90 years] and 165 women [mean age, 68 years; range, 33-92 years]) from two institutions between 2007 and 2013. Histopathologic analysis was available for one nodule per patient. Corresponding nodule of interest was identified on axial CT images by a radiologist with manual annotation. Nodule shape, wavelet (Gabor), and texture-based (Haralick and Laws energy) features were extracted from intra- and perinodular regions. Features were pruned to train machine learning classifiers with 145 patients. In a test set of 145 patients, classifier results were compared against a convolutional neural network (CNN) and diagnostic readings of two radiologists. Results Support vector machine classifier with intranodular radiomic features achieved an area under the receiver operating characteristic curve (AUC) of 0.75 on the test set. Combining radiomics of intranodular with perinodular regions improved the AUC to 0.80. On the same test set, CNN resulted in an AUC of 0.76. Radiologist readers achieved AUCs of 0.61 and 0.60, respectively. Conclusion Radiomic features from intranodular and perinodular regions of nodules can distinguish non-small cell lung cancer adenocarcinomas from benign granulomas at noncontrast CT. © RSNA, 2018 Online supplemental material is available for this article. See also the editorial by Nishino in this issue.",2019,10.1148/radiol.2018180910,diagnosis,True
Phased searching with NEAT in a time-scaled framework: experiments on a computer-aided detection system for lung nodules,"OBJECTIVE: In the field of computer-aided detection (CAD) systems for lung nodules in computed tomography (CT) scans, many image features are presented and many artificial neural network (ANN) classifiers with various structural topologies are analyzed; frequently, the classifier topologies are selected by trial-and-error experiments. To avoid these trial and error approaches, we present a novel classifier that evolves ANNs using genetic algorithms, called ""Phased Searching with NEAT in a Time or Generation-Scaled Framework"", integrating feature selection with the classification task. METHODS AND MATERIALS: We analyzed our method's performance on 360 CT scans from the public Lung Image Database Consortium database. We compare our method's performance with other more-established classifiers, namely regular NEAT, Feature-Deselective NEAT (FD-NEAT), fixed-topology ANNs, and support vector machines (SVMs) using ten-fold cross-validation experiments of all 360 scans. RESULTS: The results show that the proposed ""Phased Searching"" method performs better and faster than regular NEAT, better than FD-NEAT, and achieves sensitivities at 3 and 4 false positives (FP) per scan that are comparable with the fixed-topology ANN and SVM classifiers, but with fewer input features. It achieves a detection sensitivity of 83.0±9.7% with an average of 4FP/scan, for nodules with a diameter greater than or equal to 3mm. It also evolves networks with shorter evolution times and with lower complexities than regular NEAT (p=0.026 and p<0.001, respectively). Analysis on the average and best network complexities evolved by regular NEAT and by our approach shows that our approach searches for good solutions in lower dimensional search spaces, and evolves networks without superfluous structure. CONCLUSIONS: We have presented a novel approach that combines feature selection with the evolution of ANN topology and weights. Compared with the original threshold-based Phased Searching method of Green, our method requires fewer parameters and converges to the optimal network complexity required for the classification task at hand. The results of the ten-fold cross-validation experiments also show that our proposed CAD system for lung nodule detection performs well with respect to other methods in the literature.",2013,10.1016/j.artmed.2013.07.002,diagnosis,True
Pneumonia Detection in Chest X-Ray Dose-Equivalent CT: Impact of Dose Reduction on Detectability by Artificial Intelligence,"RATIONALE AND OBJECTIVES: There has been a significant increase of immunocompromised patients in recent years due to new treatment modalities for previously fatal diseases. This comes at the cost of an elevated risk for infectious diseases, most notably pathogens affecting the respiratory tract. Because early diagnosis and treatment of pneumonia can help reducing morbidity and mortality, we assessed the performance of a deep neural network in the detection of pulmonary infection in chest X-ray dose-equivalent computed tomography (CT). MATERIALS AND METHODS: The 100 patients included in this retrospective study were referred to our department for suspicion of pulmonary infection and/or follow-up of known pulmonary nodules. Every patient was scanned with a standard dose (1.43 ± 0.54 mSv) and a 20 times dose-reduced (0.07 ± 0.03 mSv) CT protocol. We trained a deep neural network to perform binary classification (pulmonary consolidation present or not) and assessed diagnostic performance on both standard dose and reduced dose CT images. RESULTS: The areas under the curve of the deep learning algorithm for the standard dose CT was 0.923 (confidence interval [CI] 95%: 0.905-0.941) and significantly higher than the areas under the curve (0.881, CI 95%: 0.859-0.903) of the reduced dose CT (p = 0.001). Sensitivity and specificity of the standard dose CT was 82.9% and 93.8%, and of the reduced dose CT 71.0% and 93.3%. CONCLUSION: Pneumonia detection with X-ray dose-equivalent CT using artificial intelligence is feasible and may contribute to a more robust and reproducible diagnostic performance. Dose reduction lowered the performance of the deep neural network, which calls for optimization and adaption of CT protocols when using AI algorithms at reduced doses.",2021,10.1016/j.acra.2020.05.031,diagnosis,False
Pneumonia detection in chest X-ray images using an ensemble of deep learning models,"Pneumonia is a respiratory infection caused by bacteria or viruses; it affects many individuals, especially in developing and underdeveloped nations, where high levels of pollution, unhygienic living conditions, and overcrowding are relatively common, together with inadequate medical infrastructure. Pneumonia causes pleural effusion, a condition in which fluids fill the lung, causing respiratory difficulty. Early diagnosis of pneumonia is crucial to ensure curative treatment and increase survival rates. Chest X-ray imaging is the most frequently used method for diagnosing pneumonia. However, the examination of chest X-rays is a challenging task and is prone to subjective variability. In this study, we developed a computer-aided diagnosis system for automatic pneumonia detection using chest X-ray images. We employed deep transfer learning to handle the scarcity of available data and designed an ensemble of three convolutional neural network models: GoogLeNet, ResNet-18, and DenseNet-121. A weighted average ensemble technique was adopted, wherein the weights assigned to the base learners were determined using a novel approach. The scores of four standard evaluation metrics, precision, recall, f1-score, and the area under the curve, are fused to form the weight vector, which in studies in the literature was frequently set experimentally, a method that is prone to error. The proposed approach was evaluated on two publicly available pneumonia X-ray datasets, provided by Kermany et al. and the Radiological Society of North America (RSNA), respectively, using a five-fold cross-validation scheme. The proposed method achieved accuracy rates of 98.81% and 86.85% and sensitivity rates of 98.80% and 87.02% on the Kermany and RSNA datasets, respectively. The results were superior to those of state-of-the-art methods and our method performed better than the widely used ensemble techniques. Statistical analyses on the datasets using McNemar's and ANOVA tests showed the robustness of the approach. The codes for the proposed work are available at https://github.com/Rohit-Kundu/Ensemble-Pneumonia-Detection.",2021,10.1371/journal.pone.0256630,diagnosis,False
Potential diagnosis of COVID-19 from chest X-ray and CT findings using semi-supervised learning,"COVID-19 is an infectious disease, which has adversely affected public health and the economy across the world. On account of the highly infectious nature of the disease, rapid automated diagnosis of COVID-19 is urgently needed. A few recent findings suggest that chest X-rays and CT scans can be used by machine learning for the diagnosis of COVID-19. Herein, we employed semi-supervised learning (SSL) approaches to detect COVID-19 cases accurately by analyzing digital chest X-rays and CT scans. On a relatively small COVID-19 radiography dataset, which contains only 219 COVID-19 positive images, 1341 normal and 1345 viral pneumonia images, our algorithm, COVIDCon, which takes advantage of data augmentation, consistency regularization, and multicontrastive learning, attains 97.07% average class prediction accuracy, with 1000 labeled images, which is 7.65% better than the next best SSL method, virtual adversarial training. COVIDCon performs even better on a larger COVID-19 CT Scan dataset that contains 82,767 images. It achieved an excellent accuracy of 99.13%, at 20,000 labels, which is 6.45% better than the next best pseudo-labeling approach. COVIDCon outperforms other state-of-the-art algorithms at every label that we have investigated. These results demonstrate COVIDCon as the benchmark SSL algorithm for potential diagnosis of COVID-19 from chest X-rays and CT-Scans. Furthermore, COVIDCon performs exceptionally well in identifying COVID-19 positive cases from a completely unseen repository with a confirmed COVID-19 case history. COVIDCon, may provide a fast, accurate, and reliable method for screening COVID-19 patients.",2022,10.1007/s13246-021-01075-2,diagnosis,False
Potential feature exploration and model development based on 18F-FDG PET/CT images for differentiating benign and malignant lung lesions,"PURPOSE: The study is to explore potential features and develop classification models for distinguishing benign and malignant lung lesions based on CT-radiomics features and PET metabolic parameters extracted from PET/CT images. MATERIALS AND METHODS: A retrospective study was conducted in baseline 18 F-flurodeoxyglucose positron emission tomography/ computed tomography (18 F-FDG PET/CT) images of 135 patients. The dataset was utilized for feature extraction of CT-radiomics features and PET metabolic parameters based on volume of interest, then went through feature selection and model development with strategy of five-fold cross-validation. Specifically, model development used support vector machine, PET metabolic parameters selection used Akaike's information criterion, and CT-radiomics were reduced by the least absolute shrinkage and selection operator method then forward selection approach. The diagnostic performances of CT-radiomics, PET metabolic parameters and combination of both were illustrated by receiver operating characteristic (ROC) curves, and compared by Delong test. Five groups of selected PET metabolic parameters and CT-radiomics were counted, and potential features were found and analyzed with Mann-Whitney U test. RESULTS: The CT-radiomics, PET metabolic parameters, and combination of both among five subsets showed mean area under the curve (AUC) of 0.820 ± 0.053, 0.874 ± 0.081, and 0.887 ± 0.046, respectively. No significant differences in ROC among models were observed through pairwise comparison in each fold (P-value from 0.09 to 0.81, Delong test). The potential features were found to be SurfaceVolumeRatio and SUVpeak (P < 0.001 of both, U test). CONCLUSION: The classification models developed by CT-radiomics features and PET metabolic parameters based on PET/CT images have substantial diagnostic capacity on lung lesions.",2019,10.1016/j.ejrad.2019.108735,diagnosis,True
Pre-processing methods in chest X-ray image classification,"BACKGROUND: The SARS-CoV-2 pandemic began in early 2020, paralyzing human life all over the world and threatening our security. Thus, the need for an effective, novel approach to diagnosing, preventing, and treating COVID-19 infections became paramount. METHODS: This article proposes a machine learning-based method for the classification of chest X-ray images. We also examined some of the pre-processing methods such as thresholding, blurring, and histogram equalization. RESULTS: We found the F1-score results rose to 97%, 96%, and 99% for the three analyzed classes: healthy, COVID-19, and pneumonia, respectively. CONCLUSION: Our research provides proof that machine learning can be used to support medics in chest X-ray classification and improving pre-processing leads to improvements in accuracy, precision, recall, and F1-scores.",2022,10.1371/journal.pone.0265949,diagnosis,False
Pre-treatment (18)F-FDG PET-based radiomics predict survival in resected non-small cell lung cancer,"AIM: To assess the prognostic value of 2-[(18)F]-fluoro-2-deoxy-d-glucose (FDG) positron-emission tomography (PET)-based radiomics using a machine learning approach in patients with non-small cell lung cancer (NSCLC). MATERIALS AND METHODS: Ninety-three patients with stage I-III NSCLC who underwent combined PET/computed tomography (CT) followed by curative resection. A total of 35 unique quantitative radiomic features was extracted from the PET images, which included imaging phenotypes such as pixel intensity, shape, and texture. Radiomic features were ranked based on score according to their correlation with disease recurrence status within a 3-year follow-up. The recurrence risk classification performances of machine learning algorithms (random forest, neural network, naive Bayes, logistic regression, and support vector machine) using the 20 best-ranked features were compared using the areas under the receiver operating characteristic curve (AUC) and validated by the random sampling method. RESULTS: Contrast and busyness texture features from neighbourhood grey-level difference matrix were found to be the two best predictors of disease recurrence. The random forest model obtained the best performance (AUC: 0.956, accuracy: 0.901, F1 score: 0.872, precision: 0.905, recall: 0.842), followed by the neural network model (AUC: 0.871, accuracy: 0.780, F1 score: 0.708, precision: 0.755, recall: 0.666). CONCLUSION: A PET-based radiomic model was developed and validated for risk classification in NSCLC. The machine learning approach with random forest classifier exhibited good performance in predicting the recurrence risk. Radiomic features may help clinicians to improve the risk stratification for clinical practice.",2019,10.1016/j.crad.2019.02.008,diagnosis,True
Predicting adenocarcinoma recurrence using computational texture models of nodule components in lung CT,"PURPOSE: To investigate the importance of presurgical computed tomography (CT) intensity and texture information from ground-glass opacities (GGO) and solid nodule components for the prediction of adenocarcinoma recurrence. METHODS: For this study, 101 patients with surgically resected stage I adenocarcinoma were selected. During the follow-up period, 17 patients had disease recurrence with six associated cancer-related deaths. GGO and solid tumor components were delineated on presurgical CT scans by a radiologist. Computational texture models of GGO and solid regions were built using linear combinations of steerable Riesz wavelets learned with linear support vector machines (SVMs). Unlike other traditional texture attributes, the proposed texture models are designed to encode local image scales and directions that are specific to GGO and solid tissue. The responses of the locally steered models were used as texture attributes and compared to the responses of unaligned Riesz wavelets. The texture attributes were combined with CT intensities to predict tumor recurrence and patient hazard according to disease-free survival (DFS) time. Two families of predictive models were compared: LASSO and SVMs, and their survival counterparts: Cox-LASSO and survival SVMs. RESULTS: The best-performing predictive model of patient hazard was associated with a concordance index (C-index) of 0.81 ± 0.02 and was based on the combination of the steered models and CT intensities with survival SVMs. The same feature group and the LASSO model yielded the highest area under the receiver operating characteristic curve (AUC) of 0.8 ± 0.01 for predicting tumor recurrence, although no statistically significant difference was found when compared to using intensity features solely. For all models, the performance was found to be significantly higher when image attributes were based on the solid components solely versus using the entire tumors (p < 3.08 × 10(-5)). CONCLUSIONS: This study constitutes a novel perspective on how to interpret imaging information from CT examinations by suggesting that most of the information related to adenocarcinoma aggressiveness is related to the intensity and morphological properties of solid components of the tumor. The prediction of adenocarcinoma relapse was found to have low specificity but very high sensitivity. Our results could be useful in clinical practice to identify patients for which no recurrence is expected with a very high confidence using a presurgical CT scan only. It also provided an accurate estimation of the risk of recurrence after a given duration t from surgical resection (i.e., C-index = 0.81 ± 0.02).",2015,10.1118/1.4916088,prognosis,True
"Predicting benign, preinvasive, and invasive lung nodules on computed tomography scans using machine learning","OBJECTIVE: The study objective was to investigate if machine learning algorithms can predict whether a lung nodule is benign, adenocarcinoma, or its preinvasive subtype from computed tomography images alone. METHODS: A dataset of chest computed tomography scans containing lung nodules was collected with their pathologic diagnosis from several sources. The dataset was split randomly into training (70%), internal validation (15%), and independent test sets (15%) at the patient level. Two machine learning algorithms were developed, trained, and validated. The first algorithm used the support vector machine model, and the second used deep learning technology: a convolutional neural network. Receiver operating characteristic analysis was used to evaluate the performance of the classification on the test dataset. RESULTS: The support vector machine/convolutional neural network-based models classified nodules into 6 categories resulting in an area under the curve of 0.59/0.65 when differentiating atypical adenomatous hyperplasia versus adenocarcinoma in situ, 0.87/0.86 with minimally invasive adenocarcinoma versus invasive adenocarcinoma, 0.76/0.72 atypical adenomatous hyperplasia + adenocarcinoma in situ versus minimally invasive adenocarcinoma, 0.89/0.87 atypical adenomatous hyperplasia + adenocarcinoma in situ versus minimally invasive adenocarcinoma + invasive adenocarcinoma, and 0.93/0.92 atypical adenomatous hyperplasia + adenocarcinoma in situ + minimally invasive adenocarcinoma versus invasive adenocarcinoma. Classifying benign versus atypical adenomatous hyperplasia + adenocarcinoma in situ + minimally invasive adenocarcinoma versus invasive adenocarcinoma resulted in a micro-average area under the curve of 0.93/0.94 for the support vector machine/convolutional neural network models, respectively. The convolutional neural network-based methods had higher sensitivities than the support vector machine-based methods but lower specificities and accuracies. CONCLUSIONS: The machine learning algorithms demonstrated reasonable performance in differentiating benign versus preinvasive versus invasive adenocarcinoma from computed tomography images alone. However, the prediction accuracy varies across its subtypes. This holds the potential for improved diagnostic capabilities with less-invasive means.",2022,10.1016/j.jtcvs.2021.02.010,diagnosis,True
Predicting clinical outcomes in COVID-19 using radiomics on chest radiographs,"OBJECTIVES: For optimal utilization of healthcare resources, there is a critical need for early identification of COVID-19 patients at risk of poor prognosis as defined by the need for intensive unit care and mechanical ventilation. We tested the feasibility of chest X-ray (CXR)-based radiomics metrics to develop machine-learning algorithms for predicting patients with poor outcomes. METHODS: In this Institutional Review Board (IRB) approved, Health Insurance Portability and Accountability Act (HIPAA) compliant, retrospective study, we evaluated CXRs performed around the time of admission from 167 COVID-19 patients. Of the 167 patients, 68 (40.72%) required intensive care during their stay, 45 (26.95%) required intubation, and 25 (14.97%) died. Lung opacities were manually segmented using ITK-SNAP (open-source software). CaPTk (open-source software) was used to perform 2D radiomics analysis. RESULTS: Of all the algorithms considered, the AdaBoost classifier performed the best with AUC = 0.72 to predict the need for intubation, AUC = 0.71 to predict death, and AUC = 0.61 to predict the need for admission to the intensive care unit (ICU). AdaBoost had similar performance with ElasticNet in predicting the need for admission to ICU. Analysis of the key radiomic metrics that drive model prediction and performance showed the importance of first-order texture metrics compared to other radiomics panel metrics. Using a Venn-diagram analysis, two first-order texture metrics and one second-order texture metric that consistently played an important role in driving model performance in all three outcome predictions were identified. CONCLUSIONS: Considering the quantitative nature and reliability of radiomic metrics, they can be used prospectively as prognostic markers to individualize treatment plans for COVID-19 patients and also assist with healthcare resource management. ADVANCES IN KNOWLEDGE: We report on the performance of CXR-based imaging metrics extracted from RT-PCR positive COVID-19 patients at admission to develop machine-learning algorithms for predicting the need for ICU, the need for intubation, and mortality, respectively.",2021,10.1259/bjr.20210221,prognosis,False
Predicting EGFR and PD-L1 Status in NSCLC Patients Using Multitask AI System Based on CT Images,"BACKGROUND: Epidermal growth factor receptor (EGFR) genotyping and programmed death ligand-1 (PD-L1) expressions are of paramount importance for treatment guidelines such as the use of tyrosine kinase inhibitors (TKIs) and immune checkpoint inhibitors (ICIs) in lung cancer. Conventional identification of EGFR or PD-L1 status requires surgical or biopsied tumor specimens, which are obtained through invasive procedures associated with risk of morbidities and may be unavailable to access tissue samples. Here, we developed an artificial intelligence (AI) system that can predict EGFR and PD-L1 status in using non-invasive computed tomography (CT) images. METHODS: A multitask AI system including deep learning (DL) module, radiomics (RA) module, and joint (JO) module combining the DL, RA, and clinical features was developed, trained, and optimized with CT images to predict the EGFR and PD-L1 status. We used feature selectors and feature fusion methods to find the best model among combinations of module types. The models were evaluated using the areas under the receiver operating characteristic curves (AUCs). RESULTS: Our multitask AI system yielded promising performance for gene expression status, subtype classification, and joint prediction. The AUCs of DL module achieved 0.842 (95% CI, 0.825-0.855) in the EGFR mutated status and 0.805 (95% CI, 0.779-0.829) in the mutated-EGFR subtypes discrimination (19Del, L858R, other mutations). DL module also demonstrated the AUCs of 0.799 (95% CI, 0.762-0.854) in the PD-L1 expression status and 0.837 (95% CI, 0.775-0.911) in the positive-PD-L1 subtypes (PD-L1 tumor proportion score, 1%-49% and ≥50%). Furthermore, the JO module of our AI system performed well in the EGFR and PD-L1 joint cohort, with an AUC of 0.928 (95% CI, 0.909-0.946) for distinguishing EGFR mutated status and 0.905 (95% CI, 0.886-0.930) for discriminating PD-L1 expression status. CONCLUSION: Our AI system has demonstrated the encouraging results for identifying gene status and further assessing the genotypes. Both clinical indicators and radiomics features showed a complementary role in prediction and provided accurate estimates to predict EGFR and PD-L1 status. Furthermore, this non-invasive, high-throughput, and interpretable AI system can be used as an assistive tool in conjunction with or in lieu of ancillary tests and extensive diagnostic workups to facilitate early intervention.",2022,10.3389/fimmu.2022.813072,diagnosis,True
Predicting EGFR mutation status in lung adenocarcinoma on computed tomography image using deep learning,"Epidermal growth factor receptor (EGFR) genotyping is critical for treatment guidelines such as the use of tyrosine kinase inhibitors in lung adenocarcinoma. Conventional identification of EGFR genotype requires biopsy and sequence testing which is invasive and may suffer from the difficulty of accessing tissue samples. Here, we propose a deep learning model to predict EGFR mutation status in lung adenocarcinoma using non-invasive computed tomography (CT).We retrospectively collected data from 844 lung adenocarcinoma patients with pre-operative CT images, EGFR mutation and clinical information from two hospitals. An end-to-end deep learning model was proposed to predict the EGFR mutation status by CT scanning.By training in 14 926 CT images, the deep learning model achieved encouraging predictive performance in both the primary cohort (n=603; AUC 0.85, 95% CI 0.83-0.88) and the independent validation cohort (n=241; AUC 0.81, 95% CI 0.79-0.83), which showed significant improvement over previous studies using hand-crafted CT features or clinical characteristics (p<0.001). The deep learning score demonstrated significant differences in EGFR-mutant and EGFR-wild type tumours (p<0.001).Since CT is routinely used in lung cancer diagnosis, the deep learning model provides a non-invasive and easy-to-use method for EGFR mutation status prediction.",2019,10.1183/13993003.00986-2018,diagnosis,True
Predicting lung nodule malignancies by combining deep convolutional neural network and handcrafted features,"To predict lung nodule malignancy with a high sensitivity and specificity for low dose CT (LDCT) lung cancer screening, we propose a fusion algorithm that combines handcrafted features (HF) into the features learned at the output layer of a 3D deep convolutional neural network (CNN). First, we extracted twenty-nine HF, including nine intensity features, eight geometric features, and twelve texture features based on grey-level co-occurrence matrix (GLCM). We then trained 3D CNNs modified from three 2D CNN architectures (AlexNet, VGG-16 Net and Multi-crop Net) to extract the CNN features learned at the output layer. For each 3D CNN, the CNN features combined with the 29 HF were used as the input for the support vector machine (SVM) coupled with the sequential forward feature selection (SFS) method to select the optimal feature subset and construct the classifiers. The fusion algorithm takes full advantage of the HF and the highest level CNN features learned at the output layer. It can overcome the disadvantage of the HF that may not fully reflect the unique characteristics of a particular lesion by combining the intrinsic CNN features. Meanwhile, it also alleviates the requirement of a large scale annotated dataset for the CNNs based on the complementary of HF. The patient cohort includes 431 malignant nodules and 795 benign nodules extracted from the LIDC/IDRI database. For each investigated CNN architecture, the proposed fusion algorithm achieved the highest AUC, accuracy, sensitivity, and specificity scores among all competitive classification models.",2019,10.1088/1361-6560/ab326a,diagnosis,True
Predicting Malignant Nodules from Screening CT Scans,"OBJECTIVES: The aim of this study was to determine whether quantitative analyses (""radiomics"") of low-dose computed tomography lung cancer screening images at baseline can predict subsequent emergence of cancer. METHODS: Public data from the National Lung Screening Trial (ACRIN 6684) were assembled into two cohorts of 104 and 92 patients with screen-detected lung cancer and then matched with cohorts of 208 and 196 screening subjects with benign pulmonary nodules. Image features were extracted from each nodule and used to predict the subsequent emergence of cancer. RESULTS: The best models used 23 stable features in a random forests classifier and could predict nodules that would become cancerous 1 and 2 years hence with accuracies of 80% (area under the curve 0.83) and 79% (area under the curve 0.75), respectively. Radiomics outperformed the Lung Imaging Reporting and Data System and volume-only approaches. The performance of the McWilliams risk assessment model was commensurate. CONCLUSIONS: The radiomics of lung cancer screening computed tomography scans at baseline can be used to assess risk for development of cancer.",2016,10.1016/j.jtho.2016.07.002,diagnosis,True
Predicting response to cancer immunotherapy using noninvasive radiomic biomarkers,"INTRODUCTION: Immunotherapy is regarded as one of the major breakthroughs in cancer treatment. Despite its success, only a subset of patients responds-urging the quest for predictive biomarkers. We hypothesize that artificial intelligence (AI) algorithms can automatically quantify radiographic characteristics that are related to and may therefore act as noninvasive radiomic biomarkers for immunotherapy response. PATIENTS AND METHODS: In this study, we analyzed 1055 primary and metastatic lesions from 203 patients with advanced melanoma and non-small-cell lung cancer (NSCLC) undergoing anti-PD1 therapy. We carried out an AI-based characterization of each lesion on the pretreatment contrast-enhanced CT imaging data to develop and validate a noninvasive machine learning biomarker capable of distinguishing between immunotherapy responding and nonresponding. To define the biological basis of the radiographic biomarker, we carried out gene set enrichment analysis in an independent dataset of 262 NSCLC patients. RESULTS: The biomarker reached significant performance on NSCLC lesions (up to 0.83 AUC, P < 0.001) and borderline significant for melanoma lymph nodes (0.64 AUC, P = 0.05). Combining these lesion-wide predictions on a patient level, immunotherapy response could be predicted with an AUC of up to 0.76 for both cancer types (P < 0.001), resulting in a 1-year survival difference of 24% (P = 0.02). We found highly significant associations with pathways involved in mitosis, indicating a relationship between increased proliferative potential and preferential response to immunotherapy. CONCLUSIONS: These results indicate that radiographic characteristics of lesions on standard-of-care imaging may function as noninvasive biomarkers for response to immunotherapy, and may show utility for improved patient stratification in both neoadjuvant and palliative settings.",2019,10.1093/annonc/mdz108,treatment,True
Predicting response to immunotherapy in advanced non-small-cell lung cancer using tumor mutational burden radiomic biomarker,"BACKGROUND: Tumor mutational burden (TMB) is a significant predictor of immune checkpoint inhibitors (ICIs) efficacy. This study investigated the correlation between deep learning radiomic biomarker and TMB, including its predictive value for ICIs treatment response in patients with advanced non-small-cell lung cancer (NSCLC). METHODS: CT images from 327 patients with TMB data (TMB median=6.067 mutations per megabase (range: 0 to 42.151)) were retrospectively collected and randomly divided into a training (n=236), validation (n=26), and test cohort (n=65). We used 3D-densenet to estimate the target tumor area, which used 1020 deep learning features to distinguish High-TMB from Low-TMB patients and establish the TMB radiomic biomarker (TMBRB). The TMBRB was developed in the training cohort combined with validation cohort and evaluated in the test cohort. The predictive value of TMBRB was assessed in a cohort of 123 NSCLC patients who had received ICIs (survival median=462 days (range: 16 to 1128)). RESULTS: TMBRB discriminated between High-TMB and Low-TMB patients in the training cohort (area under the curve (AUC): 0.85, 95% CI: 0.84 to 0.87))and test cohort (AUC: 0.81, 95% CI: 0.77 to 0.85). In this study, the predictive value of TMBRB was better than that of a histological subtype (AUC of training cohort: 0.75, 95% CI: 0.72 to 0.77; AUC of test cohort: 0.71, 95% CI: 0.66 to 0.76) or Radiomic model (AUC of training cohort: 0.75, 95% CI: 0.72 to 0.77; AUC of test cohort: 0.74, 95% CI: 0.69 to 0.79). When predicting immunotherapy efficacy, TMBRB divided patients into a high- and low-risk group with distinctly different overall survival (OS; HR: 0.54, 95% CI: 0.31 to 0.95; p=0.030) and progression-free survival (PFS; HR: 1.78, 95% CI: 1.07 to 2.95; p=0.023). Moreover, TMBRB had a better predictive ability when combined with the Eastern Cooperative Oncology Group performance status (OS: p=0.007; PFS: p=0.003). Visual analysis revealed that tumor microenvironment was important for predicting TMB. CONCLUSION: By combining deep learning technology and CT images, we developed an individual non-invasive biomarker that could distinguish High-TMB from Low-TMB, which might inform decisions on the use of ICIs in patients with advanced NSCLC.",2020,10.1136/jitc-2020-000550,treatment,True
"Predicting tumor hypoxia in non-small cell lung cancer by combining CT, FDG PET and dynamic contrast-enhanced CT","BACKGROUND: Most solid tumors contain inadequately oxygenated (i.e., hypoxic) regions, which tend to be more aggressive and treatment resistant. Hypoxia PET allows visualization of hypoxia and may enable treatment adaptation. However, hypoxia PET imaging is expensive, time-consuming and not widely available. We aimed to predict hypoxia levels in non-small cell lung cancer (NSCLC) using more easily available imaging modalities: FDG-PET/CT and dynamic contrast-enhanced CT (DCE-CT). MATERIAL AND METHODS: For 34 NSCLC patients, included in two clinical trials, hypoxia HX4-PET/CT, planning FDG-PET/CT and DCE-CT scans were acquired before radiotherapy. Scans were non-rigidly registered to the planning CT. Tumor blood flow (BF) and blood volume (BV) were calculated by kinetic analysis of DCE-CT images. Within the gross tumor volume, independent clusters, i.e., supervoxels, were created based on FDG-PET/CT. For each supervoxel, tumor-to-background ratios (TBR) were calculated (median SUV/aorta SUV(mean)) for HX4-PET/CT and supervoxel features (median, SD, entropy) for the other modalities. Two random forest models (cross-validated: 10 folds, five repeats) were trained to predict the hypoxia TBR; one based on CT, FDG, BF and BV, and one with only CT and FDG features. Patients were split in a training (trial NCT01024829) and independent test set (trial NCT01210378). For each patient, predicted, and observed hypoxic volumes (HV) (TBR > 1.2) were compared. RESULTS: Fifteen patients (3291 supervoxels) were used for training and 19 patients (1502 supervoxels) for testing. The model with all features (RMSE training: 0.19 ± 0.01, test: 0.27) outperformed the model with only CT and FDG-PET features (RMSE training: 0.20 ± 0.01, test: 0.29). All tumors of the test set were correctly classified as normoxic or hypoxic (HV > 1 cm(3)) by the best performing model. CONCLUSIONS: We created a data-driven methodology to predict hypoxia levels and hypoxia spatial patterns using CT, FDG-PET and DCE-CT features in NSCLC. The model correctly classifies all tumors, and could therefore, aid tumor hypoxia classification and patient stratification.",2017,10.1080/0284186x.2017.1349332,diagnosis,True
"Predicting Unnecessary Nodule Biopsies from a Small, Unbalanced, and Pathologically Proven Dataset by Transfer Learning","This study explores an automatic diagnosis method to predict unnecessary nodule biopsy from a small, unbalanced, and pathologically proven database. The automatic diagnosis method is based on a convolutional neural network (CNN) model. Because of the small and unbalanced samples, the presented method aims to improve the transfer learning capability via the VGG16 architecture and optimize the related transfer learning parameters. For comparison purpose, a traditional machine learning method is implemented, which extracts the texture features and classifies the features by support vector machine (SVM). The database includes 68 biopsied nodules, 16 are pathologically proven benign and the remaining 52 are malignant. To consider the volumetric data by the CNN model, each image slice from each nodule volume is selected randomly until all image slices of each nodule are utilized. The leave-one-out and 10-folder cross validations are applied to train and test the randomly selected 68 image slices (one image slice from one nodule) in each experiment, respectively. The averages over all the experimental outcomes are the final results. The experiments revealed that the features from both the medical and the natural images share the similarity of focusing on simpler and less-abstract objects, leading to the conclusion that not the more the transfer convolutional layers, the better the classification results. Transfer learning from other larger datasets can supply additional information to small and unbalanced datasets to improve the classification performance. The presented method has shown the potential to adapt CNN architecture to improve the prediction of unnecessary nodule biopsy from small, unbalanced, and pathologically proven volumetric dataset.",2020,10.1007/s10278-019-00306-z,diagnosis,True
"Prediction of COVID Criticality Score with Laboratory, Clinical and CT Images using Hybrid Regression Models","BACKGROUND AND OBJECTIVE: Rapid and precise diagnosis of COVID-19 is very critical in hotspot regions. The main aim of this proposed work is to investigate the baseline, laboratory and CT features of COVID-19 affected patients of two groups (Early and Critical stages). The detection model for COVID-19 is built depending upon the manifestations that define the severity of the disease. METHODS: The CT scan images are fed into the various deep learning, machine learning and hybrid learning models to mine the necessary features and predict CT Score. The predicted CT score along with other clinical, laboratory and CT scan image features are then passed to train the various Regression models for predicting the COVID Criticality (CC) Score. These baseline, laboratory and CT features of COVID-19 are reduced using Statistical analysis and Univariate logistic regression analysis. RESULTS: When analysing the prediction of CT scores using images alone, AlexNet+Lasso yields better outcome with regression score of 0.9643 and RMSE of 0.0023 when compared with Decision tree (RMSE of 0.0034; Regression score of 0.9578) and GRU (RMSE of 0.1253; regression score of 0.9323). When analysing the prediction of CC scores using CT scores and other baseline, laboratory and CT features, VGG-16+Linear Regression yields better results with regression score of 0.9911 and RMSE of 0.0002 when compared with Linear SVR (RMSE of 0.0006; Regression score of 0.9911) and LSTM (RMSE of 0.0005; Regression score of 0.9877). The correlation analysis is performed to identify the significance of utilizing other features in prediction of CC Score. The correlation coefficient of CT scores with actual value is 0.93 and 0.92 for Early stage group and Critical stage group respectively. The correlation coefficient of CC scores with actual value is 0.96 for Early stage group and 0.95 for Critical stage group.The classification of COVID-19 patients are carried out with the help of predicted CC Scores. CONCLUSIONS: This proposed work is carried out in the motive of helping radiologists in faster categorization of COVID patients as Early or Severe staged using CC Scores. The automated prediction of COVID Criticality Score using our diagnostic model can help radiologists and physicians save time for carrying out further treatment and procedures.",2021,10.1016/j.cmpb.2021.106336,diagnosis,True
Prediction of disease progression in patients with COVID-19 by artificial intelligence assisted lesion quantification,"To investigate the value of artificial intelligence (AI) assisted quantification on initial chest CT for prediction of disease progression and clinical outcome in patients with coronavirus disease 2019 (COVID-19). Patients with confirmed COVID-19 infection and initially of non-severe type were retrospectively included. The initial CT scan on admission was used for imaging analysis. The presence of ground glass opacity (GGO), consolidation and other findings were visually evaluated. CT severity score was calculated according to the extent of lesion involvement. In addition, AI based quantification of GGO and consolidation volume were also performed. 123 patients (mean age: 64.43 ± 14.02; 62 males) were included. GGO + consolidation was more frequently revealed in progress-to-severe group whereas pure GGO was more likely to be found in non-severe group. Compared to non-severe group, patients in progress-to-severe group had larger GGO volume (167.33 ± 167.88 cm(3) versus 101.12 ± 127 cm(3), p = 0.013) as well as consolidation volume (40.85 ± 60.4 cm(3) versus 6.63 ± 14.91 cm(3), p < 0.001). Among imaging parameters, consolidation volume had the largest area under curve (AUC) in discriminating non-severe from progress-to-severe group (AUC = 0.796, p < 0.001) and patients with or without critical events (AUC = 0.754, p < 0.001). According to multivariate regression, consolidation volume and age were two strongest predictors for disease progression (hazard ratio: 1.053 and 1.071, p: 0.006 and 0.008) whereas age and diabetes were predictors for unfavorable outcome. Consolidation volume quantified on initial chest CT was the strongest predictor for disease severity progression and larger consolidation volume was associated with unfavorable clinical outcome.",2020,10.1038/s41598-020-79097-1,prognosis,True
Prediction of lung cancer incidence on the low-dose computed tomography arm of the National Lung Screening Trial: A dynamic Bayesian network,"INTRODUCTION: Identifying high-risk lung cancer individuals at an early disease stage is the most effective way of improving survival. The landmark National Lung Screening Trial (NLST) demonstrated the utility of low-dose computed tomography (LDCT) imaging to reduce mortality (relative to X-ray screening). As a result of the NLST and other studies, imaging-based lung cancer screening programs are now being implemented. However, LDCT interpretation results in a high number of false positives. A set of dynamic Bayesian networks (DBN) were designed and evaluated to provide insight into how longitudinal data can be used to help inform lung cancer screening decisions. METHODS: The LDCT arm of the NLST dataset was used to build and explore five DBNs for high-risk individuals. Three of these DBNs were built using a backward construction process, and two using structure learning methods. All models employ demographics, smoking status, cancer history, family lung cancer history, exposure risk factors, comorbidities related to lung cancer, and LDCT screening outcome information. Given the uncertainty arising from lung cancer screening, a cancer state-space model based on lung cancer staging was utilized to characterize the cancer status of an individual over time. The models were evaluated on balanced training and test sets of cancer and non-cancer cases to deal with data imbalance and overfitting. RESULTS: Results were comparable to expert decisions. The average area under the curve (AUC) of the receiver operating characteristic (ROC) for the three intervention points of the NLST trial was higher than 0.75 for all models. Evaluation of the models on the complete LDCT arm of the NLST dataset (N=25,486) demonstrated satisfactory generalization. Consensus of predictions over similar cases is reported in concordance statistics between the models' and the physicians' predictions. The models' predictive ability with respect to missing data was also evaluated with the sample of cases that missed the second screening exam of the trial (N=417). The DBNs outperformed comparison models such as logistic regression and naïve Bayes. CONCLUSION: The lung cancer screening DBNs demonstrated high discrimination and predictive power with the majority of cancer and non-cancer cases.",2016,10.1016/j.artmed.2016.07.001,diagnosis,True
Prediction of lung cancer risk at follow-up screening with low-dose CT: a training and validation study of a deep learning method,"BACKGROUND: Current lung cancer screening guidelines use mean diameter, volume or density of the largest lung nodule in the prior computed tomography (CT) or appearance of new nodule to determine the timing of the next CT. We aimed at developing a more accurate screening protocol by estimating the 3-year lung cancer risk after two screening CTs using deep machine learning (ML) of radiologist CT reading and other universally available clinical information. METHODS: A deep machine learning (ML) algorithm was developed from 25,097 participants who had received at least two CT screenings up to two years apart in the National Lung Screening Trial. Double-blinded validation was performed using 2,294 participants from the Pan-Canadian Early Detection of Lung Cancer Study (PanCan). Performance of ML score to inform lung cancer incidence was compared with Lung-RADS and volume doubling time using time-dependent ROC analysis. Exploratory analysis was performed to identify individuals with aggressive cancers and higher mortality rates. FINDINGS: In the PanCan validation cohort, ML showed excellent discrimination with a 1-, 2- and 3-year time-dependent AUC values for cancer diagnosis of 0·968±0·013, 0·946±0·013 and 0·899±0·017. Although high ML score cohort included only 10% of the PanCan sample, it identified 94%, 85%, and 71% of incident and interval lung cancers diagnosed within 1, 2, and 3 years, respectively, after the second screening CT. Furthermore, individuals with high ML score had significantly higher mortality rates (HR=16·07, p<0·001) compared to those with lower risk. INTERPRETATION: ML tool that recognizes patterns in both temporal and spatial changes as well as synergy among changes in nodule and non-nodule features may be used to accurately guide clinical management after the next scheduled repeat screening CT.",2019,10.1016/s2589-7500(19)30159-1,diagnosis,True
Prediction of lung emphysema in COPD by spirometry and clinical symptoms: results from COSYCONET,"BACKGROUND: Lung emphysema is an important phenotype of chronic obstructive pulmonary disease (COPD), and CT scanning is strongly recommended to establish the diagnosis. This study aimed to identify criteria by which physicians with limited technical resources can improve the diagnosis of emphysema. METHODS: We studied 436 COPD patients with prospective CT scans from the COSYCONET cohort. All items of the COPD Assessment Test (CAT) and the St George's Respiratory Questionnaire (SGRQ), the modified Medical Research Council (mMRC) scale, as well as data from spirometry and CO diffusing capacity, were used to construct binary decision trees. The importance of parameters was checked by the Random Forest and AdaBoost machine learning algorithms. RESULTS: When relying on questionnaires only, items CAT 1 & 7 and SGRQ 8 & 12 sub-item 3 were most important for the emphysema- versus airway-dominated phenotype, and among the spirometric measures FEV(1)/FVC. The combination of CAT item 1 (≤ 2) with mMRC (> 1) and FEV(1)/FVC, could raise the odds for emphysema by factor 7.7. About 50% of patients showed combinations of values that did not markedly alter the likelihood for the phenotypes, and these could be easily identified in the trees. Inclusion of CO diffusing capacity revealed the transfer coefficient as dominant measure. The results of machine learning were consistent with those of the single trees. CONCLUSIONS: Selected items (cough, sleep, breathlessness, chest condition, slow walking) from comprehensive COPD questionnaires in combination with FEV(1)/FVC could raise or lower the likelihood for lung emphysema in patients with COPD. The simple, parsimonious approach proposed by us might help if diagnostic resources regarding respiratory diseases are limited. Trial registration ClinicalTrials.gov, Identifier: NCT01245933, registered 18 November 2010, https://clinicaltrials.gov/ct2/show/record/NCT01245933 .",2021,10.1186/s12931-021-01837-2,prognosis,True
Prediction of mediastinal lymph node metastasis based on (18)F-FDG PET/CT imaging using support vector machine in non-small cell lung cancer,"OBJECTIVE: The purpose of this study was to develop a classification method based on support vector machine (SVM) to improve the diagnostic performance of (18)F-fluorodeoxyglucose (FDG) positron emission tomography/computed tomography (PET/CT) to detect the lymph node (LN) metastasis in non-small cell lung cancer (NSCLC). METHOD: Two hundred nineteen lymph nodes (37 metastatic) from 71 patients were evaluated in this study. SVM models were developed with 7 LN features. The area under the curve (AUC) and accuracy of 9 models were compared to select the best model. The best SVM model was simplified on the basis of the feature weights and value distribution to further suit the clinical application. RESULTS: The maximum, minimum, and mean accuracy of the best model was 91.89% (68/74, 95% CI 83.11~96.54%), 66.22% (49/74, 95% CI 54.85~75.98%), and 80.09% (59,266/74,000, 95% CI 70.27~89.19%), respectively, with an AUC of 0.94, 0.66, and 0.81, respectively. The best SVM model was finally simplified into a score rule: LNs with scores more than 3.0 were considered as malignant ones, whereas LNs with scores less than 1.5 tended to be benign ones. For the LNs with scores within a range of 1.5-3.0, metastasis was suspected. CONCLUSION: An SVM model based on (18)F-FDG PET/CT images was able to predict the metastatic LNs for patients with NSCLC. The ratio of the maximum of standard uptake value of LNs to aortic arch played a major role in the model. After simplification, the model could be transferred into a scoring method which may partly help clinicians determine the clinical staging of patients with NSCLC relatively easier. KEY POINTS: • The SVM model based on (18)F-FDG PET/CT features may help clinicians to make a decision for metastatic mediastinal lymph nodes in patients with NSCLC. • The SUR(blood) plays a major role in the SVM model. • The score rule based on the SVM model simplified the complexity of the model and may partly help clinicians determine the clinical staging of patients with NSCLC relatively easier.",2021,10.1007/s00330-020-07466-5,diagnosis,True
Prediction of Obstructive Lung Disease from Chest Radiographs via Deep Learning Trained on Pulmonary Function Data,"BACKGROUND: Chronic obstructive pulmonary disease (COPD), the third leading cause of death worldwide, is often underdiagnosed. PURPOSE: To develop machine learning methods to predict COPD using chest radiographs and a convolutional neural network (CNN) trained with near-concurrent pulmonary function test (PFT) data. Comparison is made to natural language processing (NLP) of the associated radiologist text reports. MATERIALS AND METHODS: This IRB-approved single-institution retrospective study uses 6749 two-view chest radiograph exams (2012-2017, 4436 unique subjects, 54% female, 46% male), same-day associated radiologist text reports, and PFT exams acquired within 180 days. The Image Model (Resnet18 pre-trained with ImageNet CNN) is trained using frontal and lateral radiographs and PFTs with 10% of the subjects for validation and 19% for testing. The NLP Model is trained using radiologist text reports and PFTs. The primary metric of model comparison is the area under the receiver operating characteristic curve (AUC). RESULTS: The Image Model achieves an AUC of 0.814 for prediction of obstructive lung disease (FEV1/FVC <0.7) from chest radiographs and performs better than the NLP Model (AUC 0.704, p<0.001) from radiologist text reports where FEV1 = forced expiratory volume in 1 second and FVC = forced vital capacity. The Image Model performs better for prediction of severe or very severe COPD (FEV1 <0.5) with an AUC of 0.837 versus the NLP model AUC of 0.770 (p<0.001). CONCLUSION: A CNN Image Model trained on physiologic lung function data (PFTs) can be applied to chest radiographs for quantitative prediction of obstructive lung disease with good accuracy.",2020,10.2147/copd.S279850,diagnosis,False
Prediction of pathologic stage in non-small cell lung cancer using machine learning algorithm based on CT image feature analysis,"PURPOSE: To explore imaging biomarkers that can be used for diagnosis and prediction of pathologic stage in non-small cell lung cancer (NSCLC) using multiple machine learning algorithms based on CT image feature analysis. METHODS: Patients with stage IA to IV NSCLC were included, and the whole dataset was divided into training and testing sets and an external validation set. To tackle imbalanced datasets in NSCLC, we generated a new dataset and achieved equilibrium of class distribution by using SMOTE algorithm. The datasets were randomly split up into a training/testing set. We calculated the importance value of CT image features by means of mean decrease gini impurity generated by random forest algorithm and selected optimal features according to feature importance (mean decrease gini impurity > 0.005). The performance of prediction model in training and testing sets were evaluated from the perspectives of classification accuracy, average precision (AP) score and precision-recall curve. The predictive accuracy of the model was externally validated using lung adenocarcinoma (LUAD) and lung squamous cell carcinoma (LUSC) samples from TCGA database. RESULTS: The prediction model that incorporated nine image features exhibited a high classification accuracy, precision and recall scores in the training and testing sets. In the external validation, the predictive accuracy of the model in LUAD outperformed that in LUSC. CONCLUSIONS: The pathologic stage of patients with NSCLC can be accurately predicted based on CT image features, especially for LUAD. Our findings extend the application of machine learning algorithms in CT image feature prediction for pathologic staging and identify potential imaging biomarkers that can be used for diagnosis of pathologic stage in NSCLC patients.",2019,10.1186/s12885-019-5646-9,diagnosis,True
Prediction of Patient Management in COVID-19 Using Deep Learning-Based Fully Automated Extraction of Cardiothoracic CT Metrics and Laboratory Findings,"OBJECTIVE: To extract pulmonary and cardiovascular metrics from chest CTs of patients with coronavirus disease 2019 (COVID-19) using a fully automated deep learning-based approach and assess their potential to predict patient management. MATERIALS AND METHODS: All initial chest CTs of patients who tested positive for severe acute respiratory syndrome coronavirus 2 at our emergency department between March 25 and April 25, 2020, were identified (n = 120). Three patient management groups were defined: group 1 (outpatient), group 2 (general ward), and group 3 (intensive care unit [ICU]). Multiple pulmonary and cardiovascular metrics were extracted from the chest CT images using deep learning. Additionally, six laboratory findings indicating inflammation and cellular damage were considered. Differences in CT metrics, laboratory findings, and demographics between the patient management groups were assessed. The potential of these parameters to predict patients' needs for intensive care (yes/no) was analyzed using logistic regression and receiver operating characteristic curves. Internal and external validity were assessed using 109 independent chest CT scans. RESULTS: While demographic parameters alone (sex and age) were not sufficient to predict ICU management status, both CT metrics alone (including both pulmonary and cardiovascular metrics; area under the curve [AUC] = 0.88; 95% confidence interval [CI] = 0.79-0.97) and laboratory findings alone (C-reactive protein, lactate dehydrogenase, white blood cell count, and albumin; AUC = 0.86; 95% CI = 0.77-0.94) were good classifiers. Excellent performance was achieved by a combination of demographic parameters, CT metrics, and laboratory findings (AUC = 0.91; 95% CI = 0.85-0.98). Application of a model that combined both pulmonary CT metrics and demographic parameters on a dataset from another hospital indicated its external validity (AUC = 0.77; 95% CI = 0.66-0.88). CONCLUSION: Chest CT of patients with COVID-19 contains valuable information that can be accessed using automated image analysis. These metrics are useful for the prediction of patient management.",2021,10.3348/kjr.2020.0994,prognosis,True
Prediction of Pulmonary Fibrosis Based on X-Rays by Deep Neural Network,"As a fatal lung disease, pulmonary fibrosis can cause irreversible damage to the lung, affect normal lung function, and eventually lead to death. At present, the pathogenesis of this kind of disease is not completely clear, and there is no radical cure. The main purpose of the treatment of this disease is to slow down the deterioration of pulmonary fibrosis. For this kind of disease, if it can be found early, it can be treated as soon as possible and the life of patients will be prolonged. Clinically, the diagnosis of pulmonary fibrosis depends on the relevant imaging examination, lung biopsy, lung function examination, and so on. Imaging data such as X-rays is a common examination means in clinical medicine and also plays an important role in the prediction of pulmonary fibrosis. Through X-ray, radiologists can clearly see the relevant lung lesions so as to make the relevant diagnosis. Based on the common medical image data, this paper designs related models to complete the prediction of pulmonary fibrosis. The model designed in this paper is mainly divided into two parts: first, this paper uses a neural network to complete the segmentation of lung organs; second, the neural network of image classification is designed to complete the process from lung image to disease prediction. In the design of these two parts, this paper improves on the basis of previous research methods. Through the design of a neural network with higher performance, more optimized results are achieved on the key indicators which can be applied to the real scene of pulmonary fibrosis prediction.",2022,10.1155/2022/3845008,diagnosis,False
Prediction of radiation pneumonitis after definitive radiotherapy for locally advanced non-small cell lung cancer using multi-region radiomics analysis,"To predict grade ≥ 2 radiation pneumonitis (RP) in patients with locally advanced non-small cell lung cancer (NSCLC) using multi-region radiomics analysis. Data from 77 patients with NSCLC who underwent definitive radiotherapy between 2008 and 2018 were analyzed. Radiomic feature extraction from the whole lung (whole-lung radiomics analysis) and imaging- and dosimetric-based segmentation (multi-region radiomics analysis) were performed. Patients with RP grade ≥ 2 or < 2 were classified. Predictors were selected with least absolute shrinkage and selection operator logistic regression and the model was built with neural network classifiers. A total of 49,383 radiomics features per patient image were extracted from the radiotherapy planning computed tomography. We identified 4 features and 13 radiomics features in the whole-lung and multi-region radiomics analysis for classification, respectively. The accuracy and area under the curve (AUC) without the synthetic minority over-sampling technique (SMOTE) were 60.8%, and 0.62 for whole-lung and 80.1%, and 0.84 for multi-region radiomics analysis. These were improved 1.7% for whole-lung and 2.1% for multi-region radiomics analysis with the SMOTE. The developed multi-region radiomics analysis can help predict grade ≥ 2 RP. The radiomics features in the median- and high-dose regions, and the local intensity roughness and variation were important factors in predicting grade ≥ 2 RP.",2021,10.1038/s41598-021-95643-x,diagnosis,True
Prediction of radiation pneumonitis with machine learning using 4D-CT based dose-function features,"In this article, we highlight the fundamental importance of the simultaneous use of dose-volume histogram (DVH) and dose-function histogram (DFH) features based on functional images calculated from 4-dimensional computed tomography (4D-CT) and deformable image registration (DIR) in developing a multivariate radiation pneumonitis (RP) prediction model. The patient characteristics, DVH features and DFH features were calculated from functional images by Hounsfield unit (HU) and Jacobian metrics, for an RP grade ≥ 2 multivariate prediction models were computed from 85 non-small cell lung cancer patients. The prediction model is developed using machine learning via a kernel-based support vector machine (SVM) machine. In the patient cohort, 21 of the 85 patients (24.7%) presented with RP grade ≥ 2. The median area under curve (AUC) was 0.58 for the generated 50 prediction models with patient clinical features and DVH features. When HU metric and Jacobian metric DFH features were added, the AUC improved to 0.73 and 0.68, respectively. We conclude that predictive RP models that incorporate DFH features were successfully developed via kernel-based SVM. These results demonstrate that effectiveness of the simultaneous use of DVH features and DFH features calculated from 4D-CT and DIR on functional image-guided radiotherapy.",2022,10.1093/jrr/rrab097,diagnosis,True
Prediction of the motion of chest internal points using a recurrent neural network trained with real-time recurrent learning for latency compensation in lung cancer radiotherapy,"During the radiotherapy treatment of patients with lung cancer, the radiation delivered to healthy tissue around the tumor needs to be minimized, which is difficult because of respiratory motion and the latency of linear accelerator (LINAC) systems. In the proposed study, we first use the Lucas-Kanade pyramidal optical flow algorithm to perform deformable image registration (DIR) of chest computed tomography (CT) scan images of four patients with lung cancer. We then track three internal points close to the lung tumor based on the previously computed deformation field and predict their position with a recurrent neural network (RNN) trained using real-time recurrent learning (RTRL) and gradient clipping. The breathing data is quite regular, sampled at approximately 2.5 Hz, and includes artificially added drift in the spine direction. The amplitude of the motion of the tracked points ranged from 12.0 mm to 22.7 mm. Finally, we propose a simple method for recovering and predicting three-dimensional (3D) tumor images from the tracked points and the initial tumor image, based on a linear correspondence model and the Nadaraya-Watson non-linear regression. The root-mean-square (RMS) error, maximum error and jitter corresponding to the RNN prediction on the test set were smaller than the same performance measures obtained with linear prediction and least mean squares (LMS). In particular, the maximum prediction error associated with the RNN, equal to 1.51 mm, is respectively 16.1% and 5.0% lower than the error given by a linear predictor and LMS. The average prediction time per time step with RTRL is equal to 119 ms, which is less than the 400 ms marker position sampling time. The tumor position in the predicted images appears visually correct, which is confirmed by the high mean cross-correlation between the original and predicted images, equal to 0.955. The standard deviation of the Gaussian kernel and the number of layers in the optical flow algorithm were the parameters having the most significant impact on registration performance. Their optimization led respectively to a 31.3% and 36.2% decrease in the registration error. Using only a single layer proved to be detrimental to the registration quality because tissue motion in the lower part of the lung has a high amplitude relative to the resolution of the CT scan images. The random initialization of the hidden units and the number of these hidden units were found to be the most important factors affecting the performance of the RNN. Increasing the number of hidden units from 15 to 250 led to a 56.3% decrease in the prediction error on the cross-validation data. Similarly, optimizing the standard deviation of the initial Gaussian distribution of the synaptic weights σ(init)(RNN) led to a 28.4% decrease in the prediction error on the cross-validation data, with the error minimized for σ(init)(RNN)=0.02 with the four patients.",2021,10.1016/j.compmedimag.2021.101941,treatment,True
Prediction of visceral pleural invasion in lung cancer on CT: deep learning model achieves a radiologist-level performance with adaptive sensitivity and specificity to clinical needs,"OBJECTIVES: To develop and validate a preoperative CT-based deep learning model for the prediction of visceral pleural invasion (VPI) in early-stage lung cancer. METHODS: In this retrospective study, dataset 1 (for training, tuning, and internal validation) included 676 patients with clinical stage IA lung adenocarcinomas resected between 2009 and 2015. Dataset 2 (for temporal validation) included 141 patients with clinical stage I adenocarcinomas resected between 2017 and 2018. A CT-based deep learning model was developed for the prediction of VPI and validated in terms of discrimination and calibration. An observer performance study and a multivariable regression analysis were performed. RESULTS: The area under the receiver operating characteristic curve (AUC) of the model was 0.75 (95% CI, 0.67-0.84), which was comparable to those of board-certified radiologists (AUC, 0.73-0.79; all p > 0.05). The model had a higher standardized partial AUC for a specificity range of 90 to 100% than the radiologists (all p < 0.05). The high sensitivity cutoff (0.245) yielded a sensitivity of 93.8% and a specificity of 31.2%, and the high specificity cutoff (0.448) resulted in a sensitivity of 47.9% and a specificity of 86.0%. Two of the three radiologists provided highly sensitive (93.8% and 97.9%) but not specific (48.4% and 40.9%) diagnoses. The model showed good calibration (p > 0.05), and its output was an independent predictor for VPI (adjusted odds ratio, 1.07; 95% CI, 1.03-1.11; p < 0.001). CONCLUSIONS: The deep learning model demonstrated a radiologist-level performance. The model could achieve either highly sensitive or highly specific diagnoses depending on clinical needs. KEY POINTS: • The preoperative CT-based deep learning model demonstrated an expert-level diagnostic performance for the presence of visceral pleural invasion in early-stage lung cancer. • Radiologists had a tendency toward highly sensitive, but not specific diagnoses for the visceral pleural invasion.",2021,10.1007/s00330-020-07431-2,diagnosis,True
"Predictive usefulness of RT-PCR testing in different patterns of Covid-19 symptomatology: analysis of a French cohort of 12,810 outpatients","Reverse transcriptase polymerase chain reaction (RT-PCR) is a key tool to diagnose Covid-19. Yet it may not be the most efficient test in all patients. In this paper, we develop a clinical strategy for prescribing RT-PCR to patients based on data from COVIDOM, a French cohort of 54,000 patients with clinically suspected Covid-19, including 12,810 patients tested by RT-PCR. We use a machine-learning algorithm (decision tree) in order to predict RT-PCR results based on the clinical presentation. We show that symptoms alone are sufficient to predict RT-PCR outcome with a mean average precision of 86%. We identify combinations of symptoms that are predictive of RT-PCR positivity (90% for anosmia/ageusia) or negativity (only 30% of RT-PCR+ for a subgroup with cardiopulmonary symptoms): in both cases, RT-PCR provides little added diagnostic value. We propose a prescribing strategy based on clinical presentation that can improve the global efficiency of RT-PCR testing.",2021,10.1038/s41598-021-99991-6,diagnosis,True
Predictive value of a novel Asian lung cancer screening nomogram based on artificial intelligence and epidemiological characteristics,"BACKGROUND: To develop and validate a risk prediction nomogram based on a deep learning convolutional neural networks (CNN) model and epidemiological characteristics for lung cancer screening in patients with small pulmonary nodules (SPN). METHODS: This study included three data sets. First, a CNN model was developed and tested on data set 1. Then, a hybrid prediction model was developed on data set 2 by multivariable binary logistic regression analysis. We combined the CNN model score and the selected epidemiological risk factors, and a risk prediction nomogram was presented. An independent multicenter cohort was used for model external validation. The performance of the nomogram was assessed with respect to its calibration and discrimination. RESULTS: The final hybrid model included the CNN model score and the screened risk factors included age, gender, smoking status and family history of cancer. The nomogram showed good discrimination and calibration with an area under the curve (AUC) of 91.6% (95% CI: 89.4%-93.5%), compare with the CNN model, the improvement was significance. The performance of the nomogram still showed good discrimination and good calibration in the multicenter validation cohort, with an AUC of 88.3% (95% CI: 83.1%-92.3%). CONCLUSIONS: Our study showed that epidemiological characteristics should be considered in lung cancer screening, which can significantly improve the efficiency of the artificial intelligence (AI) model alone. We combined the CNN model score with Asian lung cancer epidemiological characteristics to develop a new nomogram to facilitate and accurately perform individualized lung cancer screening, especially for Asians.",2021,10.1111/1759-7714.14140,diagnosis,True
Predictors at Admission of Mechanical Ventilation and Death in an Observational Cohort of Adults Hospitalized With Coronavirus Disease 2019,"BACKGROUND: Coronavirus disease (COVID-19) can cause severe illness and death. Predictors of poor outcome collected on hospital admission may inform clinical and public health decisions. METHODS: We conducted a retrospective observational cohort investigation of 297 adults admitted to 8 academic and community hospitals in Georgia, United States, during March 2020. Using standardized medical record abstraction, we collected data on predictors including admission demographics, underlying medical conditions, outpatient antihypertensive medications, recorded symptoms, vital signs, radiographic findings, and laboratory values. We used random forest models to calculate adjusted odds ratios (aORs) and 95% confidence intervals (CIs) for predictors of invasive mechanical ventilation (IMV) and death. RESULTS: Compared with age <45 years, ages 65-74 years and ≥75 years were predictors of IMV (aORs, 3.12 [95% CI, 1.47-6.60] and 2.79 [95% CI, 1.23-6.33], respectively) and the strongest predictors for death (aORs, 12.92 [95% CI, 3.26-51.25] and 18.06 [95% CI, 4.43-73.63], respectively). Comorbidities associated with death (aORs, 2.4-3.8; P < .05) included end-stage renal disease, coronary artery disease, and neurologic disorders, but not pulmonary disease, immunocompromise, or hypertension. Prehospital use vs nonuse of angiotensin receptor blockers (aOR, 2.02 [95% CI, 1.03-3.96]) and dihydropyridine calcium channel blockers (aOR, 1.91 [95% CI, 1.03-3.55]) were associated with death. CONCLUSIONS: After adjustment for patient and clinical characteristics, older age was the strongest predictor of death, exceeding comorbidities, abnormal vital signs, and laboratory test abnormalities. That coronary artery disease, but not chronic lung disease, was associated with death among hospitalized patients warrants further investigation, as do associations between certain antihypertensive medications and death.",2021,10.1093/cid/ciaa1459,prognosis,False
Preoperative CT-based Deep Learning Model for Predicting Disease-Free Survival in Patients with Lung Adenocarcinomas,"Background Deep learning models have the potential for lung cancer prognostication, but model output as an independent prognostic factor must be validated with clinical risk factors. Purpose To develop and validate a preoperative CT-based deep learning model for predicting disease-free survival in patients with lung adenocarcinoma. Materials and Methods In this retrospective study, a deep learning model was trained to extract prognostic information from preoperative CT examinations. Data set 1 for training, tuning, and internal validation consisted of patients with T1-4N0M0 adenocarcinoma resected between 2009 and 2015. Data set 2 for external validation included patients with clinical T1-2aN0M0 (stage I) adenocarcinomas resected in 2014. Discrimination was assessed by using Harrell C index and benchmarked against the clinical T category. The Greenwood-Nam-D'Agostino test was used for model calibration. The multivariable-adjusted hazard ratios (HRs) were analyzed with clinical prognostic factors by using the Cox regression. Results Evaluated were 800 patients (median age, 64 years; interquartile range, 56-70 years; 450 women) in data set 1 and 108 patients (median age, 63 years; interquartile range, 57-71 years; 60 women) in data set 2. The C indexes were 0.74-0.80 in the internal validation and 0.71-0.78 in the external validation, both comparable with the clinical T category (0.78 in the internal validation and 0.74 in the external validation; all P > .05). The model exhibited good calibration in all data sets (P > .05). Multivariable Cox regression revealed that model outputs were independent prognostic factors (hazard ratio [HR] of the categorical output, 2.5 [95% confidence interval {CI}: 1.03, 5.9; P = .04] in the internal validation and 3.6 [95% CI: 1.6, 8.5; P = .003] in the external validation). Other than the deep learning model, only smoking status (HR, 3.4; 95% CI: 1.4, 8.5; P = .007) contributed further to prediction of disease-free survival for patients after resection of clinical stage I adenocarcinomas. Conclusion A deep learning model for chest CT predicted disease-free survival for patients undergoing an operation for clinical stage I lung adenocarcinoma. © RSNA, 2020 Online supplemental material is available for this article. See also the editorial by Shaffer in this issue.",2020,10.1148/radiol.2020192764,prognosis,True
Preoperative CT-based radiomics combined with intraoperative frozen section is predictive of invasive adenocarcinoma in pulmonary nodules: a multicenter study,"OBJECTIVES: Develop a CT-based radiomics model and combine it with frozen section (FS) and clinical data to distinguish invasive adenocarcinomas (IA) from preinvasive lesions/minimally invasive adenocarcinomas (PM). METHODS: This multicenter study cohort of 623 lung adenocarcinomas was split into training (n = 331), testing (n = 143), and external validation dataset (n = 149). Random forest models were built using selected radiomics features, results from FS, lesion volume, clinical and semantic features, and combinations thereof. The area under the receiver operator characteristic curves (AUC) was used to evaluate model performances. The diagnosis accuracy, calibration, and decision curves of models were tested. RESULTS: The radiomics-based model shows good predictive performance and diagnostic accuracy for distinguishing IA from PM, with AUCs of 0.89, 0.89, and 0.88, in the training, testing, and validation datasets, respectively, and with corresponding accuracies of 0.82, 0.79, and 0.85. Adding lesion volume and FS significantly increases the performance of the model with AUCs of 0.96, 0.97, and 0.96, and with accuracies of 0.91, 0.94, and 0.93 in the three datasets. There is no significant difference in AUC between the FS model enriched with radiomics and volume against an FS model enriched with volume alone, while the former has higher accuracy. The model combining all available information shows minor non-significant improvements in AUC and accuracy compared with an FS model enriched with radiomics and volume. CONCLUSIONS: Radiomics signatures are potential biomarkers for the risk of IA, especially in combination with FS, and could help guide surgical strategy for pulmonary nodules patients. KEY POINTS: • A CT-based radiomics model may be a valuable tool for preoperative prediction of invasive adenocarcinoma for patients with pulmonary nodules. • Radiomics combined with frozen sections could help in guiding surgery strategy for patients with pulmonary nodules.",2020,10.1007/s00330-019-06597-8,diagnosis,True
Preoperative diagnosis of malignant pulmonary nodules in lung cancer screening with a radiomics nomogram,"BACKGROUND: Lung cancer is the most commonly diagnosed cancer worldwide. Its survival rate can be significantly improved by early screening. Biomarkers based on radiomics features have been found to provide important physiological information on tumors and considered as having the potential to be used in the early screening of lung cancer. In this study, we aim to establish a radiomics model and develop a tool to improve the discrimination between benign and malignant pulmonary nodules. METHODS: A retrospective study was conducted on 875 patients with benign or malignant pulmonary nodules who underwent computed tomography (CT) examinations between June 2013 and June 2018. We assigned 612 patients to a training cohort and 263 patients to a validation cohort. Radiomics features were extracted from the CT images of each patient. Least absolute shrinkage and selection operator (LASSO) was used for radiomics feature selection and radiomics score calculation. Multivariate logistic regression analysis was used to develop a classification model and radiomics nomogram. Radiomics score and clinical variables were used to distinguish benign and malignant pulmonary nodules in logistic model. The performance of the radiomics nomogram was evaluated by the area under the curve (AUC), calibration curve and Hosmer-Lemeshow test in both the training and validation cohorts. RESULTS: A radiomics score was built and consisted of 20 features selected by LASSO from 1288 radiomics features in the training cohort. The multivariate logistic model and radiomics nomogram were constructed using the radiomics score and patients' age. Good discrimination of benign and malignant pulmonary nodules was obtained from the training cohort (AUC, 0.836; 95% confidence interval [CI]: 0.793-0.879) and validation cohort (AUC, 0.809; 95% CI: 0.745-0.872). The Hosmer-Lemeshow test also showed good performance for the logistic regression model in the training cohort (P = 0.765) and validation cohort (P = 0.064). Good alignment with the calibration curve indicated the good performance of the nomogram. CONCLUSIONS: The established radiomics nomogram is a noninvasive preoperative prediction tool for malignant pulmonary nodule diagnosis. Validation revealed that this nomogram exhibited excellent discrimination and calibration capacities, suggesting its clinical utility in the early screening of lung cancer.",2020,10.1002/cac2.12002,diagnosis,True
Prior-Attention Residual Learning for More Discriminative COVID-19 Screening in CT Images,"We propose a conceptually simple framework for fast COVID-19 screening in 3D chest CT images. The framework can efficiently predict whether or not a CT scan contains pneumonia while simultaneously identifying pneumonia types between COVID-19 and Interstitial Lung Disease (ILD) caused by other viruses. In the proposed method, two 3D-ResNets are coupled together into a single model for the two above-mentioned tasks via a novel prior-attention strategy. We extend residual learning with the proposed prior-attention mechanism and design a new so-called prior-attention residual learning (PARL) block. The model can be easily built by stacking the PARL blocks and trained end-to-end using multi-task losses. More specifically, one 3D-ResNet branch is trained as a binary classifier using lung images with and without pneumonia so that it can highlight the lesion areas within the lungs. Simultaneously, inside the PARL blocks, prior-attention maps are generated from this branch and used to guide another branch to learn more discriminative representations for the pneumonia-type classification. Experimental results demonstrate that the proposed framework can significantly improve the performance of COVID-19 screening. Compared to other methods, it achieves a state-of-the-art result. Moreover, the proposed method can be easily extended to other similar clinical applications such as computer-aided detection and diagnosis of pulmonary nodules in CT images, glaucoma lesions in Retina fundus images, etc.",2020,10.1109/tmi.2020.2994908,diagnosis,True
Prognostic and Predictive Values of Metabolic Parameters of (18)F-FDG PET/CT in Patients With Non-Small Cell Lung Cancer Treated With Chemotherapy,"OBJECTIVES: Increasing interests have been focused on using artificial intelligence (AI) to extend prognostic value of medical imaging. Feature extraction is a critical step for successful application of AI. The aim of this study was to explore several metabolic parameters measured by (18)F-fluorodeoxyglucose positron emission tomography/computed tomography (PET/CT) as potential AI features in predicting the effectiveness of chemotherapy in patients with non-small cell lung cancer (NSCLC). METHODS: A set of metabolic parameters of PET/CT and clinical characteristics were detected from 137 patients with NSCLC treated with at least 1 cycle of chemotherapy. Survival receiver-operating characteristic (ROC) analysis was used to define the more significant parameters chosen for the following survival analysis. Patient survival was analyzed by Kaplan-Meier method, log-rank test, and Cox regression. RESULTS: Survival ROC showed that maximum standardized uptake value (SUVmax), metabolic tumor volume 50% (MTV50), and total lesion glycolysis 50% (TLG50) had larger area under the curve, and the optimal cutoff values were 11.72, 4.04, and 34.55, respectively. Univariate and multivariate analyses synergistically showed that late PET/CT stage and MTV50 >4.04 were independent factors of poor survival in patients with NSCLC who received chemotherapy. CONCLUSIONS: Several potential prognostic biomarkers of PET/CT imaging have been extracted for predicting survival and selecting patients with NSCLC who are more likely to benefit from chemotherapy. The identification may accelerate the development of AI methods to improve treatment outcome for NSCLC.",2019,10.1177/1536012119846025,prognosis,True
Prognostic Implications of CT Feature Analysis in Patients with COVID-19: a Nationwide Cohort Study,"BACKGROUND: Few studies have classified chest computed tomography (CT) findings of coronavirus disease 2019 (COVID-19) and analyzed their correlations with prognosis. The present study aimed to evaluate retrospectively the clinical and chest CT findings of COVID-19 and to analyze CT findings and determine their relationships with clinical severity. METHODS: Chest CT and clinical features of 271 COVID-19 patients were assessed. The presence of CT findings and distribution of parenchymal abnormalities were evaluated, and CT patterns were classified as bronchopneumonia, organizing pneumonia (OP), or diffuse alveolar damage (DAD). Total extents were assessed using a visual scoring system and artificial intelligence software. Patients were allocated to two groups based on clinical outcomes, that is, to a severe group (requiring O₂ therapy or mechanical ventilation, n = 55) or a mild group (not requiring O₂ therapy or mechanical ventilation, n = 216). Clinical and CT features of these two groups were compared and univariate and multivariate logistic regression analyses were performed to identify independent prognostic factors. RESULTS: Age, lymphocyte count, levels of C-reactive protein, and procalcitonin were significantly different in the two groups. Forty-five of the 271 patients had normal chest CT findings. The most common CT findings among the remaining 226 patients were ground-glass opacity (98%), followed by consolidation (53%). CT findings were classified as OP (93%), DAD (4%), or bronchopneumonia (3%) and all nine patients with DAD pattern were included in the severe group. Uivariate and multivariate analyses showed an elevated procalcitonin (odds ratio [OR], 2.521; 95% confidence interval [CI], 1.001-6.303, P = 0.048), and higher visual CT scores (OR, 1.137; 95% CI, 1.042-1.236; P = 0.003) or higher total extent by AI measurement (OR, 1.048; 95% CI, 1.020-1.076; P < 0.001) were significantly associated with a severe clinical course. CONCLUSION: CT findings of COVID-19 pneumonia can be classified into OP, DAD, or bronchopneumonia patterns and all patients with DAD pattern were included in severe group. Elevated inflammatory markers and higher CT scores were found to be significant predictors of poor prognosis in patients with COVID-19 pneumonia.",2021,10.3346/jkms.2021.36.e51,prognosis,True
Prognostic value of anthropometric measures extracted from whole-body CT using deep learning in patients with non-small-cell lung cancer,"INTRODUCTION: The aim of the study was to extract anthropometric measures from CT by deep learning and to evaluate their prognostic value in patients with non-small-cell lung cancer (NSCLC). METHODS: A convolutional neural network was trained to perform automatic segmentation of subcutaneous adipose tissue (SAT), visceral adipose tissue (VAT), and muscular body mass (MBM) from low-dose CT images in 189 patients with NSCLC who underwent pretherapy PET/CT. After a fivefold cross-validation in a subset of 35 patients, anthropometric measures extracted by deep learning were normalized to the body surface area (BSA) to control the various patient morphologies. VAT/SAT ratio and clinical parameters were included in a Cox proportional-hazards model for progression-free survival (PFS) and overall survival (OS). RESULTS: Inference time for a whole volume was about 3 s. Mean Dice similarity coefficients in the validation set were 0.95, 0.93, and 0.91 for SAT, VAT, and MBM, respectively. For PFS prediction, T-stage, N-stage, chemotherapy, radiation therapy, and VAT/SAT ratio were associated with disease progression on univariate analysis. On multivariate analysis, only N-stage (HR = 1.7 [1.2-2.4]; p = 0.006), radiation therapy (HR = 2.4 [1.0-5.4]; p = 0.04), and VAT/SAT ratio (HR = 10.0 [2.7-37.9]; p < 0.001) remained significant prognosticators. For OS, male gender, smoking status, N-stage, a lower SAT/BSA ratio, and a higher VAT/SAT ratio were associated with mortality on univariate analysis. On multivariate analysis, male gender (HR = 2.8 [1.2-6.7]; p = 0.02), N-stage (HR = 2.1 [1.5-2.9]; p < 0.001), and the VAT/SAT ratio (HR = 7.9 [1.7-37.1]; p < 0.001) remained significant prognosticators. CONCLUSION: The BSA-normalized VAT/SAT ratio is an independent predictor of both PFS and OS in NSCLC patients. KEY POINTS: • Deep learning will make CT-derived anthropometric measures clinically usable as they are currently too time-consuming to calculate in routine practice. • Whole-body CT-derived anthropometrics in non-small-cell lung cancer are associated with progression-free survival and overall survival. • A priori medical knowledge can be implemented in the neural network loss function calculation.",2020,10.1007/s00330-019-06630-w,prognosis,True
Prognostication of patients with COVID-19 using artificial intelligence based on chest x-rays and clinical data: a retrospective study,"BACKGROUND: Chest x-ray is a relatively accessible, inexpensive, fast imaging modality that might be valuable in the prognostication of patients with COVID-19. We aimed to develop and evaluate an artificial intelligence system using chest x-rays and clinical data to predict disease severity and progression in patients with COVID-19. METHODS: We did a retrospective study in multiple hospitals in the University of Pennsylvania Health System in Philadelphia, PA, USA, and Brown University affiliated hospitals in Providence, RI, USA. Patients who presented to a hospital in the University of Pennsylvania Health System via the emergency department, with a diagnosis of COVID-19 confirmed by RT-PCR and with an available chest x-ray from their initial presentation or admission, were retrospectively identified and randomly divided into training, validation, and test sets (7:1:2). Using the chest x-rays as input to an EfficientNet deep neural network and clinical data, models were trained to predict the binary outcome of disease severity (ie, critical or non-critical). The deep-learning features extracted from the model and clinical data were used to build time-to-event models to predict the risk of disease progression. The models were externally tested on patients who presented to an independent multicentre institution, Brown University affiliated hospitals, and compared with severity scores provided by radiologists. FINDINGS: 1834 patients who presented via the University of Pennsylvania Health System between March 9 and July 20, 2020, were identified and assigned to the model training (n=1285), validation (n=183), or testing (n=366) sets. 475 patients who presented via the Brown University affiliated hospitals between March 1 and July 18, 2020, were identified for external testing of the models. When chest x-rays were added to clinical data for severity prediction, area under the receiver operating characteristic curve (ROC-AUC) increased from 0·821 (95% CI 0·796-0·828) to 0·846 (0·815-0·852; p<0·0001) on internal testing and 0·731 (0·712-0·738) to 0·792 (0·780-0 ·803; p<0·0001) on external testing. When deep-learning features were added to clinical data for progression prediction, the concordance index (C-index) increased from 0·769 (0·755-0·786) to 0·805 (0·800-0·820; p<0·0001) on internal testing and 0·707 (0·695-0·729) to 0·752 (0·739-0·764; p<0·0001) on external testing. The image and clinical data combined model had significantly better prognostic performance than combined severity scores and clinical data on internal testing (C-index 0·805 vs 0·781; p=0·0002) and external testing (C-index 0·752 vs 0·715; p<0·0001). INTERPRETATION: In patients with COVID-19, artificial intelligence based on chest x-rays had better prognostic performance than clinical data or radiologist-derived severity scores. Using artificial intelligence, chest x-rays can augment clinical data in predicting the risk of progression to critical illness in patients with COVID-19. FUNDING: Brown University, Amazon Web Services Diagnostic Development Initiative, Radiological Society of North America, National Cancer Institute and National Institute of Biomedical Imaging and Bioengineering of the National Institutes of Health.",2021,10.1016/s2589-7500(21)00039-x,prognosis,False
Proposing a deep learning-based method for improving the diagnostic certainty of pulmonary nodules in CT scan of chest,"OBJECTIVE: To compare the performance of a deep learning (DL)-based method for diagnosing pulmonary nodules compared with radiologists' diagnostic approach in computed tomography (CT) of the chest. MATERIALS AND METHODS: A total of 150 pathologically confirmed pulmonary nodules (60% malignant) assessed and reported by radiologists were included. CT images were processed by the proposed DL-based method to generate the probability of malignancy (0-100%), and the nodules were divided into the groups of benign (0-39.9%), indeterminate (40.0-59.9%), and malignant (60.0-100%). Taking the pathological results as the gold standard, we compared the diagnostic performance of the proposed DL-based method with the radiologists' diagnostic approach using the McNemar-Bowker test. RESULTS: There was a statistically significant difference between the diagnosis results of the proposed DL-based method and the radiologists' diagnostic approach (p < 0.001). Moreover, there was no statistically significant difference in the composition of the diagnosis results between the proposed DL-based method and the radiologists' diagnostic approach (all p > 0.05). The difference in diagnostic accuracy between the proposed DL-based method (70%) and radiologists' diagnostic performance (64%) was not statistically significant (p = 0.243). CONCLUSIONS: The proposed DL-based method achieved an accuracy comparable with the radiologists' diagnostic approach in clinical practice. Furthermore, its advantage in improving diagnostic certainty may raise the radiologists' confidence in diagnosing pulmonary nodules and may help clinical management. Therefore, the proposed DL-based method showed great potential in a certain clinical application. KEY POINTS: • Deep learning-based method for diagnosing the pulmonary nodules in computed tomography provides a higher diagnostic certainty.",2021,10.1007/s00330-021-07919-5,diagnosis,True
Prospective Case-Control Study of Cardiovascular Abnormalities 6 Months Following Mild COVID-19 in Healthcare Workers,"OBJECTIVES: The purpose of this study was to detect cardiovascular changes after mild severe acute respiratory syndrome-coronavirus-2 infection. BACKGROUND: Concern exists that mild coronavirus disease 2019 may cause myocardial and vascular disease. METHODS: Participants were recruited from COVIDsortium, a 3-hospital prospective study of 731 health care workers who underwent first-wave weekly symptom, polymerase chain reaction, and serology assessment over 4 months, with seroconversion in 21.5% (n = 157). At 6 months post-infection, 74 seropositive and 75 age-, sex-, and ethnicity-matched seronegative control subjects were recruited for cardiovascular phenotyping (comprehensive phantom-calibrated cardiovascular magnetic resonance and blood biomarkers). Analysis was blinded, using objective artificial intelligence analytics where available. RESULTS: A total of 149 subjects (mean age 37 years, range 18 to 63 years, 58% women) were recruited. Seropositive infections had been mild with case definition, noncase definition, and asymptomatic disease in 45 (61%), 18 (24%), and 11 (15%), respectively, with 1 person hospitalized (for 2 days). Between seropositive and seronegative groups, there were no differences in cardiac structure (left ventricular volumes, mass, atrial area), function (ejection fraction, global longitudinal shortening, aortic distensibility), tissue characterization (T(1), T(2), extracellular volume fraction mapping, late gadolinium enhancement) or biomarkers (troponin, N-terminal pro-B-type natriuretic peptide). With abnormal defined by the 75 seronegatives (2 SDs from mean, e.g., ejection fraction <54%, septal T(1) >1,072 ms, septal T(2) >52.4 ms), individuals had abnormalities including reduced ejection fraction (n = 2, minimum 50%), T(1) elevation (n = 6), T(2) elevation (n = 9), late gadolinium enhancement (n = 13, median 1%, max 5% of myocardium), biomarker elevation (borderline troponin elevation in 4; all N-terminal pro-B-type natriuretic peptide normal). These were distributed equally between seropositive and seronegative individuals. CONCLUSIONS: Cardiovascular abnormalities are no more common in seropositive versus seronegative otherwise healthy, workforce representative individuals 6 months post-mild severe acute respiratory syndrome-coronavirus-2 infection.",2021,10.1016/j.jcmg.2021.04.011,diagnosis,False
Pulmonary Artery-Vein Classification in CT Images Using Deep Learning,"Recent studies show that pulmonary vascular diseases may specifically affect arteries or veins through different physiologic mechanisms. To detect changes in the two vascular trees, physicians manually analyze the chest computed tomography (CT) image of the patients in search of abnormalities. This process is time consuming, difficult to standardize, and thus not feasible for large clinical studies or useful in real-world clinical decision making. Therefore, automatic separation of arteries and veins in CT images is becoming of great interest, as it may help physicians to accurately diagnose pathological conditions. In this paper, we present a novel, fully automatic approach to classify vessels from chest CT images into arteries and veins. The algorithm follows three main steps: first, a scale-space particles segmentation to isolate vessels; then a 3-D convolutional neural network (CNN) to obtain a first classification of vessels; finally, graph-cuts' optimization to refine the results. To justify the usage of the proposed CNN architecture, we compared different 2-D and 3-D CNNs that may use local information from bronchus- and vessel-enhanced images provided to the network with different strategies. We also compared the proposed CNN approach with a random forests (RFs) classifier. The methodology was trained and evaluated on the superior and inferior lobes of the right lung of 18 clinical cases with noncontrast chest CT scans, in comparison with manual classification. The proposed algorithm achieves an overall accuracy of 94%, which is higher than the accuracy obtained using other CNN architectures and RF. Our method was also validated with contrast-enhanced CT scans of patients with chronic thromboembolic pulmonary hypertension to demonstrate that our model generalizes well to contrast-enhanced modalities. The proposed method outperforms state-of-the-art methods, paving the way for future use of 3-D CNN for artery/vein classification in CT images.",2018,10.1109/tmi.2018.2833385,diagnosis,True
Pulmonary COVID-19: Learning Spatiotemporal Features Combining CNN and LSTM Networks for Lung Ultrasound Video Classification,"Deep Learning is a very active and important area for building Computer-Aided Diagnosis (CAD) applications. This work aims to present a hybrid model to classify lung ultrasound (LUS) videos captured by convex transducers to diagnose COVID-19. A Convolutional Neural Network (CNN) performed the extraction of spatial features, and the temporal dependence was learned using a Long Short-Term Memory (LSTM). Different types of convolutional architectures were used for feature extraction. The hybrid model (CNN-LSTM) hyperparameters were optimized using the Optuna framework. The best hybrid model was composed of an Xception pre-trained on ImageNet and an LSTM containing 512 units, configured with a dropout rate of 0.4, two fully connected layers containing 1024 neurons each, and a sequence of 20 frames in the input layer (20×2018). The model presented an average accuracy of 93% and sensitivity of 97% for COVID-19, outperforming models based purely on spatial approaches. Furthermore, feature extraction using transfer learning with models pre-trained on ImageNet provided comparable results to models pre-trained on LUS images. The results corroborate with other studies showing that this model for LUS classification can be an important tool in the fight against COVID-19 and other lung diseases.",2021,10.3390/s21165486,diagnosis,False
Pulmonary lesion subtypes recognition of COVID-19 from radiomics data with three-dimensional texture characterization in computed tomography images,"BACKGROUND: The COVID-19 disease is putting unprecedented pressure on the global healthcare system. The CT (computed tomography) examination as a auxiliary confirmed diagnostic method can help clinicians quickly detect lesions locations of COVID-19 once screening by PCR test. Furthermore, the lesion subtypes classification plays a critical role in the consequent treatment decision. Identifying the subtypes of lesions accurately can help doctors discover changes in lesions in time and better assess the severity of COVID-19. METHOD: The most four typical lesion subtypes of COVID-19 are discussed in this paper, which are GGO (ground-glass opacity), cord, solid and subsolid. A computer-aided diagnosis approach of lesion subtype is proposed in this paper. The radiomics data of lesions are segmented from COVID-19 patients CT images with diagnosis and lesions annotations by radiologists. Then the three-dimensional texture descriptors are applied on the volume data of lesions as well as shape and first-order features. The massive feature data are selected by HAFS (hybrid adaptive feature selection) algorithm and a classification model is trained at the same time. The classifier is used to predict lesion subtypes as side decision information for radiologists. RESULTS: There are 3734 lesions extracted from the dataset with 319 patients collection and then 189 radiomics features are obtained finally. The random forest classifier is trained with data augmentation that the number of different subtypes of lesions is imbalanced in initial dataset. The experimental results show that the accuracy of the four subtypes of lesions is (93.06%, 96.84%, 99.58%, and 94.30%), the recall is (95.52%, 91.58%, 95.80% and 80.75%) and the f-score is (93.84%, 92.37%, 95.47%, and 84.42%). CONCLUSION: The three-dimensional radiomics features used in this paper can better express the high-level information of COVID-19 lesions in CT slices. HAFS method aggregates the results of multiple feature selection algorithms intersects with traditional methods to filter out redundant features more accurately. After selection, the subtype of COVID-19 lesion can be judged by inputting the features into the RF (random forest) model, which can help clinicians more accurately identify the subtypes of COVID-19 lesions and provide help for further research.",2021,10.1186/s12938-021-00961-w,diagnosis,True
Pulmonary Nodule Classification with Deep Convolutional Neural Networks on Computed Tomography Images,"Computer aided detection (CAD) systems can assist radiologists by offering a second opinion on early diagnosis of lung cancer. Classification and feature representation play critical roles in false-positive reduction (FPR) in lung nodule CAD. We design a deep convolutional neural networks method for nodule classification, which has an advantage of autolearning representation and strong generalization ability. A specified network structure for nodule images is proposed to solve the recognition of three types of nodules, that is, solid, semisolid, and ground glass opacity (GGO). Deep convolutional neural networks are trained by 62,492 regions-of-interest (ROIs) samples including 40,772 nodules and 21,720 nonnodules from the Lung Image Database Consortium (LIDC) database. Experimental results demonstrate the effectiveness of the proposed method in terms of sensitivity and overall accuracy and that it consistently outperforms the competing methods.",2016,10.1155/2016/6215085,diagnosis,True
Pulmonary nodule classification with deep residual networks,"PURPOSE : Lung cancer has the highest death rate among all cancers in the USA. In this work we focus on improving the ability of computer-aided diagnosis (CAD) systems to predict the malignancy of nodules from cropped CT images of lung nodules. METHODS: We evaluate the effectiveness of very deep convolutional neural networks at the task of expert-level lung nodule malignancy classification. Using the state-of-the-art ResNet architecture as our basis, we explore the effect of curriculum learning, transfer learning, and varying network depth on the accuracy of malignancy classification. RESULTS: Due to a lack of public datasets with standardized problem definitions and train/test splits, studies in this area tend to not compare directly against other existing work. This makes it hard to know the relative improvement in the new solution. In contrast, we directly compare our system against two state-of-the-art deep learning systems for nodule classification on the LIDC/IDRI dataset using the same experimental setup and data set. The results show that our system achieves the highest performance in terms of all metrics measured including sensitivity, specificity, precision, AUROC, and accuracy. CONCLUSIONS: The proposed method of combining deep residual learning, curriculum learning, and transfer learning translates to high nodule classification accuracy. This reveals a promising new direction for effective pulmonary nodule CAD systems that mirrors the success of recent deep learning advances in other image-based application domains.",2017,10.1007/s11548-017-1605-6,diagnosis,True
Pulmonary Nodule Detection in CT Images: False Positive Reduction Using Multi-View Convolutional Networks,"We propose a novel Computer-Aided Detection (CAD) system for pulmonary nodules using multi-view convolutional networks (ConvNets), for which discriminative features are automatically learnt from the training data. The network is fed with nodule candidates obtained by combining three candidate detectors specifically designed for solid, subsolid, and large nodules. For each candidate, a set of 2-D patches from differently oriented planes is extracted. The proposed architecture comprises multiple streams of 2-D ConvNets, for which the outputs are combined using a dedicated fusion method to get the final classification. Data augmentation and dropout are applied to avoid overfitting. On 888 scans of the publicly available LIDC-IDRI dataset, our method reaches high detection sensitivities of 85.4% and 90.1% at 1 and 4 false positives per scan, respectively. An additional evaluation on independent datasets from the ANODE09 challenge and DLCST is performed. We showed that the proposed multi-view ConvNets is highly suited to be used for false positive reduction of a CAD system.",2016,10.1109/tmi.2016.2536809,diagnosis,True
Pulmonary nodule detection in CT scans with equivariant CNNs,"Convolutional Neural Networks (CNNs) require a large amount of annotated data to learn from, which is often difficult to obtain for medical imaging problems. In this work we show that the sample complexity of CNNs can be significantly improved by using 3D roto-translation group convolutions instead of standard translational convolutions. 3D CNNs with group convolutions (3D G-CNNs) were applied to the problem of false positive reduction for pulmonary nodule detection in CT scans, and proved to be substantially more effective in terms of accuracy, sensitivity to malignant nodules, and speed of convergence compared to a strong and comparable baseline architecture with regular convolutions, extensive data augmentation and a similar number of parameters. For every dataset size tested, the G-CNN achieved a FROC score close to the CNN trained on ten times more data.",2019,10.1016/j.media.2019.03.010,diagnosis,True
Pulmonary Nodule Detection Model Based on SVM and CT Image Feature-Level Fusion with Rough Sets,"In order to improve the detection accuracy of pulmonary nodules in CT image, considering two problems of pulmonary nodules detection model, including unreasonable feature structure and nontightness of feature representation, a pulmonary nodules detection algorithm is proposed based on SVM and CT image feature-level fusion with rough sets. Firstly, CT images of pulmonary nodule are analyzed, and 42-dimensional feature components are extracted, including six new 3-dimensional features proposed by this paper and others 2-dimensional and 3-dimensional features. Secondly, these features are reduced for five times with rough set based on feature-level fusion. Thirdly, a grid optimization model is used to optimize the kernel function of support vector machine (SVM), which is used as a classifier to identify pulmonary nodules. Finally, lung CT images of 70 patients with pulmonary nodules are collected as the original samples, which are used to verify the effectiveness and stability of the proposed model by four groups' comparative experiments. The experimental results show that the effectiveness and stability of the proposed model based on rough set feature-level fusion are improved in some degrees.",2016,10.1155/2016/8052436,diagnosis,True
Pulmonary nodule detection on chest radiographs using balanced convolutional neural network and classic candidate detection,"Computer-aided detection (CADe) systems play a crucial role in pulmonary nodule detection via chest radiographs (CXRs). A two-stage CADe scheme usually includes nodule candidate detection and false positive reduction. A pure deep learning model, such as faster region convolutional neural network (faster R-CNN), has been successfully applied for nodule candidate detection via computed tomography (CT). The model is yet to achieve a satisfactory performance in CXR, because the size of the CXR is relatively large and the nodule in CXR has been obscured by structures such as ribs. In contrast, the CNN has proved effective for false positive reduction compared to the shallow method. In this paper, we developed a CADe scheme using the balanced CNN with classic candidate detection. First, the scheme applied a multi-segment active shape model to accurately segment pulmonary parenchyma. The grayscale morphological enhancement technique was then used to improve the conspicuity of the nodule structure. Based on the nodule enhancement image, 200 nodule candidates were selected and a region of interest (ROI) was cropped for each. Nodules in CXR exhibit a large variation in density, and rib crossing and vessel tissue usually present similar features to the nodule. Compared to the original ROI image, the nodule enhancement ROI image has potential discriminative features from false positive reduction. In this study, the nodule enhancement ROI image, corresponding segmentation result, and original ROI image were encoded into a red-green-blue (RGB) color image instead of the duplicated original ROI image as input of the CNN (GoogLeNet) for false positive reduction. With the Japanese Society of Radiological Technology database, the CADe scheme achieved high performance of the published literatures (a sensitivity of 91.4 % and 97.1 %, with 2.0 false positives per image (FPs/image) and 5.0 FPs/image, respectively) for nodule cases.",2020,10.1016/j.artmed.2020.101881,diagnosis,False
Pulmonary nodule detection using hybrid two-stage 3D CNNs,"PURPOSE: Early detection of pulmonary nodules is an effective way to improve patients' chances of survival. In this work, we propose a novel and efficient way to build a computer-aided detection (CAD) system for pulmonary nodules based on computed tomography (CT) scans. METHODS: The system can be roughly divided into two steps: nodule candidate detection and false positive reduction. Considering the three-dimensional (3D) nature of nodules, the CAD system adopts 3D convolutional neural networks (CNNs) in both stages. Specifically, in the first stage, a segmentation-based 3D CNN with a hybrid loss is designed to segment nodules. According to the probability maps produced by the segmentation network, a threshold method and connected component analysis are applied to generate nodule candidates. In the second stage, we employ three classification-based 3D CNNs with different types of inputs to reduce false positives. In addition to simple raw data input, we also introduce hybrid inputs to make better use of the output of the previous segmentation network. In experiments, we use data augmentation and batch normalization to avoid overfitting. RESULTS: We evaluate the system on 888 CT scans from the publicly available LIDC-IDRI dataset, and our method achieves the best performance by comparing with the state-of-the-art methods, which has a high detection sensitivity of 97.5% with an average of only one false positive per scan. An additional evaluation on 115 CT scans from local hospitals is also performed. CONCLUSIONS: Experimental results demonstrate that our method is highly suited for the detection of pulmonary nodules.",2020,10.1002/mp.14161,diagnosis,True
Pulmonary nodule segmentation with CT sample synthesis using adversarial networks,"PURPOSE: Segmentation of pulmonary nodules is critical for the analysis of nodules and lung cancer diagnosis. We present a novel framework of segmentation for various types of nodules using convolutional neural networks (CNNs). METHODS: The proposed framework is composed of two major parts. The first part is to increase the variety of samples and build a more balanced dataset. A conditional generative adversarial network (cGAN) is employed to produce synthetic CT images. Semantic labels are generated to impart spatial contextual knowledge to the network. Nine attribute scoring labels are combined as well to preserve nodule features. To refine the realism of synthesized samples, reconstruction error loss is introduced into cGAN. The second part is to train a nodule segmentation network on the extended dataset. We build a three-dimensional (3D) CNN model that exploits heterogeneous maps including edge maps and local binary pattern maps. The incorporation of these maps informs the model of texture patterns and boundary information of nodules, which assists high-level feature learning for segmentation. Residual unit, which learns to reduce residual error, is adopted to accelerate training and improve accuracy. RESULTS: Validation on LIDC-IDRI dataset demonstrates that the generated samples are realistic. The mean squared error and average cosine similarity between real and synthesized samples are 1.55 × 10-2 and 0.9534, respectively. The Dice coefficient, positive predicted value, sensitivity, and accuracy are, respectively, 0.8483, 0.8895, 0.8511, and 0.9904 for the segmentation results. CONCLUSIONS: The proposed 3D CNN segmentation framework, based on the use of synthesized samples and multiple maps with residual learning, achieves more accurate nodule segmentation compared to existing state-of-the-art methods. The proposed CT image synthesis method can not only output samples close to real images but also allow for stochastic variation in image diversity.",2019,10.1002/mp.13349,diagnosis,True
Pulmonary nodules detection assistant platform: An effective computer aided system for early pulmonary nodules detection in physical examination,"BACKGROUND AND OBJECTIVE: Early detection of the pulmonary nodule from physical examination low-dose computer tomography (LDCT) images is an effective measure to reduce the mortality rate of lung cancer. Although there are many computer aided diagnosis (CAD) methods used for detecting pulmonary nodules, there are few CAD systems for small pulmonary nodule detection with a large amount of physical examination LDCT images. METHODS: In this work, we designed a CAD system called Pulmonary Nodules Detection Assistant Platform for early pulmonary nodules detection and classification based on the physical examination LDCT images. Based on the preprocessed physical examination CT images, the three-dimensional (3D) CNN-based model is presented to detect candidate pulmonary nodules and output detection results with quantitative parameters, the 3D ResNet is used to classify the detected nodules into intrapulmonary nodules and pleural nodules to reduce the physician workloads, and the Fully Connected Neural Network (FCNN) is used to classify ground-glass opacity (GGO) nodules and non-GGO nodules to help doctor pay more attention to those suspected early lung cancer nodules. RESULTS: Experiments are performed on our 1000 samples of physical examinations (LNPE1000) with an average diameter of 5.3 mm and LUNA16 dataset with an average diameter of 8.31 mm, which show that the designed CAD system is automatic and efficient for detecting smaller and larger nodules from different datasets, especially for the detection of smaller nodules with diameter between 3 mm and 6 mm in physical examinations. The accuracy of pulmonary nodule detection reaches 0.879 with an average of 1 false positive per CT in LNPE1000 dataset, which is comparable to the experienced physicians. The classification accuracy reaches 0.911 between intrapulmonary and pleural nodules, and 0.950 between GGO and non-GGO nodules, respectively. CONCLUSION: Experimental results show that the proposed pulmonary nodule detection model is robust for different datasets, which can successfully detect smaller and larger nodules in CT images obtained by physical examination. The interactive platform of the designed CAD system has been on trial in a hospital by combining with manual reading, which helps doctors analyze clinical data dynamically and improves the nodule detection efficiency in physical examination applications.",2022,10.1016/j.cmpb.2022.106680,diagnosis,True
Pulmonary nodules detection based on multi-scale attention networks,"Pulmonary nodules are the main manifestation of early lung cancer. Therefore, accurate detection of nodules in CT images is vital for lung cancer diagnosis. A 3D automatic detection system of pulmonary nodules based on multi-scale attention networks is proposed in this paper to use multi-scale features of nodules and avoid network over-fitting problems. The system consists of two parts, nodule candidate detection (determining the locations of candidate nodules), false positive reduction (minimizing the number of false positive nodules). Specifically, with Res2Net structure, using pre-activation operation and convolutional quadruplet attention module, the 3D multi-scale attention block is designed. It makes full use of multi-scale information of pulmonary nodules by extracting multi-scale features at a granular level and alleviates over-fitting by pre-activation. The U-Net-like encoder-decoder structure is combined with multi-scale attention blocks as the backbone network of Faster R-CNN for detection of candidate nodules. Then a 3D deep convolutional neural network based on multi-scale attention blocks is designed for false positive reduction. The extensive experiments on LUNA16 and TianChi competition datasets demonstrate that the proposed approach can effectively improve the detection sensitivity and control the number of false positive nodules, which has clinical application value.",2022,10.1038/s41598-022-05372-y,diagnosis,True
Pulmonary ventilation imaging in asthma and cystic fibrosis using oxygen-enhanced 3D radial ultrashort echo time MRI,"BACKGROUND: A previous study demonstrated the feasibility of using 3D radial ultrashort echo time (UTE) oxygen-enhanced MRI (UTE OE-MRI) for functional imaging of healthy human lungs. The repeatability of quantitative measures from UTE OE-MRI needs to be established prior to its application in clinical research. PURPOSE: To evaluate repeatability of obstructive patterns in asthma and cystic fibrosis (CF) with UTE OE-MRI with isotropic spatial resolution and full chest coverage. STUDY TYPE: Volunteer and patient repeatability. POPULATION: Eighteen human subjects (five asthma, six CF, and seven normal subjects). FIELD STRENGTH/SEQUENCE: Respiratory-gated free-breathing 3D radial UTE (80 μs) sequence at 1.5T. ASSESSMENT: Two 3D radial UTE volumes were acquired sequentially under normoxic and hyperoxic conditions. A subset of subjects underwent repeat acquisitions on either the same day or ≤15 days apart. Asthma and CF subjects also underwent spirometry. A workflow including deformable registration and retrospective lung density correction was used to compute 3D isotropic percent signal enhancement (PSE) maps. Median PSE (MPSE) and ventilation defect percent (VDP) of the lung were measured from the PSE map. STATISTICAL TESTS: The relations between MPSE, VDP, and spirometric measures were assessed using Spearman correlations. The test-retest repeatability was evaluated using Bland-Altman analysis and intraclass correlation coefficients (ICC). RESULTS: Ventilation measures in normal subjects (MPSE = 8.0%, VDP = 3.3%) were significantly different from those in asthma (MPSE = 6.0%, P = 0.042; VDP = 21.7%, P = 0.018) and CF group (MPSE = 4.5%, P = 0.0006; VDP = 27.2%, P = 0.002). MPSE correlated significantly with forced expiratory lung volume in 1 second percent predicted (ρ = 0.72, P = 0.017). The ICC of the test-retest VDP and MPSE were both ≥0.90. In all subject groups, an anterior/posterior gradient was observed with higher MPSE and lower VDP in the posterior compared to anterior regions (P ≤ 0.0021 for all comparisons). DATA CONCLUSION: 3D radial UTE OE-MRI supports quantitative differentiation of diseased vs. healthy lungs using either whole lung VDP or MPSE with excellent test-retest repeatability. LEVEL OF EVIDENCE: 2 Technical Efficacy: Stage 1 J. Magn. Reson. Imaging 2018;47:1287-1297.",2018,10.1002/jmri.25877,diagnosis,False
Quantification of pulmonary involvement in COVID-19 pneumonia by means of a cascade of two U-nets: training and assessment on multiple datasets using different annotation criteria,"PURPOSE: This study aims at exploiting artificial intelligence (AI) for the identification, segmentation and quantification of COVID-19 pulmonary lesions. The limited data availability and the annotation quality are relevant factors in training AI-methods. We investigated the effects of using multiple datasets, heterogeneously populated and annotated according to different criteria. METHODS: We developed an automated analysis pipeline, the LungQuant system, based on a cascade of two U-nets. The first one (U-net[Formula: see text]) is devoted to the identification of the lung parenchyma; the second one (U-net[Formula: see text]) acts on a bounding box enclosing the segmented lungs to identify the areas affected by COVID-19 lesions. Different public datasets were used to train the U-nets and to evaluate their segmentation performances, which have been quantified in terms of the Dice Similarity Coefficients. The accuracy in predicting the CT-Severity Score (CT-SS) of the LungQuant system has been also evaluated. RESULTS: Both the volumetric DSC (vDSC) and the accuracy showed a dependency on the annotation quality of the released data samples. On an independent dataset (COVID-19-CT-Seg), both the vDSC and the surface DSC (sDSC) were measured between the masks predicted by LungQuant system and the reference ones. The vDSC (sDSC) values of 0.95±0.01 and 0.66±0.13 (0.95±0.02 and 0.76±0.18, with 5 mm tolerance) were obtained for the segmentation of lungs and COVID-19 lesions, respectively. The system achieved an accuracy of 90% in CT-SS identification on this benchmark dataset. CONCLUSION: We analysed the impact of using data samples with different annotation criteria in training an AI-based quantification system for pulmonary involvement in COVID-19 pneumonia. In terms of vDSC measures, the U-net segmentation strongly depends on the quality of the lesion annotations. Nevertheless, the CT-SS can be accurately predicted on independent test sets, demonstrating the satisfactory generalization ability of the LungQuant.",2022,10.1007/s11548-021-02501-2,diagnosis,True
Quantifying the incremental value of deep learning: Application to lung nodule detection,"We present a case study for implementing a machine learning algorithm with an incremental value framework in the domain of lung cancer research. Machine learning methods have often been shown to be competitive with prediction models in some domains; however, implementation of these methods is in early development. Often these methods are only directly compared to existing methods; here we present a framework for assessing the value of a machine learning model by assessing the incremental value. We developed a machine learning model to identify and classify lung nodules and assessed the incremental value added to existing risk prediction models. Multiple external datasets were used for validation. We found that our image model, trained on a dataset from The Cancer Imaging Archive (TCIA), improves upon existing models that are restricted to patient characteristics, but it was inconclusive about whether it improves on models that consider nodule features. Another interesting finding is the variable performance on different datasets, suggesting population generalization with machine learning models may be more challenging than is often considered.",2020,10.1371/journal.pone.0231468,diagnosis,True
Quantitative Analysis and Automated Lung Ultrasound Scoring for Evaluating COVID-19 Pneumonia With Neural Networks,"As being radiation-free, portable, and capable of repetitive use, ultrasonography is playing an important role in diagnosing and evaluating the COVID-19 Pneumonia (PN) in this epidemic. By virtue of lung ultrasound scores (LUSS), lung ultrasound (LUS) was used to estimate the excessive lung fluid that is an important clinical manifestation of COVID-19 PN, with high sensitivity and specificity. However, as a qualitative method, LUSS suffered from large interobserver variations and requirement for experienced clinicians. Considering this limitation, we developed a quantitative and automatic lung ultrasound scoring system for evaluating the COVID-19 PN. A total of 1527 ultrasound images prospectively collected from 31 COVID-19 PN patients with different clinical conditions were evaluated and scored with LUSS by experienced clinicians. All images were processed via a series of computer-aided analysis, including curve-to-linear conversion, pleural line detection, region-of-interest (ROI) selection, and feature extraction. A collection of 28 features extracted from the ROI was specifically defined for mimicking the LUSS. Multilayer fully connected neural networks, support vector machines, and decision trees were developed for scoring LUS images using the fivefold cross validation. The model with 128×256 two fully connected layers gave the best accuracy of 87%. It is concluded that the proposed method could assess the ultrasound images by assigning LUSS automatically with high accuracy, potentially applicable to the clinics.",2021,10.1109/tuffc.2021.3070696,diagnosis,True
Quantitative assessment of lung involvement on chest CT at admission: Impact on hypoxia and outcome in COVID-19 patients,"BACKGROUND: The aim of this study was to quantify COVID-19 pneumonia features using CT performed at time of admission to emergency department in order to predict patients' hypoxia during the hospitalization and outcome. METHODS: Consecutive chest CT performed in the emergency department between March 1st and April 7th 2020 for COVID-19 pneumonia were analyzed. The three features of pneumonia (GGO, semi-consolidation and consolidation) and the percentage of well-aerated lung were quantified using a HU threshold based software. ROC curves identified the optimal cut-off values of CT parameters to predict hypoxia worsening and hospital discharge. Multiple Cox proportional hazards regression was used to analyze the capability of CT quantitative features, demographic and clinical variables to predict the time to hospital discharge. RESULTS: Seventy-seven patients (median age 56-years-old, 51 men) with COVID-19 pneumonia at CT were enrolled. The quantitative features of COVID-19 pneumonia were not associated to age, sex and time-from-symptoms onset, whereas higher number of comorbidities was correlated to lower well-aerated parenchyma ratio (rho = -0.234, p = 0.04) and increased semi-consolidation ratio (rho = -0.303, p = 0.008). Well-aerated lung (≤57%), semi-consolidation (≥17%) and consolidation (≥9%) predicted worst hypoxemia during hospitalization, with moderate areas under curves (AUC 0.76, 0.75, 0.77, respectively). Multiple Cox regression identified younger age (p < 0.01), female sex (p < 0.001), longer time-from-symptoms onset (p = 0.049), semi-consolidation ≤17% (p < 0.01) and consolidation ≤13% (p = 0.03) as independent predictors of shorter time to hospital discharge. CONCLUSION: Quantification of pneumonia features on admitting chest CT predicted hypoxia worsening during hospitalization and time to hospital discharge in COVID-19 patients.",2021,10.1016/j.clinimag.2021.04.033,diagnosis,True
Quantitative CT analysis of pulmonary nodules for lung adenocarcinoma risk classification based on an exponential weighted grey scale angular density distribution feature,"BACKGROUND AND OBJECTIVES: To improve lung nodule classification efficiency, we propose a lung nodule CT image characterization method. We propose a multi-directional feature extraction method to effectively represent nodules of different risk levels. The proposed feature combined with pattern recognition model to classify lung adenocarcinomas risk to four categories: Atypical Adenomatous Hyperplasia (AAH), Adenocarcinoma In Situ (AIS), Minimally Invasive Adenocarcinoma (MIA), and Invasive Adenocarcinoma (IA). METHODS: First, we constructed the reference map using an integral image and labelled this map using a K-means approach. The density distribution map of the lung nodule image was generated after scanning all pixels in the nodule image. An exponential function was designed to weight the angular histogram for each component of the distribution map, and the features of the image were described. Then, quantitative measurement was performed using a Random Forest classifier. The evaluation data were obtained from the LIDC-IDRI database and the CT database which provided by Shanghai Zhongshan hospital (ZSDB). In the LIDC-IDRI, the nodules are categorized into three configurations with five ranks of malignancy (""1"" to ""5""). In the ZSDB, the nodule categories are AAH, AIS, MIA, and IA. RESULTS: The average of Student's t-test p-values were less than 0.02. The AUCs for the LIDC-IDRI database were 0.9568, 0.9320, and 0.8288 for Configurations 1, 2, and 3, respectively. The AUCs for the ZSDB were 0.9771, 0.9917, 0.9590, and 0.9971 for AAH, AIS, MIA and IA, respectively. CONCLUSION: The experimental results demonstrate that the proposed method outperforms the state-of-the-art and is robust for different lung CT image datasets.",2018,10.1016/j.cmpb.2018.04.001,diagnosis,True
Quantitative CT for detecting COVID‑19 pneumonia in suspected cases,"BACKGROUND: Corona Virus Disease 2019 (COVID-19) is currently a worldwide pandemic and has a huge impact on public health and socio-economic development. The purpose of this study is to explore the diagnostic value of the quantitative computed tomography (CT) method by using different threshold segmentation techniques to distinguish between patients with or without COVID-19 pneumonia. METHODS: A total of 47 patients with suspected COVID-19 were retrospectively analyzed, including nine patients with positive real-time fluorescence reverse transcription polymerase chain reaction (RT-PCR) test (confirmed case group) and 38 patients with negative RT-PCR test (excluded case group). An improved 3D convolutional neural network (VB-Net) was used to automatically extract lung lesions. Eight different threshold segmentation methods were used to define the ground glass opacity (GGO) and consolidation. The receiver operating characteristic (ROC) curves were used to compare the performance of various parameters with different thresholds for diagnosing COVID-19 pneumonia. RESULTS: The volume of GGO (VOGGO) and GGO percentage in the whole lung (GGOPITWL) were the most effective values for diagnosing COVID-19 at a threshold of - 300 HU, with areas under the curve (AUCs) of 0.769 and 0.769, sensitivity of 66.67 and 66.67%, specificity of 94.74 and 86.84%. Compared with VOGGO or GGOPITWL at a threshold of - 300 Hounsfield units (HU), the consolidation percentage in the whole lung (CPITWL) with thresholds at - 400 HU, - 350 HU, and - 250 HU were statistically different. There were statistical differences in the infection volume and percentage of the whole lung, right lung, and lobes between the two groups. VOGGO, GGOPITWL, and volume of consolidation (VOC) were also statistically different at the threshold of - 300 HU. CONCLUSIONS: Quantitative CT provides an image quantification method for the auxiliary diagnosis of COVID-19 and is expected to assist in confirming patients with COVID-19 pneumonia in suspected cases.",2021,10.1186/s12879-021-06556-z,diagnosis,True
Quantitative evaluation of COVID-19 pneumonia severity by CT pneumonia analysis algorithm using deep learning technology and blood test results,"PURPOSE: To evaluate whether early chest computed tomography (CT) lesions quantified by an artificial intelligence (AI)-based commercial software and blood test values at the initial presentation can differentiate the severity of COVID-19 pneumonia. MATERIALS AND METHODS: This retrospective study included 100 SARS-CoV-2-positive patients with mild (n = 23), moderate (n = 37) or severe (n = 40) pneumonia classified according to the Japanese guidelines. Univariate Kruskal-Wallis and multivariate ordinal logistic analyses were used to examine whether CT parameters (opacity score, volume of opacity, % opacity, volume of high opacity, % high opacity and mean HU total on CT) as well as blood test parameters [procalcitonin, estimated glomerular filtration rate (eGFR), C-reactive protein, % lymphocyte, ferritin, aspartate aminotransferase, lactate dehydrogenase, alanine aminotransferase, creatine kinase, hemoglobin A1c, prothrombin time, activated partial prothrombin time (APTT), white blood cell count and creatinine] differed by disease severity. RESULTS: All CT parameters and all blood test parameters except procalcitonin and APPT were significantly different among mild, moderate and severe groups. By multivariate analysis, mean HU total and eGFR were two independent factors associated with severity (p < 0.0001). Cutoff values for mean HU total and eGFR were, respectively, - 801 HU and 77 ml/min/1.73 m(2) between mild and moderate pneumonia and - 704 HU and 53 ml/min/1.73 m(2) between moderate and severe pneumonia. CONCLUSION: The mean HU total of the whole lung, determined by the AI algorithm, and eGFR reflect the severity of COVID-19 pneumonia.",2021,10.1007/s11604-021-01134-4,diagnosis,True
Quantitative lung lesion features and temporal changes on chest CT in patients with common and severe SARS-CoV-2 pneumonia,"The purpose of this study was to describe the temporal evolution of quantitative lung lesion features on chest computed tomography (CT) in patients with common and severe types of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) pneumonia. Records of patients diagnosed with SARS-CoV-2 pneumonia were reviewed retrospectively from 24 January 2020 to 15 March 2020. Patients were classified into common and severe groups according to the diagnostic criteria of severe pneumonia. The quantitative CT features of lung lesions were automatically calculated using artificial intelligence algorithms, and the percentages of ground-glass opacity volume (PGV), consolidation volume (PCV) and total lesion volume (PTV) were determined in both lungs. PGV, PCV and PTV were analyzed based on the time from the onset of initial symptoms in the common and severe groups. In the common group, PTV increased slowly and peaked at approximately 12 days from the onset of the initial symptoms. In the severe group, PTV peaked at approximately 17 days. The severe pneumonia group exhibited increased PGV, PCV and PTV compared with the common group. These features started to appear in Stage 2 (4-7 days from onset of initial symptoms) and were observed in all subsequent stages (p<0.05). In severe SARS-CoV-2 pneumonia patients, PGV, PCV and PTV began to significantly increase in Stage 2 and decrease in Stage 5 (22-30 days). Compared with common SARS-CoV-2 pneumonia patients, the patients in the severe group exhibited increased PGV, PCV and PTV as well as a later peak time of lesion and recovery time.",2020,10.1371/journal.pone.0236858,diagnosis,True
Quantitative vessel tortuosity: A potential CT imaging biomarker for distinguishing lung granulomas from adenocarcinomas,"Adenocarcinomas and active granulomas can both have a spiculated appearance on computed tomography (CT) and both are often fluorodeoxyglucose (FDG) avid on positron emission tomography (PET) scan, making them difficult to distinguish. Consequently, patients with benign granulomas are often subjected to invasive surgical biopsies or resections. In this study, quantitative vessel tortuosity (QVT), a novel CT imaging biomarker to distinguish between benign granulomas and adenocarcinomas on routine non-contrast lung CT scans is introduced. Our study comprised of CT scans of 290 patients from two different institutions, one cohort for training (N = 145) and the other (N = 145) for independent validation. In conjunction with a machine learning classifier, the top informative and stable QVT features yielded an area under receiver operating characteristic curve (ROC AUC) of 0.85 in the independent validation set. On the same cohort, the corresponding AUCs for two human experts including a radiologist and a pulmonologist were found to be 0.61 and 0.60, respectively. QVT features also outperformed well known shape and textural radiomic features which had a maximum AUC of 0.73 (p-value = 0.002), as well as features learned using a convolutional neural network AUC = 0.76 (p-value = 0.028). Our results suggest that QVT features could potentially serve as a non-invasive imaging biomarker to distinguish granulomas from adenocarcinomas on non-contrast CT scans.",2018,10.1038/s41598-018-33473-0,diagnosis,True
Quantum algorithm for quicker clinical prognostic analysis: an application and experimental study using CT scan images of COVID-19 patients,"BACKGROUND: In medical diagnosis and clinical practice, diagnosing a disease early is crucial for accurate treatment, lessening the stress on the healthcare system. In medical imaging research, image processing techniques tend to be vital in analyzing and resolving diseases with a high degree of accuracy. This paper establishes a new image classification and segmentation method through simulation techniques, conducted over images of COVID-19 patients in India, introducing the use of Quantum Machine Learning (QML) in medical practice. METHODS: This study establishes a prototype model for classifying COVID-19, comparing it with non-COVID pneumonia signals in Computed tomography (CT) images. The simulation work evaluates the usage of quantum machine learning algorithms, while assessing the efficacy for deep learning models for image classification problems, and thereby establishes performance quality that is required for improved prediction rate when dealing with complex clinical image data exhibiting high biases. RESULTS: The study considers a novel algorithmic implementation leveraging quantum neural network (QNN). The proposed model outperformed the conventional deep learning models for specific classification task. The performance was evident because of the efficiency of quantum simulation and faster convergence property solving for an optimization problem for network training particularly for large-scale biased image classification task. The model run-time observed on quantum optimized hardware was 52 min, while on K80 GPU hardware it was 1 h 30 min for similar sample size. The simulation shows that QNN outperforms DNN, CNN, 2D CNN by more than 2.92% in gain in accuracy measure with an average recall of around 97.7%. CONCLUSION: The results suggest that quantum neural networks outperform in COVID-19 traits' classification task, comparing to deep learning w.r.t model efficacy and training time. However, a further study needs to be conducted to evaluate implementation scenarios by integrating the model within medical devices.",2021,10.1186/s12911-021-01588-6,diagnosis,True
Radiation Versus Immune Checkpoint Inhibitor Associated Pneumonitis: Distinct Radiologic Morphologies,"BACKGROUND: Patients with non-small cell lung cancer may develop pneumonitis after thoracic radiotherapy (RT) and immune checkpoint inhibitors (ICIs). We hypothesized that distinct morphologic features are associated with different pneumonitis etiologies. MATERIALS AND METHODS: We systematically compared computed tomography (CT) features of RT- versus ICI-pneumonitis. Clinical and imaging features were tested for association with pneumonitis severity. Lastly, we constructed an exploratory radiomics-based machine learning (ML) model to discern pneumonitis etiology. RESULTS: Between 2009 and 2019, 82 patients developed pneumonitis: 29 after thoracic RT, 23 after ICI, and 30 after RT + ICI. Fifty patients had grade 2 pneumonitis, 22 grade 3, and 7 grade 4. ICI-pneumonitis was more likely bilateral (65% vs. 28%; p = .01) and involved more lobes (66% vs. 45% involving at least three lobes) and was less likely to have sharp border (17% vs. 59%; p = .004) compared with RT-pneumonitis. Pneumonitis morphology after RT + ICI was heterogeneous, with 47% bilateral, 37% involving at least three lobes, and 40% sharp borders. Among all patients, risk factors for severe pneumonitis included poor performance status, smoking history, worse lung function, and bilateral and multifocal involvement on CT. An ML model based on seven radiomic features alone could distinguish ICI- from RT-pneumonitis with an area under the receiver-operating curve of 0.76 and identified the predominant etiology after RT + ICI concordant with multidisciplinary consensus. CONCLUSION: RT- and ICI-pneumonitis exhibit distinct spatial features on CT. Bilateral and multifocal lung involvement is associated with severe pneumonitis. Integrating these morphologic features in the clinical management of patients who develop pneumonitis after RT and ICIs may improve treatment decision-making. IMPLICATIONS FOR PRACTICE: Patients with non-small cell lung cancer often receive thoracic radiation and immune checkpoint inhibitors (ICIs), both of which can cause pneumonitis. This study identified similarities and differences in pneumonitis morphology on computed tomography (CT) scans among pneumonitis due to radiotherapy (RT) alone, ICI alone, and the combination of both. Patients who have bilateral CT changes involving at least three lobes are more likely to have ICI-pneumonitis, whereas those with unilateral CT changes with sharp borders are more likely to have radiation pneumonitis. After RT and/or ICI, severe pneumonitis is associated with bilateral and multifocal CT changes. These results can help guide clinicians in triaging patients who develop pneumonitis after radiation and during ICI treatment.",2021,10.1002/onco.13900,diagnosis,True
Radiogenomic Models Using Machine Learning Techniques to Predict EGFR Mutations in Non-Small Cell Lung Cancer,"BACKGROUND: The purpose of this study was to build radiogenomics models from texture signatures derived from computed tomography (CT) and (18)F-FDG PET-CT (FDG PET-CT) images of non-small cell lung cancer (NSCLC) with and without epidermal growth factor receptor (EGFR) mutations. METHODS: Fifty patients diagnosed with NSCLC between 2011 and 2015 and with known EGFR mutation status were retrospectively identified. Texture features extracted from pretreatment CT and FDG PET-CT images by manual contouring of the primary tumor were used to develop multivariate logistic regression (LR) models to predict EGFR mutations in exon 19 and exon 20. RESULTS: An LR model evaluating FDG PET-texture features was able to differentiate EGFR mutant from wild type with an area under the curve (AUC), sensitivity, specificity, and accuracy of 0.87, 0.76, 0.66, and 0.71, respectively. The model derived from CT texture features had an AUC, sensitivity, specificity, and accuracy of 0.83, 0.84, 0.73, and 0.78, respectively. FDG PET-texture features that could discriminate between mutations in EGFR exon 19 and 21 demonstrated AUC, sensitivity, specificity, and accuracy of 0.86, 0.84, 0.73, and 0.78, respectively. Based on CT texture features, the AUC, sensitivity, specificity, and accuracy were 0.75, 0.81, 0.69, and 0.75, respectively. CONCLUSION: Non-small cell lung cancer texture analysis using FGD-PET and CT images can identify tumors with mutations in EGFR. Imaging signatures could be valuable for pretreatment assessment and prognosis in precision therapy.",2021,10.1177/0846537119899526,diagnosis,True
Radiological Image Traits Predictive of Cancer Status in Pulmonary Nodules,"Purpose: We propose a systematic methodology to quantify incidentally identified pulmonary nodules based on observed radiological traits (semantics) quantified on a point scale and a machine-learning method using these data to predict cancer status.Experimental Design: We investigated 172 patients who had low-dose CT images, with 102 and 70 patients grouped into training and validation cohorts, respectively. On the images, 24 radiological traits were systematically scored and a linear classifier was built to relate the traits to malignant status. The model was formed both with and without size descriptors to remove bias due to nodule size. The multivariate pairs formed on the training set were tested on an independent validation data set to evaluate their performance.Results: The best 4-feature set that included a size measurement (set 1), was short axis, contour, concavity, and texture, which had an area under the receiver operator characteristic curve (AUROC) of 0.88 (accuracy = 81%, sensitivity = 76.2%, specificity = 91.7%). If size measures were excluded, the four best features (set 2) were location, fissure attachment, lobulation, and spiculation, which had an AUROC of 0.83 (accuracy = 73.2%, sensitivity = 73.8%, specificity = 81.7%) in predicting malignancy in primary nodules. The validation test AUROC was 0.8 (accuracy = 74.3%, sensitivity = 66.7%, specificity = 75.6%) and 0.74 (accuracy = 71.4%, sensitivity = 61.9%, specificity = 75.5%) for sets 1 and 2, respectively.Conclusions: Radiological image traits are useful in predicting malignancy in lung nodules. These semantic traits can be used in combination with size-based measures to enhance prediction accuracy and reduce false-positives. Clin Cancer Res; 23(6); 1442-9. ©2016 AACR.",2017,10.1158/1078-0432.Ccr-15-3102,diagnosis,True
Radiologist-supervised Transfer Learning: Improving Radiographic Localization of Pneumonia and Prognostication of Patients With COVID-19,"PURPOSE: To assess the potential of a transfer learning strategy leveraging radiologist supervision to enhance convolutional neural network-based (CNN) localization of pneumonia on radiographs and to further assess the prognostic value of CNN severity quantification on patients evaluated for COVID-19 pneumonia, for whom severity on the presenting radiograph is a known predictor of mortality and intubation. MATERIALS AND METHODS: We obtained an initial CNN previously trained to localize pneumonia along with 25,684 radiographs used for its training. We additionally curated 1466 radiographs from patients who had a computed tomography (CT) performed on the same day. Regional likelihoods of pneumonia were then annotated by cardiothoracic radiologists, referencing these CTs. Combining data, a preexisting CNN was fine-tuned using transfer learning. Whole-image and regional performance of the updated CNN was assessed using receiver-operating characteristic area under the curve and Dice. Finally, the value of CNN measurements was assessed with survival analysis on 203 patients with COVID-19 and compared against modified radiographic assessment of lung edema (mRALE) score. RESULTS: Pneumonia detection area under the curve improved on both internal (0.756 to 0.841) and external (0.864 to 0.876) validation data. Dice overlap also improved, particularly in the lung bases (R: 0.121 to 0.433, L: 0.111 to 0.486). There was strong correlation between radiologist mRALE score and CNN fractional area of involvement (ρ=0.85). Survival analysis showed similar, strong prognostic ability of the CNN and mRALE for mortality, likelihood of intubation, and duration of hospitalization among patients with COVID-19. CONCLUSIONS: Radiologist-supervised transfer learning can enhance the ability of CNNs to localize and quantify the severity of disease. Closed-loop systems incorporating radiologists may be beneficial for continued improvement of artificial intelligence algorithms.",2022,10.1097/rti.0000000000000618,diagnosis,True
Radiologists versus Deep Convolutional Neural Networks: A Comparative Study for Diagnosing COVID-19,"The reverse transcriptase polymerase chain reaction (RT-PCR) is still the routinely used test for the diagnosis of SARS-CoV-2 (COVID-19). However, according to several reports, RT-PCR showed a low sensitivity and multiple tests may be required to rule out false negative results. Recently, chest computed tomography (CT) has been an efficient tool to diagnose COVID-19 as it is directly affecting the lungs. In this paper, we investigate the application of pre-trained models in diagnosing patients who are positive for COVID-19 and differentiating it from normal patients, who tested negative for coronavirus. The study aims to compare the generalization capabilities of deep learning models with two thoracic radiologists in diagnosing COVID-19 chest CT images. A dataset of 3000 images was obtained from the Near East Hospital, Cyprus, and used to train and to test the three employed pre-trained models. In a test set of 250 images used to evaluate the deep neural networks and the radiologists, it was found that deep networks (ResNet-18, ResNet-50, and DenseNet-201) can outperform the radiologists in terms of higher accuracy (97.8%), sensitivity (98.1%), specificity (97.3%), precision (98.4%), and F1-score (198.25%), in classifying COVID-19 images.",2021,10.1155/2021/5527271,diagnosis,True
Radiomic Analysis of CT Predicts Tumor Response in Human Lung Cancer with Radiotherapy,"PURPOSE: Radiomics features can be positioned to monitor changes throughout treatment. In this study, we evaluated machine learning for predicting tumor response by analyzing CT images of lung cancer patients treated with radiotherapy. EXPERIMENTAL DESIGN: For this retrospective study, screening or standard diagnostic CT images were collected for 100 patients (mean age, 67 years; range, 55-82 years; 64 men [mean age, 68 years; range, 55-82 years] and 36 women [mean age, 65 years; range, 60-72 years]) from two institutions between 2013 and 2017. Radiomics analysis was available for each patient. Features were pruned to train machine learning classifiers with 50 patients, then trained in the test dataset. RESULT: A support vector machine classifier with 2 radiomic features (flatness and coefficient of variation) achieved an area under the receiver operating characteristic curve (AUC) of 0.91 on the test set. CONCLUSION: The 2 radiomic features, flatness, and coefficient of variation, from the volume of interest of lung tumor, can be the biomarkers for predicting tumor response at CT.",2020,10.1007/s10278-020-00385-3,diagnosis,True
Radiomic analysis of planning computed tomograms for predicting radiation-induced lung injury and outcome in lung cancer patients treated with robotic stereotactic body radiation therapy,"OBJECTIVES: To predict radiation-induced lung injury and outcome in non-small cell lung cancer (NSCLC) patients treated with robotic stereotactic body radiation therapy (SBRT) from radiomic features of the primary tumor. METHODS: In all, 110 patients with primary stage I/IIa NSCLC were analyzed for local control (LC), disease-free survival (DFS), overall survival (OS) and development of local lung injury up to fibrosis (LF). First-order (histogram), second-order (GLCM, Gray Level Co-occurrence Matrix) and shape-related radiomic features were determined from the unprocessed or filtered planning CT images of the gross tumor volume (GTV), subjected to LASSO (Least Absolute Shrinkage and Selection Operator) regularization and used to construct continuous and dichotomous risk scores for each endpoint. RESULTS: Continuous scores comprising 1-5 histogram or GLCM features had a significant (p = 0.0001-0.032) impact on all endpoints that was preserved in a multifactorial Cox regression analysis comprising additional clinical and dosimetric factors. At 36 months, LC did not differ between the dichotomous risk groups (93% vs. 85%, HR 0.892, 95%CI 0.222-3.590), while DFS (45% vs. 17%, p < 0.05, HR 0.457, 95%CI 0.240-0.868) and OS (80% vs. 37%, p < 0.001, HR 0.190, 95%CI 0.065-0.556) were significantly lower in the high-risk groups. Also, the frequency of LF differed significantly between the two risk groups (63% vs. 20% at 24 months, p < 0.001, HR 0.158, 95%CI 0.054-0.458). CONCLUSION: Radiomic analysis of the gross tumor volume may help to predict DFS and OS and the development of local lung fibrosis in early stage NSCLC patients treated with stereotactic radiotherapy.",2019,10.1007/s00066-019-01452-7,diagnosis,True
Radiomic Detection of EGFR Mutations in NSCLC,"Radiomics is defined as the use of automated or semi-automated post-processing and analysis of multiple features derived from imaging exams. Extracted features might generate models able to predict the molecular profile of solid tumors. The aim of this study was to develop a predictive algorithm to define the mutational status of EGFR in treatment-naïve patients with advanced non-small cell lung cancer (NSCLC). CT scans from 109 treatment-naïve patients with NSCLC (21 EGFR-mutant and 88 EGFR-wild type) underwent radiomics analysis to develop a machine learning model able to recognize EGFR-mutant from EGFR-WT patients via CT scans. A ""test-retest"" approach was used to identify stable radiomics features. The accuracy of the model was tested on an external validation set from another institution and on a dataset from the Cancer Imaging Archive (TCIA). The machine learning model that considered both radiomic and clinical features (gender and smoking status) reached a diagnostic accuracy of 88.1% in our dataset with an AUC at the ROC curve of 0.85, whereas the accuracy values in the datasets from TCIA and the external institution were 76.6% and 83.3%, respectively. Furthermore, 17 distinct radiomics features detected at baseline CT scan were associated with subsequent development of T790M during treatment with an EGFR inhibitor. In conclusion, our machine learning model was able to identify EGFR-mutant patients in multiple validation sets with globally good accuracy, especially after data optimization. More comprehensive training sets might result in further improvement of radiomics-based algorithms. SIGNIFICANCE: These findings demonstrate that data normalization and ""test-retest"" methods might improve the performance of machine learning models on radiomics images and increase their reliability when used on external validation datasets.",2021,10.1158/0008-5472.Can-20-0999,diagnosis,True
Radiomic signature as a diagnostic factor for histologic subtype classification of non-small cell lung cancer,"OBJECTIVES: To distinguish squamous cell carcinoma (SCC) from lung adenocarcinoma (ADC) based on a radiomic signature METHODS: This study involved 129 patients with non-small cell lung cancer (NSCLC) (81 in the training cohort and 48 in the independent validation cohort). Approximately 485 features were extracted from a manually outlined tumor region. The LASSO logistic regression model selected the key features of a radiomic signature. Receiver operating characteristic curve and area under the curve (AUC) were used to evaluate the performance of the radiomic signature in the training and validation cohorts. RESULTS: Five features were selected to construct the radiomic signature for histologic subtype classification. The performance of the radiomic signature to distinguish between lung ADC and SCC in both training and validation cohorts was good, with an AUC of 0.905 (95% confidence interval [CI]: 0.838 to 0.971), sensitivity of 0.830, and specificity of 0.929. In the validation cohort, the radiomic signature showed an AUC of 0.893 (95% CI: 0.789 to 0.996), sensitivity of 0.828, and specificity of 0.900. CONCLUSIONS: A unique radiomic signature was constructed for use as a diagnostic factor for discriminating lung ADC from SCC. Patients with NSCLC will benefit from the proposed radiomic signature. KEY POINTS: • Machine learning can be used for auxiliary distinguish in lung cancer. • Radiomic signature can discriminate lung ADC from SCC. • Radiomics can help to achieve precision medical treatment.",2018,10.1007/s00330-017-5221-1,diagnosis,True
Radiomic-Based Pathological Response Prediction from Primary Tumors and Lymph Nodes in NSCLC,"INTRODUCTION: Noninvasive biomarkers that capture the total tumor burden could provide important complementary information for precision medicine to aid clinical decision making. We investigated the value of radiomic data extracted from pretreatment computed tomography images of the primary tumor and lymph nodes in predicting pathological response after neoadjuvant chemoradiation before surgery. METHODS: A total of 85 patients with resectable locally advanced (stage II-III) NSCLC (median age 60.3 years, 65% female) treated from 2003 to 2013 were included in this institutional review board-approved study. Radiomics analysis was performed on 85 primary tumors and 178 lymph nodes to discriminate between pathological complete response (pCR) and gross residual disease (GRD). Twenty nonredundant and stable features (10 from each site) were evaluated by using the area under the curve (AUC) (all p values were corrected for multiple hypothesis testing). Classification performance of each feature set was evaluated by random forest and nested cross validation. RESULTS: Three radiomic features (describing primary tumor sphericity and lymph node homogeneity) were significantly predictive of pCR with similar performances (all AUC = 0.67, p < 0.05). Two features (quantifying lymph node homogeneity) were predictive of GRD (AUC range 0.72-0.75, p < 0.05) and performed significantly better than the primary features (AUC = 0.62). Multivariate analysis showed that for pCR, the radiomic features set alone had the best-performing classification (median AUC = 0.68). Furthermore, for GRD classification, the combination of radiomic and clinical data significantly outperformed all other feature sets (median AUC = 0.73). CONCLUSION: Lymph node phenotypic information was significantly predictive for pathological response and showed higher classification performance than radiomic features obtained from the primary tumor.",2017,10.1016/j.jtho.2016.11.2226,diagnosis,True
Radiomics analysis of pulmonary nodules in low-dose CT for early detection of lung cancer,"PURPOSE: To develop a radiomics prediction model to improve pulmonary nodule (PN) classification in low-dose CT. To compare the model with the American College of Radiology (ACR) Lung CT Screening Reporting and Data System (Lung-RADS) for early detection of lung cancer. METHODS: We examined a set of 72 PNs (31 benign and 41 malignant) from the Lung Image Database Consortium image collection (LIDC-IDRI). One hundred three CT radiomic features were extracted from each PN. Before the model building process, distinctive features were identified using a hierarchical clustering method. We then constructed a prediction model by using a support vector machine (SVM) classifier coupled with a least absolute shrinkage and selection operator (LASSO). A tenfold cross-validation (CV) was repeated ten times (10 × 10-fold CV) to evaluate the accuracy of the SVM-LASSO model. Finally, the best model from the 10 × 10-fold CV was further evaluated using 20 × 5- and 50 × 2-fold CVs. RESULTS: The best SVM-LASSO model consisted of only two features: the bounding box anterior-posterior dimension (BB_AP) and the standard deviation of inverse difference moment (SD_IDM). The BB_AP measured the extension of a PN in the anterior-posterior direction and was highly correlated (r = 0.94) with the PN size. The SD_IDM was a texture feature that measured the directional variation of the local homogeneity feature IDM. Univariate analysis showed that both features were statistically significant and discriminative (P = 0.00013 and 0.000038, respectively). PNs with larger BB_AP or smaller SD_IDM were more likely malignant. The 10 × 10-fold CV of the best SVM model using the two features achieved an accuracy of 84.6% and 0.89 AUC. By comparison, Lung-RADS achieved an accuracy of 72.2% and 0.77 AUC using four features (size, type, calcification, and spiculation). The prediction improvement of SVM-LASSO comparing to Lung-RADS was statistically significant (McNemar's test P = 0.026). Lung-RADS misclassified 19 cases because it was mainly based on PN size, whereas the SVM-LASSO model correctly classified 10 of these cases by combining a size (BB_AP) feature and a texture (SD_IDM) feature. The performance of the SVM-LASSO model was stable when leaving more patients out with five- and twofold CVs (accuracy 84.1% and 81.6%, respectively). CONCLUSION: We developed an SVM-LASSO model to predict malignancy of PNs with two CT radiomic features. We demonstrated that the model achieved an accuracy of 84.6%, which was 12.4% higher than Lung-RADS.",2018,10.1002/mp.12820,diagnosis,True
Radiomics analysis using stability selection supervised component analysis for right-censored survival data,"Radiomics is a newly emerging field that involves the extraction of massive quantitative features from biomedical images by using data-characterization algorithms. Distinctive imaging features identified from biomedical images can be used for prognosis and therapeutic response prediction, and they can provide a noninvasive approach for personalized therapy. So far, many of the published radiomics studies utilize existing out of the box algorithms to identify the prognostic markers from biomedical images that are not specific to radiomics data. To better utilize biomedical images, we propose a novel machine learning approach, stability selection supervised principal component analysis (SSSuperPCA) that identifies stable features from radiomics big data coupled with dimension reduction for right-censored survival outcomes. The proposed approach allows us to identify a set of stable features that are highly associated with the survival outcomes in a simple yet meaningful manner, while controlling the per-family error rate. We evaluate the performance of SSSuperPCA using simulations and real data sets for non-small cell lung cancer and head and neck cancer, and compare it with other machine learning algorithms. The results demonstrate that our method has a competitive edge over other existing methods in identifying the prognostic markers from biomedical imaging data for the prediction of right-censored survival outcomes.",2020,10.1016/j.compbiomed.2020.103959,diagnosis,True
Radiomics and gene expression profile to characterise the disease and predict outcome in patients with lung cancer,"OBJECTIVE: The objectives of our study were to assess the association of radiomic and genomic data with histology and patient outcome in non-small cell lung cancer (NSCLC). METHODS: In this retrospective single-centre observational study, we selected 151 surgically treated patients with adenocarcinoma or squamous cell carcinoma who performed baseline [18F] FDG PET/CT. A subgroup of patients with cancer tissue samples at the Institutional Biobank (n = 74/151) was included in the genomic analysis. Features were extracted from both PET and CT images using an in-house tool. The genomic analysis included detection of genetic variants, fusion transcripts, and gene expression. Generalised linear model (GLM) and machine learning (ML) algorithms were used to predict histology and tumour recurrence. RESULTS: Standardised uptake value (SUV) and kurtosis (among the PET and CT radiomic features, respectively), and the expression of TP63, EPHA10, FBN2, and IL1RAP were associated with the histotype. No correlation was found between radiomic features/genomic data and relapse using GLM. The ML approach identified several radiomic/genomic rules to predict the histotype successfully. The ML approach showed a modest ability of PET radiomic features to predict relapse, while it identified a robust gene expression signature able to predict patient relapse correctly. The best-performing ML radiogenomic rule predicting the outcome resulted in an area under the curve (AUC) of 0.87. CONCLUSIONS: Radiogenomic data may provide clinically relevant information in NSCLC patients regarding the histotype, aggressiveness, and progression. Gene expression analysis showed potential new biomarkers and targets valuable for patient management and treatment. The application of ML allows to increase the efficacy of radiogenomic analysis and provides novel insights into cancer biology.",2021,10.1007/s00259-021-05371-7,diagnosis,True
Radiomics Approach to Prediction of Occult Mediastinal Lymph Node Metastasis of Lung Adenocarcinoma,"OBJECTIVE: The purpose of this study was to evaluate the prognostic impact of radiomic features from CT scans in predicting occult mediastinal lymph node (LN) metastasis of lung adenocarcinoma. MATERIALS AND METHODS: A total of 492 patients with lung adenocarcinoma who underwent preoperative unenhanced chest CT were enrolled in the study. A total of 300 radiomics features quantifying tumor intensity, texture, and wavelet were extracted from the segmented entire-tumor volume of interest of the primary tumor. A radiomics signature was generated by use of the relief-based feature method and the support vector machine classification method. A ROC regression curve was drawn for the predictive performance of radiomics features. Multivariate logistic regression models based on clinicopathologic and radiomics features were compared for discriminating mediastinal LN metastasis. RESULTS: Clinical variables (sex, tumor diameter, tumor location) and predominant subtype were risk factors for pathologic mediastinal LN metastasis. The accuracy of radiomics signature for predicting mediastinal LN metastasis was 91.1% in ROC analysis (AUC, 0.972; sensitivity, 94.8%; specificity, 92%). Radiomics signature (Akaike information criterion [AIC] value, 80.9%) showed model fit superior to that of the clinicohistopathologic model (AIC value, 61.1%) for predicting mediastinal LN metastasis. CONCLUSION: The radiomics signature of a primary tumor based on CT scans can be used for quantitative and noninvasive prediction of occult mediastinal LN metastasis of lung adenocarcinoma.",2018,10.2214/ajr.17.19074,diagnosis,True
Radiomics combined with clinical characteristics predicted the progression-free survival time in first-line targeted therapy for advanced non-small cell lung cancer with EGFR mutation,"OBJECTIVE: This study was to explore the most appropriate radiomics modeling method to predict the progression-free survival of EGFR-TKI treatment in advanced non-small cell lung cancer with EGFR mutations. Different machine learning methods may vary considerably and the selection of a proper model is essential for accurate treatment outcome prediction. Our study were established 176 discrimination models constructed with 22 feature selection methods and 8 classifiers. The predictive performance of each model were evaluated using the AUC, ACC, sensitivity and specificity, where the optimal model was identified. RESULTS: There were totally 107 radiomics features and 7 clinical features obtained from each patient. After feature selection, the top-ten most relevant features were fed to train 176 models. Significant performance variations were observed in the established models, with the best performance achieved by the logistic regression model using gini-index feature selection (AUC = 0.797, ACC = 0.722, sensitivity = 0.758, specificity = 0.693). The median R-score was 0.518 (IQR, 0.023-0.987), and the patients were divided into high-risk and low-risk groups based on this cut-off value. The KM survival curves of the two groups demonstrated evident stratification results (p = 0.000).",2022,10.1186/s13104-022-06019-x,diagnosis,True
"Radiomics complements clinical, radiological, and technical features to assess local control of colorectal cancer lung metastases treated with radiofrequency ablation","OBJECTIVES: Radiofrequency ablation (RFA) of lung metastases of colorectal origin can improve patient survival and quality of life. Our aim was to identify pre- and per-RFA features predicting local control of lung metastases following RFA. METHODS: This case-control single-center retrospective study included 119 lung metastases treated with RFA in 48 patients (median age: 60 years). Clinical, technical, and radiological data before and on early CT scan (at 48 h) were retrieved. After CT scan preprocessing, 64 radiomics features were extracted from pre-RFA and early control CT scans. Log-rank tests were used to detect categorical variables correlating with post-RFA local tumor progression-free survival (LTPFS). Radiomics prognostic scores (RPS) were developed on reproducible radiomics features using Monte-Carlo cross-validated LASSO Cox regressions. RESULTS: Twenty-six of 119 (21.8%) nodules demonstrated local progression (median delay: 11.2 months). In univariate analysis, four non-radiomics variables correlated with post-RFA-LTPFS: nodule size (> 15 mm, p < 0.001), chosen electrode (with difference between covered array and nodule diameter < 20 mm or non-expandable electrode, p = 0.03), per-RFA intra-alveolar hemorrhage (IAH, p = 0.002), and nodule location into the ablation zone (not seen or in contact with borders, p = 0.005). The highest prognostic performance was reached with the multivariate model including a RPS built on 4 radiomics features from pre-RFA and early revaluation CT scans (cross-validated concordance index= 0.74) in which this RPS remained an independent predictor (cross-validated HR = 3.49, 95% confidence interval = [1.76 - 6.96]). CONCLUSIONS: Technical, radiological, and radiomics features of the lung metastases before RFA and of the ablation zone at 48 h can help discriminate nodules at risk of local progression that could benefit from complementary local procedure. KEY POINTS: • The highest prognostic performance to predict post-RFA LTPFS was reached with a parsimonious model including a radiomics score built with 4 radiomics features. • Nodule size, difference between electrode diameter, use of non-expandable electrode, per-RFA hemorrhage, and a tumor not seen or in contact with the ablation zone borders at 48-h CT were correlated with post-RFA LTPFS.",2021,10.1007/s00330-021-07998-4,diagnosis,True
Radiomics for Classification of Lung Cancer Histological Subtypes Based on Nonenhanced Computed Tomography,"OBJECTIVES: To evaluate the performance of using radiomics method to classify lung cancer histological subtypes based on nonenhanced computed tomography images. MATERIALS AND METHODS: 278 patients with pathologically confirmed lung cancer were collected, including 181 nonsmall cell lung cancer (NSCLC) and 97 small cell lung cancers (SCLC) patients. Among the NSCLC patients, 88 patients were adenocarcinomas (AD) and 93 patients were squamous cell carcinomas (SCC). In total, 1695 quantitative radiomic features (QRF) were calculated from the primary lung cancer tumor in each patient. To build radiomic classification model based on the extracted QRFs, several machine-learning algorithms were applied sequentially. First, unsupervised hierarchical clustering was used to exclude highly correlated QRFs; second, the minimum Redundancy Maximum Relevance feature selection algorithm was employed to select informative and nonredundant QRFs; finally, the Incremental Forward Search and Support Vector Machine classification algorithms were used to combine the selected QRFs and build the model. In our work, to study the phenotypic differences among lung cancer histological subtypes, four classification models were built. They were models of SCLC vs NSCLC, SCLC vs AD, SCLC vs SCC, and AD vs SCC. The performance of the classification models was evaluated by the area under the receiver operating characteristic curve (AUC) estimated by three-fold cross-validation. RESULTS: The AUC (95% confidence interval) for the model of SCLC vs NSCLC was 0.741(0.678, 0.795). For the models of SCLC vs AD and SCLC vs SCC, the AUCs were 0.822(0.755, 0.875) and 0.665(0.583, 0.738), respectively. The AUC for the model of AD vs SCC was 0.655(0.570, 0.731). Several QRFs (""Law_15,"" ""LoG_Uniformity,"" ""GLCM_Contrast,"" and ""Compactness Factor"") that characterize tumor heterogeneity and shape were selected as the significant features to build the models. CONCLUSION: Our results show that phenotypic differences exist among different lung cancer histological subtypes on nonenhanced computed tomography image.",2019,10.1016/j.acra.2018.10.013,diagnosis,True
Radiomics for lung adenocarcinoma manifesting as pure ground-glass nodules: invasive prediction,"OBJECTIVES: To investigate the value of radiomics based on CT imaging in predicting invasive adenocarcinoma manifesting as pure ground-glass nodules (pGGNs). METHODS: This study enrolled 395 pGGNs with histopathology-confirmed benign nodules or adenocarcinoma. A total of 396 radiomic features were extracted from each labeled nodule. A Rad-score was constructed with the least absolute shrinkage and selection operator (LASSO) in the training set. Multivariate logistic regression analysis was conducted to establish the radiographic model and the combined radiographic-radiomics model. The predictive performance was validated by receiver operating characteristic (ROC) curve. Based on the multivariate logistic regression analysis, an individual prediction nomogram was developed and the clinical utility was assessed. RESULTS: Five radiomic features and four radiographic features were selected for predicting the invasive lesions. The combined radiographic-radiomics model (AUC 0.77; 95% CI, 0.69-0.86) performed better than the radiographic model (AUC 0.71; 95% CI, 0.62-0.81) and Rad-score (AUC 0.72; 95% CI, 0.63-0.81) in the validation set. The clinical utility of the individualized prediction nomogram developed using the Rad-score, margin, spiculation, and size was confirmed in the validation set. The decision curve analysis (DCA) indicated that using a model with Rad-score to predict the invasive lesion would be more beneficial than that without Rad-score and the clinical model. CONCLUSIONS: The proposed radiomics-based nomogram that incorporated the Rad-score, margin, spiculation, and size may be utilized as a noninvasive biomarker for the assessment of invasive prediction in patients with pGGNs. KEY POINTS: • CT-based radiomics analysis helps invasive prediction manifested as pGGNs. • The combined radiographic-radiomics model may be utilized as a noninvasive biomarker for predicting invasive lesion for pGGNs. • Radiomics-based individual nomogram may serve as a vital decision support tool to identify invasive pGGNs, obviating further workup and blind follow-up.",2020,10.1007/s00330-020-06776-y,diagnosis,True
Radiomics is feasible for prediction of spread through air spaces in patients with nonsmall cell lung cancer,"Tumor spread through air spaces (STAS) in non-small-cell lung cancer (NSCLC) is known to influence a poor patient outcome, even in patients presenting with early-stage disease. However, the pre-operative diagnosis of STAS remains challenging. With the progress of radiomics-based analyses several attempts have been made to predict STAS based on radiological findings. In the present study, patients with NSCLC which is located peripherally and tumors ≤ 2 cm in size on computed tomography (CT) that were potential candidates for sublobar resection were enrolled in this study. The radiologic features of the targeted tumors on thin-section CT were extracted using the PyRadiomics v3.0 software package, and a predictive model for STAS was built using the t-test and XGBoost. Thirty-five out of 226 patients had a STAS histology. The predictive model of STAS indicated an area under the receiver-operator characteristic curve (AUC) of 0.77. There was no significant difference in the overall survival (OS) for lobectomy between the predicted-STAS (+) and (-) groups (p = 0.19), but an unfavorable OS for sublobar resection was indicated in the predicted-STAS (+) group (p < 0.01). These results suggest that radiomics with machine-learning helped to develop a favorable model of STAS (+) NSCLC, which might be useful for the proper selection of candidates who should undergo sublobar resection.",2021,10.1038/s41598-021-93002-4,prognosis,True
Radiomics nomogram for preoperative differentiation of lung tuberculoma from adenocarcinoma in solitary pulmonary solid nodule,"PURPOSE: To investigate the preoperative differential diagnostic performance of a radiomics nomogram in tuberculous granuloma (TBG) and lung adenocarcinoma (LAC) appearing as solitary pulmonary solid nodules (SPSNs). METHOD: We retrospectively recruited 426 patients with SPSNs from two centers and assigned them to training (n = 123), internal validation (n = 121), and external validation cohorts (n = 182). A model of deep learning (DL) was built for tumor segmentation from routine computed tomography (CT) images and extraction of 3D radiomics features. We used the least absolute shrinkage and selection operator (LASSO) logistic regression to build a radiomics signature. A clinical model was developed with clinical factors, including age, gender, and CT-based subjective findings (eg, lesion size, lesion location, lesion margin, lobulated sharp, and spiculation sign). We constructed individualized radiomics nomograms incorporating the radiomics signature and clinical factors to validate the diagnostic ability. RESULTS: Three factors - radiomics signature, age, and spiculation sign - were found to be independent predictors and were used to build the radiomics nomogram, which showed better diagnostic accuracy than any single model (all net reclassification improvement p < 0.05). The area under curve yielded was 0.9660 (95% confidence interval [CI], 0.9390-0.9931), 0.9342 (95% CI, 0.8944-0.9739), and 0.9064 (95% CI, 0.8639-0.9490) for the training, internal validation, and external validation cohorts, respectively. Decision curve analysis (DCA) and stratification analysis showed the nomogram has potential for generalizability. CONCLUSION: The radiomics nomogram we developed can preoperatively distinguish between LAC and TBG in patient with a SPSN.",2020,10.1016/j.ejrad.2020.109022,diagnosis,True
Radiomics of (18)F-FDG PET/CT images predicts clinical benefit of advanced NSCLC patients to checkpoint blockade immunotherapy,"INTRODUCTION: Immunotherapy has improved outcomes for patients with non-small cell lung cancer (NSCLC), yet durable clinical benefit (DCB) is experienced in only a fraction of patients. Here, we test the hypothesis that radiomics features from baseline pretreatment (18)F-FDG PET/CT scans can predict clinical outcomes of NSCLC patients treated with checkpoint blockade immunotherapy. METHODS: This study included 194 patients with histologically confirmed stage IIIB-IV NSCLC with pretreatment PET/CT images. Radiomics features were extracted from PET, CT, and PET+CT fusion images based on minimum Kullback-Leibler divergence (KLD) criteria. The radiomics features from 99 retrospective patients were used to train a multiparametric radiomics signature (mpRS) to predict DCB using an improved least absolute shrinkage and selection operator (LASSO) method, which was subsequently validated in both retrospective (N = 47) and prospective test cohorts (N = 48). Using these cohorts, the mpRS was also used to predict progression-free survival (PFS) and overall survival (OS) by training nomogram models using multivariable Cox regression analyses with additional clinical characteristics incorporated. RESULTS: The mpRS could predict patients who will receive DCB, with areas under receiver operating characteristic curves (AUCs) of 0.86 (95%CI 0.79-0.94), 0.83 (95%CI 0.71-0.94), and 0.81 (95%CI 0.68-0.92) in the training, retrospective test, and prospective test cohorts, respectively. In the same three cohorts, respectively, nomogram models achieved C-indices of 0.74 (95%CI 0.68-0.80), 0.74 (95%CI 0.66-0.82), and 0.77 (95%CI 0.69-0.84) to predict PFS and C-indices of 0.83 (95%CI 0.77-0.88), 0.83 (95%CI 0.71-0.94), and 0.80 (95%CI 0.69-0.91) to predict OS. CONCLUSION: PET/CT-based signature can be used prior to initiation of immunotherapy to identify NSCLC patients most likely to benefit from immunotherapy. As such, these data may be leveraged to improve more precise and individualized decision support in the treatment of patients with advanced NSCLC.",2020,10.1007/s00259-019-04625-9,diagnosis,True
Radiomics-based features for pattern recognition of lung cancer histopathology and metastases,"BACKGROUND AND OBJECTIVES: lung cancer is the leading cause of cancer-related deaths in the world, and its poor prognosis varies markedly according to tumor staging. Computed tomography (CT) is the imaging modality of choice for lung cancer evaluation, being used for diagnosis and clinical staging. Besides tumor stage, other features, like histopathological subtype, can also add prognostic information. In this work, radiomics-based CT features were used to predict lung cancer histopathology and metastases using machine learning models. METHODS: local image datasets of confirmed primary malignant pulmonary tumors were retrospectively evaluated for testing and validation. CT images acquired with same protocol were semiautomatically segmented. Tumors were characterized by clinical features and computer attributes of intensity, histogram, texture, shape, and volume. Three machine learning classifiers used up to 100 selected features to perform the analysis. RESULTS: radiomics-based features yielded areas under the receiver operating characteristic curve of 0.89, 0.97, and 0.92 at testing and 0.75, 0.71, and 0.81 at validation for lymph nodal metastasis, distant metastasis, and histopathology pattern recognition, respectively. CONCLUSIONS: the radiomics characterization approach presented great potential to be used in a computational model to aid lung cancer histopathological subtype diagnosis as a ""virtual biopsy"" and metastatic prediction for therapy decision support without the necessity of a whole-body imaging scanning.",2018,10.1016/j.cmpb.2018.02.015,diagnosis,True
"Radiomics-based machine learning differentiates ""ground-glass"" opacities due to COVID-19 from acute non-COVID-19 lung disease","Ground-glass opacities (GGOs) are a non-specific high-resolution computed tomography (HRCT) finding tipically observed in early Coronavirus disesase 19 (COVID-19) pneumonia. However, GGOs are also seen in other acute lung diseases, thus making challenging the differential diagnosis. To this aim, we investigated the performance of a radiomics-based machine learning method to discriminate GGOs due to COVID-19 from those due to other acute lung diseases. Two sets of patients were included: a first set of 28 patients (COVID) diagnosed with COVID-19 infection confirmed by real-time polymerase chain reaction (RT-PCR) between March and April 2020 having (a) baseline HRCT at hospital admission and (b) predominant GGOs pattern on HRCT; a second set of 30 patients (nCOVID) showing (a) predominant GGOs pattern on HRCT performed between August 2019 and April 2020 and (b) availability of final diagnosis. Two readers independently segmented GGOs on HRCTs using a semi-automated approach, and radiomics features were extracted using a standard open source software (PyRadiomics). Partial least square (PLS) regression was used as the multivariate machine-learning algorithm. A leave-one-out nested cross-validation was implemented. PLS β-weights of radiomics features, including the 5% features with the largest β-weights in magnitude (top 5%), were obtained. The diagnostic performance of the radiomics model was assessed through receiver operating characteristic (ROC) analysis. The Youden's test assessed sensitivity and specificity of the classification. A null hypothesis probability threshold of 5% was chosen (p < 0.05). The predictive model delivered an AUC of 0.868 (Youden's index = 0.68, sensitivity = 93%, specificity 75%, p = 4.2 × 10(-7)). Of the seven features included in the top 5% features, five were texture-related. A radiomics-based machine learning signature showed the potential to accurately differentiate GGOs due to COVID-19 pneumonia from those due to other acute lung diseases. Most of the discriminant radiomics features were texture-related. This approach may assist clinician to adopt the appropriate management early, while improving the triage of patients.",2021,10.1038/s41598-021-96755-0,diagnosis,True
Radiomics-based prediction for tumour spread through air spaces in stage I lung adenocarcinoma using machine learning,"OBJECTIVES: As evidence has proven that sublobar resection is oncologically contraindicated by tumour spread through air spaces (STAS), its preoperative recognition is vital in customizing surgical strategies. We aimed to assess the value of radiomics in predicting STAS in stage I lung adenocarcinoma. METHODS: We retrospectively reviewed the patients with stage I lung adenocarcinoma, who accepted curative resection in our institution between January 2011 and December 2013. Using 'PyRadiomics' package, 88 radiomics features were extracted from computed tomography (CT) images and a prediction model was consequently constructed using Naïve Bayes machine-learning approach. The accuracy of the model was assessed through receiver operating curve analysis, and the performance of the model was validated both internally and externally. RESULTS: A total of 233 patients were included as the training cohort with 69 (29.6%) patients being STAS (+). Patients with STAS had worse recurrence-free survival and overall survival (P < 0.001). After feature extraction, 5 most contributing radiomics features were selected out to develop a Naïve Bayes model. In the internal validation, the model exhibited good performance with an area under the curve value of 0.63 (0.55-0.71). External validation was conducted on a test cohort with 112 patients and produced an area under the curve value of 0.69. CONCLUSIONS: CT-based radiomics is valuable in preoperatively predicting STAS in stage I lung adenocarcinoma, which may aid surgeons in determining the optimal surgical approach.",2020,10.1093/ejcts/ezaa011,diagnosis,True
Radiomics-based Prognosis Analysis for Non-Small Cell Lung Cancer,"Radiomics characterizes tumor phenotypes by extracting large numbers of quantitative features from radiological images. Radiomic features have been shown to provide prognostic value in predicting clinical outcomes in several studies. However, several challenges including feature redundancy, unbalanced data, and small sample sizes have led to relatively low predictive accuracy. In this study, we explore different strategies for overcoming these challenges and improving predictive performance of radiomics-based prognosis for non-small cell lung cancer (NSCLC). CT images of 112 patients (mean age 75 years) with NSCLC who underwent stereotactic body radiotherapy were used to predict recurrence, death, and recurrence-free survival using a comprehensive radiomics analysis. Different feature selection and predictive modeling techniques were used to determine the optimal configuration of prognosis analysis. To address feature redundancy, comprehensive analysis indicated that Random Forest models and Principal Component Analysis were optimum predictive modeling and feature selection methods, respectively, for achieving high prognosis performance. To address unbalanced data, Synthetic Minority Over-sampling technique was found to significantly increase predictive accuracy. A full analysis of variance showed that data endpoints, feature selection techniques, and classifiers were significant factors in affecting predictive accuracy, suggesting that these factors must be investigated when building radiomics-based predictive models for cancer prognosis.",2017,10.1038/srep46349,prognosis,True
Radiomics-guided deep neural networks stratify lung adenocarcinoma prognosis from CT scans,"Deep learning (DL) is a breakthrough technology for medical imaging with high sample size requirements and interpretability issues. Using a pretrained DL model through a radiomics-guided approach, we propose a methodology for stratifying the prognosis of lung adenocarcinomas based on pretreatment CT. Our approach allows us to apply DL with smaller sample size requirements and enhanced interpretability. Baseline radiomics and DL models for the prognosis of lung adenocarcinomas were developed and tested using local (n = 617) cohort. The DL models were further tested in an external validation (n = 70) cohort. The local cohort was divided into training and test cohorts. A radiomics risk score (RRS) was developed using Cox-LASSO. Three pretrained DL networks derived from natural images were used to extract the DL features. The features were further guided using radiomics by retaining those DL features whose correlations with the radiomics features were high and Bonferroni-corrected p-values were low. The retained DL features were subject to a Cox-LASSO when constructing DL risk scores (DRS). The risk groups stratified by the RRS and DRS showed a significant difference in training, testing, and validation cohorts. The DL features were interpreted using existing radiomics features, and the texture features explained the DL features well.",2021,10.1038/s42003-021-02814-7,prognosis,True
Rapidly deploying a COVID-19 decision support system in one of the largest Brazilian hospitals,"The COVID-19 pandemic generated research interest in automated models to perform classification and segmentation from medical imaging of COVID-19 patients, However, applications in real-world scenarios are still needed. We describe the development and deployment of COVID-19 decision support and segmentation system. A partnership with a Brazilian radiologist consortium, gave us access to 1000s of labeled computed tomography (CT) and X-ray images from São Paulo Hospitals. The system used EfficientNet and EfficientDet networks, state-of-the-art convolutional neural networks for natural images classification and segmentation, in a real-time scalable scenario in communication with a Picture Archiving and Communication System (PACS). Additionally, the system could reject non-related images, using header analysis and classifiers. We achieved CT and X-ray classification accuracies of 0.94 and 0.98, respectively, and Dice coefficient for lung and covid findings segmentations of 0.98 and 0.73, respectively. The median response time was 7 s for X-ray and 4 min for CT.",2021,10.1177/14604582211033017,diagnosis,True
RCoNet: Deformable Mutual Information Maximization and High-Order Uncertainty-Aware Learning for Robust COVID-19 Detection,"The novel 2019 Coronavirus (COVID-19) infection has spread worldwide and is currently a major healthcare challenge around the world. Chest computed tomography (CT) and X-ray images have been well recognized to be two effective techniques for clinical COVID-19 disease diagnoses. Due to faster imaging time and considerably lower cost than CT, detecting COVID-19 in chest X-ray (CXR) images is preferred for efficient diagnosis, assessment, and treatment. However, considering the similarity between COVID-19 and pneumonia, CXR samples with deep features distributed near category boundaries are easily misclassified by the hyperplanes learned from limited training data. Moreover, most existing approaches for COVID-19 detection focus on the accuracy of prediction and overlook uncertainty estimation, which is particularly important when dealing with noisy datasets. To alleviate these concerns, we propose a novel deep network named RCoNet (k)(s) for robust COVID-19 detection which employs Deformable Mutual Information Maximization (DeIM), Mixed High-order Moment Feature (MHMF), and Multiexpert Uncertainty-aware Learning (MUL). With DeIM, the mutual information (MI) between input data and the corresponding latent representations can be well estimated and maximized to capture compact and disentangled representational characteristics. Meanwhile, MHMF can fully explore the benefits of using high-order statistics and extract discriminative features of complex distributions in medical imaging. Finally, MUL creates multiple parallel dropout networks for each CXR image to evaluate uncertainty and thus prevent performance degradation caused by the noise in the data. The experimental results show that RCoNet (k)(s) achieves the state-of-the-art performance on an open-source COVIDx dataset of 15 134 original CXR images across several metrics. Crucially, our method is shown to be more effective than existing methods with the presence of noise in the data.",2021,10.1109/tnnls.2021.3086570,diagnosis,False
Re-Identification and growth detection of pulmonary nodules without image registration using 3D siamese neural networks,"Lung cancer follow-up is a complex, error prone, and time consuming task for clinical radiologists. Several lung CT scan images taken at different time points of a given patient need to be individually inspected, looking for possible cancerogenous nodules. Radiologists mainly focus their attention in nodule size, density, and growth to assess the existence of malignancy. In this study, we present a novel method based on a 3D siamese neural network, for the re-identification of nodules in a pair of CT scans of the same patient without the need for image registration. The network was integrated into a two-stage automatic pipeline to detect, match, and predict nodule growth given pairs of CT scans. Results on an independent test set reported a nodule detection sensitivity of 94.7%, an accuracy for temporal nodule matching of 88.8%, and a sensitivity of 92.0% with a precision of 88.4% for nodule growth detection.",2021,10.1016/j.media.2020.101823,diagnosis,True
Real-time markerless tumour tracking with patient-specific deep learning using a personalised data generation strategy: proof of concept by phantom study,"OBJECTIVE: For real-time markerless tumour tracking in stereotactic lung radiotherapy, we propose a different approach which uses patient-specific deep learning (DL) using a personalised data generation strategy, avoiding the need for collection of a large patient data set. We validated our strategy with digital phantom simulation and epoxy phantom studies. METHODS: We developed lung tumour tracking for radiotherapy using a convolutional neural network trained for each phantom's lesion by using multiple digitally reconstructed radiographs (DRRs) generated from each phantom's treatment planning four-dimensional CT. We trained tumour-bone differentiation using large numbers of training DRRs generated with various projection geometries to simulate tumour motion. We solved the problem of using DRRs for training and X-ray images for tracking using the training DRRs with random contrast transformation and random noise addition. RESULTS: We defined adequate tracking accuracy as the percentage frames satisfying <1 mm tracking error of the isocentre. In the simulation study, we achieved 100% tracking accuracy in 3 cm spherical and 1.5×2.25×3 cm ovoid masses. In the phantom study, we achieved 100 and 94.7% tracking accuracy in 3 cm and 2 cm spherical masses, respectively. This required 32.5 ms/frame (30.8 fps) real-time processing. CONCLUSIONS: We proved the potential feasibility of a real-time markerless tumour tracking framework for stereotactic lung radiotherapy based on patient-specific DL with personalised data generation with digital phantom and epoxy phantom studies. ADVANCES IN KNOWLEDGE: Using DL with personalised data generation is an efficient strategy for real-time lung tumour tracking.",2020,10.1259/bjr.20190420,diagnosis,False
Real-time tumor tracking using fluoroscopic imaging with deep neural network analysis,"PURPOSE: To improve respiratory gating accuracy and treatment throughput, we developed a fluoroscopic markerless tumor tracking algorithm based on a deep neural network (DNN). METHODS: In the learning stage, target positions were projected onto digitally reconstructed radiography (DRR) images from four-dimensional computed tomography (4DCT). DRR images were cropped into subimages of the target or surrounding regions to build a network that takes input of the image pattern of subimages and produces a target probability map (TPM) for estimating the target position. Using multiple subimages, a DNN was trained to generate a TPM based on the target position projected onto the DRRs. In the tracking stage, the network takes in the subimages cropped from fluoroscopic images at the same position of the subimages on the DRRs and produces TPMs, which are used to estimate target positions. We integrated the lateral correction to modify an estimated target position by using a linear regression model. We tracked five lung and five liver cases, and calculated tracking accuracy (Euclidian distance in 3D space) by subtracting the estimated position from the reference. RESULTS: Tracking accuracy averaged over all patients was 1.64 ± 0.73 mm. Accuracy for liver cases (1.37 ± 0.81 mm) was better than that for lung cases (1.90 ± 0.65 mm). Computation time was <40 ms for a pair of fluoroscopic images. CONCLUSIONS: Our markerless tracking algorithm successfully estimated tumor positions. We believe our results will provide useful information to advance tumor tracking technology.",2019,10.1016/j.ejmp.2019.02.006,diagnosis,False
Recognition of COVID-19 from CT Scans Using Two-Stage Deep-Learning-Based Approach: CNR-IEMN,"Since the appearance of the COVID-19 pandemic (at the end of 2019, Wuhan, China), the recognition of COVID-19 with medical imaging has become an active research topic for the machine learning and computer vision community. This paper is based on the results obtained from the 2021 COVID-19 SPGC challenge, which aims to classify volumetric CT scans into normal, COVID-19, or community-acquired pneumonia (Cap) classes. To this end, we proposed a deep-learning-based approach (CNR-IEMN) that consists of two main stages. In the first stage, we trained four deep learning architectures with a multi-tasks strategy for slice-level classification. In the second stage, we used the previously trained models with an XG-boost classifier to classify the whole CT scan into normal, COVID-19, or Cap classes. Our approach achieved a good result on the validation set, with an overall accuracy of 87.75% and 96.36%, 52.63%, and 95.83% sensitivities for COVID-19, Cap, and normal, respectively. On the other hand, our approach achieved fifth place on the three test datasets of SPGC in the COVID-19 challenge, where our approach achieved the best result for COVID-19 sensitivity. In addition, our approach achieved second place on two of the three testing sets.",2021,10.3390/s21175878,diagnosis,True
Recognition of Peripheral Lung Cancer and Focal Pneumonia on Chest Computed Tomography Images Based on Convolutional Neural Network,"Introduction: Chest computed tomography (CT) is important for the early screening of lung diseases and clinical diagnosis, particularly during the COVID-19 pandemic. We propose a method for classifying peripheral lung cancer and focal pneumonia on chest CT images and undertake 5 window settings to study the effect on the artificial intelligence processing results. Methods: A retrospective collection of CT images from 357 patients with peripheral lung cancer having solitary solid nodule or focal pneumonia with a solitary consolidation was applied. We segmented and aligned the lung parenchyma based on some morphological methods and cropped this region of the lung parenchyma with the minimum 3D bounding box. Using these 3D cropped volumes of all cases, we designed a 3D neural network to classify them into 2 categories. We also compared the classification results of the 3 physicians with different experience levels on the same dataset. Results: We conducted experiments using 5 window settings. After cropping and alignment based on an automatic preprocessing procedure, our neural network achieved an average classification accuracy of 91.596% under a 5-fold cross-validation in the full window, in which the area under the curve (AUC) was 0.946. The classification accuracy and AUC value were 90.48% and 0.957 for the junior physician, 94.96% and 0.989 for the intermediate physician, and 96.92% and 0.980 for the senior physician, respectively. After removing the error prediction, the accuracy improved significantly, reaching 98.79% in the self-defined window2. Conclusion: Using the proposed neural network, in separating peripheral lung cancer and focal pneumonia in chest CT data, we achieved an accuracy competitive to that of a junior physician. Through a data ablation study, the proposed 3D CNN can achieve a slightly higher accuracy compared with senior physicians in the same subset. The self-defined window2 was the best for data training and evaluation.",2022,10.1177/15330338221085375,diagnosis,True
Recurrent attention network for false positive reduction in the detection of pulmonary nodules in thoracic CT scans,"PURPOSE: Multiview two-dimensional (2D) convolutional neural networks (CNNs) and three-dimensional (3D) CNNs have been successfully used for analyzing volumetric data in many state-of-the-art medical imaging applications. We propose an alternative modular framework that analyzes volumetric data with an approach that is analogous to radiologists' interpretation, and apply the framework to reduce false positives that are generated in computer-aided detection (CADe) systems for pulmonary nodules in thoracic computed tomography (CT) scans. METHODS: In our approach, a deep network consisting of 2D CNNs first processes slices individually. The features extracted in this stage are then passed to a recurrent neural network (RNN), thereby modeling consecutive slices as a sequence of temporal data and capturing the contextual information across all three dimensions in the volume of interest. Outputs of the RNN layer are weighed before the final fully connected layer, enabling the network to scale the importance of different slices within a volume of interest in an end-to-end training framework. RESULTS: We validated the proposed architecture on the false positive reduction track of the lung nodule analysis (LUNA) challenge for pulmonary nodule detection in chest CT scans, and obtained competitive results compared to 3D CNNs. Our results show that the proposed approach can encode the 3D information in volumetric data effectively by achieving a sensitivity >0.8 with just 1/8 false positives per scan. CONCLUSIONS: Our experimental results demonstrate the effectiveness of temporal analysis of volumetric images for the application of false positive reduction in chest CT scans and show that state-of-the-art 2D architectures from the literature can be directly applied to analyzing volumetric medical data. As newer and better 2D architectures are being developed at a much faster rate compared to 3D architectures, our approach makes it easy to obtain state-of-the-art performance on volumetric data using new 2D architectures.",2020,10.1002/mp.14076,diagnosis,True
Recurrent feature fusion learning for multi-modality pet-ct tumor segmentation,"BACKGROUND AND OBJECTIVE: [18f]-fluorodeoxyglucose (fdg) positron emission tomography - computed tomography (pet-ct) is now the preferred imaging modality for staging many cancers. Pet images characterize tumoral glucose metabolism while ct depicts the complementary anatomical localization of the tumor. Automatic tumor segmentation is an important step in image analysis in computer aided diagnosis systems. Recently, fully convolutional networks (fcns), with their ability to leverage annotated datasets and extract image feature representations, have become the state-of-the-art in tumor segmentation. There are limited fcn based methods that support multi-modality images and current methods have primarily focused on the fusion of multi-modality image features at various stages, i.e., early-fusion where the multi-modality image features are fused prior to fcn, late-fusion with the resultant features fused and hyper-fusion where multi-modality image features are fused across multiple image feature scales. Early- and late-fusion methods, however, have inherent, limited freedom to fuse complementary multi-modality image features. The hyper-fusion methods learn different image features across different image feature scales that can result in inaccurate segmentations, in particular, in situations where the tumors have heterogeneous textures. METHODS: we propose a recurrent fusion network (rfn), which consists of multiple recurrent fusion phases to progressively fuse the complementary multi-modality image features with intermediary segmentation results derived at individual recurrent fusion phases: (1) the recurrent fusion phases iteratively learn the image features and then refine the subsequent segmentation results; and, (2) the intermediary segmentation results allows our method to focus on learning the multi-modality image features around these intermediary segmentation results, which minimize the risk of inconsistent feature learning. RESULTS: we evaluated our method on two pathologically proven non-small cell lung cancer pet-ct datasets. We compared our method to the commonly used fusion methods (early-fusion, late-fusion and hyper-fusion) and the state-of-the-art pet-ct tumor segmentation methods on various network backbones (resnet, densenet and 3d-unet). Our results show that the rfn provides more accurate segmentation compared to the existing methods and is generalizable to different datasets. CONCLUSIONS: we show that learning through multiple recurrent fusion phases allows the iterative re-use of multi-modality image features that refines tumor segmentation results. We also identify that our rfn produces consistent segmentation results across different network architectures.",2021,10.1016/j.cmpb.2021.106043,diagnosis,True
Reducing False-Positives in Lung Nodules Detection Using Balanced Datasets,"Malignant pulmonary nodules are one of the main manifestations of lung cancer in early CT image screening. Since lung cancer may have no early obvious symptoms, it is important to develop a computer-aided detection (CAD) system to assist doctors to detect the malignant pulmonary nodules in the early stage of lung cancer CT diagnosis. Due to the recent successful applications of deep learning in image processing, more and more researchers have been trying to apply it to the diagnosis of pulmonary nodules. However, due to the ratio of nodules and non-nodules samples used in the training and testing datasets usually being different from the practical ratio of lung cancer, the CAD classification systems may easily produce higher false-positives while using this imbalanced dataset. This work introduces a filtering step to remove the irrelevant images from the dataset, and the results show that the false-positives can be reduced and the accuracy can be above 98%. There are two steps in nodule detection. Firstly, the images with pulmonary nodules are screened from the whole lung CT images of the patients. Secondly, the exact locations of pulmonary nodules will be detected using Faster R-CNN. Final results show that this method can effectively detect the pulmonary nodules in the CT images and hence potentially assist doctors in the early diagnosis of lung cancer.",2021,10.3389/fpubh.2021.671070,diagnosis,True
Registration-based lung mechanical analysis of chronic obstructive pulmonary disease (COPD) using a supervised machine learning framework,"RATIONALE AND OBJECTIVES: This study evaluated the performance of computed tomography (CT)-derived biomechanical based features of lung function and the presence and severity of chronic obstructive pulmonary disease (COPD). It performed well when compared to CT-derived density and textural features of lung function and the presence and severity of COPD. MATERIALS AND METHODS: A total of 162 subjects (Global Initiative for Chronic Obstructive Lung Disease [GOLD] stages 0-4 and nonsmokers) subjects with CT scan performed at total lung capacity or expiration to functional residual capacity were evaluated. CT-derived biomechanical, density, and textural feature sets were compared to forced expiratory volume in 1 second (FEV1)%, FEV1/forced vital capacity, and total St. George's respiratory questionnaire scores. The ability of these feature sets to assess the presence and severity of COPD was also evaluated. Optimal features are selected by linear forward feature selection and the classification is done using k nearest neighbor learning algorithm. RESULTS: The proposed biomechanical features showed good correlations with the pulmonary function tests and health status metrics. In COPD versus non-COPD classification, biomechanical feature set achieved an area under the curve (AUC) of 0.85 performing well in comparison to density (AUC = 0.83) and texture (AUC = 0.89) feature sets. Classifying the subjects into the severity of GOLD stage using biomechanical features (AUC = 0.81) performed better than the density- and texture-based feature sets, AUC = 0.76 and 0.73, respectively. The biomechanical features performed better alone than in combination with the other two feature sets. CONCLUSION: This study shows the effectiveness of CT-derived biomechanical measures in the assessment of airflow obstruction and quality of life in subjects with COPD. CT-derived biomechanical features performed well in assessing the presence and severity of COPD.",2013,10.1016/j.acra.2013.01.019,diagnosis,True
Relational Modeling for Robust and Efficient Pulmonary Lobe Segmentation in CT Scans,"Pulmonary lobe segmentation in computed tomography scans is essential for regional assessment of pulmonary diseases. Recent works based on convolution neural networks have achieved good performance for this task. However, they are still limited in capturing structured relationships due to the nature of convolution. The shape of the pulmonary lobes affect each other and their borders relate to the appearance of other structures, such as vessels, airways, and the pleural wall. We argue that such structural relationships play a critical role in the accurate delineation of pulmonary lobes when the lungs are affected by diseases such as COVID-19 or COPD. In this paper, we propose a relational approach (RTSU-Net) that leverages structured relationships by introducing a novel non-local neural network module. The proposed module learns both visual and geometric relationships among all convolution features to produce self-attention weights. With a limited amount of training data available from COVID-19 subjects, we initially train and validate RTSU-Net on a cohort of 5000 subjects from the COPDGene study (4000 for training and 1000 for evaluation). Using models pre-trained on COPDGene, we apply transfer learning to retrain and evaluate RTSU-Net on 470 COVID-19 suspects (370 for retraining and 100 for evaluation). Experimental results show that RTSU-Net outperforms three baselines and performs robustly on cases with severe lung infection due to COVID-19.",2020,10.1109/tmi.2020.2995108,diagnosis,True
Reproducible Machine Learning Methods for Lung Cancer Detection Using Computed Tomography Images: Algorithm Development and Validation,"BACKGROUND: Chest computed tomography (CT) is crucial for the detection of lung cancer, and many automated CT evaluation methods have been proposed. Due to the divergent software dependencies of the reported approaches, the developed methods are rarely compared or reproduced. OBJECTIVE: The goal of the research was to generate reproducible machine learning modules for lung cancer detection and compare the approaches and performances of the award-winning algorithms developed in the Kaggle Data Science Bowl. METHODS: We obtained the source codes of all award-winning solutions of the Kaggle Data Science Bowl Challenge, where participants developed automated CT evaluation methods to detect lung cancer (training set n=1397, public test set n=198, final test set n=506). The performance of the algorithms was evaluated by the log-loss function, and the Spearman correlation coefficient of the performance in the public and final test sets was computed. RESULTS: Most solutions implemented distinct image preprocessing, segmentation, and classification modules. Variants of U-Net, VGGNet, and residual net were commonly used in nodule segmentation, and transfer learning was used in most of the classification algorithms. Substantial performance variations in the public and final test sets were observed (Spearman correlation coefficient = .39 among the top 10 teams). To ensure the reproducibility of results, we generated a Docker container for each of the top solutions. CONCLUSIONS: We compared the award-winning algorithms for lung cancer detection and generated reproducible Docker images for the top solutions. Although convolutional neural networks achieved decent accuracy, there is plenty of room for improvement regarding model generalizability.",2020,10.2196/16709,diagnosis,True
Res-trans networks for lung nodule classification,"PURPOSE: Lung cancer usually presents as pulmonary nodules on early diagnostic images, and accurately estimating the malignancy of pulmonary nodules is crucial to the prevention and diagnosis of lung cancer. Recently, deep learning algorithms based on convolutional neural networks have shown potential for pulmonary nodules classification. However, the size of the nodules is very diverse, ranging from 3 to 30 mm, which makes classifying them to be a challenging task. In this study, we propose a novel architecture called Res-trans networks to classify nodules in computed tomography (CT) scans. METHODS: We designed local and global blocks to extract features that capture the long-range dependencies between pixels to adapt to the correct classification of lung nodules of different sizes. Specifically, we designed residual blocks with convolutional operations to extract local features and transformer blocks with self-attention to capture global features. Moreover, the Res-trans network has a sequence fusion block that aggregates and extracts the sequence feature information output by the transformer block that improves classification accuracy. RESULTS: Our proposed method is extensively evaluated on the public LIDC-IDRI dataset, which contains 1,018 CT scans. A tenfold cross-validation result shows that our method obtains better performance with AUC = 0.9628 and Accuracy = 0.9292 compared with recently leading methods. CONCLUSION: In this paper, a network that can capture local and global features is proposed to classify nodules in chest CT. Experimental results show that our proposed method has better classification performance and can help radiologists to accurately analyze lung nodules.",2022,10.1007/s11548-022-02576-5,diagnosis,True
Research on Lung Ultrasound Image Classification Based on Compressed Sensing,"Pneumothorax is a common injury in disaster rescue, traffic accidents, and war trauma environments and requires early diagnosis and treatment. The commonly used X-ray, CT, and other diagnostic instruments are not suitable for rescue sites due to their large size, heavy weight, and difficulty in transportation. Ultrasound equipment is easy to carry and suitable for rescue environments. However, ultrasound images are noisy, have low resolution, and are difficult to get started, which affects the efficiency of diagnosis. This paper studies the effect of lung ultrasound image recognition and classification based on compressed sensing and BP neural network. We use ultrasound equipment to build a lung simulation model, collect five typical features of lung ultrasound images in M-mode, and build a dataset. Using compressed sensing theory, we design sparse matrix and observation matrix and perform data compression on the image data in the dataset to obtain observation values. We design a BP neural network, input the observations into the network for training, and compare it with the commonly used VGG16 network. The method proposed in this paper has higher recognition accuracy and significantly fewer parameters than VGG16, so it is suitable for use in embedded devices.",2022,10.1155/2022/1414723,diagnosis,False
Revisiting segmentation of lung tumors from CT images,"Lung cancer is a leading cause of death throughout the world. Because the prompt diagnosis of tumors allows oncologists to discern their nature, type, and mode of treatment, tumor detection and segmentation from CT scan images is a crucial field of study. This paper investigates lung tumor segmentation via a two-dimensional Discrete Wavelet Transform (DWT) on the LOTUS dataset (31,247 training, and 4458 testing samples) and a Deeply Supervised MultiResUNet model. Coupling the DWT, which is used to achieve a more meticulous textural analysis while integrating information from neighboring CT slices, with the deep supervision of the model architecture results in an improved dice coefficient of 0.8472. A key characteristic of our approach is its avoidance of 3D kernels (despite being used for a 3D segmentation task), thereby making it quite lightweight.",2022,10.1016/j.compbiomed.2022.105385,diagnosis,True
Risk prediction of pleural effusion in lung malignancy patients treated with CT-guided percutaneous microwave ablation: a nomogram and artificial neural network model,"OBJECTIVES: To develop an effective nomogram and artificial neural network (ANN) model for predicting pleural effusion after percutaneous microwave ablation (MWA) in lung malignancy (LM) patients. METHODS: LM patients treated with MWA were randomly allocated to either the training cohort or the validation cohort (7:3). The predictors of pleural effusion identified by univariable and multivariable analyses in the training cohort were used to develop a nomogram and ANN model. The C-statistic was used to evaluate the predictive accuracy in both the training and validation cohorts. RESULTS: A total of 496 patients (training cohort: n = 357; validation cohort: n = 139) were enrolled in this study. The predictors selected into the nomogram for pleural effusion included the maximum power (hazard ratio [HR], 1.060; 95% confidence interval [CI], 1.022-1.100, p = 0.002), the number of pleural punctures (HR, 2.280; 95% CI, 1.103-4.722; p = 0.026) and the minimum distance from needle to pleura (HR, 0.840; 95% CI, 0.775-0.899; p < 0.001). The C-statistic showed good predictive performance in both cohorts, with a C-statistic of 0.866 (95% CI, 0.787-0.945) internally and 0.782 (95% CI, 0.644-0.920) externally (training cohort and validation cohort, respectively). The optimal cutoff value for the risk of pleural effusion was 0.16. CONCLUSIONS: Maximum power, number of pleural punctures and minimum distance from needle to pleura were predictors of pleural effusion after MWA in LM patients. The nomogram and ANN model could effectively predict the risk of pleural effusion after MWA. Patients showing a high risk (>0.16) on the nomogram should be monitored for pleural effusion.",2021,10.1080/02656736.2021.1885755,diagnosis,True
Robust feature selection to predict tumor treatment outcome,"OBJECTIVE: Recurrence of cancer after treatment increases the risk of death. The ability to predict the treatment outcome can help to design the treatment planning and can thus be beneficial to the patient. We aim to select predictive features from clinical and PET (positron emission tomography) based features, in order to provide doctors with informative factors so as to anticipate the outcome of the patient treatment. METHODS: In order to overcome the small sample size problem of datasets usually met in the medical domain, we propose a novel wrapper feature selection algorithm, named HFS (hierarchical forward selection), which searches forward in a hierarchical feature subset space. Feature subsets are iteratively evaluated with the prediction performance using SVM (support vector machine). All feature subsets performing better than those at the preceding iteration are retained. Moreover, as SUV (standardized uptake value) based features have been recognized as significant predictive factors for a patient outcome, we propose to incorporate this prior knowledge into the selection procedure to improve its robustness and reduce its computational cost. RESULTS: Two real-world datasets from cancer patients are included in the evaluation. We extract dozens of clinical and PET-based features to characterize the patient's state, including SUV parameters and texture features. We use leave-one-out cross-validation to evaluate the prediction performance, in terms of prediction accuracy and robustness. Using SVM as the classifier, our HFS method produces accuracy values of 100% and 94% on the two datasets, respectively, and robustness values of 89% and 96%. Without accuracy loss, the prior-based version (pHFS) improves the robustness up to 100% and 98% on the two datasets, respectively. CONCLUSIONS: Compared with other feature selection methods, the proposed HFS and pHFS provide the most promising results. For our HFS method, we have empirically shown that the addition of prior knowledge improves the robustness and accelerates the convergence.",2015,10.1016/j.artmed.2015.07.002,diagnosis,True
Robust radiogenomics approach to the identification of EGFR mutations among patients with NSCLC from three different countries using topologically invariant Betti numbers,"OBJECTIVES: To propose a novel robust radiogenomics approach to the identification of epidermal growth factor receptor (EGFR) mutations among patients with non-small cell lung cancer (NSCLC) using Betti numbers (BNs). MATERIALS AND METHODS: Contrast enhanced computed tomography (CT) images of 194 multi-racial NSCLC patients (79 EGFR mutants and 115 wildtypes) were collected from three different countries using 5 manufacturers' scanners with a variety of scanning parameters. Ninety-nine cases obtained from the University of Malaya Medical Centre (UMMC) in Malaysia were used for training and validation procedures. Forty-one cases collected from the Kyushu University Hospital (KUH) in Japan and fifty-four cases obtained from The Cancer Imaging Archive (TCIA) in America were used for a test procedure. Radiomic features were obtained from BN maps, which represent topologically invariant heterogeneous characteristics of lung cancer on CT images, by applying histogram- and texture-based feature computations. A BN-based signature was determined using support vector machine (SVM) models with the best combination of features that maximized a robustness index (RI) which defined a higher total area under receiver operating characteristics curves (AUCs) and lower difference of AUCs between the training and the validation. The SVM model was built using the signature and optimized in a five-fold cross validation. The BN-based model was compared to conventional original image (OI)- and wavelet-decomposition (WD)-based models with respect to the RI between the validation and the test. RESULTS: The BN-based model showed a higher RI of 1.51 compared with the models based on the OI (RI: 1.33) and the WD (RI: 1.29). CONCLUSION: The proposed model showed higher robustness than the conventional models in the identification of EGFR mutations among NSCLC patients. The results suggested the robustness of the BN-based approach against variations in image scanner/scanning parameters.",2021,10.1371/journal.pone.0244354,diagnosis,True
"Role of Hybrid Deep Neural Networks (HDNNs), Computed Tomography, and Chest X-rays for the Detection of COVID-19","COVID-19 syndrome has extensively escalated worldwide with the induction of the year 2020 and has resulted in the illness of millions of people. COVID-19 patients bear an elevated risk once the symptoms deteriorate. Hence, early recognition of diseased patients can facilitate early intervention and avoid disease succession. This article intends to develop a hybrid deep neural networks (HDNNs), using computed tomography (CT) and X-ray imaging, to predict the risk of the onset of disease in patients suffering from COVID-19. To be precise, the subjects were classified into 3 categories namely normal, Pneumonia, and COVID-19. Initially, the CT and chest X-ray images, denoted as 'hybrid images' (with resolution 1080 × 1080) were collected from different sources, including GitHub, COVID-19 radiography database, Kaggle, COVID-19 image data collection, and Actual Med COVID-19 Chest X-ray Dataset, which are open source and publicly available data repositories. The 80% hybrid images were used to train the hybrid deep neural network model and the remaining 20% were used for the testing purpose. The capability and prediction accuracy of the HDNNs were calculated using the confusion matrix. The hybrid deep neural network showed a 99% classification accuracy on the test set data.",2021,10.3390/ijerph18063056,diagnosis,True
SC-Dynamic R-CNN: A Self-Calibrated Dynamic R-CNN Model for Lung Cancer Lesion Detection,"Lung cancer has complex biological characteristics and a high degree of malignancy. It has always been the number one ""killer"" in cancer, threatening human life and health. The diagnosis and early treatment of lung cancer still require improvement and further development. With high morbidity and mortality, there is an urgent need for an accurate diagnosis method. However, the existing computer-aided detection system has a complicated process and low detection accuracy. To solve this problem, this paper proposed a two-stage detection method based on the dynamic region-based convolutional neural network (Dynamic R-CNN). We divide lung cancer into squamous cell carcinoma, adenocarcinoma, and small cell carcinoma. By adding the self-calibrated convolution module into the feature network, we extracted more abundant lung cancer features and proposed a new regression loss function to further improve the detection performance of lung cancer. After experimental verification, the mAP (mean average precision) of the model can reach 88.1% on the lung cancer dataset and it performed particularly well with a high IoU (intersection over union) threshold. This method has a good performance in the detection of lung cancer and can improve the efficiency of doctors' diagnoses. It can avoid false detection and miss detection to a certain extent.",2022,10.1155/2022/9452157,diagnosis,True
SCPM-Net: An anchor-free 3D lung nodule detection network using sphere representation and center points matching,"Automatic and accurate lung nodule detection from 3D Computed Tomography (CT) scans plays a vital role in efficient lung cancer screening. Despite the state-of-the-art performance obtained by recent anchor-based detectors using Convolutional Neural Networks (CNNs) for this task, they require predetermined anchor parameters such as the size, number, and aspect ratio of anchors, and have limited robustness when dealing with lung nodules with a massive variety of sizes. To overcome these problems, we propose a 3D sphere representation-based center-points matching detection network (SCPM-Net) that is anchor-free and automatically predicts the position, radius, and offset of nodules without manual design of nodule/anchor parameters. The SCPM-Net consists of two novel components: sphere representation and center points matching. First, to match the nodule annotation in clinical practice, we replace the commonly used bounding box with our proposed bounding sphere to represent nodules with the centroid, radius, and local offset in 3D space. A compatible sphere-based intersection over-union loss function is introduced to train the lung nodule detection network stably and efficiently. Second, we empower the network anchor-free by designing a positive center-points selection and matching (CPM) process, which naturally discards pre-determined anchor boxes. An online hard example mining and re-focal loss subsequently enable the CPM process to be more robust, resulting in more accurate point assignment and mitigation of class imbalance. In addition, to better capture spatial information and 3D context for the detection, we propose to fuse multi-level spatial coordinate maps with the feature extractor and combine them with 3D squeeze-and-excitation attention modules. Experimental results on the LUNA16 dataset showed that our proposed SCPM-Net framework achieves superior performance compared with existing anchor-based and anchor-free methods for lung nodule detection with the average sensitivity at 7 predefined FPs/scan of 89.2%. Moreover, our sphere representation is verified to achieve higher detection accuracy than the traditional bounding box representation of lung nodules. Code is available at: https://github.com/HiLab-git/SCPM-Net.",2022,10.1016/j.media.2021.102287,diagnosis,True
Screening of COVID-19 based on the extracted radiomics features from chest CT images,"BACKGROUND AND OBJECTIVE: Radiomics has been widely used in quantitative analysis of medical images for disease diagnosis and prognosis assessment. The objective of this study is to test a machine-learning (ML) method based on radiomics features extracted from chest CT images for screening COVID-19 cases. METHODS: The study is carried out on two groups of patients, including 138 patients with confirmed and 140 patients with suspected COVID-19. We focus on distinguishing pneumonia caused by COVID-19 from the suspected cases by segmentation of whole lung volume and extraction of 86 radiomics features. Followed by feature extraction, nine feature-selection procedures are used to identify valuable features. Then, ten ML classifiers are applied to classify and predict COVID-19 cases. Each ML models is trained and tested using a ten-fold cross-validation method. The predictive performance of each ML model is evaluated using the area under the curve (AUC) and accuracy. RESULTS: The range of accuracy and AUC is from 0.32 (recursive feature elimination [RFE]+Multinomial Naive Bayes [MNB] classifier) to 0.984 (RFE+bagging [BAG], RFE+decision tree [DT] classifiers) and 0.27 (mutual information [MI]+MNB classifier) to 0.997 (RFE+k-nearest neighborhood [KNN] classifier), respectively. There is no direct correlation among the number of the selected features, accuracy, and AUC, however, with changes in the number of the selected features, the accuracy and AUC values will change. Feature selection procedure RFE+BAG classifier and RFE+DT classifier achieve the highest prediction accuracy (accuracy: 0.984), followed by MI+Gaussian Naive Bayes (GNB) and logistic regression (LGR)+DT classifiers (accuracy: 0.976). RFE+KNN classifier as a feature selection procedure achieve the highest AUC (AUC: 0.997), followed by RFE+BAG classifier (AUC: 0.991) and RFE+gradient boosting decision tree (GBDT) classifier (AUC: 0.99). CONCLUSION: This study demonstrates that the ML model based on RFE+KNN classifier achieves the highest performance to differentiate patients with a confirmed infection caused by COVID-19 from the suspected cases.",2021,10.3233/xst-200831,diagnosis,True
Segmentation and suppression of pulmonary vessels in low-dose chest CT scans,"PURPOSE: The suppression of pulmonary vessels in chest computed tomography (CT) images can enhance the conspicuity of lung nodules, thereby improving the detection rate of early lung cancer. This study aimed to develop two key techniques in vessel suppression, that is, segmentation and removal of pulmonary vessels while preserving the nodules. METHODS: Pulmonary vessel segmentation and removal methods in CT images were developed. The vessel segmentation method used a framework of two cascaded convolutional neural networks (CNNs). A bi-class segmentation network was utilized in the first step to extract high-intensity structures, including both vessels and nonvascular tissues such as nodules. A tri-class segmentation network was employed in the second step to distinguish the vessels from nonvascular tissues (mainly nodules) and the lung parenchyma. In the vessel removal method, the voxels in the segmented vessels were replaced with randomly selected voxels from the surrounding lung parenchyma. The dataset in this study comprised 50 three-dimensional (3D) low-dose chest CT images. The labels for vessel and nodule segmentation were annotated with a semi automatic approach. The two cascaded networks for pulmonary vessel segmentation were trained with CT images of 40 cases and tested with CT images of ten cases. Pulmonary vessels were removed from the ten testing scans based on the predicted segmentation results. In addition to qualitative evaluation to the effects of segmentation and removal, the segmentation results were quantitatively evaluated using Dice coefficient (DICE), Jaccard index (JAC), and volumetric similarity (VS) and the removal results were evaluated using contrast-to-noise ratio (CNR). RESULTS: In the first step of vessel segmentation, the mean DICE, JAC, and VS for high-intensity tissues, including both vessels and nodules, were 0.943, 0.893, and 0.991, respectively. In the second step, all the nodules were separated from the vessels, and the mean DICE, JAC, and VS for the vessels were 0.941, 0.890, and 0.991, respectively. After vessel removal, the mean CNR for nodules was improved from 4.23 (6.26 dB) to 6.95 (8.42 dB). CONCLUSIONS: Quantitative and qualitative evaluations demonstrated that the proposed method achieved a high accuracy for pulmonary vessel segmentation and a good effect on pulmonary vessel suppression.",2019,10.1002/mp.13648,diagnosis,True
Segmentation of CT Lung Images Using FCM with Active Contour and CNN Classifier,"OBJECTIVE: Lung cancer is one of the unsafe diseases for human which reduces the patient life time. Generally, most of the lung cancers are identified after it has been spread into the lung parts and moreover it is difficult to find the lung cancer at the early stage. It requires radiologist and special doctors to find the tumoral tissue of the lung cancer. For this reason, the recommended work helps to segment the tumoral tissue of CT lung image in an effective way. METHODS: The research work uses hybrid segmentation technique to separate the lung cancer cells to diagnose the lung tumour. It is a technique which combines active contour along with Fuzzy c means to diagnose the tumoral tissue. Further the segmented portion was trained by Convolutional Neural Network (CNN) in order to classify the segmented region as normal or abnormal. RESULTS: The evaluation of the proposed method was done by analyzing the results of test image with the ground truth image. Finally, the results of the implemented technique provided good accuracy, Peak signal to noise ratio (PSNR), Mean Square Error (MSE) value. In future the other techniques can be utilized to improve the details before segmentation. The proposed work provides 96.67 % accuracy. CONCLUSION: Hybrid segmentation technique involves several steps like preprocessing, binarization, thresholding, segmentation and feature extraction using GLCM.",2022,10.31557/apjcp.2022.23.3.905,diagnosis,True
Segmentation of infected region in CT images of COVID-19 patients based on QC-HC U-net,"Since the outbreak of COVID-19 in 2019, the rapid spread of the epidemic has brought huge challenges to medical institutions. If the pathological region in the COVID-19 CT image can be automatically segmented, it will help doctors quickly determine the patient's infection, thereby speeding up the diagnosis process. To be able to automatically segment the infected area, we proposed a new network structure and named QC-HC U-Net. First, we combine residual connection and dense connection to form a new connection method and apply it to the encoder and the decoder. Second, we choose to add Hypercolumns in the decoder section. Compared with the benchmark 3D U-Net, the improved network can effectively avoid vanishing gradient while extracting more features. To improve the situation of insufficient data, resampling and data enhancement methods are selected in this paper to expand the datasets. We used 63 cases of MSD lung tumor data for training and testing, continuously verified to ensure the training effect of this model, and then selected 20 cases of public COVID-19 data for training and testing. Experimental results showed that in the segmentation of COVID-19, the specificity and sensitivity were 85.3% and 83.6%, respectively, and in the segmentation of MSD lung tumors, the specificity and sensitivity were 81.45% and 80.93%, respectively, without any fitting.",2021,10.1038/s41598-021-01502-0,diagnosis,True
Segmentation of pulmonary nodules in computed tomography using a regression neural network approach and its application to the Lung Image Database Consortium and Image Database Resource Initiative dataset,"We present new pulmonary nodule segmentation algorithms for computed tomography (CT). These include a fully-automated (FA) system, a semi-automated (SA) system, and a hybrid system. Like most traditional systems, the new FA system requires only a single user-supplied cue point. On the other hand, the SA system represents a new algorithm class requiring 8 user-supplied control points. This does increase the burden on the user, but we show that the resulting system is highly robust and can handle a variety of challenging cases. The proposed hybrid system starts with the FA system. If improved segmentation results are needed, the SA system is then deployed. The FA segmentation engine has 2 free parameters, and the SA system has 3. These parameters are adaptively determined for each nodule in a search process guided by a regression neural network (RNN). The RNN uses a number of features computed for each candidate segmentation. We train and test our systems using the new Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI) data. To the best of our knowledge, this is one of the first nodule-specific performance benchmarks using the new LIDC-IDRI dataset. We also compare the performance of the proposed methods with several previously reported results on the same data used by those other methods. Our results suggest that the proposed FA system improves upon the state-of-the-art, and the SA system offers a considerable boost over the FA system.",2015,10.1016/j.media.2015.02.002,diagnosis,True
Segmenting lung tumors on longitudinal imaging studies via a patient-specific adaptive convolutional neural network,"PURPOSE: To design a deep learning algorithm that automatically delineates lung tumors seen on weekly magnetic resonance imaging (MRI) scans acquired during radiotherapy and facilitates the analysis of geometric tumor changes. METHODS: This longitudinal imaging study comprised 9 lung cancer patients who had 6-7 weekly T2-weighted MRI scans during radiotherapy. Tumors on all scans were manually contoured as the ground truth. Meanwhile, a patient-specific adaptive convolutional neural network (A-net) was developed to simulate the workflow of adaptive radiotherapy and to utilize past weekly MRI and tumor contours to segment tumors on the current weekly MRI. To augment the training data, each voxel inside the volume of interest was expanded to a 3 × 3 cm patch as the input, whereas the classification of the corresponding patch, background or tumor, was the output. Training was updated weekly to incorporate the latest MRI scan. For comparison, a population-based neural network was implemented, trained, and validated on the leave-one-out scheme. Both algorithms were evaluated by their precision, DICE coefficient, and root mean square surface distance between the manual and computerized segmentations. RESULTS: Training of A-net converged well within 2 h of computations on a computer cluster. A-net segmented the weekly MR with a precision, DICE, and root mean square surface distance of 0.81 ± 0.10, 0.82 ± 0.10, and 2.4 ± 1.4 mm, and outperformed the population-based algorithm with 0.63 ± 0.21, 0.64 ± 0.19, and 4.1 ± 3.0 mm, respectively. CONCLUSION: A-net can be feasibly integrated into the clinical workflow of a longitudinal imaging study and become a valuable tool to facilitate decision- making in adaptive radiotherapy.",2019,10.1016/j.radonc.2018.10.037,diagnosis,True
Self-Ensembling Co-Training Framework for Semi-Supervised COVID-19 CT Segmentation,"The coronavirus disease 2019 (COVID-19) has become a severe worldwide health emergency and is spreading at a rapid rate. Segmentation of COVID lesions from computed tomography (CT) scans is of great importance for supervising disease progression and further clinical treatment. As labeling COVID-19 CT scans is labor-intensive and time-consuming, it is essential to develop a segmentation method based on limited labeled data to conduct this task. In this paper, we propose a self-ensembled co-training framework, which is trained by limited labeled data and large-scale unlabeled data, to automatically extract COVID lesions from CT scans. Specifically, to enrich the diversity of unsupervised information, we build a co-training framework consisting of two collaborative models, in which the two models teach each other during training by using their respective predicted pseudo-labels of unlabeled data. Moreover, to alleviate the adverse impacts of noisy pseudo-labels for each model, we propose a self-ensembling strategy to perform consistency regularization for the up-to-date predictions of unlabeled data, in which the predictions of unlabeled data are gradually ensembled via moving average at the end of every training epoch. We evaluate our framework on a COVID-19 dataset containing 103 CT scans. Experimental results show that our proposed method achieves better performance in the case of only 4 labeled CT scans compared to the state-of-the-art semi-supervised segmentation networks.",2021,10.1109/jbhi.2021.3103646,diagnosis,True
"Self-supervised deep learning model for COVID-19 lung CT image segmentation highlighting putative causal relationship among age, underlying disease and COVID-19","BACKGROUND: Coronavirus disease 2019 (COVID-19) is very contagious. Cases appear faster than the available Polymerase Chain Reaction test kits in many countries. Recently, lung computerized tomography (CT) has been used as an auxiliary COVID-19 testing approach. Automatic analysis of the lung CT images is needed to increase the diagnostic efficiency and release the human participant. Deep learning is successful in automatically solving computer vision problems. Thus, it can be introduced to the automatic and rapid COVID-19 CT diagnosis. Many advanced deep learning-based computer vison techniques were developed to increase the model performance but have not been introduced to medical image analysis. METHODS: In this study, we propose a self-supervised two-stage deep learning model to segment COVID-19 lesions (ground-glass opacity and consolidation) from chest CT images to support rapid COVID-19 diagnosis. The proposed deep learning model integrates several advanced computer vision techniques such as generative adversarial image inpainting, focal loss, and lookahead optimizer. Two real-life datasets were used to evaluate the model's performance compared to the previous related works. To explore the clinical and biological mechanism of the predicted lesion segments, we extract some engineered features from the predicted lung lesions. We evaluate their mediation effects on the relationship of age with COVID-19 severity, as well as the relationship of underlying diseases with COVID-19 severity using statistic mediation analysis. RESULTS: The best overall F1 score is observed in the proposed self-supervised two-stage segmentation model (0.63) compared to the two related baseline models (0.55, 0.49). We also identified several CT image phenotypes that mediate the potential causal relationship between underlying diseases with COVID-19 severity as well as the potential causal relationship between age with COVID-19 severity. CONCLUSIONS: This work contributes a promising COVID-19 lung CT image segmentation model and provides predicted lesion segments with potential clinical interpretability. The model could automatically segment the COVID-19 lesions from the raw CT images with higher accuracy than related works. The features of these lesions are associated with COVID-19 severity through mediating the known causal of the COVID-19 severity (age and underlying diseases).",2021,10.1186/s12967-021-02992-2,diagnosis,True
Semantic segmentation and detection of mediastinal lymph nodes and anatomical structures in CT data for lung cancer staging,"PURPOSE: Accurate lung cancer diagnosis is crucial to select the best course of action for treating the patient. From a simple chest CT volume, it is necessary to identify whether the cancer has spread to nearby lymph nodes or not. It is equally important to know precisely where each malignant lymph node is with respect to the surrounding anatomical structures and the airways. In this paper, we introduce a new data-set containing annotations of fifteen different anatomical structures in the mediastinal area, including lymph nodes of varying sizes. We present a 2D pipeline for semantic segmentation and instance detection of anatomical structures and potentially malignant lymph nodes in the mediastinal area. METHODS: We propose a 2D pipeline combining the strengths of U-Net for pixel-wise segmentation using a loss function dealing with data imbalance and Mask R-CNN providing instance detection and improved pixel-wise segmentation within bounding boxes. A final stage performs pixel-wise labels refinement and 3D instance detection using a tracking approach along the slicing dimension. Detected instances are represented by a 3D pixel-wise mask, bounding volume, and centroid position. RESULTS: We validated our approach following a fivefold cross-validation over our new data-set of fifteen lung cancer patients. For the semantic segmentation task, we reach an average Dice score of 76% over all fifteen anatomical structures. For the lymph node instance detection task, we reach 75% recall for 9 false positives per patient, with an average centroid position estimation error of 3 mm in each dimension. CONCLUSION: Fusing 2D networks' results increases pixel-wise segmentation results while enabling good instance detection. Better leveraging of the 3D information and station mapping for the detected lymph nodes are the next steps.",2019,10.1007/s11548-019-01948-8,diagnosis,True
Semi-supervised adversarial model for benign-malignant lung nodule classification on chest CT,"Classification of benign-malignant lung nodules on chest CT is the most critical step in the early detection of lung cancer and prolongation of patient survival. Despite their success in image classification, deep convolutional neural networks (DCNNs) always require a large number of labeled training data, which are not available for most medical image analysis applications due to the work required in image acquisition and particularly image annotation. In this paper, we propose a semi-supervised adversarial classification (SSAC) model that can be trained by using both labeled and unlabeled data for benign-malignant lung nodule classification. This model consists of an adversarial autoencoder-based unsupervised reconstruction network R, a supervised classification network C, and learnable transition layers that enable the adaption of the image representation ability learned by R to C. The SSAC model has been extended to the multi-view knowledge-based collaborative learning, aiming to employ three SSACs to characterize each nodule's overall appearance, heterogeneity in shape and texture, respectively, and to perform such characterization on nine planar views. The MK-SSAC model has been evaluated on the benchmark LIDC-IDRI dataset and achieves an accuracy of 92.53% and an AUC of 95.81%, which are superior to the performance of other lung nodule classification and semi-supervised learning approaches.",2019,10.1016/j.media.2019.07.004,diagnosis,True
Semi-Supervised Deep Transfer Learning for Benign-Malignant Diagnosis of Pulmonary Nodules in Chest CT Images,"Lung cancer is the leading cause of cancer deaths worldwide. Accurately diagnosing the malignancy of suspected lung nodules is of paramount clinical importance. However, to date, the pathologically-proven lung nodule dataset is largely limited and is highly imbalanced in benign and malignant distributions. In this study, we proposed a Semi-supervised Deep Transfer Learning (SDTL) framework for benign-malignant pulmonary nodule diagnosis. First, we utilize a transfer learning strategy by adopting a pre-trained classification network that is used to differentiate pulmonary nodules from nodule-like tissues. Second, since the size of samples with pathological-proven is small, an iterated feature-matching-based semi-supervised method is proposed to take advantage of a large available dataset with no pathological results. Specifically, a similarity metric function is adopted in the network semantic representation space for gradually including a small subset of samples with no pathological results to iteratively optimize the classification network. In this study, a total of 3,038 pulmonary nodules (from 2,853 subjects) with pathologically-proven benign or malignant labels and 14,735 unlabeled nodules (from 4,391 subjects) were retrospectively collected. Experimental results demonstrate that our proposed SDTL framework achieves superior diagnosis performance, with accuracy = 88.3%, AUC = 91.0% in the main dataset, and accuracy = 74.5%, AUC = 79.5% in the independent testing dataset. Furthermore, ablation study shows that the use of transfer learning provides 2% accuracy improvement, and the use of semi-supervised learning further contributes 2.9% accuracy improvement. Results implicate that our proposed classification network could provide an effective diagnostic tool for suspected lung nodules, and might have a promising application in clinical practice.",2022,10.1109/tmi.2021.3123572,diagnosis,True
Semi-supervised learning for an improved diagnosis of COVID-19 in CT images,"Coronavirus disease 2019 (COVID-19) has been spread out all over the world. Although a real-time reverse-transcription polymerase chain reaction (RT-PCR) test has been used as a primary diagnostic tool for COVID-19, the utility of CT based diagnostic tools have been suggested to improve the diagnostic accuracy and reliability. Herein we propose a semi-supervised deep neural network for an improved detection of COVID-19. The proposed method utilizes CT images in a supervised and unsupervised manner to improve the accuracy and robustness of COVID-19 diagnosis. Both labeled and unlabeled CT images are employed. Labeled CT images are used for supervised leaning. Unlabeled CT images are utilized for unsupervised learning in a way that the feature representations are invariant to perturbations in CT images. To systematically evaluate the proposed method, two COVID-19 CT datasets and three public CT datasets with no COVID-19 CT images are employed. In distinguishing COVID-19 from non-COVID-19 CT images, the proposed method achieves an overall accuracy of 99.83%, sensitivity of 0.9286, specificity of 0.9832, and positive predictive value (PPV) of 0.9192. The results are consistent between the COVID-19 challenge dataset and the public CT datasets. For discriminating between COVID-19 and common pneumonia CT images, the proposed method obtains 97.32% accuracy, 0.9971 sensitivity, 0.9598 specificity, and 0.9326 PPV. Moreover, the comparative experiments with respect to supervised learning and training strategies demonstrate that the proposed method is able to improve the diagnostic accuracy and robustness without exhaustive labeling. The proposed semi-supervised method, exploiting both supervised and unsupervised learning, facilitates an accurate and reliable diagnosis for COVID-19, leading to an improved patient care and management.",2021,10.1371/journal.pone.0249450,diagnosis,True
Semi-Supervised Medical Image Classification With Relation-Driven Self-Ensembling Model,"Training deep neural networks usually requires a large amount of labeled data to obtain good performance. However, in medical image analysis, obtaining high-quality labels for the data is laborious and expensive, as accurately annotating medical images demands expertise knowledge of the clinicians. In this paper, we present a novel relation-driven semi-supervised framework for medical image classification. It is a consistency-based method which exploits the unlabeled data by encouraging the prediction consistency of given input under perturbations, and leverages a self-ensembling model to produce high-quality consistency targets for the unlabeled data. Considering that human diagnosis often refers to previous analogous cases to make reliable decisions, we introduce a novel sample relation consistency (SRC) paradigm to effectively exploit unlabeled data by modeling the relationship information among different samples. Superior to existing consistency-based methods which simply enforce consistency of individual predictions, our framework explicitly enforces the consistency of semantic relation among different samples under perturbations, encouraging the model to explore extra semantic information from unlabeled data. We have conducted extensive experiments to evaluate our method on two public benchmark medical image classification datasets, i.e., skin lesion diagnosis with ISIC 2018 challenge and thorax disease classification with ChestX-ray14. Our method outperforms many state-of-the-art semi-supervised learning methods on both single-label and multi-label image classification scenarios.",2020,10.1109/tmi.2020.2995518,diagnosis,False
Sensitivity analysis of FDG PET tumor voxel cluster radiomics and dosimetry for predicting mid-chemoradiation regional response of locally advanced lung cancer,"We investigated the sensitivity of regional tumor response prediction to variability in voxel clustering techniques, imaging features, and machine learning algorithms in 25 patients with locally advanced non-small cell lung cancer (LA-NSCLC) enrolled on the FLARE-RT clinical trial. Metabolic tumor volumes (MTV) from pre-chemoradiation (PETpre) and mid-chemoradiation fluorodeoxyglucose-positron emission tomography (FDG PET) images (PETmid) were subdivided into K-means or hierarchical voxel clusters by standardized uptake values (SUV) and 3D-positions. MTV cluster separability was evaluated by CH index, and morphologic changes were captured by Dice similarity and centroid Euclidean distance. PETpre conventional features included SUVmean, MTV/MTV cluster size, and mean radiation dose. PETpre radiomics consisted of 41 intensity histogram and 3D texture features (PET Oncology Radiomics Test Suite) extracted from MTV or MTV clusters. Machine learning models (multiple linear regression, support vector regression, logistic regression, support vector machines) of conventional features or radiomic features were constructed to predict PETmid response. Leave-one-out-cross-validated root-mean-squared-error (RMSE) for continuous response regression (ΔSUVmean) and area-under-receiver-operating-characteristic-curve (AUC) for binary response classification were calculated. K-means MTV 2-clusters (MTVhi, MTVlo) achieved maximum CH index separability (Friedman p < 0.001). Between PETpre and PETmid, MTV cluster pairs overlapped (Dice 0.70-0.87) and migrated 0.6-1.1 cm. PETmid ΔSUVmean response prediction was superior in MTV and MTVlo (RMSE = 0.17-0.21) compared to MTVhi (RMSE = 0.42-0.52, Friedman p < 0.001). PETmid ΔSUVmean response class prediction performance trended higher in MTVlo (AUC = 0.83-0.88) compared to MTVhi (AUC = 0.44-0.58, Friedman p = 0.052). Models were more sensitive to MTV/MTV cluster regions (Friedman p = 0.026) than feature sets/algorithms (Wilcoxon signed-rank p = 0.36). Top-ranked radiomic features included GLZSM-LZHGE (large-zone-high-SUV), GTSDM-CP (cluster-prominence), GTSDM-CS (cluster-shade) and NGTDM-CNT (contrast). Top-ranked features were consistent between MTVhi and MTVlo cluster pairs but varied between MTVhi-MTVlo clusters, reflecting distinct regional radiomic phenotypes. Variability in tumor voxel cluster response prediction can inform robust radiomic target definition for risk-adaptive chemoradiation in patients with LA-NSCLC. FLARE-RT trial: NCT02773238.",2020,10.1088/1361-6560/abb0c7,diagnosis,False
Setting up an Easy-to-Use Machine Learning Pipeline for Medical Decision Support: A Case Study for COVID-19 Diagnosis Based on Deep Learning with CT Scans,"Coronavirus disease (COVID-19) constitutes an ongoing global health problem with significant morbidity and mortality. It usually presents characteristic findings on a chest CT scan, which may lead to early detection of the disease. A timely and accurate diagnosis of COVID-19 is the cornerstone for the prompt management of the patients. The aim of the present study was to evaluate the performance of an automated machine learning algorithm in the diagnosis of Covid-19 pneumonia using chest CT scans. Diagnostic performance was assessed by the area under the receiver operating characteristic curve (AUC), sensitivity, and positive predictive value. The method's average precision was 0.932. We suggest that auto-ML platforms help users with limited ML expertise train image recognition models by only uploading the examined dataset and performing some basic settings. Such methods could deliver significant potential benefits for patients in the future by allowing for earlier disease detection and care.",2020,10.3233/shti200481,diagnosis,True
Severity and Consolidation Quantification of COVID-19 From CT Images Using Deep Learning Based on Hybrid Weak Labels,"Early and accurate diagnosis of Coronavirus disease (COVID-19) is essential for patient isolation and contact tracing so that the spread of infection can be limited. Computed tomography (CT) can provide important information in COVID-19, especially for patients with moderate to severe disease as well as those with worsening cardiopulmonary status. As an automatic tool, deep learning methods can be utilized to perform semantic segmentation of affected lung regions, which is important to establish disease severity and prognosis prediction. Both the extent and type of pulmonary opacities help assess disease severity. However, manually pixel-level multi-class labelling is time-consuming, subjective, and non-quantitative. In this article, we proposed a hybrid weak label-based deep learning method that utilize both the manually annotated pulmonary opacities from COVID-19 pneumonia and the patient-level disease-type information available from the clinical report. A UNet was firstly trained with semantic labels to segment the total infected region. It was used to initialize another UNet, which was trained to segment the consolidations with patient-level information using the Expectation-Maximization (EM) algorithm. To demonstrate the performance of the proposed method, multi-institutional CT datasets from Iran, Italy, South Korea, and the United States were utilized. Results show that our proposed method can predict the infected regions as well as the consolidation regions with good correlation to human annotation.",2020,10.1109/jbhi.2020.3030224,diagnosis,True
Severity of Chest Imaging is Correlated with Risk of Acute Neuroimaging Findings among Patients with COVID-19,"BACKGROUND AND PURPOSE: Severe respiratory distress in patients with COVID-19 has been associated with higher rate of neurologic manifestations. Our aim was to investigate whether the severity of chest imaging findings among patients with coronavirus disease 2019 (COVID-19) correlates with the risk of acute neuroimaging findings. MATERIALS AND METHODS: This retrospective study included all patients with COVID-19 who received care at our hospital between March 3, 2020, and May 6, 2020, and underwent chest imaging within 10 days of neuroimaging. Chest radiographs were assessed using a previously validated automated neural network algorithm for COVID-19 (Pulmonary X-ray Severity score). Chest CTs were graded using a Chest CT Severity scoring system based on involvement of each lobe. Associations between chest imaging severity scores and acute neuroimaging findings were assessed using multivariable logistic regression. RESULTS: Twenty-four of 93 patients (26%) included in the study had positive acute neuroimaging findings, including intracranial hemorrhage (n = 7), infarction (n = 7), leukoencephalopathy (n = 6), or a combination of findings (n = 4). The average length of hospitalization, prevalence of intensive care unit admission, and proportion of patients requiring intubation were significantly greater in patients with acute neuroimaging findings than in patients without them (P < .05 for all). Compared with patients without acute neuroimaging findings, patients with acute neuroimaging findings had significantly higher mean Pulmonary X-ray Severity scores (5.0 [SD, 2.9] versus 9.2 [SD, 3.4], P < .001) and mean Chest CT Severity scores (9.0 [SD, 5.1] versus 12.1 [SD, 5.0], P = .041). The pulmonary x-ray severity score was a significant predictor of acute neuroimaging findings in patients with COVID-19. CONCLUSIONS: Patients with COVID-19 and acute neuroimaging findings had more severe findings on chest imaging on both radiographs and CT compared with patients with COVID-19 without acute neuroimaging findings. The severity of findings on chest radiography was a strong predictor of acute neuroimaging findings in patients with COVID-19.",2021,10.3174/ajnr.A7032,diagnosis,True
Shape and margin-aware lung nodule classification in low-dose CT images via soft activation mapping,"A number of studies on lung nodule classification lack clinical/biological interpretations of the features extracted by convolutional neural network (CNN). The methods like class activation mapping (CAM) and gradient-based CAM (Grad-CAM) are tailored for interpreting localization and classification tasks while they ignored fine-grained features. Therefore, CAM and Grad-CAM cannot provide optimal interpretation for lung nodule categorization task in low-dose CT images, in that fine-grained pathological clues like discrete and irregular shape and margins of nodules are capable of enhancing sensitivity and specificity of nodule classification with regards to CNN. In this paper, we first develop a soft activation mapping (SAM) to enable fine-grained lung nodule shape & margin (LNSM) feature analysis with a CNN so that it can access rich discrete features. Secondly, by combining high-level convolutional features with SAM, we further propose a high-level feature enhancement scheme (HESAM) to localize LNSM features. Experiments on the LIDC-IDRI dataset indicate that 1) SAM captures more fine-grained and discrete attention regions than existing methods, 2) HESAM localizes more accurately on LNSM features and obtains the state-of-the-art predictive performance, reducing the false positive rate, and 3) we design and conduct a visually matching experiment which incorporates radiologists study to increase the confidence level of applying our method to clinical diagnosis.",2020,10.1016/j.media.2019.101628,diagnosis,True
Short-term Reproducibility of Pulmonary Nodule and Mass Detection in Chest Radiographs: Comparison among Radiologists and Four Different Computer-Aided Detections with Convolutional Neural Net,"To investigate the reproducibility of computer-aided detection (CAD) for detection of pulmonary nodules and masses for consecutive chest radiographies (CXRs) of the same patient within a short-term period. A total of 944 CXRs (Chest PA) with nodules and masses, recorded between January 2010 and November 2016 at the Asan Medical Center, were obtained. In all, 1092 regions of interest for the nodules and mass were delineated using an in-house software. All CXRs were randomly split into 6:2:2 sets for training, development, and validation. Furthermore, paired follow-up CXRs (n = 121) acquired within one week in the validation set, in which expert thoracic radiologists confirmed no changes, were used to evaluate the reproducibility of CAD by two radiologists (R1 and R2). The reproducibility comparison of four different convolutional neural net algorithms and two chest radiologists (with 13- and 14-years' experience) was conducted. Model performances were evaluated by figure-of-merit (FOM) analysis of the jackknife free-response receiver operating curve and reproducibility rates were evaluated in terms of percent positive agreement (PPA) and Chamberlain's percent positive agreement (CPPA). Reproducibility analysis of the four CADs and R1 and R2 showed variations in the PPA and CPPA. Model performance of YOLO (You Only Look Once) v2 based eDenseYOLO showed a higher FOM (0.89; 0.85-0.93) than RetinaNet (0.89; 0.85-0.93) and atrous spatial pyramid pooling U-Net (0.85; 0.80-0.89). eDenseYOLO showed higher PPAs (97.87%) and CPPAs (95.80%) than Mask R-CNN, RetinaNet, ASSP U-Net, R1, and R2 (PPA: 96.52%, 94.23%, 95.04%, 96.55%, and 94.98%; CPPA: 93.18%, 89.09%, 90.57%, 93.33%, and 90.43%). There were moderate variations in the reproducibility of CAD with different algorithms, which likely indicates that measurement of reproducibility is necessary for evaluating CAD performance in actual clinical environments.",2019,10.1038/s41598-019-55373-7,diagnosis,False
Similarity measurement of lung masses for medical image retrieval using kernel based semisupervised distance metric,"PURPOSE: To develop a new algorithm to measure the similarity between the query lung mass and reference lung mass data set for content-based medical image retrieval (CBMIR). METHODS: A lung mass data set including 746 mass regions of interest (ROIs) was assembled. Among them, 375 ROIs depicted malignant lesions and 371 depicted benign lesions. Each mass ROI is represented by a vector of 26 texture features. A kernel function was employed to map the original data in input space to a feature space. In this space, a semisupervised distance metric was learned, which used differential scatter discriminant criterion to represent the semantic relevance, and the regularization term to represent the visual similarity. The learned distance metric can measure the similarity of the query mass and reference mass data set. The clustering accuracy is used to configure the parameters. The retrieval accuracy and classification accuracy are used as the performance assessment index. RESULTS: After configuring the parameters, a mean clustering accuracy of 0.87 can be achieved. For retrieval accuracy, our algorithm achieves better performance than other state-of-the-art retrieval algorithms when applying a leave-one-out validation method to the testing data set. For classification accuracy, the area under the ROC curve of our algorithm can be achieved as 0.941 ± 0.006. The running times of 346 query images with the proposed algorithm are 5.399 and 6.0 s, respectively. CONCLUSIONS: The study results demonstrated the proposed algorithm outperforms the compared algorithms, when taking the semantic relevant and visual similarity into account in kernel space. The algorithm can be used in a CBMIR system for a query mass to retrieve similarity masses, which can help doctors make better decisions.",2016,10.1118/1.4966030,diagnosis,True
Simulated four-dimensional CT for markerless tumor tracking using a deep learning network with multi-task learning,"INTRODUCTION: Our markerless tumor tracking algorithm requires 4DCT data to train models. 4DCT cannot be used for markerless tracking for respiratory-gated treatment due to inaccuracies and a high radiation dose. We developed a deep neural network (DNN) to generate 4DCT from 3DCT data. METHODS: We used 2420 thoracic 4DCT datasets from 436 patients to train a DNN, designed to export 9 deformation vector fields (each field representing one-ninth of the respiratory cycle) from each CT dataset based on a 3D convolutional autoencoder with shortcut connections using deformable image registration. Then 3DCT data at exhale were transformed using the predicted deformation vector fields to obtain simulated 4DCT data. We compared markerless tracking accuracy between original and simulated 4DCT datasets for 20 patients. Our tracking algorithm used a machine learning approach with patient-specific model parameters. For the training stage, a pair of digitally reconstructed radiography images was generated using 4DCT for each patient. For the prediction stage, the tracking algorithm calculated tumor position using incoming fluoroscopic image data. RESULTS: Diaphragmatic displacement averaged over 40 cases for the original 4DCT were slightly higher (<1.3 mm) than those for the simulated 4DCT. Tracking positional errors (95th percentile of the absolute value of displacement, ""simulated 4DCT"" minus ""original 4DCT"") averaged over the 20 cases were 0.56 mm, 0.65 mm, and 0.96 mm in the X, Y and Z directions, respectively. CONCLUSIONS: We developed a DNN to generate simulated 4DCT data that are useful for markerless tumor tracking when original 4DCT is not available. Using this DNN would accelerate markerless tumor tracking and increase treatment accuracy in thoracoabdominal treatment.",2020,10.1016/j.ejmp.2020.10.023,diagnosis,True
Simultaneous cosegmentation of tumors in PET-CT images using deep fully convolutional networks,"PURPOSE: To investigate the use and efficiency of 3-D deep learning, fully convolutional networks (DFCN) for simultaneous tumor cosegmentation on dual-modality nonsmall cell lung cancer (NSCLC) and positron emission tomography (PET)-computed tomography (CT) images. METHODS: We used DFCN cosegmentation for NSCLC tumors in PET-CT images, considering both the CT and PET information. The proposed DFCN-based cosegmentation method consists of two coupled three-dimensional (3D)-UNets with an encoder-decoder architecture, which can communicate with the other in order to share complementary information between PET and CT. The weighted average sensitivity and positive predictive values denoted as Scores, dice similarity coefficients (DSCs), and the average symmetric surface distances were used to assess the performance of the proposed approach on 60 pairs of PET/CTs. A Simultaneous Truth and Performance Level Estimation Algorithm (STAPLE) of 3 expert physicians' delineations were used as a reference. The proposed DFCN framework was compared to 3 graph-based cosegmentation methods. RESULTS: Strong agreement was observed when using the STAPLE references for the proposed DFCN cosegmentation on the PET-CT images. The average DSCs on CT and PET are 0.861 ± 0.037 and 0.828 ± 0.087, respectively, using DFCN, compared to 0.638 ± 0.165 and 0.643 ± 0.141, respectively, when using the graph-based cosegmentation method. The proposed DFCN cosegmentation using both PET and CT also outperforms the deep learning method using either PET or CT alone. CONCLUSIONS: The proposed DFCN cosegmentation is able to outperform existing graph-based segmentation methods. The proposed DFCN cosegmentation shows promise for further integration with quantitative multimodality imaging tools in clinical trials.",2019,10.1002/mp.13331,diagnosis,True
Single-view 2D CNNs with fully automatic non-nodule categorization for false positive reduction in pulmonary nodule detection,"BACKGROUND AND OBJECTIVE: In pulmonary nodule detection, the first stage, candidate detection, aims to detect suspicious pulmonary nodules. However, detected candidates include many false positives and thus in the following stage, false positive reduction, such false positives are reliably reduced. Note that this task is challenging due to 1) the imbalance between the numbers of nodules and non-nodules and 2) the intra-class diversity of non-nodules. Although techniques using 3D convolutional neural networks (CNNs) have shown promising performance, they suffer from high computational complexity which hinders constructing deep networks. To efficiently address these problems, we propose a novel framework using the ensemble of 2D CNNs using single views, which outperforms existing 3D CNN-based methods. METHODS: Our ensemble of 2D CNNs utilizes single-view 2D patches to improve both computational and memory efficiency compared to previous techniques exploiting 3D CNNs. We first categorize non-nodules on the basis of features encoded by an autoencoder. Then, all 2D CNNs are trained by using the same nodule samples, but with different types of non-nodules. By extending the learning capability, this training scheme resolves difficulties of extracting representative features from non-nodules with large appearance variations. Note that, instead of manual categorization requiring the heavy workload of radiologists, we propose to automatically categorize non-nodules based on the autoencoder and k-means clustering. RESULTS: We performed extensive experiments to validate the effectiveness of our framework based on the database of the lung nodule analysis 2016 challenge. The superiority of our framework is demonstrated through comparing the performance of five frameworks trained with differently constructed training sets. Our proposed framework achieved state-of-the-art performance (0.922 of the competition performance metric score) with low computational demands (789K of parameters and 1024M of floating point operations per second). CONCLUSION: We presented a novel false positive reduction framework, the ensemble of single-view 2D CNNs with fully automatic non-nodule categorization, for pulmonary nodule detection. Unlike previous 3D CNN-based frameworks, we utilized 2D CNNs using 2D single views to improve computational efficiency. Also, our training scheme using categorized non-nodules, extends the learning capability of representative features of different non-nodules. Our framework achieved state-of-the-art performance with low computational complexity.",2018,10.1016/j.cmpb.2018.08.012,diagnosis,True
Six artificial intelligence paradigms for tissue characterisation and classification of non-COVID-19 pneumonia against COVID-19 pneumonia in computed tomography lungs,"BACKGROUND: COVID-19 pandemic has currently no vaccines. Thus, the only feasible solution for prevention relies on the detection of COVID-19-positive cases through quick and accurate testing. Since artificial intelligence (AI) offers the powerful mechanism to automatically extract the tissue features and characterise the disease, we therefore hypothesise that AI-based strategies can provide quick detection and classification, especially for radiological computed tomography (CT) lung scans. METHODOLOGY: Six models, two traditional machine learning (ML)-based (k-NN and RF), two transfer learning (TL)-based (VGG19 and InceptionV3), and the last two were our custom-designed deep learning (DL) models (CNN and iCNN), were developed for classification between COVID pneumonia (CoP) and non-COVID pneumonia (NCoP). K10 cross-validation (90% training: 10% testing) protocol on an Italian cohort of 100 CoP and 30 NCoP patients was used for performance evaluation and bispectrum analysis for CT lung characterisation. RESULTS: Using K10 protocol, our results showed the accuracy in the order of DL > TL > ML, ranging the six accuracies for k-NN, RF, VGG19, IV3, CNN, iCNN as 74.58 ± 2.44%, 96.84 ± 2.6, 94.84 ± 2.85%, 99.53 ± 0.75%, 99.53 ± 1.05%, and 99.69 ± 0.66%, respectively. The corresponding AUCs were 0.74, 0.94, 0.96, 0.99, 0.99, and 0.99 (p-values < 0.0001), respectively. Our Bispectrum-based characterisation system suggested CoP can be separated against NCoP using AI models. COVID risk severity stratification also showed a high correlation of 0.7270 (p < 0.0001) with clinical scores such as ground-glass opacities (GGO), further validating our AI models. CONCLUSIONS: We prove our hypothesis by demonstrating that all the six AI models successfully classified CoP against NCoP due to the strong presence of contrasting features such as ground-glass opacities (GGO), consolidations, and pleural effusion in CoP patients. Further, our online system takes < 2 s for inference.",2021,10.1007/s11548-021-02317-0,diagnosis,True
Small lung nodules detection based on local variance analysis and probabilistic neural network,"BACKGROUND AND OBJECTIVE: In medical examinations doctors use various techniques in order to provide to the patients an accurate analysis of their actual state of health. One of the commonly used methodologies is the x-ray screening. This examination very often help to diagnose some diseases of chest organs. The most frequent cause of wrong diagnosis lie in the radiologist's difficulty in interpreting the presence of lungs carcinoma in chest X-ray. In such circumstances, an automated approach could be highly advantageous as it provides important help in medical diagnosis. METHODS: In this paper we propose a new classification method of the lung carcinomas. This method start with the localization and extraction of the lung nodules by computing, for each pixel of the original image, the local variance obtaining an output image (variance image) with the same size of the original image. In the variance image we find the local maxima and then by using the locations of these maxima in the original image we found the contours of the possible nodules in lung tissues. However after this segmentation stage we find many false nodules. Therefore to discriminate the true ones we use a probabilistic neural network as classifier. RESULTS: The performance of our approach is 92% of correct classifications, while the sensitivity is 95% and the specificity is 89.7%. The misclassification errors are due to the fact that network confuses false nodules with the true ones (6%) and true nodules with the false ones (2%). CONCLUSIONS: Several researchers have proposed automated algorithms to detect and classify pulmonary nodules but these methods fail to detect low-contrast nodules and have a high computational complexity, in contrast our method is relatively simple but at the same time provides good results and can detect low-contrast nodules. Furthermore, in this paper is presented a new algorithm for training the PNN neural networks that allows to obtain PNNs with many fewer neurons compared to the neural networks obtained by using the training algorithms present in the literature. So considerably lowering the computational burden of the trained network and at same time keeping the same performances.",2018,10.1016/j.cmpb.2018.04.025,diagnosis,True
Solitary solid pulmonary nodules: a CT-based deep learning nomogram helps differentiate tuberculosis granulomas from lung adenocarcinomas,"OBJECTIVES: To evaluate the differential diagnostic performance of a computed tomography (CT)-based deep learning nomogram (DLN) in identifying tuberculous granuloma (TBG) and lung adenocarcinoma (LAC) presenting as solitary solid pulmonary nodules (SSPNs). METHODS: Routine CT images of 550 patients with SSPNs were retrospectively obtained from two centers. A convolutional neural network was used to extract deep learning features from all lesions. The training set consisted of data for 218 patients. The least absolute shrinkage and selection operator logistic regression was used to create a deep learning signature (DLS). Clinical factors and CT-based subjective findings were combined in a clinical model. An individualized DLN incorporating DLS, clinical factors, and CT-based subjective findings was constructed to validate the diagnostic ability. The performance of the DLN was assessed by discrimination and calibration using internal (n = 140) and external validation cohorts (n = 192). RESULTS: DLS, gender, age, and lobulated shape were found to be independent predictors and were used to build the DLN. The combination showed better diagnostic accuracy than any single model evaluated using the net reclassification improvement method (p < 0.05). The areas under the curve in the training, internal validation, and external validation cohorts were 0.889 (95% confidence interval [CI], 0.839-0.927), 0.879 (95% CI, 0.813-0.928), and 0.809 (95% CI, 0.746-0.862), respectively. Decision curve analysis and stratification analysis showed that the DLN has potential generalization ability. CONCLUSIONS: The CT-based DLN can preoperatively distinguish between LAC and TBG in patients presenting with SSPNs. KEY POINTS: • The deep learning nomogram was developed to preoperatively differentiate TBG from LAC in patients with SSPNs. • The performance of the deep learning feature was superior to that of the radiomics feature. • The deep learning nomogram achieved superior performance compared to the deep learning signature, the radiomics signature, or the clinical model alone.",2020,10.1007/s00330-020-07024-z,diagnosis,True
SOM-LWL method for identification of COVID-19 on chest X-rays,"The outbreak of coronavirus disease 2019 (COVID-19) has had an immense impact on world health and daily life in many countries. Sturdy observing of the initial site of infection in patients is crucial to gain control in the struggle with COVID-19. The early automated detection of the recent coronavirus disease (COVID-19) will help to limit its dissemination worldwide. Many initial studies have focused on the identification of the genetic material of coronavirus and have a poor detection rate for long-term surgery. The first imaging procedure that played an important role in COVID-19 treatment was the chest X-ray. Radiological imaging is often used as a method that emphasizes the performance of chest X-rays. Recent findings indicate the presence of COVID-19 in patients with irregular findings on chest X-rays. There are many reports on this topic that include machine learning strategies for the identification of COVID-19 using chest X-rays. Other current studies have used non-public datasets and complex artificial intelligence (AI) systems. In our research, we suggested a new COVID-19 identification technique based on the locality-weighted learning and self-organization map (LWL-SOM) strategy for detecting and capturing COVID-19 cases. We first grouped images from chest X-ray datasets based on their similar features in different clusters using the SOM strategy in order to discriminate between the COVID-19 and non-COVID-19 cases. Then, we built our intelligent learning model based on the LWL algorithm to diagnose and detect COVID-19 cases. The proposed SOM-LWL model improved the correlation coefficient performance results between the Covid19, no-finding, and pneumonia cases; pneumonia and no-finding cases; Covid19 and pneumonia cases; and Covid19 and no-finding cases from 0.9613 to 0.9788, 0.6113 to 1 0.8783 to 0.9999, and 0.8894 to 1, respectively. The proposed LWL-SOM had better results for discriminating COVID-19 and non-COVID-19 patients than the current machine learning-based solutions using AI evaluation measures.",2021,10.1371/journal.pone.0247176,diagnosis,False
Somatic Mutations Drive Distinct Imaging Phenotypes in Lung Cancer,"Tumors are characterized by somatic mutations that drive biological processes ultimately reflected in tumor phenotype. With regard to radiographic phenotypes, generally unconnected through present understanding to the presence of specific mutations, artificial intelligence methods can automatically quantify phenotypic characters by using predefined, engineered algorithms or automatic deep-learning methods, a process also known as radiomics. Here we demonstrate how imaging phenotypes can be connected to somatic mutations through an integrated analysis of independent datasets of 763 lung adenocarcinoma patients with somatic mutation testing and engineered CT image analytics. We developed radiomic signatures capable of distinguishing between tumor genotypes in a discovery cohort (n = 353) and verified them in an independent validation cohort (n = 352). All radiomic signatures significantly outperformed conventional radiographic predictors (tumor volume and maximum diameter). We found a radiomic signature related to radiographic heterogeneity that successfully discriminated between EGFR(+) and EGFR(-) cases (AUC = 0.69). Combining this signature with a clinical model of EGFR status (AUC = 0.70) significantly improved prediction accuracy (AUC = 0.75). The highest performing signature was capable of distinguishing between EGFR(+) and KRAS(+) tumors (AUC = 0.80) and, when combined with a clinical model (AUC = 0.81), substantially improved its performance (AUC = 0.86). A KRAS(+)/KRAS(-) radiomic signature also showed significant albeit lower performance (AUC = 0.63) and did not improve the accuracy of a clinical predictor of KRAS status. Our results argue that somatic mutations drive distinct radiographic phenotypes that can be predicted by radiomics. This work has implications for the use of imaging-based biomarkers in the clinic, as applied noninvasively, repeatedly, and at low cost. Cancer Res; 77(14); 3922-30. ©2017 AACR.",2017,10.1158/0008-5472.Can-17-0122,diagnosis,True
Spatial Pyramid Pooling With 3D Convolution Improves Lung Cancer Detection,"Lung cancer is the leading cause of cancer deaths. Low-dose computed tomography (CT)screening has been shown to significantly reduce lung cancer mortality but suffers from a high false positive rate that leads to unnecessary diagnostic procedures. The development of deep learning techniques has the potential to help improve lung cancer screening technology. Here we present the algorithm, DeepScreener, which can predict a patient's cancer status from a volumetric lung CT scan. DeepScreener is based on our model of Spatial Pyramid Pooling, which ranked 16th of 1972 teams (top 1 percent)in the Data Science Bowl 2017 competition (DSB2017), evaluated with the challenge datasets. Here we test the algorithm with an independent set of 1449 low-dose CT scans of the National Lung Screening Trial (NLST)cohort, and we find that DeepScreener has consistent performance of high accuracy. Furthermore, by combining Spatial Pyramid Pooling and 3D Convolution, it achieves an AUC of 0.892, surpassing the previous state-of-the-art algorithms using only 3D convolution. The advancement of deep learning algorithms can potentially help improve lung cancer detection with low-dose CT scans.",2022,10.1109/tcbb.2020.3027744,diagnosis,True
Spectral augmentation for heart chambers segmentation on conventional contrasted and unenhanced CT scans: an in-depth study,"PURPOSE: Recently, machine learning has outperformed established tools for automated segmentation in medical imaging. However, segmentation of cardiac chambers still proves challenging due to the variety of contrast agent injection protocols used in clinical practice, inducing disparities of contrast between cavities. Hence, training a generalist network requires large training datasets representative of these protocols. Furthermore, segmentation on unenhanced CT scans is further hindered by the challenge of obtaining ground truths from these images. Newly available spectral CT scanners allow innovative image reconstructions such as virtual non-contrast (VNC) imaging, mimicking non-contrasted conventional CT studies from a contrasted scan. Recent publications have demonstrated that networks can be trained using VNC to segment contrasted and unenhanced conventional CT scans to reduce annotated data requirements and the need for annotations on unenhanced scans. We propose an extensive evaluation of this statement. METHOD: We undertake multiple trainings of a 3D multi-label heart segmentation network with (HU-VNC) and without (HUonly) VNC as augmentation, using decreasing training dataset sizes (114, 76, 57, 38, 29, 19 patients). At each step, both networks are tested on a multi-vendor, multi-centric dataset of 122 patients, including different protocols: pulmonary embolism (PE), chest-abdomen-pelvis (CAP), heart CT angiography (CTA) and true non-contrast scans (TNC). An in-depth comparison of resulting Dice coefficients and distance metrics is performed for the networks trained on the largest dataset. RESULTS: HU-VNC-trained on 57 patients significantly outperforms HUonly trained on 114 regarding CAP and TNC scans (mean Dice coefficients of 0.881/0.835 and 0.882/0.416, respectively). When trained on the largest dataset, significant improvements in all labels are noted for TNC and CAP scans (mean Dice coefficient of 0.882/0.416 and 0.891/0.835, respectively). CONCLUSION: Adding VNC images as training augmentation allows the network to perform on unenhanced scans and improves segmentations on other imaging protocols, while using a reduced training dataset.",2021,10.1007/s11548-021-02468-0,diagnosis,True
Spiculation Sign Recognition in a Pulmonary Nodule Based on Spiking Neural P Systems,"The spiculation sign is one of the main signs to distinguish benign and malignant pulmonary nodules. In order to effectively extract the image feature of a pulmonary nodule for the spiculation sign distinguishment, a new spiculation sign recognition model is proposed based on the doctors' diagnosis process of pulmonary nodules. A maximum density projection model is established to fuse the local three-dimensional information into the two-dimensional image. The complete boundary of a pulmonary nodule is extracted by the improved Snake model, which can take full advantage of the parallel calculation of the Spike Neural P Systems to build a new neural network structure. In this paper, our experiments show that the proposed algorithm can accurately extract the boundary of a pulmonary nodule and effectively improve the recognition rate of the spiculation sign.",2020,10.1155/2020/6619076,diagnosis,True
SSA-Net: Spatial self-attention network for COVID-19 pneumonia infection segmentation with semi-supervised few-shot learning,"Coronavirus disease (COVID-19) broke out at the end of 2019, and has resulted in an ongoing global pandemic. Segmentation of pneumonia infections from chest computed tomography (CT) scans of COVID-19 patients is significant for accurate diagnosis and quantitative analysis. Deep learning-based methods can be developed for automatic segmentation and offer a great potential to strengthen timely quarantine and medical treatment. Unfortunately, due to the urgent nature of the COVID-19 pandemic, a systematic collection of CT data sets for deep neural network training is quite difficult, especially high-quality annotations of multi-category infections are limited. In addition, it is still a challenge to segment the infected areas from CT slices because of the irregular shapes and fuzzy boundaries. To solve these issues, we propose a novel COVID-19 pneumonia lesion segmentation network, called Spatial Self-Attention network (SSA-Net), to identify infected regions from chest CT images automatically. In our SSA-Net, a self-attention mechanism is utilized to expand the receptive field and enhance the representation learning by distilling useful contextual information from deeper layers without extra training time, and spatial convolution is introduced to strengthen the network and accelerate the training convergence. Furthermore, to alleviate the insufficiency of labeled multi-class data and the long-tailed distribution of training data, we present a semi-supervised few-shot iterative segmentation framework based on re-weighting the loss and selecting prediction values with high confidence, which can accurately classify different kinds of infections with a small number of labeled image data. Experimental results show that SSA-Net outperforms state-of-the-art medical image segmentation networks and provides clinically interpretable saliency maps, which are useful for COVID-19 diagnosis and patient triage. Meanwhile, our semi-supervised iterative segmentation model can improve the learning ability in small and unbalanced training set and can achieve higher performance.",2022,10.1016/j.media.2022.102459,diagnosis,True
StackNet-DenVIS: a multi-layer perceptron stacked ensembling approach for COVID-19 detection using X-ray images,"The highly contagious nature of Coronavirus disease 2019 (Covid-19) resulted in a global pandemic. Due to the relatively slow and taxing nature of conventional testing for Covid-19, a faster method needs to be in place. The current researches have suggested that visible irregularities found in the chest X-ray of Covid-19 positive patients are indicative of the presence of the disease. Hence, Deep Learning and Image Classification techniques can be employed to learn from these irregularities, and classify accordingly with high accuracy. This research presents an approach to create a classifier model named StackNet-DenVIS which is designed to act as a screening process before conducting the existing swab tests. Using a novel approach, which incorporates Transfer Learning and Stacked Generalization, the model aims to lower the False Negative rate of classification compensating for the 30% False Negative rate of the swab tests. A dataset gathered from multiple reliable sources consisting of 9953 Chest X-rays (868 Covid and 9085 Non-Covid) was used. Also, this research demonstrates handling data imbalance using various techniques involving Generative Adversarial Networks and sampling techniques. The accuracy, sensitivity, and specificity obtained on our proposed model were 95.07%, 99.40% and 94.61% respectively. To the best of our knowledge, the combination of accuracy and false negative rate obtained by this paper outperforms the current implementations. We must also highlight that our proposed architecture also considers other types of viral pneumonia. Given the unprecedented sensitivity of our model we are optimistic it contributes to a better Covid-19 detection.",2020,10.1007/s13246-020-00952-6,diagnosis,False
Standardization of imaging features for radiomics analysis,"Radiomics has the potential to provide tumor characteristics with noninvasive and repeatable way. The purpose of this paper is to evaluate the standardization effect of imaging features for radiomics analysis. For this purpose, we prepared two CT databases ; one includes 40 non-small cell lung cancer (NSCLC) patients for whom tumor biopsies was performed before stereotactic body radiation therapy in The University of Tokyo Hospital, and the other includes 29 early-stage NSCLC datasets from the Cancer Imaging Archive. The former was used as the training data, whereas the later was used as the test data in the evaluation of the prediction model. In total, 476 imaging features were extracted from each data. Then, both training and test data were standardized as the min-max normalization, the z-score normalization, and the whitening from the principle component analysis. All of standardization strategies improved the accuracy for the histology prediction. The area under the receiver observed characteristics curve was 0.725, 0.789, and 0.785 in above standardizations, respectively. Radiomics analysis has shown that robust features have a high prognostic power in predicting early-stage NSCLC histology subtypes. The performance was able to be improved by standardizing the data in the feature space. J. Med. Invest. 66 : 35-37, February, 2019.",2019,10.2152/jmi.66.35,diagnosis,True
Statistical modeling can determine what factors are predictive of appropriate follow-up in patients presenting with incidental pulmonary nodules on CT,"PURPOSE: To assess the performance of statistical modeling in predicting follow-up adherence of incidentally detected pulmonary nodules (IPN) on CT, based on patient variables (PV), radiology report related variables (RRRV) and physician-patient communication variables (PPCV). METHODS: 200 patients with IPN on CT were retrospectively identified and randomly selected. PV (age, gender, smoking status, ethnicity), RRRV (nodule size, patient context, whether follow-up recommendations were provided) and PPCV (whether referring physician documented IPN and ordered follow-up on the electronic medical record) were recorded. Primary outcome was whether patients received appropriate follow-up within +/- 1 month of the recommended time frame. Statistical methods included logistic regression and machine learning (K-nearest neighbors and support vector machine). RESULTS: Adherence was low, with or without recommendations provided in the radiology report (23.4 %-27.4 %). Whether the referring physician ordered follow-up was the dominant predictor of adherence in all models. The following variables were statistically significant predictors of whether referring physician ordered follow-up: recommendations provided in the radiology report, smoking status, patient context and nodule size (FDR logworth of respectively 21.18, 11.66, 2.35, 1.63, p < 0.05). Prediction accuracy varied from 72 % (PV) to 93 % (PPCV, all variables). CONCLUSION: PPCV are the most important predictors of adherence. Amongst all variables, patient context, smoking status, nodule size, and whether the radiologist provided follow-up recommendations in the report were all statistically significant predictors of patient follow-up adherence, supporting the utility of statistical modeling for analytics, quality assurance and optimization of outcomes related to IPN.",2020,10.1016/j.ejrad.2020.109062,diagnosis,True
Strategies to develop radiomics and machine learning models for lung cancer stage and histology prediction using small data samples,"Predictive models based on radiomics and machine-learning (ML) need large and annotated datasets for training, often difficult to collect. We designed an operative pipeline for model training to exploit data already available to the scientific community. The aim of this work was to explore the capability of radiomic features in predicting tumor histology and stage in patients with non-small cell lung cancer (NSCLC). We analyzed the radiotherapy planning thoracic CT scans of a proprietary sample of 47 subjects (L-RT) and integrated this dataset with a publicly available set of 130 patients from the MAASTRO NSCLC collection (Lung1). We implemented intra- and inter-sample cross-validation strategies (CV) for evaluating the ML predictive model performances with not so large datasets. We carried out two classification tasks: histology classification (3 classes) and overall stage classification (two classes: stage I and II). In the first task, the best performance was obtained by a Random Forest classifier, once the analysis has been restricted to stage I and II tumors of the Lung1 and L-RT merged dataset (AUC = 0.72 ± 0.11). For the overall stage classification, the best results were obtained when training on Lung1 and testing of L-RT dataset (AUC = 0.72 ± 0.04 for Random Forest and AUC = 0.84 ± 0.03 for linear-kernel Support Vector Machine). According to the classification task to be accomplished and to the heterogeneity of the available dataset(s), different CV strategies have to be explored and compared to make a robust assessment of the potential of a predictive model based on radiomics and ML.",2021,10.1016/j.ejmp.2021.08.015,diagnosis,True
Stratifying the early radiologic trajectory in dyspneic patients with COVID-19 pneumonia,"OBJECTIVE: This study aimed to stratify the early pneumonia trajectory on chest radiographs and compare patient characteristics in dyspneic patients with coronavirus disease 2019 (COVID-19). MATERIALS AND METHODS: We retrospectively included 139 COVID-19 patients with dyspnea (87 men, 62.7±16.3 years) and serial chest radiographs from January to September 2020. Radiographic pneumonia extent was quantified as a percentage using a previously-developed deep learning algorithm. A group-based trajectory model was used to categorize the pneumonia trajectory after symptom onset during hospitalization. Clinical findings, and outcomes were compared, and Cox regression was performed for survival analysis. RESULTS: Radiographic pneumonia trajectories were categorized into four groups. Group 1 (n = 83, 59.7%) had negligible pneumonia, and group 2 (n = 29, 20.9%) had mild pneumonia. Group 3 (n = 13, 9.4%) and group 4 (n = 14, 10.1%) showed similar considerable pneumonia extents at baseline, but group 3 had decreasing pneumonia extent at 1-2 weeks, while group 4 had increasing pneumonia extent. Intensive care unit admission and mortality were significantly more frequent in groups 3 and 4 than in groups 1 and 2 (P < .05). Groups 3 and 4 shared similar clinical and laboratory findings, but thrombocytopenia (<150×103/μL) was exclusively observed in group 4 (P = .016). When compared to groups 1 and 2, group 4 (hazard ratio, 63.3; 95% confidence interval, 7.9-504.9) had a two-fold higher risk for mortality than group 3 (hazard ratio, 31.2; 95% confidence interval, 3.5-280.2), and this elevated risk was maintained after adjusting confounders. CONCLUSION: Monitoring the early radiologic trajectory beyond baseline further prognosticated at-risk COVID-19 patients, who potentially had thrombo-inflammatory responses.",2021,10.1371/journal.pone.0259010,diagnosis,True
Structure Correction for Robust Volume Segmentation in Presence of Tumors,"CNN based lung segmentation models in absence of diverse training dataset fail to segment lung volumes in presence of severe pathologies such as large masses, scars, and tumors. To rectify this problem, we propose a multi-stage algorithm for lung volume segmentation from CT scans. The algorithm uses a 3D CNN in the first stage to obtain a coarse segmentation of the left and right lungs. In the second stage, shape correction is performed on the segmentation mask using a 3D structure correction CNN. A novel data augmentation strategy is adopted to train a 3D CNN which helps in incorporating global shape prior. Finally, the shape corrected segmentation mask is up-sampled and refined using a parallel flood-fill operation. The proposed multi-stage algorithm is robust in the presence of large nodules/tumors and does not require labeled segmentation masks for entire pathological lung volume for training. Through extensive experiments conducted on publicly available datasets such as NSCLC, LUNA, and LOLA11 we demonstrate that the proposed approach improves the recall of large juxtapleural tumor voxels by at least 15% over state-of-the-art models without sacrificing segmentation accuracy in case of normal lungs. The proposed method also meets the requirement of CAD software by performing segmentation within 5 seconds which is significantly faster than present methods.",2021,10.1109/jbhi.2020.3004296,diagnosis,True
Synthetic CT image generation of shape-controlled lung cancer using semi-conditional InfoGAN and its applicability for type classification,"PURPOSE: In recent years, convolutional neural network (CNN), an artificial intelligence technology with superior image recognition, has become increasingly popular and frequently used for classification tasks in medical imaging. However, the amount of labelled data available for classifying medical images is often significantly less than that of natural images, and the handling of rare diseases is often challenging. To overcome these problems, data augmentation has been performed using generative adversarial networks (GANs). However, conventional GAN cannot effectively handle the various shapes of tumours because it randomly generates images. In this study, we introduced semi-conditional InfoGAN, which enables some labels to be added to InfoGAN, for the generation of shape-controlled tumour images. InfoGAN is a derived model of GAN, and it can represent object features in images without any label. METHODS: Chest computed tomography images of 66 patients diagnosed with three histological types of lung cancer (adenocarcinoma, squamous cell carcinoma, and small cell lung cancer) were used for analysis. To investigate the applicability of the generated images, we classified the histological types of lung cancer using a CNN that was pre-trained with the generated images. RESULTS: As a result of the training, InfoGAN was possible to generate images that controlled the diameters of each lesion and the presence or absence of the chest wall. The classification accuracy of the pre-trained CNN was 57.7%, which was higher than that of the CNN trained only with real images (34.2%), thereby suggesting the potential of image generation. CONCLUSION: The applicability of semi-conditional InfoGAN for feature learning and representation in medical images was demonstrated in this study. InfoGAN can perform constant feature learning and generate images with a variety of shapes using a small dataset.",2021,10.1007/s11548-021-02308-1,diagnosis,True
Synthetic pulmonary perfusion images from 4DCT for functional avoidance using deep learning,"Purpose.To develop and evaluate the performance of a deep learning model to generate synthetic pulmonary perfusion images from clinical 4DCT images for patients undergoing radiotherapy for lung cancer.Methods. A clinical data set of 58 pre- and post-radiotherapy(99m)Tc-labeled MAA-SPECT perfusion studies (32 patients) each with contemporaneous 4DCT studies was collected. Using the inhale and exhale phases of the 4DCT, a 3D-residual network was trained to create synthetic perfusion images utilizing the MAA-SPECT as ground truth. The training process was repeated for a 50-imaging study, five-fold validation with twenty model instances trained per fold. The highest performing model instance from each fold was selected for inference upon the eight-study test set. A manual lung segmentation was used to compute correlation metrics constrained to the voxels within the lungs. From the pre-treatment test cases (N = 5), 50th percentile contours of well-perfused lung were generated from both the clinical and synthetic perfusion images and the agreement was quantified.Results. Across the hold-out test set, our deep learning model predicted perfusion with a Spearman correlation coefficient of 0.70 (IQR: 0.61-0.76) and a Pearson correlation coefficient of 0.66 (IQR: 0.49-0.73). The agreement of the functional avoidance contour pairs was Dice of 0.803 (IQR: 0.750-0.810) and average surface distance of 5.92 mm (IQR: 5.68-7.55).Conclusion. We demonstrate that from 4DCT alone, a deep learning model can generate synthetic perfusion images with potential application in functional avoidance treatment planning.",2021,10.1088/1361-6560/ac16ec,diagnosis,True
Target dose conversion modeling from pencil beam (PB) to Monte Carlo (MC) for lung SBRT,"BACKGROUND: A challenge preventing routine clinical implementation of Monte Carlo (MC)-based lung SBRT is the difficulty of reinterpreting historical outcome data calculated with inaccurate dose algorithms, because the target dose was found to decrease to varying degrees when recalculated with MC. The large variability was previously found to be affected by factors such as tumour size, location, and lung density, usually through sub-group comparisons. We hereby conducted a pilot study to systematically and quantitatively analyze these patient factors and explore accurate target dose conversion models, so that large-scale historical outcome data can be correlated with more accurate MC dose without recalculation. METHODS: Twenty-one patients that underwent SBRT for early-stage lung cancer were replanned with 6MV 360° dynamic conformal arcs using pencil-beam (PB) and recalculated with MC. The percent D95 difference (PB-MC) was calculated for the PTV and GTV. Using single linear regression, this difference was correlated with the following quantitative patient indices: maximum tumour diameter (MaxD); PTV and GTV volumes; minimum distance from tumour to soft tissue (dmin); and mean density and standard deviation of the PTV, GTV, PTV margin, lung, and 2 mm, 15 mm, 50 mm shells outside the PTV. Multiple linear regression and artificial neural network (ANN) were employed to model multiple factors and improve dose conversion accuracy. RESULTS: Single linear regression with PTV D95 deficiency identified the strongest correlation on mean-density (location) indices, weaker on lung density, and the weakest on size indices, with the following R(2) values in decreasing orders: shell2mm (0.71), PTV (0.68), PTV margin (0.65), shell15mm (0.62), shell50mm (0.49), lung (0.40), dmin (0.22), GTV (0.19), MaxD (0.17), PTV volume (0.15), and GTV volume (0.08). A multiple linear regression model yielded the significance factor of 3.0E-7 using two independent features: mean density of shell2mm (P = 1.6E-7) and PTV volume (P = 0.006). A 4-feature ANN model slightly improved the modeling accuracy. CONCLUSION: Quantifiable density features were proposed, replacing simple central/peripheral location designation, which showed strong correlations with PB-to-MC target dose conversion magnitude, followed by lung density and target size. Density in the immediate outer and inner areas of the PTV showed the strongest correlations. A multiple linear regression model with one such feature and PTV volume established a high significance factor, improving dose conversion accuracy.",2016,10.1186/s13014-016-0661-3,treatment,True
TBNet: a context-aware graph network for tuberculosis diagnosis,"Tuberculosis (TB) is an infectious bacterial disease. It can affect the human lungs, brain, bones, and kidneys. Pulmonary tuberculosis is the most common. This airborne bacterium can be transmitted with the droplets by coughing and sneezing. So far, the most convenient and effective method for diagnosing TB is through medical imaging. Computed tomography (CT) is the first choice for lung imaging in clinics because the conditions of the lungs can be interpreted from CT images. However, manual screening poses an enormous burden for radiologists, resulting in high inter-observer variances. Hence, developing computer-aided diagnosis systems to implement automatic TB diagnosis is an emergent and significant task for researchers and practitioners. This paper proposed a novel context-aware graph neural network called TBNet to detect TB from chest CT images METHODS: Traditional convolutional neural networks can extract high-level image features to achieve good classification performance on the ImageNet dataset. However, we observed that the spatial relationships between the feature vectors are beneficial for the classification because the feature vector may share some common characteristics with its neighboring feature vectors. To utilize this context information for the classification of chest CT images, we proposed to use a feature graph to generate context-aware features. Finally, a context-aware random vector functional-link net served as the classifier of the TBNet to identify these context-aware features as TB or normal RESULTS: The proposed TBNet produced state-of-the-art classification performance for detecting TB from healthy samples in the experiments CONCLUSIONS: Our TBNet can be an accurate and effective verification tool for manual screening in clinical diagnosis.",2022,10.1016/j.cmpb.2021.106587,diagnosis,True
Teacher-student approach for lung tumor segmentation from mixed-supervised datasets,"PURPOSE: Cancer is among the leading causes of death in the developed world, and lung cancer is the most lethal type. Early detection is crucial for better prognosis, but can be resource intensive to achieve. Automating tasks such as lung tumor localization and segmentation in radiological images can free valuable time for radiologists and other clinical personnel. Convolutional neural networks may be suited for such tasks, but require substantial amounts of labeled data to train. Obtaining labeled data is a challenge, especially in the medical domain. METHODS: This paper investigates the use of a teacher-student design to utilize datasets with different types of supervision to train an automatic model performing pulmonary tumor segmentation on computed tomography images. The framework consists of two models: the student that performs end-to-end automatic tumor segmentation and the teacher that supplies the student additional pseudo-annotated data during training. RESULTS: Using only a small proportion of semantically labeled data and a large number of bounding box annotated data, we achieved competitive performance using a teacher-student design. Models trained on larger amounts of semantic annotations did not perform better than those trained on teacher-annotated data. Our model trained on a small number of semantically labeled data achieved a mean dice similarity coefficient of 71.0 on the MSD Lung dataset. CONCLUSIONS: Our results demonstrate the potential of utilizing teacher-student designs to reduce the annotation load, as less supervised annotation schemes may be performed, without any real degradation in segmentation accuracy.",2022,10.1371/journal.pone.0266147,diagnosis,True
Technical Note: 3D localization of lung tumors on cone beam CT projections via a convolutional recurrent neural network,"PURPOSE: To design a convolutional recurrent neural network (CRNN) that calculates three-dimensional (3D) positions of lung tumors from continuously acquired cone beam computed tomography (CBCT) projections, and facilitates the sorting and reconstruction of 4D-CBCT images. METHOD: Under an IRB-approved clinical lung protocol, kilovoltage (kV) projections of the setup CBCT were collected in free-breathing. Concurrently, an electromagnetic signal-guided system recorded motion traces of three transponders implanted in or near the tumor. Convolutional recurrent neural network was designed to utilize a convolutional neural network (CNN) for extracting relevant features of the kV projections around the tumor, followed by a recurrent neural network for analyzing the temporal patterns of the moving features. Convolutional recurrent neural network was trained on the simultaneously collected kV projections and motion traces, subsequently utilized to calculate motion traces solely based on the continuous feed of kV projections. To enhance performance, CRNN was also facilitated by frequent calibrations (e.g., at 10° gantry rotation intervals) derived from cross-correlation-based registrations between kV projections and templates created from the planning 4DCT. Convolutional recurrent neural network was validated on a leave-one-out strategy using data from 11 lung patients, including 5500 kV images. The root-mean-square error between the CRNN and motion traces was calculated to evaluate the localization accuracy. RESULT: Three-dimensional displacement around the simulation position shown in the Calypso traces was 3.4 ± 1.7 mm. Using motion traces as ground truth, the 3D localization error of CRNN with calibrations was 1.3 ± 1.4 mm. CRNN had a success rate of 86 ± 8% in determining whether the motion was within a 3D displacement window of 2 mm. The latency was 20 ms when CRNN ran on a high-performance computer cluster. CONCLUSIONS: CRNN is able to provide accurate localization of lung tumors with aid from frequent recalibrations using the conventional cross-correlation-based registration approach, and has the potential to remove reliance on the implanted fiducials.",2020,10.1002/mp.14007,treatment,True
Test-retest reproducibility of a deep learning-based automatic detection algorithm for the chest radiograph,"OBJECTIVES: To perform test-retest reproducibility analyses for deep learning-based automatic detection algorithm (DLAD) using two stationary chest radiographs (CRs) with short-term intervals, to analyze influential factors on test-retest variations, and to investigate the robustness of DLAD to simulated post-processing and positional changes. METHODS: This retrospective study included patients with pulmonary nodules resected in 2017. Preoperative CRs without interval changes were used. Test-retest reproducibility was analyzed in terms of median differences of abnormality scores, intraclass correlation coefficients (ICC), and 95% limits of agreement (LoA). Factors associated with test-retest variation were investigated using univariable and multivariable analyses. Shifts in classification between the two CRs were analyzed using pre-determined cutoffs. Radiograph post-processing (blurring and sharpening) and positional changes (translations in x- and y-axes, rotation, and shearing) were simulated and agreement of abnormality scores between the original and simulated CRs was investigated. RESULTS: Our study analyzed 169 patients (median age, 65 years; 91 men). The median difference of abnormality scores was 1-2% and ICC ranged from 0.83 to 0.90. The 95% LoA was approximately ± 30%. Test-retest variation was negatively associated with solid portion size (β, - 0.50; p = 0.008) and good nodule conspicuity (β, - 0.94; p < 0.001). A small fraction (15/169) showed discordant classifications when the high-specificity cutoff (46%) was applied to the model outputs (p = 0.04). DLAD was robust to the simulated positional change (ICC, 0.984, 0.996), but relatively less robust to post-processing (ICC, 0.872, 0.968). CONCLUSIONS: DLAD was robust to the test-retest variation. However, inconspicuous nodules may cause fluctuations of the model output and subsequent misclassifications. KEY POINTS: • The deep learning-based automatic detection algorithm was robust to the test-retest variation of the chest radiographs in general. • The test-retest variation was negatively associated with solid portion size and good nodule conspicuity. • High-specificity cutoff (46%) resulted in discordant classifications of 8.9% (15/169; p = 0.04) between the test-retest radiographs.",2020,10.1007/s00330-019-06589-8,diagnosis,True
Texture Analysis in the Evaluation of COVID-19 Pneumonia in Chest X-Ray Images: A Proof of Concept Study,"BACKGROUND: One of the most challenging aspects related to Covid-19 is to establish the presence of infection in an early phase of the disease. Texture analysis might be an additional tool for the evaluation of Chest X-ray in patients with clinical suspicion of Covid-19 related pneumonia. OBJECTIVE: To evaluate the diagnostic performance of texture analysis and machine learning models for the diagnosis of Covid-19 interstitial pneumonia in Chest X-ray images. METHODS: Chest X-ray images were accessed from a publicly available repository(https://www.kaggle. com/tawsifurrahman/covid19-radiography-database). Lung areas were manually segmented using a polygonal region of interest covering both lung areas, using MaZda, a freely available software for texture analysis. A total of 308 features per ROI was extracted. One hundred-ten Covid-19 Chest X-ray images were selected for the final analysis. RESULTS: Six models, namely NB, GLM, DL, GBT, ANN, and PLS-DA were selected and ensembled. According to Youden's index, the Covid-19 Ensemble Machine Learning Score showing the highest area under the curve (0.971±0.015) was 132.57. Assuming this cut-off the Ensemble model performance was estimated by evaluating both true and false positive/negative, resulting in 91.8% accuracy with 93% sensitivity and 90% specificity. Moving the cut-off value to -100, although the accuracy resulted lower (90.6%), the Ensemble Machine Learning showed 100% sensitivity, with 80% specificity. CONCLUSION: Texture analysis of Chest X-ray images and machine learning algorithms may help in differentiating patients with Covid-19 pneumonia. Despite several limitations, this study can lay the ground for future research works in this field and help to develop more rapid and accurate screening tools for these patients.",2021,10.2174/1573405617999210112195450,diagnosis,False
Texture feature-based machine learning classifier could assist in the diagnosis of COVID-19,"PURPOSE: Differentiating COVID-19 from other acute infectious pneumonias rapidly is challenging at present. This study aims to improve the diagnosis of COVID-19 using computed tomography (CT). METHOD: COVID-19 was confirmed mainly by virus nucleic acid testing and epidemiological history according to WHO interim guidance, while other infectious pneumonias were diagnosed by antigen testing. The texture features were extracted from CT images by two radiologists with 5 years of work experience using modified wavelet transform and matrix computation analyses. The random forest (RF) classifier was applied to identify COVID-19 patients and images. RESULTS: We retrospectively analysed the data of 95 individuals (291 images) with COVID-19 and 96 individuals (279 images) with other acute infectious pneumonias, including 50 individuals (160 images) with influenza A/B. In total, 6 texture features showed a positive association with COVID-19, while 4 features were negatively associated. The mean AUROC, accuracy, sensitivity, and specificity values of the 5-fold test sets were 0.800, 0.722, 0.770, and 0.680 for image classification and 0.858, 0.826, 0.809, and 0.842 for individual classification, respectively. The feature 'Correlation' contributed most both at the image level and individual level, even compared with the clinical factors. In addition, the texture features could discriminate COVID-19 from influenza A/B, with an AUROC of 0.883 for images and 0.957 for individuals. CONCLUSIONS: The developed texture feature-based RF classifier could assist in the diagnosis of COVID-19, which could be a rapid screening tool in the era of pandemic.",2021,10.1016/j.ejrad.2021.109602,diagnosis,True
The clinical classification of patients with COVID-19 pneumonia was predicted by Radiomics using chest CT,"In 2020, the new type of coronal pneumonitis became a pandemic in the world, and has firstly been reported in Wuhan, China. Chest CT is a vital component in the diagnostic algorithm for patients with suspected or confirmed COVID-19 infection. Therefore, it is necessary to conduct automatic and accurate detection of COVID-19 by chest CT.The clinical classification of patients with COVID-19 pneumonia was predicted by Radiomics using chest CT.From the COVID-19 cases in our institution, 136 moderate patients and 83 severe patients were screened, and their clinical and laboratory data on admission were collected for statistical analysis. Initial CT Radiomics were modeled by automatic machine learning, and diagnostic performance was evaluated according to AUC, TPR, TNR, PPV and NPV of the subjects. At the same time, the initial CT main features of the two groups were analyzed semi-quantitatively, and the results were statistically analyzed.There was a statistical difference in age between the moderate group and the severe group. The model cohort showed TPR 96.9%, TNR 99.1%, PPV98.4%, NPV98.2%, and AUC 0.98. The test cohort showed TPR 94.4%, TNR100%, PPV100%, NPV96.2%, and AUC 0.97. There was statistical difference between the two groups with grade 1 score (P = .001), the AUC of grade 1 score, grade 2 score, grade 3 score and CT score were 0.619, 0.519, 0.478 and 0.548, respectively.Radiomics' Auto ML model was built by CT image of initial COVID -19 pneumonia, and it proved to be effectively used to predict the clinical classification of COVID-19 pneumonia. CT features have limited ability to predict the clinical typing of Covid-19 pneumonia.",2021,10.1097/md.0000000000025307,diagnosis,True
The deep learning model combining CT image and clinicopathological information for predicting ALK fusion status and response to ALK-TKI therapy in non-small cell lung cancer patients,"PURPOSE: This study aimed to investigate the deep learning model (DLM) combining computed tomography (CT) images and clinicopathological information for predicting anaplastic lymphoma kinase (ALK) fusion status in non-small cell lung cancer (NSCLC) patients. MATERIALS AND METHODS: Preoperative CT images, clinicopathological information as well as the ALK fusion status from 937 patients in three hospitals were retrospectively collected to train and validate the DLM for the prediction of ALK fusion status in tumors. Another cohort of patients (n = 91) received ALK tyrosine kinase inhibitor (TKI) treatment was also included to evaluate the value of the DLM in predicting the clinical outcomes of the patients. RESULTS: The performances of the DLM trained only by CT images in the primary and validation cohorts were AUC = 0.8046 (95% CI 0.7715-0.8378) and AUC = 0.7754 (95% CI 0.7199-0.8310), respectively, while the DLM trained by both CT images and clinicopathological information exhibited better performance for the prediction of ALK fusion status (AUC = 0.8540, 95% CI 0.8257-0.8823 in the primary cohort, p < 0.001; AUC = 0.8481, 95% CI 0.8036-0.8926 in the validation cohort, p < 0.001). In addition, the deep learning scores of the DLMs showed significant differences between the wild-type and ALK infusion tumors. In the ALK-target therapy cohort (n = 91), the patients predicted as ALK-positive by the DLM showed better performance of progression-free survival than the patients predicted as ALK-negative (16.8 vs. 7.5 months, p = 0.010). CONCLUSION: Our findings showed that the DLM trained by both CT images and clinicopathological information could effectively predict the ALK fusion status and treatment responses of patients. For the small size of the ALK-target therapy cohort, larger data sets would be collected to further validate the performance of the model for predicting the response to ALK-TKI treatment.",2021,10.1007/s00259-020-04986-6,treatment,True
The detection of lung cancer using massive artificial neural network based on soft tissue technique,"BACKGROUND: A proposed computer aided detection (CAD) scheme faces major issues during subtle nodule recognition. However, radiologists have not noticed subtle nodules in beginning stage of lung cancer while a proposed CAD scheme recognizes non subtle nodules using x-ray images. METHOD: Such an issue has been resolved by creating MANN (Massive Artificial Neural Network) based soft tissue technique from the lung segmented x-ray image. A soft tissue image recognizes nodule candidate for feature extortion and classification. X-ray images are downloaded using Japanese society of radiological technology (JSRT) image set. This image set includes 233 images (140 nodule x-ray images and 93 normal x-ray images). A mean size for a nodule is 17.8 mm and it is validated with computed tomography (CT) image. Thirty percent (42/140) abnormal represents subtle nodules and it is split into five stages (tremendously subtle, very subtle, subtle, observable, relatively observable) by radiologists. RESULT: A proposed CAD scheme without soft tissue technique attained 66.42% (93/140) sensitivity and 66.76% accuracy having 2.5 false positives per image. Utilizing soft tissue technique, many nodules superimposed by ribs as well as clavicles have identified (sensitivity is 72.85% (102/140) and accuracy is 72.96% at one false positive rate). CONCLUSION: In particular, a proposed CAD system determine sensitivity and accuracy in support of subtle nodules (sensitivity is 14/42 = 33.33% and accuracy is 33.66%) is statistically higher than CAD (sensitivity is 13/42 = 30.95% and accuracy is 30.97%) scheme without soft tissue technique. A proposed CAD scheme attained tremendously minimum false positive rate and it is a promising technique in support of cancerous recognition due to improved sensitivity and specificity.",2020,10.1186/s12911-020-01220-z,diagnosis,False
The effect of pulmonary vessel suppression on computerized detection of nodules in chest CT scans,"PURPOSE: In chest computed tomography (CT) scans, pulmonary vessel suppression can make pulmonary nodules more evident, and therefore may increase the detectability of early lung cancer. The purpose of this study was to develop a computer-aided detection (CAD) system with a vessel suppression function and to verify the effectiveness of the vessel suppression on the performance of the pulmonary nodule CAD system. METHODS: A CAD system with a vessel suppression function capable of suppressing vessels and detecting nodules was developed. First, a convolutional neural network (CNN)-based pulmonary vessel suppression technique was employed to remove the vessels from lungs while preserving the nodules. Then, a CNN-based pulmonary nodule detector was utilized to sequentially generate nodule candidates and reduce false positives (FPs). The performance levels of CAD systems with and without the vessel suppression function were compared using 888 three-dimensional chest CT scans from the Lung Nodule Analysis 2016 (LUNA16) dataset. The pulmonary nodule detection results were quantitatively evaluated using the average sensitivity at seven predefined FP rates: 0.125, 0.25, 0.5, 1, 2, 4, and 8 FPs per scan. RESULTS: The developed pulmonary nodule CAD system improved the average sensitivity to 0.977 from 0.950 owing to the addition of the vessel suppression function. CONCLUSIONS: The vessel suppression function considerably improved the performance of the CAD system for pulmonary nodule detection. In practice, it would be embedded in CAD systems to assist radiologists in detecting pulmonary nodules in chest CT scans.",2020,10.1002/mp.14401,diagnosis,True
The effects of physics-based data augmentation on the generalizability of deep neural networks: Demonstration on nodule false-positive reduction,"PURPOSE: An important challenge for deep learning models is generalizing to new datasets that may be acquired with acquisition protocols different from the training set. It is not always feasible to expand training data to the range encountered in clinical practice. We introduce a new technique, physics-based data augmentation (PBDA), that can emulate new computed tomography (CT) data acquisition protocols. We demonstrate two forms of PBDA, emulating increases in slice thickness and reductions of dose, on the specific problem of false-positive reduction in the automatic detection of lung nodules. METHODS: We worked with CT images from the lung image database consortium (LIDC) collection. We employed a hybrid ensemble convolutional neural network (CNN), which consists of multiple CNN modules (VGG, DenseNet, ResNet), for a classification task of determining whether an image patch was a suspicious nodule or a false positive. To emulate a reduction in tube current, we injected noise by simulating forward projection, noise addition, and backprojection corresponding to 1.5 mAs (a ""chest x-ray"" dose). To simulate thick slice CT scans from thin slice CT scans, we grouped and averaged spatially contiguous CT within thin slice data. The neural network was trained with 10% of the LIDC dataset that was selected to have either the highest tube current or the thinnest slices. The network was tested on the remaining data. We compared PBDA to a baseline with standard geometric augmentations (such as shifts and rotations) and Gaussian noise addition. RESULTS: PBDA improved the performance of the networks when generalizing to the test dataset in a limited number of cases. We found that the best performance was obtained by applying augmentation at very low doses (1.5 mAs), about an order of magnitude less than most screening protocols. In the baseline augmentation, a comparable level of Gaussian noise was injected. For dose reduction PBDA, the average sensitivity of 0.931 for the hybrid ensemble network was not statistically different from the average sensitivity of 0.935 without PBDA. Similarly for slice thickness PBDA, the average sensitivity of 0.900 when augmenting with doubled simulated slice thicknesses was not statistically different from the average sensitivity of 0.895 without PBDA. While there were cases detailed in this paper in which we observed improvements, the overall picture was one that suggests PBDA may not be an effective data enrichment tool. CONCLUSIONS: PBDA is a newly proposed strategy for mitigating the performance loss of neural networks related to the variation of acquisition protocol between the training dataset and the data that is encountered in deployment or testing. We found that PBDA does not provide robust improvements with the four neural networks (three modules and the ensemble) tested and for the specific task of false-positive reduction in nodule detection.",2019,10.1002/mp.13755,diagnosis,True
The human-AI scoring system: A new method for CT-based assessment of COVID-19 severity,"BACKGROUND: Chest computed tomography (CT) plays an important role in the diagnosis and assessment of coronavirus disease 2019 (COVID-19). OBJECTIVE: To evaluate the value of an artificial intelligence (AI) scoring system for radiologically assessing the severity of COVID-19. MATERIALS AND METHODS: Chest CT images of 81 patients (61 of normal type and 20 of severe type) with confirmed COVID-19 were used. The test data were anonymized. The scores achieved by four methods (junior radiologists; AI scoring system; human-AI segmentation system; human-AI scoring system) were compared with that by two experienced radiologists (reference score). The mean absolute errors (MAEs) between the four methods and experienced radiologists were calculated separately. The Wilcoxon test is used to predict the significance of the severity of COVID-19. Then use Spearman correlation analysis ROC analysis was used to evaluate the performance of different scores. RESULTS: The AI score had a relatively low MAE (1.67-2.21). Score of human-AI scoring system had the lowest MAE (1.67), a diagnostic value almost equal to reference score (r= 0.97), and a strongest correlation with clinical severity (r= 0.59, p< 0.001). The AUCs of reference score, score of junior radiologists, AI score, score of human-AI segmentation system, and score of human-AI scoring system were 0.874, 0.841, 0.852, 0.857 and 0.865, respectively. CONCLUSION: The human-AI scoring system can help radiologists to improve the accuracy of COVID-19 severity assessment.",2022,10.3233/thc-213199,diagnosis,True
The importance of standardisation - COVID-19 CT & Radiograph Image Data Stock for deep learning purpose,"With the number of affected individuals still growing world-wide, the research on COVID-19 is continuously expanding. The deep learning community concentrates their efforts on exploring if neural networks can potentially support the diagnosis using CT and radiograph images of patients' lungs. The two most popular publicly available datasets for COVID-19 classification are COVID-CT and COVID-19 Image Data Collection. In this work, we propose a new dataset which we call COVID-19 CT & Radiograph Image Data Stock. It contains both CT and radiograph samples of COVID-19 lung findings and combines them with additional data to ensure a sufficient number of diverse COVID-19-negative samples. Moreover, it is supplemented with a carefully defined split. The aim of COVID-19 CT & Radiograph Image Data Stock is to create a public pool of CT and radiograph images of lungs to increase the efficiency of distinguishing COVID-19 disease from other types of pneumonia and from healthy chest. We hope that the creation of this dataset would allow standardisation of the approach taken for training deep neural networks for COVID-19 classification and eventually for building more reliable models.",2020,10.1016/j.compbiomed.2020.104092,diagnosis,True
The incremental value of computed tomography of COVID-19 pneumonia in predicting ICU admission,"Triage is crucial for patient's management and estimation of the required intensive care unit (ICU) beds is fundamental for health systems during the COVID-19 pandemic. We assessed whether chest computed tomography (CT) of COVID-19 pneumonia has an incremental role in predicting patient's admission to ICU. We performed volumetric and texture analysis of the areas of the affected lung in CT of 115 outpatients with COVID-19 infection presenting to the emergency room with dyspnea and unresponsive hypoxyemia. Admission blood laboratory including lymphocyte count, serum lactate dehydrogenase, D-dimer and C-reactive protein and the ratio between the arterial partial pressure of oxygen and inspired oxygen were collected. By calculating the areas under the receiver-operating characteristic curves (AUC), we compared the performance of blood laboratory-arterial gas analyses features alone and combined with the CT features in two hybrid models (Hybrid radiological and Hybrid radiomics)for predicting ICU admission. Following a machine learning approach, 63 patients were allocated to the training and 52 to the validation set. Twenty-nine (25%) of patients were admitted to ICU. The Hybrid radiological model comprising the lung %consolidation performed significantly (p = 0.04) better in predicting ICU admission in the validation (AUC = 0.82; 95% confidence interval 0.73-0.97) set than the blood laboratory-arterial gas analyses features alone (AUC = 0.71; 95% confidence interval 0.56-0.86). A risk calculator for ICU admission was derived and is available at: https://github.com/cgplab/covidapp . The volume of the consolidated lung in CT of patients with COVID-19 pneumonia has a mild but significant incremental value in predicting ICU admission.",2021,10.1038/s41598-021-95114-3,diagnosis,True
The Influence of a Coherent Annotation and Synthetic Addition of Lung Nodules for Lung Segmentation in CT Scans,"Lung cancer is a highly prevalent pathology and a leading cause of cancer-related deaths. Most patients are diagnosed when the disease has manifested itself, which usually is a sign of lung cancer in an advanced stage and, as a consequence, the 5-year survival rates are low. To increase the chances of survival, improving the cancer early detection capacity is crucial, for which computed tomography (CT) scans represent a key role. The manual evaluation of the CTs is a time-consuming task and computer-aided diagnosis (CAD) systems can help relieve that burden. The segmentation of the lung is one of the first steps in these systems, yet it is very challenging given the heterogeneity of lung diseases usually present and associated with cancer development. In our previous work, a segmentation model based on a ResNet34 and U-Net combination was developed on a cross-cohort dataset that yielded good segmentation masks for multiple pathological conditions but misclassified some of the lung nodules. The multiple datasets used for the model development were originated from different annotation protocols, which generated inconsistencies for the learning process, and the annotations are usually not adequate for lung cancer studies since they did not comprise lung nodules. In addition, the initial datasets used for training presented a reduced number of nodules, which was showed not to be enough to allow the segmentation model to learn to include them as a lung part. In this work, an objective protocol for the lung mask's segmentation was defined and the previous annotations were carefully reviewed and corrected to create consistent and adequate ground-truth masks for the development of the segmentation model. Data augmentation with domain knowledge was used to create lung nodules in the cases used to train the model. The model developed achieved a Dice similarity coefficient (DSC) above 0.9350 for all test datasets and it showed an ability to cope, not only with a variety of lung patterns, but also with the presence of lung nodules as well. This study shows the importance of using consistent annotations for the supervised learning process, which is a very time-consuming task, but that has great importance to healthcare applications. Due to the lack of massive datasets in the medical field, which consequently brings a lack of wide representativity, data augmentation with domain knowledge could represent a promising help to overcome this limitation for learning models development.",2022,10.3390/s22093443,diagnosis,True
The Invasiveness Classification of Ground-Glass Nodules Using 3D Attention Network and HRCT,"The early stage lung cancer often appears as ground-glass nodules (GGNs). The diagnosis of GGN as preinvasive lesion (PIL) or invasive adenocarcinoma (IA) is very important for further treatment planning. This paper proposes an automatic GGNs' invasiveness classification algorithm for the adenocarcinoma. 1431 clinical cases and a total of 1624 GGNs (3-30 mm) were collected from Shanghai Cancer Center for the study. The data is in high-resolution computed tomography (HRCT) format. Firstly, the automatic GGN detector which is composed by a 3D U-Net and a 3D multi-receptive field (multi-RF) network detects the location of GGNs. Then, a deep 3D convolutional neural network (3D-CNN) called Attention-v1 is used to identify the GGNs' invasiveness. The attention mechanism was introduced to the 3D-CNN. This paper conducted a contract experiment to compare the performance of Attention-v1, ResNet, and random forest algorithm. ResNet is one of the most advanced convolutional neural network structures. The competition performance metrics (CPM) of automatic GGN detector reached 0.896. The accuracy, sensitivity, specificity, and area under curve (AUC) value of Attention-v1 structure are 85.2%, 83.7%, 86.3%, and 92.6%. The algorithm proposed in this paper outperforms ResNet and random forest in sensitivity, accuracy, and AUC value. The deep 3D-CNN's classification result is better than traditional machine learning method. Attention mechanism improves 3D-CNN's performance compared with the residual block. The automatic GGN detector with the addition of Attention-v1 can be used to construct the GGN invasiveness classification algorithm to help the patients and doctors in treatment.",2020,10.1007/s10278-020-00355-9,diagnosis,True
The method and efficacy of support vector machine classifiers based on texture features and multi-resolution histogram from (18)F-FDG PET-CT images for the evaluation of mediastinal lymph nodes in patients with lung cancer,"OBJECTIVES: In clinical practice, image analysis is dependent on simply visual perception and the diagnostic efficacy of this analysis pattern is limited for mediastinal lymph nodes in patients with lung cancer. In order to improve diagnostic efficacy, we developed a new computer-based algorithm and tested its diagnostic efficacy. METHODS: 132 consecutive patients with lung cancer underwent (18)F-FDG PET/CT examination before treatment. After all data were imported into the database of an on-line medical image analysis platform, the diagnostic efficacy of visual analysis was first evaluated without knowing pathological results, and the maximum short diameter and maximum standardized uptake value (SUVmax) were measured. Then lymph nodes were segmented manually. Three classifiers based on support vector machine (SVM) were constructed from CT, PET, and combined PET-CT images, respectively. The diagnostic efficacy of SVM classifiers was obtained and evaluated. RESULTS: According to ROC curves, the areas under curves for maximum short diameter and SUVmax were 0.684 and 0.652, respectively. The areas under the ROC curve for SVM1, SVM2, and SVM3 were 0.689, 0.579, and 0.685, respectively. CONCLUSION: The algorithm based on SVM was potential in the diagnosis of mediastinal lymph nodes.",2015,10.1016/j.ejrad.2014.11.006,diagnosis,True
The predictive power of artificial intelligence on mediastinal lymphnode metastasis,"OBJECTIVE: The aim of this study was to create the preoperative predictive model on mediastinal lymph-node metastasis based on artificial intelligence in surgically resected lung adenocarcinoma. METHODS: We enrolled 301 surgical resections of patients with clinical stage N0-1 lung adenocarcinoma, who received positron emission tomography preoperatively between 2015 and 2019. We randomly assigned the patients into two groups: the training (n = 201) and validation groups (n = 100). The training group was used to obtain basic data for learning by artificial intelligence, whereas the validation group was used to verify the constructed algorithm. We used an automatic machine learning platform, to create artificial intelligence model. For comparison, multivariate analysis was performed in the training group, whereas for calculating and verifying the prediction accuracy rate, significant predicting factors were applied to the validation group. RESULTS: Of the 301 patients, 41 patients were diagnosed as mediastinal lymph node metastasis. In multivariate analysis, the maximum standardized uptake value was an individual predictive factor. The accuracy rate of artificial intelligence model was 84%, and the specificity was 98% which were higher than those of the maximum standardized uptake value (61% and 57%). However, in terms of sensitivity, artificial intelligence model remarked low at 12%. CONCLUSIONS: An artificial intelligence-based diagnostic algorithm showed remarkable specificity compared with the maximum standardized uptake value. Although this model is not ready to practical use and the result was preliminary because of poor sensitivity, artificial intelligence could be able to complement the shortcomings of existing diagnostic modalities.",2021,10.1007/s11748-021-01671-9,diagnosis,True
The Predictive Role of Artificial Intelligence-Based Chest CT Quantification in Patients with COVID-19 Pneumonia,"We sought to analyze the prognostic value of laboratory and clinical data, and an artificial intelligence (AI)-based algorithm for Coronavirus disease 2019 (COVID-19) severity scoring, on CT-scans of patients hospitalized with COVID-19. Moreover, we aimed to determine personalized probabilities of clinical deterioration. Data of symptomatic patients with COVID-19 who underwent chest-CT-examination at the time of hospital admission between April and November 2020 were analyzed. COVID-19 severity score was automatically quantified for each pulmonary lobe as the percentage of affected lung parenchyma with the AI-based algorithm. Clinical deterioration was defined as a composite of admission to the intensive care unit, need for invasive mechanical ventilation, use of vasopressors or in-hospital mortality. In total 326 consecutive patients were included in the analysis (mean age 66.7 ± 15.3 years, 52.1% male) of whom 85 (26.1%) experienced clinical deterioration. In the multivariable regression analysis prior myocardial infarction (OR = 2.81, 95% CI = 1.12-7.04, p = 0.027), immunodeficiency (OR = 2.08, 95% CI = 1.02-4.25, p = 0.043), C-reactive protein (OR = 1.73, 95% CI = 1.32-2.33, p < 0.001) and AI-based COVID-19 severity score (OR = 1.08; 95% CI = 1.02-1.15, p = 0.013) appeared to be independent predictors of clinical deterioration. Personalized probability values were determined. AI-based COVID-19 severity score assessed at hospital admission can provide additional information about the prognosis of COVID-19, possibly serving as a useful tool for individualized risk-stratification.",2021,10.3390/tomography7040058,prognosis,True
The usage of deep neural network improves distinguishing COVID-19 from other suspected viral pneumonia by clinicians on chest CT: a real-world study,"OBJECTIVES: Based on the current clinical routine, we aimed to develop a novel deep learning model to distinguish coronavirus disease 2019 (COVID-19) pneumonia from other types of pneumonia and validate it with a real-world dataset (RWD). METHODS: A total of 563 chest CT scans of 380 patients (227/380 were diagnosed with COVID-19 pneumonia) from 5 hospitals were collected to train our deep learning (DL) model. Lung regions were extracted by U-net, then transformed and fed to pre-trained ResNet-50-based IDANNet (Identification and Analysis of New covid-19 Net) to produce a diagnostic probability. Fivefold cross-validation was employed to validate the application of our model. Another 318 scans of 316 patients (243/316 were diagnosed with COVID-19 pneumonia) from 2 other hospitals were enrolled prospectively as the RWDs to testify our DL model's performance and compared it with that from 3 experienced radiologists. RESULTS: A three-dimensional DL model was successfully established. The diagnostic threshold to differentiate COVID-19 and non-COVID-19 pneumonia was 0.685 with an AUC of 0.906 (95% CI: 0.886-0.913) in the internal validation group. In the RWD cohort, our model achieved an AUC of 0.868 (95% CI: 0.851-0.876) with the sensitivity of 0.811 and the specificity of 0.822, non-inferior to the performance of 3 experienced radiologists, suggesting promising clinical practical usage. CONCLUSIONS: The established DL model was able to achieve accurate identification of COVID-19 pneumonia from other suspected ones in the real-world situation, which could become a reliable tool in clinical routine. KEY POINTS: • In an internal validation set, our DL model achieved the best performance to differentiate COVID-19 from non-COVID-19 pneumonia with a sensitivity of 0.836, a specificity of 0.800, and an AUC of 0.906 (95% CI: 0.886-0.913) when the threshold was set at 0.685. • In the prospective RWD cohort, our DL diagnostic model achieved a sensitivity of 0.811, a specificity of 0.822, and AUC of 0.868 (95% CI: 0.851-0.876), non-inferior to the performance of 3 experienced radiologists. • The attention heatmaps were fully generated by the model without additional manual annotation and the attention regions were highly aligned with the ROIs acquired by human radiologists for diagnosis.",2021,10.1007/s00330-020-07553-7,diagnosis,True
"The utility of a convolutional neural network (CNN) model score for cancer risk in indeterminate small solid pulmonary nodules, compared to clinical practice according to British Thoracic Society guidelines","PURPOSE: To determine how implementation of an artificial intelligence nodule algorithm, the Lung Cancer Prediction Convolutional Neural Network (LCP-CNN), at the point of incidental nodule detection would have influenced further investigation and management using a series of threshold scores at both the benign and malignant end of the spectrum. METHOD: An observational retrospective study was performed in the assessment of nodules between 5-15 mm (158 benign, 32 malignant) detected on CT scans, which were performed as part of routine practice. The LCP-CNN was applied to the baseline CT scan producing a percentage score, and subsequent imaging and management determined for each threshold group. We hypothesized that the 5% low risk threshold group requires only one follow-up, the 0.56% very low risk threshold group requires no follow-up and the 80% high risk threshold group warrants expedited intervention. RESULTS: The 158 benign nodules had an LCP-CNN score between 0.1 and 70.8%, median 5.5% (IQR 1.4-18.0), whilst the 32 cancer nodules had an LCP-CNN score between 10.1 and 98.7%, median 59.0% (IQR 37.1-83.9). 24/61 CT scans in the 0.56-5% group (n = 37) and 21/21 CT scans <0.56% group (n = 13) could be obviated resulting in an overall reduction of 18.6% (45/242) CT scans in the benign cohort. In the 80% group (n = 10), expedited intervention of malignant nodules could result in a 3.6-month reduction in time delay in 5 cancer patients. CONCLUSION: We show the potential of artificial intelligence to reduce the need for follow-up scans and intervention in low-scoring benign nodules, whilst potentially accelerating the investigation and treatment of high-scoring cancer nodules.",2021,10.1016/j.ejrad.2021.109553,prognosis,True
"The Value of Artificial Intelligence Film Reading System Based on Deep Learning in the Diagnosis of Non-Small-Cell Lung Cancer and the Significance of Efficacy Monitoring: A Retrospective, Clinical, Nonrandomized, Controlled Study","OBJECTIVE: To explore the value of artificial intelligence (AI) film reading system based on deep learning in the diagnosis of non-small-cell lung cancer (NSCLC) and the significance of curative effect monitoring. METHODS: We retrospectively selected 104 suspected NSCLC cases from the self-built chest CT pulmonary nodule database in our hospital, and all of them were confirmed by pathological examination. The lung CT images of the selected patients were introduced into the AI reading system of pulmonary nodules, and the recording software automatically identified the nodules, and the results were compared with the results of the original image report. The nodules detected by the AI software and film readers were evaluated by two chest experts and recorded their size and characteristics. Comparison of calculation sensitivity, false positive rate evaluation of the NSCLC software, and physician's efficiency of nodule detection whether there was a significant difference between the two groups. RESULTS: The sensitivity, specificity, accuracy, positive predictive rate, and false positive rate of NSCLC diagnosed by radiologists were 72.94% (62/85), 92.06% (58/63), 81.08% (62+58/148), 92.53% (62/67), and 7.93% (5/63), respectively. The sensitivity, specificity, accuracy, positive prediction rate, and false positive rate of AI film reading system in the diagnosis of NSCLC were 94.12% (80/85), 77.77% (49/63), 87.161% (80 + 49/148), 85.11% (80/94), and 22.22% (14/63), respectively. Compared with radiologists, the sensitivity and false positive rate of artificial intelligence film reading system in the diagnosis of NSCLC were higher (P < 0.05). The sensitivity, specificity, accuracy, positive prediction rate, and negative prediction rate of artificial intelligence film reading system in evaluating the efficacy of patients with NSCLC were 87.50% (63/72), 69.23% (9/13), 84.70% (63 + 9)/85, 94.02% (63/67), and 50% (9/18), respectively. CONCLUSION: The AI film reading system based on deep learning has higher sensitivity for the diagnosis of NSCLC than radiologists and can be used as an auxiliary detection tool for doctors to screen for NSCLC, but its false positive rate is relatively high. Attention should be paid to identification. Meanwhile, the AI film reading system based on deep learning also has a certain guiding significance for the diagnosis and treatment monitoring of NSCLC.",2022,10.1155/2022/2864170,diagnosis,True
The value of nodal information in predicting lung cancer relapse using 4DPET/4DCT,"PURPOSE: There is evidence that computed tomography (CT) and positron emission tomography (PET) imaging metrics are prognostic and predictive in nonsmall cell lung cancer (NSCLC) treatment outcomes. However, few studies have explored the use of standardized uptake value (SUV)-based image features of nodal regions as predictive features. The authors investigated and compared the use of tumor and node image features extracted from the radiotherapy target volumes to predict relapse in a cohort of NSCLC patients undergoing chemoradiation treatment. METHODS: A prospective cohort of 25 patients with locally advanced NSCLC underwent 4DPET/4DCT imaging for radiation planning. Thirty-seven image features were derived from the CT-defined volumes and SUVs of the PET image from both the tumor and nodal target regions. The machine learning methods of logistic regression and repeated stratified five-fold cross-validation (CV) were used to predict local and overall relapses in 2 yr. The authors used well-known feature selection methods (Spearman's rank correlation, recursive feature elimination) within each fold of CV. Classifiers were ranked on their Matthew's correlation coefficient (MCC) after CV. Area under the curve, sensitivity, and specificity values are also presented. RESULTS: For predicting local relapse, the best classifier found had a mean MCC of 0.07 and was composed of eight tumor features. For predicting overall relapse, the best classifier found had a mean MCC of 0.29 and was composed of a single feature: the volume greater than 0.5 times the maximum SUV (N). CONCLUSIONS: The best classifier for predicting local relapse had only tumor features. In contrast, the best classifier for predicting overall relapse included a node feature. Overall, the methods showed that nodes add value in predicting overall relapse but not local relapse.",2015,10.1118/1.4926755,prognosis,True
Think positive: An interpretable neural network for image recognition,"The COVID-19 pandemic is an ongoing pandemic and is placing additional burden on healthcare systems around the world. Timely and effectively detecting the virus can help to reduce the spread of the disease. Although, RT-PCR is still a gold standard for COVID-19 testing, deep learning models to identify the virus from medical images can also be helpful in certain circumstances. In particular, in situations when patients undergo routine X-rays and/or CT-scans tests but within a few days of such tests they develop respiratory complications. Deep learning models can also be used for pre-screening prior to RT-PCR testing. However, the transparency/interpretability of the reasoning process of predictions made by such deep learning models is essential. In this paper, we propose an interpretable deep learning model that uses positive reasoning process to make predictions. We trained and tested our model over the dataset of chest CT-scan images of COVID-19 patients, normal people and pneumonia patients. Our model gives the accuracy, precision, recall and F-score equal to 99.48%, 0.99, 0.99 and 0.99, respectively.",2022,10.1016/j.neunet.2022.03.034,diagnosis,True
Three cuts method for identification of COPD,"Two main forms of COPD (Chronic Obstructive Pulmonary Disease) refer to a group of lung diseases that block airflow and cause a huge degree of human suffering. A new method for identifying and estimating the severity of COPD from three-dimensional (3-D) pulmonary X-ray CT images would be helpful for evaluation of treatment effects and early diagnosing is presented in this paper. This method has five main steps. Firstly, corresponding positions of lungs in inspiration and expiration are found based on anatomical structures. Secondly, lung regions are segmented from the CT images by active contours. Next, the left and right lungs are separated using a sequence of morphological operations. Then, parenchyma variations of three main cuts which selected by a feed-forward neural network are found based on the inspiratory and expiratory states. Finally, a pattern classifier is used to decide about the disease and its severity. Twenty patients with air-trapping problems and twelve normal adults were enrolled in this study. Based on the results, a mathematical model was developed to relate variations of lung volumes to severity of disease. The sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV) and the accuracy of our method for right regions were %81.6, %80.5, %87.5, %72.5 and %81.3 respectively. And these parameters for left regions were %90, %83.3, %90, %83.3 and %87.5 respectively. The proposed method may assist radiologists in detection of Asthma and COPD as a computer aided diagnosis (CAD) system.",2013,,diagnosis,True
Three-dimensional dose prediction for lung IMRT patients with deep neural networks: robust learning from heterogeneous beam configurations,"PURPOSE: The use of neural networks to directly predict three-dimensional dose distributions for automatic planning is becoming popular. However, the existing methods use only patient anatomy as input and assume consistent beam configuration for all patients in the training database. The purpose of this work was to develop a more general model that considers variable beam configurations in addition to patient anatomy to achieve more comprehensive automatic planning with a potentially easier clinical implementation, without the need to train specific models for different beam settings. METHODS: The proposed anatomy and beam (AB) model is based on our newly developed deep learning architecture, and hierarchically densely connected U-Net (HD U-Net), which combines U-Net and DenseNet. The AB model contains 10 input channels: one for beam setup and the other 9 for anatomical information (PTV and organs). The beam setup information is represented by a 3D matrix of the non-modulated beam's eye view ray-tracing dose distribution. We used a set of images from 129 patients with lung cancer treated with IMRT with heterogeneous beam configurations (4-9 beams of various orientations) for training/validation (100 patients) and testing (29 patients). Mean squared error was used as the loss function. We evaluated the model's accuracy by comparing the mean dose, maximum dose, and other relevant dose-volume metrics for the predicted dose distribution against those of the clinically delivered dose distribution. Dice similarity coefficients were computed to address the spatial correspondence of the isodose volumes between the predicted and clinically delivered doses. The model was also compared with our previous work, the anatomy only (AO) model, which does not consider beam setup information and uses only 9 channels for anatomical information. RESULTS: The AB model outperformed the AO model, especially in the low and medium dose regions. In terms of dose-volume metrics, AB outperformed AO by about 1-2%. The largest improvement was found to be about 5% in lung volume receiving a dose of 5Gy or more (V(5) ). The improvement for spinal cord maximum dose was also important, that is, 3.6% for cross-validation and 2.6% for testing. The AB model achieved Dice scores for isodose volumes as much as 10% higher than the AO model in low and medium dose regions and about 2-5% higher in high dose regions. CONCLUSIONS: The AO model, which does not use beam configuration as input, can still predict dose distributions with reasonable accuracy in high dose regions but introduces large errors in low and medium dose regions for IMRT cases with variable beam numbers and orientations. The proposed AB model outperforms the AO model substantially in low and medium dose regions, and slightly in high dose regions, by considering beam setup information through a cumulative non-modulated beam's eye view ray-tracing dose distribution. This new model represents a major step forward towards predicting 3D dose distributions in real clinical practices, where beam configuration could vary from patient to patient, from planner to planner, and from institution to institution.",2019,10.1002/mp.13597,treatment,True
Three-dimensional SVM with latent variable: application for detection of lung lesions in CT images,"The study aims to improve the performance of current computer-aided schemes for the detection of lung lesions, especially the low-contrast in gray density or irregular in shape. The relative position between suspected lesion and whole lung is, for the first time, added as a latent feature to enrich current Three-dimensional (3D) features such as shape, texture. Subsequently, 3D matrix patterns-based Support Vector Machine (SVM) with the latent variable, referred to as L-SVM3Dmatrix, was constructed accordingly. A CT image database containing 750 abnormal cases with 1050 lesions was used to train and evaluate several similar computer-aided detection (CAD) schemes: traditional features-based SVM (SVMfeature), 3D matrix patterns-based SVM (SVM3Dmatrix) and L-SVM3Dmatrix. The classifier performances were evaluated by computing the area under the ROC curve (AUC), using a 5-fold cross-validation. The L-SVM3Dmatrix sensitivity was 93.0 with 1.23% percentage of False Positive (FP), the SVM3Dmatrix sensitivity was 88.4 with 1.49% percentage of FP, and the SVMfeature sensitivity was 87.2 with 1.78% percentage of FP. The L-SVM3Dmatrix outperformed other current lung CAD schemes, especially regarding the difficult lesions.",2015,10.1007/s10916-014-0171-5,diagnosis,True
Three-Dimensional Texture Feature Analysis of Pulmonary Nodules in CT Images: Lung Cancer Predictive Models Based on Support Vector Machine Classifier,"To extract texture features of pulmonary nodules from three-dimensional views and to assess if predictive models of lung CT images from a three-dimensional texture feature could improve assessments conducted by radiologists. Clinical and CT imaging data for three dimensions (axial, coronal, and sagittal) in pulmonary nodules in 285 patients were collected from multiple centers and the Cancer Imaging Archive after ethics committee approval. Three-dimensional texture feature values (contourlets), and clinical and computed tomography (CT) imaging data were built into support vector machine (SVM) models to predict lung cancer, using four evaluation methods (disjunctive, conjunctive, voting, and synthetic); sensitivity, specificity, the Youden index, discriminant power (DP), and F value were calculated to assess model effectiveness. Additionally, diagnostic accuracy (three-dimensional model, axial model, and radiologist assessment) was assessed using the area under the curves for receiver operating characteristic (ROC) curves. Cross-sectional data from 285 patients (median age, 62 [range, 45-83] years; 115 males [40.4%]) were evaluated. Integrating three-dimensional assessments, the voting method had relatively high effectiveness based on both sensitivity (0.98) and specificity (0.79), which could improve radiologist diagnosis (maximum sensitivity, 0.75; maximum specificity, 0.51) for 23% and 28% respectively. Furthermore, the three-dimensional texture feature model of the voting method has the best diagnosis of precision rate (95.4%). Of all three-dimensional texture feature methods, the result of the voting method was the best, maintaining both high sensitivity and specificity scores. Additionally, the three-dimensional texture feature models were superior to two-dimensional models and radiologist-based assessments.",2020,10.1007/s10278-019-00238-8,diagnosis,True
Three-Terminal Ovonic Threshold Switch (3T-OTS) with Tunable Threshold Voltage for Versatile Artificial Sensory Neurons,"Inspired by information processing in biological systems, sensor-combined edge-computing systems attract attention requesting artificial sensory neurons as essential ingredients. Here, we introduce a simple and versatile structure of artificial sensory neurons based on a novel three-terminal Ovonic threshold switch (3T-OTS), which features an electrically controllable threshold voltage (V(th)). Combined with a sensor driving an output voltage, this 3T-OTS generates spikes with a frequency depending on an external stimulus. As a proof of concept, we have built an artificial retinal ganglion cell (RGC) by combining a 3T-OTS and a photodiode. Furthermore, this artificial RGC is combined with the reservoir-computing technique to perform a classification of chest X-ray images for normal, viral pneumonia, and COVID-19 infections, releasing the recognition accuracy of about 86.5%. These results indicate that the 3T-OTS is highly promising for applications in neuromorphic sensory systems, providing a building block for energy-efficient in-sensor computing devices.",2022,10.1021/acs.nanolett.1c04125,diagnosis,False
Toward an Expert Level of Lung Cancer Detection and Classification Using a Deep Convolutional Neural Network,"BACKGROUND: Computed tomography (CT) is essential for pulmonary nodule detection in diagnosing lung cancer. As deep learning algorithms have recently been regarded as a promising technique in medical fields, we attempt to integrate a well-trained deep learning algorithm to detect and classify pulmonary nodules derived from clinical CT images. MATERIALS AND METHODS: Open-source data sets and multicenter data sets have been used in this study. A three-dimensional convolutional neural network (CNN) was designed to detect pulmonary nodules and classify them into malignant or benign diseases based on pathologically and laboratory proven results. RESULTS: The sensitivity and specificity of this well-trained model were found to be 84.4% (95% confidence interval [CI], 80.5%-88.3%) and 83.0% (95% CI, 79.5%-86.5%), respectively. Subgroup analysis of smaller nodules (<10 mm) have demonstrated remarkable sensitivity and specificity, similar to that of larger nodules (10-30 mm). Additional model validation was implemented by comparing manual assessments done by different ranks of doctors with those performed by three-dimensional CNN. The results show that the performance of the CNN model was superior to manual assessment. CONCLUSION: Under the companion diagnostics, the three-dimensional CNN with a deep learning algorithm may assist radiologists in the future by providing accurate and timely information for diagnosing pulmonary nodules in regular clinical practices. IMPLICATIONS FOR PRACTICE: The three-dimensional convolutional neural network described in this article demonstrated both high sensitivity and high specificity in classifying pulmonary nodules regardless of diameters as well as superiority compared with manual assessment. Although it still warrants further improvement and validation in larger screening cohorts, its clinical application could definitely facilitate and assist doctors in clinical practice.",2019,10.1634/theoncologist.2018-0908,diagnosis,True
Toward automatic prediction of EGFR mutation status in pulmonary adenocarcinoma with 3D deep learning,"To develop a deep learning system based on 3D convolutional neural networks (CNNs), and to automatically predict EGFR-mutant pulmonary adenocarcinoma in CT images. A dataset of 579 nodules with EGFR mutation status labels of mutant (Mut) or wild-type (WT) was retrospectively analyzed. A deep learning system, namely 3D DenseNets, was developed to process 3D patches of nodules from CT data, and learn strong representations with supervised end-to-end training. The 3D DenseNets were trained with a training subset of 348 nodules and tuned with a development subset of 116 nodules. A strong data augmentation technique, mixup, was used for better generalization. We evaluated our model on a holdout subset of 115 nodules. An independent public dataset of 37 nodules from the cancer imaging archive (TCIA) was also used to test the generalization of our method. Conventional radiomics analysis was also performed for comparison. Our method achieved promising performance on predicting EGFR mutation status, with AUCs of 75.8% and 75.0% for our holdout test set and public test set, respectively. Moreover, strong relations were found between deep learning feature and conventional radiomics, while deep learning worked through an enhanced radiomics manner, that is, deep learned radiomics (DLR), in terms of robustness, compactness and expressiveness. The proposed deep learning system predicts EGFR-mutant of lung adenocarcinomas in CT images noninvasively and automatically, indicating its potential to help clinical decision-making by identifying eligible patients of pulmonary adenocarcinoma for EGFR-targeted therapy.",2019,10.1002/cam4.2233,diagnosis,True
Toward predicting the evolution of lung tumors during radiotherapy observed on a longitudinal MR imaging study via a deep learning algorithm,"PURPOSE: To predict the spatial and temporal trajectories of lung tumor during radiotherapy monitored under a longitudinal magnetic resonance imaging (MRI) study via a deep learning algorithm for facilitating adaptive radiotherapy (ART). METHODS: We monitored 10 lung cancer patients by acquiring weekly MRI-T2w scans over a course of radiotherapy. Under an ART workflow, we developed a predictive neural network (P-net) to predict the spatial distributions of tumors in the coming weeks utilizing images acquired earlier in the course. The three-step P-net consisted of a convolutional neural network to extract relevant features of the tumor and its environment, followed by a recurrence neural network constructed with gated recurrent units to analyze trajectories of tumor evolution in response to radiotherapy, and finally an attention model to weight the importance of weekly observations and produce the predictions. The performance of P-net was measured with Dice and root mean square surface distance (RMSSD) between the algorithm-predicted and experts-contoured tumors under a leave-one-out scheme. RESULTS: Tumor shrinkage was 60% ± 27% (mean ± standard deviation) by the end of radiotherapy across nine patients. Using images from the first three weeks, P-net predicted tumors on future weeks (4, 5, 6) with a Dice and RMSSD of (0.78 ± 0.22, 0.69 ± 0.24, 0.69 ± 0.26), and (2.1 ± 1.1 mm, 2.3 ± 0.8 mm, 2.6 ± 1.4 mm), respectively. CONCLUSION: The proposed deep learning algorithm can capture and predict spatial and temporal patterns of tumor regression in a longitudinal imaging study. It closely follows the clinical workflow, and could facilitate the decision-making of ART. A prospective study including more patients is warranted.",2019,10.1002/mp.13765,treatment,False
Towards automatic pulmonary nodule management in lung cancer screening with deep learning,"The introduction of lung cancer screening programs will produce an unprecedented amount of chest CT scans in the near future, which radiologists will have to read in order to decide on a patient follow-up strategy. According to the current guidelines, the workup of screen-detected nodules strongly relies on nodule size and nodule type. In this paper, we present a deep learning system based on multi-stream multi-scale convolutional networks, which automatically classifies all nodule types relevant for nodule workup. The system processes raw CT data containing a nodule without the need for any additional information such as nodule segmentation or nodule size and learns a representation of 3D data by analyzing an arbitrary number of 2D views of a given nodule. The deep learning system was trained with data from the Italian MILD screening trial and validated on an independent set of data from the Danish DLCST screening trial. We analyze the advantage of processing nodules at multiple scales with a multi-stream convolutional network architecture, and we show that the proposed deep learning system achieves performance at classifying nodule type that surpasses the one of classical machine learning approaches and is within the inter-observer variability among four experienced human observers.",2017,10.1038/srep46479,diagnosis,True
Towards large-scale case-finding: training and validation of residual networks for detection of chronic obstructive pulmonary disease using low-dose CT,"BACKGROUND: Chronic obstructive pulmonary disease (COPD) is underdiagnosed in the community. Thoracic CT scans are widely used for diagnostic and screening purposes for lung cancer. In this proof-of-concept study, we aimed to evaluate a software pipeline for the automated detection of COPD, based on deep learning and a dataset of low-dose CTs that were performed for early detection of lung cancer. METHODS: We examined the use of deep residual networks, a type of artificial residual network, for the automated detection of COPD. Three versions of the residual networks were independently trained to perform COPD diagnosis using random subsets of CT scans collected from the PanCan study, which enrolled ex-smokers and current smokers at high risk of lung cancer, and evaluated the networks using three-fold cross-validation experiments. External validation was performed using 2153 CT scans acquired from a separate cohort of individuals with COPD in the ECLIPSE study. Spirometric data were used to define COPD, with stages defined according to the GOLD criteria. FINDINGS: The best performing networks achieved an area under the receiver operating characteristic curve (AUC) of 0·889 (SD 0·017) in three-fold cross-validation experiments. When the same set of networks was applied to the ECLIPSE cohort without any modifications to the trained models, they achieved an AUC of 0·886 (0·017), a positive predictive value of 0·847 (0·056), and a negative predictive value of 0·755 (0·097), which is a greater performance than the best quantitative CT measure, the percentage of lung volumes of less than or equal to -950 Hounsfield units (AUC 0·742). INTERPRETATION: Our proposed approach could identify patients with COPD among ex-smokers and current smokers without a previous diagnosis of COPD, with clinically acceptable performance. The use of deep residual networks on chest CT scans could be an effective case-finding tool for COPD detection and diagnosis, particularly in ex-smokers and current smokers who are being screened for lung cancer. FUNDING: Data Science Institute, University of British Columbia; Canadian Institutes of Health Research.",2020,10.1016/s2589-7500(20)30064-9,diagnosis,True
Towards radiologist-level cancer risk assessment in CT lung screening using deep learning,"PURPOSE: Lung cancer is the leading cause of cancer mortality in the US, responsible for more deaths than breast, prostate, colon and pancreas cancer combined and large population studies have indicated that low-dose computed tomography (CT) screening of the chest can significantly reduce this death rate. Recently, the usefulness of Deep Learning (DL) models for lung cancer risk assessment has been demonstrated. However, in many cases model performances are evaluated on small/medium size test sets, thus not providing strong model generalization and stability guarantees which are necessary for clinical adoption. In this work, our goal is to contribute towards clinical adoption by investigating a deep learning framework on larger and heterogeneous datasets while also comparing to state-of-the-art models. METHODS: Three low-dose CT lung cancer screening datasets were used: National Lung Screening Trial (NLST, n = 3410), Lahey Hospital and Medical Center (LHMC, n = 3154) data, Kaggle competition data (from both stages, n = 1397 + 505) and the University of Chicago data (UCM, a subset of NLST, annotated by radiologists, n = 132). At the first stage, our framework employs a nodule detector; while in the second stage, we use both the image context around the nodules and nodule features as inputs to a neural network that estimates the malignancy risk for the entire CT scan. We trained our algorithm on a part of the NLST dataset, and validated it on the other datasets. Special care was taken to ensure there was no patient overlap between the train and validation sets. RESULTS AND CONCLUSIONS: The proposed deep learning model is shown to: (a) generalize well across all three data sets, achieving AUC between 86% to 94%, with our external test-set (LHMC) being at least twice as large compared to other works; (b) have better performance than the widely accepted PanCan Risk Model, achieving 6 and 9% better AUC score in our two test sets; (c) have improved performance compared to the state-of-the-art represented by the winners of the Kaggle Data Science Bowl 2017 competition on lung cancer screening; (d) have comparable performance to radiologists in estimating cancer risk at a patient level.",2021,10.1016/j.compmedimag.2021.101883,diagnosis,True
Towards robust diagnosis of COVID-19 using vision self-attention transformer,"The outbreak of COVID-19, since its appearance, has affected about 200 countries and endangered millions of lives. COVID-19 is extremely contagious disease, and it can quickly incapacitate the healthcare systems if infected cases are not handled timely. Several Conventional Neural Networks (CNN) based techniques have been developed to diagnose the COVID-19. These techniques require a large, labelled dataset to train the algorithm fully, but there are not too many labelled datasets. To mitigate this problem and facilitate the diagnosis of COVID-19, we developed a self-attention transformer-based approach having self-attention mechanism using CT slices. The architecture of transformer can exploit the ample unlabelled datasets using pre-training. The paper aims to compare the performances of self-attention transformer-based approach with CNN and Ensemble classifiers for diagnosis of COVID-19 using binary Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) infection and multi-class Hybrid-learning for UnbiaSed predicTion of COVID-19 (HUST-19) CT scan dataset. To perform this comparison, we have tested Deep learning-based classifiers and ensemble classifiers with proposed approach using CT scan images. Proposed approach is more effective in detection of COVID-19 with an accuracy of 99.7% on multi-class HUST-19, whereas 98% on binary class SARS-CoV-2 dataset. Cross corpus evaluation achieves accuracy of 93% by training the model with Hust19 dataset and testing using Brazilian COVID dataset.",2022,10.1038/s41598-022-13039-x,diagnosis,True
Transfer learning based novel ensemble classifier for COVID-19 detection from chest CT-scans,"Coronavirus Disease 2019 (COVID-19) is a deadly infection that affects the respiratory organs in humans as well as animals. By 2020, this disease turned out to be a pandemic affecting millions of individuals across the globe. Conducting rapid tests for a large number of suspects preventing the spread of the virus has become a challenge. In the recent past, several deep learning based approaches have been developed for automating the process of detecting COVID-19 infection from Lung Computerized Tomography (CT) scan images. However, most of them rely on a single model prediction for the final decision which may or may not be accurate. In this paper, we propose a novel ensemble approach that aggregates the strength of multiple deep neural network architectures before arriving at the final decision. We use various pre-trained models such as VGG16, VGG19, InceptionV3, ResNet50, ResNet50V2, InceptionResNetV2, Xception, and MobileNet and fine-tune them using Lung CT Scan images. All these trained models are further used to create a strong ensemble classifier that makes the final prediction. Our experiments exhibit that the proposed ensemble approach is superior to existing ensemble approaches and set state-of-the-art results for detecting COVID-19 infection from lung CT scan images.",2022,10.1016/j.compbiomed.2021.105127,diagnosis,True
Transfer Learning for Multicenter Classification of Chronic Obstructive Pulmonary Disease,"Chronic obstructive pulmonary disease (COPD) is a lung disease that can be quantified using chest computed tomography scans. Recent studies have shown that COPD can be automatically diagnosed using weakly supervised learning of intensity and texture distributions. However, up till now such classifiers have only been evaluated on scans from a single domain, and it is unclear whether they would generalize across domains, such as different scanners or scanning protocols. To address this problem, we investigate classification of COPD in a multicenter dataset with a total of 803 scans from three different centers, four different scanners, with heterogenous subject distributions. Our method is based on Gaussian texture features, and a weighted logistic classifier, which increases the weights of samples similar to the test data. We show that Gaussian texture features outperform intensity features previously used in multicenter classification tasks. We also show that a weighting strategy based on a classifier that is trained to discriminate between scans from different domains can further improve the results. To encourage further research into transfer learning methods for the classification of COPD, upon acceptance of this paper we will release two feature datasets used in this study on http://bigr.nl/research/projects/copd.",2018,10.1109/jbhi.2017.2769800,diagnosis,True
Transfer learning-based ensemble support vector machine model for automated COVID-19 detection using lung computerized tomography scan data,"The novel discovered disease coronavirus popularly known as COVID-19 is caused due to severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) and declared a pandemic by the World Health Organization (WHO). An early-stage detection of COVID-19 is crucial for the containment of the pandemic it has caused. In this study, a transfer learning-based COVID-19 screening technique is proposed. The motivation of this study is to design an automated system that can assist medical staff especially in areas where trained staff are outnumbered. The study investigates the potential of transfer learning-based models for automatically diagnosing diseases like COVID-19 to assist the medical force, especially in times of an outbreak. In the proposed work, a deep learning model, i.e., truncated VGG16 (Visual Geometry Group from Oxford) is implemented to screen COVID-19 CT scans. The VGG16 architecture is fine-tuned and used to extract features from CT scan images. Further principal component analysis (PCA) is used for feature selection. For the final classification, four different classifiers, namely deep convolutional neural network (DCNN), extreme learning machine (ELM), online sequential ELM, and bagging ensemble with support vector machine (SVM) are compared. The best performing classifier bagging ensemble with SVM within 385 ms achieved an accuracy of 95.7%, the precision of 95.8%, area under curve (AUC) of 0.958, and an F1 score of 95.3% on 208 test images. The results obtained on diverse datasets prove the superiority and robustness of the proposed work. A pre-processing technique has also been proposed for radiological data. The study further compares pre-trained CNN architectures and classification models against the proposed technique.",2021,10.1007/s11517-020-02299-2,diagnosis,True
Truncated inception net: COVID-19 outbreak screening using chest X-rays,"Since December 2019, the Coronavirus Disease (COVID-19) pandemic has caused world-wide turmoil in a short period of time, and the infection, caused by SARS-CoV-2, is spreading rapidly. AI-driven tools are used to identify Coronavirus outbreaks as well as forecast their nature of spread, where imaging techniques are widely used, such as CT scans and chest X-rays (CXRs). In this paper, motivated by the fact that X-ray imaging systems are more prevalent and cheaper than CT scan systems, a deep learning-based Convolutional Neural Network (CNN) model, which we call Truncated Inception Net, is proposed to screen COVID-19 positive CXRs from other non-COVID and/or healthy cases. To validate our proposal, six different types of datasets were employed by taking the following CXRs: COVID-19 positive, Pneumonia positive, Tuberculosis positive, and healthy cases into account. The proposed model achieved an accuracy of 99.96% (AUC of 1.0) in classifying COVID-19 positive cases from combined Pneumonia and healthy cases. Similarly, it achieved an accuracy of 99.92% (AUC of 0.99) in classifying COVID-19 positive cases from combined Pneumonia, Tuberculosis, and healthy CXRs. To the best of our knowledge, as of now, the achieved results outperform the existing AI-driven tools for screening COVID-19 using the acquired CXRs, and proves the viability of using the proposed Truncated Inception Net as a screening tool.",2020,10.1007/s13246-020-00888-x,diagnosis,False
Tumor immune profiles noninvasively estimated by FDG PET with deep learning correlate with immunotherapy response in lung adenocarcinoma,"Rationale: The clinical application of biomarkers reflecting tumor immune microenvironment is hurdled by the invasiveness of obtaining tissues despite its importance in immunotherapy. We developed a deep learning-based biomarker which noninvasively estimates a tumor immune profile with fluorodeoxyglucose positron emission tomography (FDG-PET) in lung adenocarcinoma (LUAD). Methods: A deep learning model to predict cytolytic activity score (CytAct) using semi-automatically segmented tumors on FDG-PET trained by a publicly available dataset paired with tissue RNA sequencing (n = 93). This model was validated in two independent cohorts of LUAD: SNUH (n = 43) and The Cancer Genome Atlas (TCGA) cohort (n = 16). The model was applied to the immune checkpoint blockade (ICB) cohort, which consists of patients with metastatic LUAD who underwent ICB treatment (n = 29). Results: The predicted CytAct showed a positive correlation with CytAct of RNA sequencing in validation cohorts (Spearman rho = 0.32, p = 0.04 in SNUH cohort; spearman rho = 0.47, p = 0.07 in TCGA cohort). In ICB cohort, the higher predicted CytAct of individual lesion was associated with more decrement in tumor size after ICB treatment (Spearman rho = -0.54, p < 0.001). Higher minimum predicted CytAct in each patient associated with significantly prolonged progression free survival and overall survival (Hazard ratio 0.25, p = 0.001 and 0.18, p = 0.004, respectively). In patients with multiple lesions, ICB responders had significantly lower variance of predicted CytActs (p = 0.005). Conclusion: The deep learning model that predicts CytAct using FDG-PET of LUAD was validated in independent cohorts. Our approach may be used to noninvasively assess an immune profile and predict outcomes of LUAD patients treated with ICB.",2020,10.7150/thno.50283,prognosis,True
Tumor Segmentation and Feature Extraction from Whole-Body FDG-PET/CT Using Cascaded 2D and 3D Convolutional Neural Networks,"(18)F-Fluorodeoxyglucose-positron emission tomography (FDG-PET) is commonly used in clinical practice and clinical drug development to identify and quantify metabolically active tumors. Manual or computer-assisted tumor segmentation in FDG-PET images is a common way to assess tumor burden, such approaches are both labor intensive and may suffer from high inter-reader variability. We propose an end-to-end method leveraging 2D and 3D convolutional neural networks to rapidly identify and segment tumors and to extract metabolic information in eyes to thighs (whole body) FDG-PET/CT scans. The developed architecture is computationally efficient and devised to accommodate the size of whole-body scans, the extreme imbalance between tumor burden and the volume of healthy tissue, and the heterogeneous nature of the input images. Our dataset consists of a total of 3664 eyes to thighs FDG-PET/CT scans, from multi-site clinical trials in patients with non-Hodgkin's lymphoma (NHL) and advanced non-small cell lung cancer (NSCLC). Tumors were segmented and reviewed by board-certified radiologists. We report a mean 3D Dice score of 88.6% on an NHL hold-out set of 1124 scans and a 93% sensitivity on 274 NSCLC hold-out scans. The method is a potential tool for radiologists to rapidly assess eyes to thighs FDG-avid tumor burden.",2020,10.1007/s10278-020-00341-1,diagnosis,True
Tumour auto-contouring on 2d cine MRI for locally advanced lung cancer: A comparative study,"BACKGROUND AND PURPOSE: Radiotherapy guidance based on magnetic resonance imaging (MRI) is currently becoming a clinical reality. Fast 2d cine MRI sequences are expected to increase the precision of radiation delivery by facilitating tumour delineation during treatment. This study compares four auto-contouring algorithms for the task of delineating the primary tumour in six locally advanced (LA) lung cancer patients. MATERIAL AND METHODS: Twenty-two cine MRI sequences were acquired using either a balanced steady-state free precession or a spoiled gradient echo imaging technique. Contours derived by the auto-contouring algorithms were compared against manual reference contours. A selection of eight image data sets was also used to assess the inter-observer delineation uncertainty. RESULTS: Algorithmically derived contours agreed well with the manual reference contours (median Dice similarity index: ⩾0.91). Multi-template matching and deformable image registration performed significantly better than feature-driven registration and the pulse-coupled neural network (PCNN). Neither MRI sequence nor image orientation was a conclusive predictor for algorithmic performance. Motion significantly degraded the performance of the PCNN. The inter-observer variability was of the same order of magnitude as the algorithmic performance. CONCLUSION: Auto-contouring of tumours on cine MRI is feasible in LA lung cancer patients. Despite large variations in implementation complexity, the different algorithms all have relatively similar performance.",2017,10.1016/j.radonc.2017.09.013,diagnosis,False
U-survival for prognostic prediction of disease progression and mortality of patients with COVID-19,"The rapid increase of patients with coronavirus disease 2019 (COVID-19) has introduced major challenges to healthcare services worldwide. Therefore, fast and accurate clinical assessment of COVID-19 progression and mortality is vital for the management of COVID-19 patients. We developed an automated image-based survival prediction model, called U-survival, which combines deep learning of chest CT images with the established survival analysis methodology of an elastic-net Cox survival model. In an evaluation of 383 COVID-19 positive patients from two hospitals, the prognostic bootstrap prediction performance of U-survival was significantly higher (P < 0.0001) than those of existing laboratory and image-based reference predictors both for COVID-19 progression (maximum concordance index: 91.6% [95% confidence interval 91.5, 91.7]) and for mortality (88.7% [88.6, 88.9]), and the separation between the Kaplan-Meier survival curves of patients stratified into low- and high-risk groups was largest for U-survival (P < 3 × 10(-14)). The results indicate that U-survival can be used to provide automated and objective prognostic predictions for the management of COVID-19 patients.",2021,10.1038/s41598-021-88591-z,prognosis,True
UBNet: Deep learning-based approach for automatic X-ray image detection of pneumonia and COVID-19 patients,"BACKGROUND: Analysis of chest X-ray images is one of the primary standards in diagnosing patients with COVID-19 and pneumonia, which is faster than using PCR Swab method. However, accuracy of using X-ray images needs to be improved. OBJECTIVE: To develop a new deep learning system of chest X-ray images and evaluate whether it can quickly and accurately detect pneumonia and COVID-19 patients. METHODS: The developed deep learning system (UBNet v3) uses three architectural hierarchies, namely first, to build an architecture containing 7 convolution layers and 3 ANN layers (UBNet v1) to classify between normal images and pneumonia images. Second, using 4 layers of convolution and 3 layers of ANN (UBNet v2) to classify between bacterial and viral pneumonia images. Third, using UBNet v1 to classify between pneumonia virus images and COVID-19 virus infected images. An open-source database with 9,250 chest X-ray images including 3,592 COVID-19 images were used in this study to train and test the developed deep learning models. RESULTS: CNN architecture with a hierarchical scheme developed in UBNet v3 using a simple architecture yielded following performance indices to detect chest X-ray images of COVID-19 patients namely, 99.6%accuracy, 99.7%precision, 99.7%sensitivity, 99.1%specificity, and F1 score of 99.74%. A desktop GUI-based monitoring and classification system supported by a simple CNN architecture can process each chest X-ray image to detect and classify COVID-19 image with an average time of 1.21 seconds. CONCLUSION: Using three hierarchical architectures in UBNet v3 improves system performance in classifying chest X-ray images of pneumonia and COVID-19 patients. A simple architecture also speeds up image processing time.",2022,10.3233/xst-211005,diagnosis,False
ULNet for the detection of coronavirus (COVID-19) from chest X-ray images,"Novel coronavirus disease 2019 (COVID-19) is an infectious disease that spreads very rapidly and threatens the health of billions of people worldwide. With the number of cases increasing rapidly, most countries are facing the problem of a shortage of testing kits and resources, and it is necessary to use other diagnostic methods as an alternative to these test kits. In this paper, we propose a convolutional neural network (CNN) model (ULNet) to detect COVID-19 using chest X-ray images. The proposed architecture is constructed by adding a new downsampling side, skip connections and fully connected layers on the basis of U-net. Because the shape of the network is similar to UL, it is named ULNet. This model is trained and tested on a publicly available Kaggle dataset (consisting of a combination of 219 COVID-19, 1314 normal and 1345 viral pneumonia chest X-ray images), including binary classification (COVID-19 vs. Normal) and multiclass classification (COVID-19 vs. Normal vs. Viral Pneumonia). The accuracy of the proposed model in the detection of COVID-19 in the binary-class and multiclass tasks is 99.53% and 95.35%, respectively. Based on these promising results, this method is expected to help doctors diagnose and detect COVID-19. Overall, our ULNet provides a quick method for identifying patients with COVID-19, which is conducive to the control of the COVID-19 pandemic.",2021,10.1016/j.compbiomed.2021.104834,diagnosis,False
UMLF-COVID: an unsupervised meta-learning model specifically designed to identify X-ray images of COVID-19 patients,"BACKGROUND: With the rapid spread of COVID-19 worldwide, quick screening for possible COVID-19 patients has become the focus of international researchers. Recently, many deep learning-based Computed Tomography (CT) image/X-ray image fast screening models for potential COVID-19 patients have been proposed. However, the existing models still have two main problems. First, most of the existing supervised models are based on pre-trained model parameters. The pre-training model needs to be constructed on a dataset with features similar to those in COVID-19 X-ray images, which limits the construction and use of the model. Second, the number of categories based on the X-ray dataset of COVID-19 and other pneumonia patients is usually imbalanced. In addition, the quality is difficult to distinguish, leading to non-ideal results with the existing model in the multi-class classification COVID-19 recognition task. Moreover, no researchers have proposed a COVID-19 X-ray image learning model based on unsupervised meta-learning. METHODS: This paper first constructed an unsupervised meta-learning model for fast screening of COVID-19 patients (UMLF-COVID). This model does not require a pre-trained model, which solves the limitation problem of model construction, and the proposed unsupervised meta-learning framework solves the problem of sample imbalance and sample quality. RESULTS: The UMLF-COVID model is tested on two real datasets, each of which builds a three-category and four-category model. And the experimental results show that the accuracy of the UMLF-COVID model is 3-10% higher than that of the existing models. CONCLUSION: In summary, we believe that the UMLF-COVID model is a good complement to COVID-19 X-ray fast screening models.",2021,10.1186/s12880-021-00704-2,diagnosis,False
Unboxing AI - Radiological Insights Into a Deep Neural Network for Lung Nodule Characterization,"RATIONALE AND OBJECTIVES: To explain predictions of a deep residual convolutional network for characterization of lung nodule by analyzing heat maps. MATERIALS AND METHODS: A 20-layer deep residual CNN was trained on 1245 Chest CTs from National Lung Screening Trial (NLST) trial to predict the malignancy risk of a nodule. We used occlusion to systematically block regions of a nodule and map drops in malignancy risk score to generate clinical attribution heatmaps on 103 nodules from Lung Image Database Consortium image collection and Image Database Resource Initiative (LIDC-IDRI) dataset, which were analyzed by a thoracic radiologist. The features were described as heat inside nodule -bright areas inside nodule, peripheral heat continuous/interrupted bright areas along nodule contours, heat in adjacent plane -brightness in scan planes juxtaposed with the nodule, satellite heat - a smaller bright spot in proximity to nodule in the same scan plane, heat map larger than nodule bright areas corresponding to the shape of the nodule seen outside the nodule margins and heat in calcification. RESULTS: These six features were assigned binary values. This feature vector was fedinto a standard J48 decision tree with 10-fold cross-validation, which gave an 85 % weighted classification accuracy with a 77.8% True Positive (TP) rate, 8% False Positive (FP) rate for benign cases and 91.8% TP and 22.2% FP rates for malignant cases. Heat Inside nodule was more frequently observed in nodules classified as malignant whereas peripheral heat, heat in adjacent plane, and satellite heat were more commonly seen in nodules classified as benign. CONCLUSION: We discuss the potential ability of a radiologist to visually parse the deep learning algorithm generated ""heat map"" to identify features aiding classification.",2020,10.1016/j.acra.2019.09.015,diagnosis,True
"Unraveling the interplay of image formation, data representation and learning in CT-based COPD phenotyping automation: The need for a meta-strategy","PURPOSE: In the literature on automated phenotyping of chronic obstructive pulmonary disease (COPD), there is a multitude of isolated classical machine learning and deep learning techniques, mostly investigating individual phenotypes, with small study cohorts and heterogeneous meta-parameters, e.g., different scan protocols or segmented regions. The objective is to compare the impact of different experimental setups, i.e., varying meta-parameters related to image formation and data representation, with the impact of the learning technique for subtyping automation for a variety of phenotypes. The identified associations of these parameters with automation performance and their interactions might be a first step towards a determination of optimal meta-parameters, i.e., a meta-strategy. METHODS: A clinical cohort of 981 patients (53.8 ± 15.1 years, 554 male) was examined. The inspiratory CT images were analyzed to automate the diagnosis of 13 COPD phenotypes given by two radiologists. A benchmark feature set that integrates many quantitative criteria was extracted from the lung and trained a variety of learning algorithms on the first 654 patients (two thirds) and the respective algorithm retrospectively assessed the remaining 327 patients (one third). The automation performance was evaluated by the area under the receiver operating characteristic curve (AUC). 1717 experiments were conducted with varying meta-parameters such as reconstruction kernel, segmented regions and input dimensionality, i.e., number of extracted features. The association of the meta-parameters with the automation performance was analyzed by multivariable general linear model decomposition of the automation performance in the contributions of meta-parameters and the learning technique. RESULTS: The automation performance varied strongly for varying meta-parameters. For emphysema-predominant phenotypes, an AUC of 93%-95% could be achieved for the best meta-configuration. The airways-predominant phenotypes led to a lower performance of 65%-85%, while smooth kernel configurations on average were unexpectedly superior to those with sharp kernels. The performance impact of meta-parameters, even that of often neglected ones like the missing-data imputation, was in general larger than that of the learning technique. Advanced learning techniques like 3D deep learning or automated machine learning yielded inferior automation performance for non-optimal meta-configurations in comparison to simple techniques with suitable meta-configurations. The best automation performance was achieved by a combination of modern learning techniques and a suitable meta-configuration. CONCLUSIONS: Our results indicate that for COPD phenotype automation, study design parameters such as reconstruction kernel and the model input dimensionality should be adapted to the learning technique and may be more important than the technique itself. To achieve optimal automation and prediction results, the interaction between input those meta-parameters and the learning technique should be considered. This might be particularly relevant for the development of specific scan protocols for novel learning algorithms, and towards an understanding of good study design for automated phenotyping.",2021,10.1002/mp.15049,diagnosis,True
Unsupervised machine learning of radiomic features for predicting treatment response and overall survival of early stage non-small cell lung cancer patients treated with stereotactic body radiation therapy,"BACKGROUND AND PURPOSE: To predict treatment response and survival of NSCLC patients receiving stereotactic body radiation therapy (SBRT), we develop an unsupervised machine learning method for stratifying patients and extracting meta-features simultaneously based on imaging data. MATERIAL AND METHODS: This study was performed based on an (18)F-FDG-PET dataset of 100 consecutive patients who were treated with SBRT for early stage NSCLC. Each patient's tumor was characterized by 722 radiomic features. An unsupervised two-way clustering method was used to identify groups of patients and radiomic features simultaneously. The groups of patients were compared in terms of survival and freedom from nodal failure. Meta-features were computed for building survival models to predict survival and free of nodal failure. RESULTS: Differences were found between 2 groups of patients when the patients were clustered into 3 groups in terms of both survival (p = 0.003) and freedom from nodal failure (p = 0.038). Average concordance measures for predicting survival and nodal failure were 0.640±0.029 and 0.664±0.063 respectively, better than those obtained by prediction models built upon clinical variables (p < 0.04). CONCLUSIONS: The evaluation results demonstrate that our method allows us to stratify patients and predict survival and freedom from nodal failure with better performance than current alternative methods.",2018,10.1016/j.radonc.2018.06.025,prognosis,False
Use of a Commercially Available Deep Learning Algorithm to Measure the Solid Portions of Lung Cancer Manifesting as Subsolid Lesions at CT: Comparisons with Radiologists and Invasive Component Size at Pathologic Examination,"Background The solid portion size of lung cancer lesions manifesting as subsolid lesions is key in their management, but the automatic measurement of such lesions by means of a deep learning (DL) algorithm needs evaluation. Purpose To evaluate the performance of a commercially available DL algorithm for automatic measurement of the solid portion of surgically proven lung adenocarcinomas manifesting as subsolid lesions. Materials and Methods Surgically proven lung adenocarcinomas manifesting as subsolid lesions on CT images between January 2018 and December 2018 were retrospectively included. Five radiologists independently measured the maximal axial diameter of the solid portion of lesions. The DL algorithm automatically segmented and measured the maximal axial diameter of the solid portion. Reader measurements, software measurements, and invasive component size at pathologic examination were compared by using intraclass correlation coefficient (ICC) and Bland-Altman plots. Results A total of 448 patients (mean age, 63 years ± 10 [standard deviation]; 264 women) with 448 lesions were evaluated (invasive component size, 3-65 mm). The measurement agreements between each radiologist and the DL algorithm were very good (ICC range, 0.82-0.89). When a radiologist was replaced with the DL algorithm, the ICCs ranged from 0.87 to 0.90, with an ICC of 0.90 among five radiologists. The mean difference between the DL algorithm and each radiologist ranged from -3.7 to 1.5 mm. The widest 95% limit of agreement between the DL algorithm and each radiologist (-15.7 to 8.3 mm) was wider than pairwise comparisons of radiologists (-7.7 to 13.0 mm). The agreement between the DL algorithm and invasive component size at pathologic evaluation was good, with an ICC of 0.67. Measurements by the DL algorithm (mean difference, -6.0 mm) and radiologists (mean difference, -7.5 to -2.3 mm) both underestimated invasive component size. Conclusion Automatic measurements of solid portions of lung cancer manifesting as subsolid lesions by the deep learning algorithm were comparable with manual measurements and showed good agreement with invasive component size at pathologic evaluation. © RSNA, 2021 Online supplemental material is available for this article.",2021,10.1148/radiol.2021202803,diagnosis,True
Use of a Dual Artificial Intelligence Platform to Detect Unreported Lung Nodules,"OBJECTIVE: To investigate the performance of Dual-AI Deep Learning Platform in detecting unreported pulmonary nodules that are 6 mm or greater, comprising computer-vision (CV) algorithm to detect pulmonary nodules, with positive results filtered by natural language processing (NLP) analysis of the dictated report. METHODS: Retrospective analysis of 5047 chest CT scans and corresponding reports. Cases which were both CV algorithm positive (nodule ≥ 6 mm) and NLP negative (nodule not reported), were outputted for review by 2 chest radiologists. RESULTS: The CV algorithm detected nodules that are 6 mm or greater in 1830 (36.3%) of 5047 cases. Three hundred fifty-five (19.4%) were unreported by the radiologist, as per NLP algorithm. Expert review determined that 139 (39.2%) of 355 cases were true positives (2.8% of all cases). One hundred thirty (36.7%) of 355 cases were unnecessary alerts-vague language in the report confounded the NLP algorithm. Eighty-six (24.2%) of 355 cases were false positives. CONCLUSIONS: Dual-AI platform detected actionable unreported nodules in 2.8% of chest CT scans, yet minimized intrusion to radiologist's workflow by avoiding alerts for most already-reported nodules.",2021,10.1097/rct.0000000000001118,diagnosis,True
Use of CT radiomics to differentiate minimally invasive adenocarcinomas and invasive adenocarcinomas presenting as pure ground-glass nodules larger than 10 mm,"PURPOSE: This study aimed to develop a model based on radiomics features extracted from computed tomography (CT) images to effectively differentiate between minimally invasive adenocarcinomas (MIAs) and invasive adenocarcinomas (IAs) manifesting as pure ground-glass nodules (pGGNs) larger than 10 mm. METHOD: This retrospective study included patients who underwent surgical resection for persistent pGGN between November 2012 and June 2018 and diagnosed with MIAs or IAs. The patients were randomly assigned to the training and test cohorts. The correlation coefficient method and the least absolute shrinkage and selection operator (LASSO) method were applied to select radiomics features useful for constructing a model whose performance was assessed by the area under the receiver operating characteristic curve (AUC-ROC). The radiomics model was compared to a standard CT model (shape, volume and mean CT value of the largest cross-section) and the combined radiomics-standard CT model using univariate and multivariate logistic regression analysis. RESULTS: The radiomics model showed better discriminative ability (training AUC, 0.879; test AUC, 0.877) than the standard CT model (training AUC, 0.820; test AUC, 0.828). The combined model (training AUC, 0.879; test AUC, 0.870) did not demonstrate improved performance compared with the radiomics model. Radiomics_score was an independent predictor of invasiveness following multivariate logistic analysis. CONCLUSIONS: For pGGNs larger than 10 mm, the radiomics model demonstrated superior diagnostic performance in differentiating between IAs and MIAs, which may be useful to clinicians for diagnosis and treatment selection.",2021,10.1016/j.ejrad.2021.109772,diagnosis,True
Usefulness of gradient tree boosting for predicting histological subtype and EGFR mutation status of non-small cell lung cancer on (18)F FDG-PET/CT,"OBJECTIVE: To develop and evaluate a radiomics approach for classifying histological subtypes and epidermal growth factor receptor (EGFR) mutation status in lung cancer on PET/CT images. METHODS: PET/CT images of lung cancer patients were obtained from public databases and used to establish two datasets, respectively to classify histological subtypes (156 adenocarcinomas and 32 squamous cell carcinomas) and EGFR mutation status (38 mutant and 100 wild-type samples). Seven types of imaging features were obtained from PET/CT images of lung cancer. Two types of machine learning algorithms were used to predict histological subtypes and EGFR mutation status: random forest (RF) and gradient tree boosting (XGB). The classifiers used either a single type or multiple types of imaging features. In the latter case, the optimal combination of the seven types of imaging features was selected by Bayesian optimization. Receiver operating characteristic analysis, area under the curve (AUC), and tenfold cross validation were used to assess the performance of the approach. RESULTS: In the classification of histological subtypes, the AUC values of the various classifiers were as follows: RF, single type: 0.759; XGB, single type: 0.760; RF, multiple types: 0.720; XGB, multiple types: 0.843. In the classification of EGFR mutation status, the AUC values were: RF, single type: 0.625; XGB, single type: 0.617; RF, multiple types: 0.577; XGB, multiple types: 0.659. CONCLUSIONS: The radiomics approach to PET/CT images, together with XGB and Bayesian optimization, is useful for classifying histological subtypes and EGFR mutation status in lung cancer.",2020,10.1007/s12149-019-01414-0,diagnosis,True
Using a Deep Learning Model to Explore the Impact of Clinical Data on COVID-19 Diagnosis Using Chest X-ray,"The coronavirus pandemic (COVID-19) is disrupting the entire world; its rapid global spread threatens to affect millions of people. Accurate and timely diagnosis of COVID-19 is essential to control the spread and alleviate risk. Due to the promising results achieved by integrating machine learning (ML), particularly deep learning (DL), in automating the multiple disease diagnosis process. In the current study, a model based on deep learning was proposed for the automated diagnosis of COVID-19 using chest X-ray images (CXR) and clinical data of the patient. The aim of this study is to investigate the effects of integrating clinical patient data with the CXR for automated COVID-19 diagnosis. The proposed model used data collected from King Fahad University Hospital, Dammam, KSA, which consists of 270 patient records. The experiments were carried out first with clinical data, second with the CXR, and finally with clinical data and CXR. The fusion technique was used to combine the clinical features and features extracted from images. The study found that integrating clinical data with the CXR improves diagnostic accuracy. Using the clinical data and the CXR, the model achieved an accuracy of 0.970, a recall of 0.986, a precision of 0.978, and an F-score of 0.982. Further validation was performed by comparing the performance of the proposed system with the diagnosis of an expert. Additionally, the results have shown that the proposed system can be used as a tool that can help the doctors in COVID-19 diagnosis.",2022,10.3390/s22020669,diagnosis,False
Using a risk model for probability of cancer in pulmonary nodules,"BACKGROUND: Considering the high morbidity and mortality of lung cancer and the high incidence of pulmonary nodules, clearly distinguishing benign from malignant lung nodules at an early stage is of great significance. However, determining the kind of lung nodule which is more prone to lung cancer remains a problem worldwide. METHODS: A total of 480 patients with pulmonary nodule data were collected from Shandong, China. We assessed the clinical characteristics and computed tomography (CT) imaging features among pulmonary nodules in patients who had undergone video-assisted thoracoscopic surgery (VATS) lobectomy from 2013 to 2018. Preliminary selection of features was based on a statistical analysis using SPSS. We used WEKA to assess the machine learning models using its multiple algorithms and selected the best decision tree model using its optimization algorithm. RESULTS: The combination of decision tree and logistics regression optimized the decision tree without affecting its AUC. The decision tree structure showed that lobulation was the most important feature, followed by spiculation, vessel convergence sign, nodule type, satellite nodule, nodule size and age of patient. CONCLUSIONS: Our study shows that decision tree analyses can be applied to screen individuals for early lung cancer with CT. Our decision tree provides a new way to help clinicians establish a logical diagnosis by a stepwise progression method, but still needs to be validated for prospective trials in a larger patient population.",2021,10.1111/1759-7714.13991,diagnosis,True
Using artificial intelligence to assist radiologists in distinguishing COVID-19 from other pulmonary infections,"BACKGROUND: Accurate and rapid diagnosis of coronavirus disease (COVID-19) is crucial for timely quarantine and treatment. PURPOSE: In this study, a deep learning algorithm-based AI model using ResUNet network was developed to evaluate the performance of radiologists with and without AI assistance in distinguishing COVID-19 infected pneumonia patients from other pulmonary infections on CT scans. METHODS: For model development and validation, a total number of 694 cases with 111,066 CT slides were retrospectively collected as training data and independent test data in the study. Among them, 118 are confirmed COVID-19 infected pneumonia cases and 576 are other pulmonary infection cases (e.g. tuberculosis cases, common pneumonia cases and non-COVID-19 viral pneumonia cases). The cases were divided into training and testing datasets. The independent test was performed by evaluating and comparing the performance of three radiologists with different years of practice experience in distinguishing COVID-19 infected pneumonia cases with and without the AI assistance. RESULTS: Our final model achieved an overall test accuracy of 0.914 with an area of the receiver operating characteristic (ROC) curve (AUC) of 0.903 in which the sensitivity and specificity are 0.918 and 0.909, respectively. The deep learning-based model then achieved a comparable performance by improving the radiologists' performance in distinguish COVOD-19 from other pulmonary infections, yielding better average accuracy and sensitivity, from 0.941 to 0.951 and from 0.895 to 0.942, respectively, when compared to radiologists without using AI assistance. CONCLUSION: A deep learning algorithm-based AI model developed in this study successfully improved radiologists' performance in distinguishing COVID-19 from other pulmonary infections using chest CT images.",2021,10.3233/xst-200735,diagnosis,True
Using Artificial Intelligence to Detect COVID-19 and Community-acquired Pneumonia Based on Pulmonary CT: Evaluation of the Diagnostic Accuracy,"Background Coronavirus disease 2019 (COVID-19) has widely spread all over the world since the beginning of 2020. It is desirable to develop automatic and accurate detection of COVID-19 using chest CT. Purpose To develop a fully automatic framework to detect COVID-19 using chest CT and evaluate its performance. Materials and Methods In this retrospective and multicenter study, a deep learning model, the COVID-19 detection neural network (COVNet), was developed to extract visual features from volumetric chest CT scans for the detection of COVID-19. CT scans of community-acquired pneumonia (CAP) and other non-pneumonia abnormalities were included to test the robustness of the model. The datasets were collected from six hospitals between August 2016 and February 2020. Diagnostic performance was assessed with the area under the receiver operating characteristic curve, sensitivity, and specificity. Results The collected dataset consisted of 4352 chest CT scans from 3322 patients. The average patient age (±standard deviation) was 49 years ± 15, and there were slightly more men than women (1838 vs 1484, respectively; P = .29). The per-scan sensitivity and specificity for detecting COVID-19 in the independent test set was 90% (95% confidence interval [CI]: 83%, 94%; 114 of 127 scans) and 96% (95% CI: 93%, 98%; 294 of 307 scans), respectively, with an area under the receiver operating characteristic curve of 0.96 (P < .001). The per-scan sensitivity and specificity for detecting CAP in the independent test set was 87% (152 of 175 scans) and 92% (239 of 259 scans), respectively, with an area under the receiver operating characteristic curve of 0.95 (95% CI: 0.93, 0.97). Conclusion A deep learning model can accurately detect coronavirus 2019 and differentiate it from community-acquired pneumonia and other lung conditions. © RSNA, 2020 Online supplemental material is available for this article.",2020,10.1148/radiol.2020200905,diagnosis,True
Using artificial intelligence to improve the diagnostic efficiency of pulmonologists in differentiating COVID-19 pneumonia from community-acquired pneumonia,"Coronavirus disease 2019 (COVID-19) has quickly turned into a global health problem. Computed tomography (CT) findings of COVID-19 pneumonia and community-acquired pneumonia (CAP) may be similar. Artificial intelligence (AI) is a popular topic among medical imaging techniques and has caused significant developments in diagnostic techniques. This retrospective study aims to analyze the contribution of AI to the diagnostic performance of pulmonologists in distinguishing COVID-19 pneumonia from CAP using CT scans. A deep learning-based AI model was created to be utilized in the detection of COVID-19, which extracted visual data from volumetric CT scans. The final data set covered a total of 2496 scans (887 patients), which included 1428 (57.2%) from the COVID-19 group and 1068 (42.8%) from the CAP group. CT slices were classified into training, validation, and test datasets in an 8:1:1. The independent test data set was analyzed by comparing the performance of four pulmonologists in differentiating COVID-19 pneumonia both with and without the help of the AI. The accuracy, sensitivity, and specificity values of the proposed AI model for determining COVID-19 in the independent test data set were 93.2%, 85.8%, and 99.3%, respectively, with the area under the receiver operating characteristic curve of 0.984. With the assistance of the AI, the pulmonologists accomplished a higher mean accuracy (88.9% vs. 79.9%, p < 0.001), sensitivity (79.1% vs. 70%, p < 0.001), and specificity (96.5% vs. 87.5%, p < 0.001). AI support significantly increases the diagnostic efficiency of pulmonologists in the diagnosis of COVID-19 via CT. Studies in the future should focus on real-time applications of AI to fight the COVID-19 infection.",2022,10.1002/jmv.27777,diagnosis,True
Using Auto-Segmentation to Reduce Contouring and Dose Inconsistency in Clinical Trials: The Simulated Impact on RTOG 0617,"PURPOSE: Contouring inconsistencies are known but understudied in clinical radiation therapy trials. We applied auto-contouring to the Radiation Therapy Oncology Group (RTOG) 0617 dose escalation trial data. We hypothesized that the trial heart doses were higher than reported due to inconsistent and insufficient heart segmentation. We tested our hypothesis by comparing doses between deep-learning (DL) segmented hearts and trial hearts. METHODS AND MATERIALS: The RTOG 0617 data were downloaded from The Cancer Imaging Archive; the 442 patients with trial hearts and dose distributions were included. All hearts were resegmented using our DL pipeline and quality assured to meet the requirements for clinical implementation. Dose (V5%, V30%, and mean heart dose) was compared between the 2 sets of hearts (Wilcoxon signed-rank test). Each dose metric was associated with overall survival (Cox proportional hazards). Lastly, 18 volume similarity metrics were assessed for the hearts and correlated with |Dose(DL) - Dose(RTOG0617)| (linear regression; significance: P ≤ .0028; corrected for 18 tests). RESULTS: Dose metrics were significantly higher for DL hearts compared with trial hearts (eg, mean heart dose: 15 Gy vs 12 Gy; P = 5.8E-16). All 3 DL heart dose metrics were stronger overall survival predictors than those of the trial hearts (median, P = 2.8E-5 vs 2.0E-4). Thirteen similarity metrics explained |Dose(DL) - Dose(RTOG0617)|; the axial distance between the 2 centers of mass was the strongest predictor (CENT(Axial); median, R(2) = 0.47; P = 6.1E-62). CENT(Axial) agreed with the qualitatively identified inconsistencies in the superior direction. The trial's qualitative heart contouring score was not correlated with |Dose(DL) - Dose(RTOG0617)| (median, R(2) = 0.01; P = .02) or with any of the similarity metrics (median, Rs = 0.13 [range, -0.22 to 0.31]). CONCLUSIONS: Using a coherent heart definition, as enabled through our open-source DL algorithm, the trial heart doses in RTOG 0617 were found to be significantly higher than previously reported, which may have led to an even more rapid mortality accumulation. Auto-segmentation is likely to reduce contouring and dose inconsistencies and increase the quality of clinical RT trials.",2021,10.1016/j.ijrobp.2020.11.011,treatment,True
Using contrast-enhanced CT and non-contrast-enhanced CT to predict EGFR mutation status in NSCLC patients-a radiomics nomogram analysis,"OBJECTIVES: To develop and validate a general radiomics nomogram capable of identifying EGFR mutation status in non-small cell lung cancer (NSCLC) patients, regardless of patient with either contrast-enhanced CT (CE-CT) or non-contrast-enhanced CT (NE-CT). METHODS: A total of 412 NSCLC patients were retrospectively enrolled in this study. Patients' radiomics features not significantly different between NE-CT and CE-CT were defined as general features, and were further used to construct the general radiomics signature. Fivefold cross-validation was used to select the best machine learning algorithm. Finally, a general radiomics nomogram was developed using general radiomics signature, and clinical and radiological characteristics. Two groups of data collected at different time periods were used as two test sets to access the discrimination and clinical usefulness. Area under the receiver operating characteristic curve (ROC-AUC) was applied to performance evaluation. RESULT: The general radiomics signature yielded the highest AUC of 0.756 and 0.739 in the two test sets, respectively. When applying to same type of CT, the performance of general radiomics signature was always similar to or higher than that of models built using only NE-CT or CE-CT features. The general radiomics nomogram combining general radiomics signature, smoking history, emphysema, and ILD achieved higher performance whether applying to NE-CT or CE-CT (test set 1, AUC = 0.833 and 0.842; test set 2, AUC = 0.839 and 0.850). CONCLUSIONS: Our work demonstrated that using general features to construct radiomics signature and nomogram could help identify EGFR mutation status of NSCLC patients and expand its scope of clinical application. KEY POINTS: • General features were proposed to construct general radiomics signature using different types of CT of different patients at the same time to identify EGFR mutation status of NSCLC patients. • The general radiomics nomogram based on general radiomics signature, and clinical and radiological characteristics could identify EGFR mutation status of patients with NSCLC and outperformed the general radiomics signature. • The general radiomics nomogram had a wider scope of clinical application; no matter which of NE-CT and CE-CT the patient has, its EGFR mutation status could be predicted.",2022,10.1007/s00330-021-08366-y,diagnosis,True
Using Deep Learning for Classification of Lung Nodules on Computed Tomography Images,"Lung cancer is the most common cancer that cannot be ignored and cause death with late health care. Currently, CT can be used to help doctors detect the lung cancer in the early stages. In many cases, the diagnosis of identifying the lung cancer depends on the experience of doctors, which may ignore some patients and cause some problems. Deep learning has been proved as a popular and powerful method in many medical imaging diagnosis areas. In this paper, three types of deep neural networks (e.g., CNN, DNN, and SAE) are designed for lung cancer calcification. Those networks are applied to the CT image classification task with some modification for the benign and malignant lung nodules. Those networks were evaluated on the LIDC-IDRI database. The experimental results show that the CNN network archived the best performance with an accuracy of 84.15%, sensitivity of 83.96%, and specificity of 84.32%, which has the best result among the three networks.",2017,10.1155/2017/8314740,diagnosis,True
Using deep learning to model the biological dose prediction on bulky lung cancer patients of partial stereotactic ablation radiotherapy,"PURPOSE: To develop a biological dose prediction model considering tissue bio-reactions in addition to patient anatomy for achieving a more comprehensive evaluation of tumor control and promoting the automatic planning of bulky lung cancer. METHODS: A database containing images and partial stereotactic ablation boost radiotherapy (P-SABR) plans of 94 bulky lung cancer patients was studied. Patient-specific parameters of gross tumor boost volume (GTVb), planning gross target volume (PGTV), and identified organs at risk (OARs) were extracted via Numpy and simple ITK. The original dose and structure maps for P-SABR patients were resampled to have a voxel resolution of 3.9 × 3.9 × 3 mm(3) . Biological equivalent dose (BED) distributions were reprogrammed based on physical dose volumes. A developed deep learning architecture, Nestnet, was adopted as the training framework. We utilized two approaches for data organization to correlate the structures and BED: (a) BED programming before training model (B-Nestnet); (b) BED programming after the training process (D-B Nestnet). The early-stop mechanism was adopted on the validation set to avoid overfitting. The evaluation criteria of predictive accuracy contain the minimum BED of GTVb and PGTV, the maximum and the mean BED of all targets, BED-volume metrics. For comparison, we also used the original Unet for BED prediction. The absolute differences were statistically analyzed with the paired-samples t test. RESULTS: The statistical outcomes demonstrate that D-B Nestnet model predicts biological dose distributions accurately. The average absolute biases of [max, mean] BED for GTVb, PGTV are [2.1%, 3.3%] and [2.1%, 4.7%], respectively. Averaging across most of OARs, the D-B Nestnet model is capable of predicting the errors of the max and mean BED within 6.3% and 6.1%, respectively. While the compared models performed worse with averaged max and mean BED prediction errors surpassing 10% on some specific OARs. CONCLUSIONS: The study developed a D-B Nestnet model capable of predicting BED distribution accurately for bulky lung cancer patients in P-SABR. The predicted BED map enables a quick intuitive evaluation of tumor ablation, modification of the ablation range to improve BED of tumor targets, and quality assessment. It represents a major step forward toward automated P-SABR planning on bulky lung cancer in real clinical practice.",2020,10.1002/mp.14518,treatment,True
Using deep-learning techniques for pulmonary-thoracic segmentations and improvement of pneumonia diagnosis in pediatric chest radiographs,"PURPOSE: To evaluate the efficacy of a deep-learning model to segment the lung and thorax regions in pediatric chest X-rays (CXRs). Validating the diagnosis of bacterial or viral pneumonia could be improved after lung segmentation. MATERIALS AND METHODS: A clinical-pediatric CXR set including 1351 patients was proposed to develop a deep-learning model for the pulmonary-thoracic segmentations. Model performance was evaluated by Jaccard's similarity coefficient (JSC) and Dice's coefficient (DC). Two adult CXR sets were used to assess the model's generalizability. According to the pulmonary-thoracic ratio, Pearson's correlation coefficient and the Bland-Altman plot were generated to demonstrate the correlation and agreement between manual and automatic segmentations. The receiver operating characteristic curves and areas under the curve (AUCs) were used to compare the pneumonia classification performance based on the lung-extracted images with that based on the original images. RESULTS: The model achieved JSCs of 0.910 and 0.950, DCs of 0.948 and 0.974 for lung and thorax segmentations, respectively. Pearson's r = 0.96, P < .0001. In the Bland-Altman plot, the mean difference was 0.0025 with a 95% confidence interval of (-0.0451, 0.0501). For testing with two adult CXR sets, the JSCs were 0.903 and 0.888, respectively, while the DCs were 0.948 and 0.937, respectively. After lung segmentation, the AUC of a classifier to identify bacterial or viral pneumonia increased from 0.815 to 0.879. CONCLUSION: We built a pediatric CXR dataset and exploited a deep-learning model for accurate pulmonary-thoracic segmentations. Lung segmentation can notably improve the diagnosis of bacterial or viral pneumonia.",2019,10.1002/ppul.24431,diagnosis,True
Using machine learning algorithms to review computed tomography scans and assess risk for cardiovascular disease: Retrospective analysis from the National Lung Screening Trial (NLST),"BACKGROUND: The National Lung Screening Trial (NLST) demonstrated that annual screening with low dose CT in high-risk population was associated with reduction in lung cancer mortality. Nonetheless, the leading cause of mortality in the study was from cardiovascular diseases. PURPOSE: To determine whether the used machine learning automatic algorithms assessing coronary calcium score (CCS), level of liver steatosis and emphysema percentage in the lungs are good predictors of cardiovascular disease (CVD) mortality and incidence when applied on low dose CT scans. MATERIALS AND METHODS: Three fully automated machine learning algorithms were used to assess CCS, level of liver steatosis and emphysema percentage in the lung. The algorithms were used on low-dose computed tomography scans acquired from 12,332 participants in NLST. RESULTS: In a multivariate analysis, association between the three algorithm scores and CVD mortality have shown an OR of 1.72 (p = 0.003), 2.62 (p < 0.0001) for CCS scores of 101-400 and above 400 respectively, and an OR of 1.12 (p = 0.044) for level of liver steatosis. Similar results were shown for the incidence of CVD, OR of 1.96 (p < 0.0001), 4.94 (p < 0.0001) for CCS scores of 101-400 and above 400 respectively. Also, emphysema percentage demonstrated an OR of 0.89 (p < 0.0001). Similar results are shown for univariate analyses of the algorithms. CONCLUSION: The three automated machine learning algorithms could help physicians to assess the incidence and risk of CVD mortality in this specific population. Application of these algorithms to existing LDCT scans can provide valuable health care information and assist in future research.",2020,10.1371/journal.pone.0236021,diagnosis,True
Using Radiomics as Prior Knowledge for Thorax Disease Classification and Localization in Chest X-rays,"Chest X-ray becomes one of the most common medical diagnoses due to its noninvasiveness. The number of chest X-ray images has skyrocketed, but reading chest X-rays still have been manually performed by radiologists, which creates huge burnouts and delays. Traditionally, radiomics, as a subfield of radiology that can extract a large number of quantitative features from medical images, demonstrates its potential to facilitate medical imaging diagnosis before the deep learning era. In this paper, we develop an end-to-end framework, ChexRadiNet, that can utilize the radiomics features to improve the abnormality classification performance. Specifically, ChexRadiNet first applies a light-weight but efficient triplet-attention mechanism to classify the chest X-rays and highlight the abnormal regions. Then it uses the generated class activation map to extract radiomic features, which further guides our model to learn more robust image features. After a number of iterations and with the help of radiomic features, our framework can converge to more accurate image regions. We evaluate the ChexRadiNet framework using three public datasets: NIH ChestX-ray, CheXpert, and MIMIC-CXR. We find that ChexRadiNet outperforms the state-of-the-art on both disease detection (0.843 in AUC) and localization (0.679 in T(IoU) = 0.1). We make the code publicly available at https://github. com/bionlplab/lung_disease_detection_amia2021, with the hope that this method can facilitate the development of automatic systems with a higher-level understanding of the radiological world.",2021,,diagnosis,False
Validation of a Deep Learning Algorithm for the Detection of Malignant Pulmonary Nodules in Chest Radiographs,"IMPORTANCE: The improvement of pulmonary nodule detection, which is a challenging task when using chest radiographs, may help to elevate the role of chest radiographs for the diagnosis of lung cancer. OBJECTIVE: To assess the performance of a deep learning-based nodule detection algorithm for the detection of lung cancer on chest radiographs from participants in the National Lung Screening Trial (NLST). DESIGN, SETTING, AND PARTICIPANTS: This diagnostic study used data from participants in the NLST ro assess the performance of a deep learning-based artificial intelligence (AI) algorithm for the detection of pulmonary nodules and lung cancer on chest radiographs using separate training (in-house) and validation (NLST) data sets. Baseline (T0) posteroanterior chest radiographs from 5485 participants (full T0 data set) were used to assess lung cancer detection performance, and a subset of 577 of these images (nodule data set) were used to assess nodule detection performance. Participants aged 55 to 74 years who currently or formerly (ie, quit within the past 15 years) smoked cigarettes for 30 pack-years or more were enrolled in the NLST at 23 US centers between August 2002 and April 2004. Information on lung cancer diagnoses was collected through December 31, 2009. Analyses were performed between August 20, 2019, and February 14, 2020. EXPOSURES: Abnormality scores produced by the AI algorithm. MAIN OUTCOMES AND MEASURES: The performance of an AI algorithm for the detection of lung nodules and lung cancer on radiographs, with lung cancer incidence and mortality as primary end points. RESULTS: A total of 5485 participants (mean [SD] age, 61.7 [5.0] years; 3030 men [55.2%]) were included, with a median follow-up duration of 6.5 years (interquartile range, 6.1-6.9 years). For the nodule data set, the sensitivity and specificity of the AI algorithm for the detection of pulmonary nodules were 86.2% (95% CI, 77.8%-94.6%) and 85.0% (95% CI, 81.9%-88.1%), respectively. For the detection of all cancers, the sensitivity was 75.0% (95% CI, 62.8%-87.2%), the specificity was 83.3% (95% CI, 82.3%-84.3%), the positive predictive value was 3.8% (95% CI, 2.6%-5.0%), and the negative predictive value was 99.8% (95% CI, 99.6%-99.9%). For the detection of malignant pulmonary nodules in all images of the full T0 data set, the sensitivity was 94.1% (95% CI, 86.2%-100.0%), the specificity was 83.3% (95% CI, 82.3%-84.3%), the positive predictive value was 3.4% (95% CI, 2.2%-4.5%), and the negative predictive value was 100.0% (95% CI, 99.9%-100.0%). In digital radiographs of the nodule data set, the AI algorithm had higher sensitivity (96.0% [95% CI, 88.3%-100.0%] vs 88.0% [95% CI, 75.3%-100.0%]; P = .32) and higher specificity (93.2% [95% CI, 89.9%-96.5%] vs 82.8% [95% CI, 77.8%-87.8%]; P = .001) for nodule detection compared with the NLST radiologists. For malignant pulmonary nodule detection on digital radiographs of the full T0 data set, the sensitivity of the AI algorithm was higher (100.0% [95% CI, 100.0%-100.0%] vs 94.1% [95% CI, 82.9%-100.0%]; P = .32) compared with the NLST radiologists, and the specificity (90.9% [95% CI, 89.6%-92.1%] vs 91.0% [95% CI, 89.7%-92.2%]; P = .91), positive predictive value (8.2% [95% CI, 4.4%-11.9%] vs 7.8% [95% CI, 4.1%-11.5%]; P = .65), and negative predictive value (100.0% [95% CI, 100.0%-100.0%] vs 99.9% [95% CI, 99.8%-100.0%]; P = .32) were similar to those of NLST radiologists. CONCLUSIONS AND RELEVANCE: In this study, the AI algorithm performed better than NLST radiologists for the detection of pulmonary nodules on digital radiographs. When used as a second reader, the AI algorithm may help to detect lung cancer.",2020,10.1001/jamanetworkopen.2020.17135,diagnosis,False
"Validation of a deep learning computer aided system for CT based lung nodule detection, classification, and growth rate estimation in a routine clinical population","OBJECTIVE: In this study, we evaluated a commercially available computer assisted diagnosis system (CAD). The deep learning algorithm of the CAD was trained with a lung cancer screening cohort and developed for detection, classification, quantification, and growth of actionable pulmonary nodules on chest CT scans. Here, we evaluated the CAD in a retrospective cohort of a routine clinical population. MATERIALS AND METHODS: In total, a number of 337 scans of 314 different subjects with reported nodules of 3-30 mm in size were included into the evaluation. Two independent thoracic radiologists alternately reviewed scans with or without CAD assistance to detect, classify, segment, and register pulmonary nodules. A third, more experienced, radiologist served as an adjudicator. In addition, the cohort was analyzed by the CAD alone. The study cohort was divided into five different groups: 1) 178 CT studies without reported pulmonary nodules, 2) 95 studies with 1-10 pulmonary nodules, 23 studies from the same patients with 3) baseline and 4) follow-up studies, and 5) 18 CT studies with subsolid nodules. A reference standard for nodules was based on majority consensus with the third thoracic radiologist as required. Sensitivity, false positive (FP) rate and Dice inter-reader coefficient were calculated. RESULTS: After analysis of 470 pulmonary nodules, the sensitivity readings for radiologists without CAD and radiologist with CAD, were 71.9% (95% CI: 66.0%, 77.0%) and 80.3% (95% CI: 75.2%, 85.0%) (p < 0.01), with average FP rate of 0.11 and 0.16 per CT scan, respectively. Accuracy and kappa of CAD for classifying solid vs sub-solid nodules was 94.2% and 0.77, respectively. Average inter-reader Dice coefficient for nodule segmentation was 0.83 (95% CI: 0.39, 0.96) and 0.86 (95% CI: 0.51, 0.95) for CAD versus readers. Mean growth percentage discrepancy of readers and CAD alone was 1.30 (95% CI: 1.02, 2.21) and 1.35 (95% CI: 1.01, 4.99), respectively. CONCLUSION: The applied CAD significantly increased radiologist's detection of actionable nodules yet also minimally increasing the false positive rate. The CAD can automatically classify and quantify nodules and calculate nodule growth rate in a cohort of a routine clinical population. Results suggest this Deep Learning software has the potential to assist chest radiologists in the tasks of pulmonary nodule detection and management within their routine clinical practice.",2022,10.1371/journal.pone.0266799,diagnosis,True
Validation of expert system enhanced deep learning algorithm for automated screening for COVID-Pneumonia on chest X-rays,"SARS-CoV2 pandemic exposed the limitations of artificial intelligence based medical imaging systems. Earlier in the pandemic, the absence of sufficient training data prevented effective deep learning (DL) solutions for the diagnosis of COVID-19 based on X-Ray data. Here, addressing the lacunae in existing literature and algorithms with the paucity of initial training data; we describe CovBaseAI, an explainable tool using an ensemble of three DL models and an expert decision system (EDS) for COVID-Pneumonia diagnosis, trained entirely on pre-COVID-19 datasets. The performance and explainability of CovBaseAI was primarily validated on two independent datasets. Firstly, 1401 randomly selected CxR from an Indian quarantine center to assess effectiveness in excluding radiological COVID-Pneumonia requiring higher care. Second, curated dataset; 434 RT-PCR positive cases and 471 non-COVID/Normal historical scans, to assess performance in advanced medical settings. CovBaseAI had an accuracy of 87% with a negative predictive value of 98% in the quarantine-center data. However, sensitivity was 0.66-0.90 taking RT-PCR/radiologist opinion as ground truth. This work provides new insights on the usage of EDS with DL methods and the ability of algorithms to confidently predict COVID-Pneumonia while reinforcing the established learning; that benchmarking based on RT-PCR may not serve as reliable ground truth in radiological diagnosis. Such tools can pave the path for multi-modal high throughput detection of COVID-Pneumonia in screening and referral.",2021,10.1038/s41598-021-02003-w,diagnosis,False
Validation of low-dose lung cancer PET-CT protocol and PET image improvement using machine learning,"PURPOSE: To conduct a simplified lesion-detection task of a low-dose (LD) PET-CT protocol for frequent lung screening using 30% of the effective PETCT dose and to investigate the feasibility of increasing clinical value of low-statistics scans using machine learning. METHODS: We acquired 33 SD PET images, of which 13 had actual LD (ALD) PET, and simulated LD (SLD) PET images at seven different count levels from the SD PET scans. We employed image quality transfer (IQT), a machine learning algorithm that performs patch-regression to map parameters from low-quality to high-quality images. At each count level, patches extracted from 23 pairs of SD/SLD PET images were used to train three IQT models - global linear, single tree, and random forest regressions with cubic patch sizes of 3 and 5 voxels. The models were then used to estimate SD images from LD images at each count level for 10 unseen subjects. Lesion-detection task was carried out on matched lesion-present and lesion-absent images. RESULTS: LD PET-CT protocol yielded lesion detectability with sensitivity of 0.98 and specificity of 1. Random forest algorithm with cubic patch size of 5 allowed further 11.7% reduction in the effective PETCT dose without compromising lesion detectability, but underestimated SUV by 30%. CONCLUSION: LD PET-CT protocol was validated for lesion detection using ALD PET scans. Substantial image quality improvement or additional dose reduction while preserving clinical values can be achieved using machine learning methods though SUV quantification may be biased and adjustment of our research protocol is required for clinical use.",2021,10.1016/j.ejmp.2020.11.027,treatment,True
Value of a deep learning-based algorithm for detecting Lung-RADS category 4 nodules on chest radiographs in a health checkup population: estimation of the sample size for a randomized controlled trial,"OBJECTIVE: To explore the value of a deep learning-based algorithm in detecting Lung CT Screening Reporting and Data System category 4 nodules on chest radiographs from an asymptomatic health checkup population. METHODS: Data from an annual retrospective cohort of individuals who underwent chest radiographs for health checkup purposes and chest CT scanning within 3 months were collected. Among 3073 individuals, 118 with category 4 nodules on CT were selected. A reader performance test was performed using those 118 radiographs and randomly selected 51 individuals without any nodules. Four radiologists independently evaluated the radiographs without and with the results of the algorithm; and sensitivities/specificities were compared. The sample size needed to confirm the difference in detection rates was calculated, i.e., the number of true-positive radiographs divided by the total number of radiographs. RESULTS: The sensitivity of the radiologists substantially increased aided by the algorithm (38.8% [183/472] to 45.1% [213/472]; p < .001) without significant change in specificity (94.1% [192/204] vs. 92.2% [188/204]; p = .22). Pooled radiologists detected more nodules with the algorithm (32.0% [156/488] vs. 38.9% [190/488]; p < .001), without alteration of false-positive rates (0.09 [62/676], both). Pooled detection rates for the annual cohort were 1.49% (183/12,292) and 1.73% (213/12,292) without and with the algorithm, respectively. A sample size of 41,776 in each arm would be required to demonstrate significant detection rate difference with < 5% type I error and > 80% power. CONCLUSION: Although readers substantially increased sensitivity in detecting nodules on chest radiographs from a health checkup population aided by the algorithm, detection rate difference was only 0.24%, requiring a sample size >80,000 for a randomized controlled trial. KEY POINTS: • Aided by a deep learning algorithm, pooled radiologists improved their sensitivity in detecting Lung-RADS category 4 nodules on chest radiographs from a health checkup population (38.8% [183/472] to 45.1% [213/472]; p < .001), without increasing false-positive rate. • The prevalence of the Lung-RADS category 4 nodules was 3.8% (118/3073) on the population, resulting in only 0.24% increase of the detection rate for the radiologists with assistance of the algorithm. • To confirm the significant detection rate increase by a randomized controlled trial, a sample size of 84,000 would be required.",2022,10.1007/s00330-021-08162-8,diagnosis,True
Verification of the machine delivery parameters of a treatment plan via deep learning,"We developed a generative adversarial network (GAN)-based deep learning approach to estimate the multileaf collimator (MLC) aperture and corresponding monitor units (MUs) from a given 3D dose distribution. The proposed design of the adversarial network, which integrates a residual block into pix2pix framework, jointly trains a 'U-Net'-like architecture as the generator and a convolutional 'PatchGAN' classifier as the discriminator. 199 patients, including nasopharyngeal, lung and rectum, treated with intensity-modulated radiotherapy and volumetric-modulated arc therapy techniques were utilized to train the network. An additional 47 patients were used to test the prediction accuracy of the proposed deep learning model. The Dice similarity coefficient (DSC) was calculated to evaluate the similarity between the MLC aperture shapes obtained from the treatment planning system (TPS) and the deep learning prediction. The average and standard deviation of the bias between the TPS-generated MUs and predicted MUs was calculated to evaluate the MU prediction accuracy. In addition, the differences between TPS and deep learning-predicted MLC leaf positions were compared. The average and standard deviation of DSC was 0.94 ± 0.043 for 47 testing patients. The average deviation of predicted MUs from the planned MUs normalized to each beam or arc was within 2% for all the testing patients. The average deviation of the predicted MLC leaf positions was around one pixel for all the testing patients. Our results demonstrated the feasibility and reliability of the proposed approach. The proposed technique has strong potential to improve the efficiency and accuracy of the patient plan quality assurance process.",2020,10.1088/1361-6560/aba165,treatment,
Viral Pneumonia Screening on Chest X-Rays Using Confidence-Aware Anomaly Detection,"Clusters of viral pneumonia occurrences over a short period may be a harbinger of an outbreak or pandemic. Rapid and accurate detection of viral pneumonia using chest X-rays can be of significant value for large-scale screening and epidemic prevention, particularly when other more sophisticated imaging modalities are not readily accessible. However, the emergence of novel mutated viruses causes a substantial dataset shift, which can greatly limit the performance of classification-based approaches. In this paper, we formulate the task of differentiating viral pneumonia from non-viral pneumonia and healthy controls into a one-class classification-based anomaly detection problem. We therefore propose the confidence-aware anomaly detection (CAAD) model, which consists of a shared feature extractor, an anomaly detection module, and a confidence prediction module. If the anomaly score produced by the anomaly detection module is large enough, or the confidence score estimated by the confidence prediction module is small enough, the input will be accepted as an anomaly case (i.e., viral pneumonia). The major advantage of our approach over binary classification is that we avoid modeling individual viral pneumonia classes explicitly and treat all known viral pneumonia cases as anomalies to improve the one-class model. The proposed model outperforms binary classification models on the clinical X-VIRAL dataset that contains 5,977 viral pneumonia (no COVID-19) cases, 37,393 non-viral pneumonia or healthy cases. Moreover, when directly testing on the X-COVID dataset that contains 106 COVID-19 cases and 107 normal controls without any fine-tuning, our model achieves an AUC of 83.61% and sensitivity of 71.70%, which is comparable to the performance of radiologists reported in the literature.",2021,10.1109/tmi.2020.3040950,diagnosis,False
Visual and software-based quantitative chest CT assessment of COVID-19: correlation with clinical findings,"PURPOSE: The aim of this study was to evaluate visual and software-based quantitative assessment of parenchymal changes and normal lung parenchyma in patients with coronavirus disease 2019 (COVID-19) pneumonia. The secondary aim of the study was to compare the radiologic findings with clinical and laboratory data. METHODS: Patients with COVID-19 who underwent chest computed tomography (CT) between March 11, 2020 and April 15, 2020 were retrospectively evaluated. Clinical and laboratory findings of patients with abnormal findings on chest CT and PCR-evidence of COVID-19 infection were recorded. Visual quantitative assessment score (VQAS) was performed according to the extent of lung opacities. Software-based quantitative assessment of the normal lung parenchyma percentage (SQNLP) was automatically quantified by a deep learning software. The presence of consolidation and crazy paving pattern (CPP) was also recorded. Statistical analyses were performed to evaluate the correlation between quantitative radiologic assessments, and clinical and laboratory findings, as well as to determine the predictive utility of radiologic findings for estimating severe pneumonia and admission to intensive care unit (ICU). RESULTS: A total of 90 patients were enrolled. Both VQAS and SQNLP were significantly correlated with multiple clinical parameters. While VQAS >8.5 (sensitivity, 84.2%; specificity, 80.3%) and SQNLP <82.45% (sensitivity, 83.1%; specificity, 84.2%) were related to severe pneumonia, VQAS >9.5 (sensitivity, 93.3%; specificity, 86.5%) and SQNLP <81.1% (sensitivity, 86.5%; specificity, 86.7%) were predictive of ICU admission. Both consolidation and CPP were more commonly seen in patients with severe pneumonia than patients with nonsevere pneumonia (P = 0.197 for consolidation; P < 0.001 for CPP). Moreover, the presence of CPP showed high specificity (97.2%) for severe pneumonia. CONCLUSION: Both SQNLP and VQAS were significantly related to the clinical findings, highlighting their clinical utility in predicting severe pneumonia, ICU admission, length of hospital stay, and management of the disease. On the other hand, presence of CPP has high specificity for severe COVID-19 pneumonia.",2020,10.5152/dir.2020.20407,diagnosis,True
Volume-of-Interest Aware Deep Neural Networks for Rapid Chest CT-Based COVID-19 Patient Risk Assessment,"Since December 2019, the world has been devastated by the Coronavirus Disease 2019 (COVID-19) pandemic. Emergency Departments have been experiencing situations of urgency where clinical experts, without long experience and mature means in the fight against COVID-19, have to rapidly decide the most proper patient treatment. In this context, we introduce an artificially intelligent tool for effective and efficient Computed Tomography (CT)-based risk assessment to improve treatment and patient care. In this paper, we introduce a data-driven approach built on top of volume-of-interest aware deep neural networks for automatic COVID-19 patient risk assessment (discharged, hospitalized, intensive care unit) based on lung infection quantization through segmentation and, subsequently, CT classification. We tackle the high and varying dimensionality of the CT input by detecting and analyzing only a sub-volume of the CT, the Volume-of-Interest (VoI). Differently from recent strategies that consider infected CT slices without requiring any spatial coherency between them, or use the whole lung volume by applying abrupt and lossy volume down-sampling, we assess only the ""most infected volume"" composed of slices at its original spatial resolution. To achieve the above, we create, present and publish a new labeled and annotated CT dataset with 626 CT samples from COVID-19 patients. The comparison against such strategies proves the effectiveness of our VoI-based approach. We achieve remarkable performance on patient risk assessment evaluated on balanced data by reaching 88.88%, 89.77%, 94.73% and 88.88% accuracy, sensitivity, specificity and F1-score, respectively.",2021,10.3390/ijerph18062842,prognosis,True
Volumetric segmentation of ground glass nodule based on 3D attentional cascaded residual U-Net and conditional random field,"BACKGROUND: Ground glass nodule (GGN) segmentation is one of the important and challenging tasks in diagnosing early-stage lung adenocarcinomas. Manually delineating of 3D GGN in a computed tomography (CT) image is a subjective, laborious, and tedious task, which presents poor repeatability. PURPOSE: To reduce the annotation burden and improve the segmentation performance, this study proposes a 3D deep learning-based volumetric segmentation model to segment the GGN in CT images. METHODS: A total of 379 GGNs were retrospectively collected from the public database, Shanghai Pulmonary Hospital (SHPH), and Fudan University Shanghai Cancer Center (FUSCC). First, a series of image preprocessing techniques involving image resampling, intensity normalization, 3D nodule patch cropping, and data augmentation, were adopted to generate the input images for the deep learning model by using CT scans. Then, a 3D attentional cascaded residual network (ACRU-Net) was proposed to develop the deep learning-based segmentation model by using the residual network and the atrous spatial pyramid pooling module. To improve the model performance, a voxel-based conditional random field (CRF) method was used to optimize the segmentation results. Finally, a balanced cross-entropy and Dice combined loss function was applied to train and build the segmentation model. RESULTS: Testing on SHPH and FUSCC datasets, the proposed method generates the Dice coefficients of 0.721 ± 0.167 and 0.733 ± 0.100, respectively, which are higher than those of 3D residual U-Net and ACRU-Net without CRF optimization. CONCLUSIONS: The results demonstrated that combining 3D ACRU-Net and CRF effectively improved the GGN segmentation performance. The proposed segmentation model may provide a potential tool to help the radiologist in the segmentation and diagnosis of 3D GGN.",2022,10.1002/mp.15423,diagnosis,True
Vulnerability of deep neural networks for detecting COVID-19 cases from chest X-ray images to universal adversarial attacks,"Owing the epidemic of the novel coronavirus disease 2019 (COVID-19), chest X-ray computed tomography imaging is being used for effectively screening COVID-19 patients. The development of computer-aided systems based on deep neural networks (DNNs) has become an advanced open source to rapidly and accurately detect COVID-19 cases because the need for expert radiologists, who are limited in number, forms a bottleneck for screening. However, thus far, the vulnerability of DNN-based systems has been poorly evaluated, although realistic and high-risk attacks using universal adversarial perturbation (UAP), a single (input image agnostic) perturbation that can induce DNN failure in most classification tasks, are available. Thus, we focus on representative DNN models for detecting COVID-19 cases from chest X-ray images and evaluate their vulnerability to UAPs. We consider non-targeted UAPs, which cause a task failure, resulting in an input being assigned an incorrect label, and targeted UAPs, which cause the DNN to classify an input into a specific class. The results demonstrate that the models are vulnerable to non-targeted and targeted UAPs, even in the case of small UAPs. In particular, the 2% norm of the UAPs to the average norm of an image in the image dataset achieves >85% and >90% success rates for the non-targeted and targeted attacks, respectively. Owing to the non-targeted UAPs, the DNN models judge most chest X-ray images as COVID-19 cases. The targeted UAPs allow the DNN models to classify most chest X-ray images into a specified target class. The results indicate that careful consideration is required in practical applications of DNNs to COVID-19 diagnosis; in particular, they emphasize the need for strategies to address security concerns. As an example, we show that iterative fine-tuning of DNN models using UAPs improves the robustness of DNN models against UAPs.",2020,10.1371/journal.pone.0243963,diagnosis,False
Vulture-Based AdaBoost-Feedforward Neural Frame Work for COVID-19 Prediction and Severity Analysis System,"In today's scenario, many scientists and medical researchers have been involved in deep research for discovering the desired medicine to reduce the spread of COVID-19 disease. However, still, it is not the end. Hence, predicting the COVID possibility in an early stage is the most required matter to reduce the death risks. Therefore, many researchers have focused on designing an early prediction mechanism in the basis of deep learning (DL), machine learning (Ml), etc., on detecting the COVID virus and severity in the human body in an earlier stage. However, the complexity of X-ray images has made it difficult to attain the finest prediction accuracy. Hence, the present research work has aimed to develop a novel Vulture Based Adaboost-Feedforward Neural (VbAFN) scheme to forecast the COVID-19 severity early. Here, the chest X-ray images were employed to identify the COVID risk feature in humans. The preprocessing function is done in the initial phase; the error-free data is imported to the classification layer for the feature extraction and segmentation process. This investigation aims to track and segment the affected parts from the trained X-ray images by the vulture fitness and to segment them with a good exactness rate. Subsequently, the designed model has gained a better segmentation accuracy of 99.9% and a lower error rate of 0.0145, which is better than other compared models. Hence, this proposed model in medical applications will offer the finest results.",2022,10.1007/s12539-022-00505-3,diagnosis,False
Wavelet decomposition facilitates training on small datasets for medical image classification by deep learning,"The adoption of low-dose computed tomography (LDCT) as the standard of care for lung cancer screening results in decreased mortality rates in high-risk population while increasing false-positive rate. Convolutional neural networks provide an ideal opportunity to improve malignant nodule detection; however, due to the lack of large adjudicated medical datasets these networks suffer from poor generalizability and overfitting. Using computed tomography images of the thorax from the National Lung Screening Trial (NLST), we compared discrete wavelet transforms (DWTs) against convolutional layers found in a CNN in order to evaluate their ability to classify suspicious lung nodules as either malignant or benign. We explored the use of the DWT as an alternative to the convolutional operations within CNNs in order to decrease the number of parameters to be estimated during training and reduce the risk of overfitting. We found that multi-level DWT performed better than convolutional layers when multiple kernel resolutions were utilized, yielding areas under the receiver-operating curve (AUC) of 94% and 92%, respectively. Furthermore, we found that multi-level DWT reduced the number of network parameters requiring evaluation when compared to a CNN and had a substantially faster convergence rate. We conclude that utilizing multi-level DWT composition in place of early convolutional layers within a DNN may improve for image classification in data-limited domains.",2021,10.1007/s00418-020-01961-y,diagnosis,True
Weakly unsupervised conditional generative adversarial network for image-based prognostic prediction for COVID-19 patients based on chest CT,"Because of the rapid spread and wide range of the clinical manifestations of the coronavirus disease 2019 (COVID-19), fast and accurate estimation of the disease progression and mortality is vital for the management of the patients. Currently available image-based prognostic predictors for patients with COVID-19 are largely limited to semi-automated schemes with manually designed features and supervised learning, and the survival analysis is largely limited to logistic regression. We developed a weakly unsupervised conditional generative adversarial network, called pix2surv, which can be trained to estimate the time-to-event information for survival analysis directly from the chest computed tomography (CT) images of a patient. We show that the performance of pix2surv based on CT images significantly outperforms those of existing laboratory tests and image-based visual and quantitative predictors in estimating the disease progression and mortality of COVID-19 patients. Thus, pix2surv is a promising approach for performing image-based prognostic predictions.",2021,10.1016/j.media.2021.102159,diagnosis,True
Weakly-supervised lesion analysis with a CNN-based framework for COVID-19,"Objective.Lesions of COVID-19 can be clearly visualized using chest CT images, and hence provide valuable evidence for clinicians when making a diagnosis. However, due to the variety of COVID-19 lesions and the complexity of the manual delineation procedure, automatic analysis of lesions with unknown and diverse types from a CT image remains a challenging task. In this paper we propose a weakly-supervised framework for this task requiring only a series of normal and abnormal CT images without the need for annotations of the specific locations and types of lesions.Approach.A deep learning-based diagnosis branch is employed for classification of the CT image and then a lesion identification branch is leveraged to capture multiple types of lesions.Main Results.Our framework is verified on publicly available datasets and CT data collected from 13 patients of the First Affiliated Hospital of Shantou University Medical College, China. The results show that the proposed framework can achieve state-of-the-art diagnosis prediction, and the extracted lesion features are capable of distinguishing between lesions showing ground glass opacity and consolidation.Significance.The proposed approach integrates COVID-19 positive diagnosis and lesion analysis into a unified framework without extra pixel-wise supervision. Further exploration also demonstrates that this framework has the potential to discover lesion types that have not been reported and can potentially be generalized to lesion detection of other chest-based diseases.",2021,10.1088/1361-6560/ac4316,diagnosis,True
xViTCOS: Explainable Vision Transformer Based COVID-19 Screening Using Radiography,"Objective: Since its outbreak, the rapid spread of COrona VIrus Disease 2019 (COVID-19) across the globe has pushed the health care system in many countries to the verge of collapse. Therefore, it is imperative to correctly identify COVID-19 positive patients and isolate them as soon as possible to contain the spread of the disease and reduce the ongoing burden on the healthcare system. The primary COVID-19 screening test, RT-PCR although accurate and reliable, has a long turn-around time. In the recent past, several researchers have demonstrated the use of Deep Learning (DL) methods on chest radiography (such as X-ray and CT) for COVID-19 detection. However, existing CNN based DL methods fail to capture the global context due to their inherent image-specific inductive bias. Methods: Motivated by this, in this work, we propose the use of vision transformers (instead of convolutional networks) for COVID-19 screening using the X-ray and CT images. We employ a multi-stage transfer learning technique to address the issue of data scarcity. Furthermore, we show that the features learned by our transformer networks are explainable. Results: We demonstrate that our method not only quantitatively outperforms the recent benchmarks but also focuses on meaningful regions in the images for detection (as confirmed by Radiologists), aiding not only in accurate diagnosis of COVID-19 but also in localization of the infected area. The code for our implementation can be found here - https://github.com/arnabkmondal/xViTCOS. Conclusion: The proposed method will help in timely identification of COVID-19 and efficient utilization of limited resources.",2022,10.1109/jtehm.2021.3134096,diagnosis,True
Fully automatic segmentation of right and left ventricle on short-axis cardiac MRI images,"Cardiac magnetic resonance imaging (CMR) is a widely used non-invasive imaging modality for evaluating cardiovascular diseases. CMR is the gold standard method for left and right ventricular functional assessment due to its ability to characterize myocardial structure and function and low intra- and inter-observer variability. However the post-processing segmentation during the functional evaluation is time-consuming and challenging. A fully automated segmentation method can assist the experts; therefore, they can do more efficient work. In this paper, a regression-based fully automated method is presented for the right- and left ventricle segmentation. For training and evaluation, our dataset contained MRI short-axis scans of 5570 patients, who underwent CMR examinations at Heart and Vascular Center, Semmelweis University Budapest. Our approach is novel and after training the state-of-the-art algorithm on our dataset, our algorithm proved to be superior on both of the ventricles. The evaluation metrics were the Dice index, Hausdorff distance and volume related parameters. We have achieved average Dice index for the left endocardium: 0.927, left epicardium: 0.940 and right endocardium: 0.873 on our dataset. We have also compared the performance of the algorithm to the human-level segmentation on both ventricles and it is similar to experienced readers for the left, and comparable for the right ventricle. We also evaluated the proposed algorithm on the ACDC dataset, which is publicly available, with and without transfer learning. The results on ACDC were also satisfying and similar to human observers. Our method is lightweight, fast to train and does not require more than 2 GB GPU memory for execution and training.",2020,10.1016/j.compmedimag.2020.101786,diagnosis,False
"Machine learning to predict the long-term risk of myocardial infarction and cardiac death based on clinical risk, coronary calcium, and epicardial adipose tissue: a prospective study","AIMS: Our aim was to evaluate the performance of machine learning (ML), integrating clinical parameters with coronary artery calcium (CAC), and automated epicardial adipose tissue (EAT) quantification, for the prediction of long-term risk of myocardial infarction (MI) and cardiac death in asymptomatic subjects. METHODS AND RESULTS: Our study included 1912 asymptomatic subjects [1117 (58.4%) male, age: 55.8 ± 9.1 years] from the prospective EISNER trial with long-term follow-up after CAC scoring. EAT volume and density were quantified using a fully automated deep learning method. ML extreme gradient boosting was trained using clinical co-variates, plasma lipid panel measurements, risk factors, CAC, aortic calcium, and automated EAT measures, and validated using repeated 10-fold cross validation. During mean follow-up of 14.5 ± 2 years, 76 events of MI and/or cardiac death occurred. ML obtained a significantly higher AUC than atherosclerotic cardiovascular disease (ASCVD) risk and CAC score for predicting events (ML: 0.82; ASCVD: 0.77; CAC: 0.77, P < 0.05 for all). Subjects with a higher ML score (by Youden's index) had high hazard of suffering events (HR: 10.38, P < 0.001); the relationships persisted in multivariable analysis including ASCVD-risk and CAC measures (HR: 2.94, P = 0.005). Age, ASCVD-risk, and CAC were prognostically important for both genders. Systolic blood pressure was more important than cholesterol in women, and the opposite in men. CONCLUSIONS: In this prospective study, machine learning used to integrate clinical and quantitative imaging-based variables significantly improves prediction of MI and cardiac death compared with standard clinical risk assessment. Following further validation, such a personalized paradigm could potentially be used to improve cardiovascular risk assessment.",2020,10.1093/cvr/cvz321,prognosis,True
