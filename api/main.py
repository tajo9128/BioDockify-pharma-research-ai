from fastapi import FastAPI, BackgroundTasks, HTTPException, Request
from contextlib import asynccontextmanager
import asyncio
from dotenv import load_dotenv
from fastapi.responses import JSONResponse
import os
import time

load_dotenv() # Load environment variables from .env

try:
    from runtime.sentry import setup_sentry
    setup_sentry()
except ImportError:
    pass

try:
    from runtime.monitoring import start_monitoring_server
except ImportError:
    pass

from pydantic import BaseModel
from typing import Dict, Any, Optional, List
import uuid

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    BioDockify Backend Lifespan Controller.
    Handles startup/shutdown logic for all modules.
    """
    await startup_event()
    yield
    logger.info("BioDockify Backend Shutdown.")

app = FastAPI(
    title="BioDockify API",
    description="Backend for BioDockify Pharma Research AI",
    version="v2.6.10",
    lifespan=lifespan
)

# Robust Imports
import logging
logger = logging.getLogger("biodockify_api")

try:
    from orchestration.planner.orchestrator import ResearchOrchestrator, OrchestratorConfig
    from orchestration.executor import ResearchExecutor
    logger.info("Orchestration modules loaded.")
except Exception as e:
    logger.error(f"Failed to load Orchestration modules: {e}")

try:
    from modules.analyst.analytics_engine import ResearchAnalyst
    logger.info("Analytics Engine loaded.")
except Exception as e:
    logger.error(f"Failed to load Analytics Engine: {e}")

try:
    from modules.backup import DriveClient, BackupManager
    logger.info("Backup Manager loaded.")
except Exception as e:
    logger.error(f"Failed to load Backup Manager: {e}")

try:
    from modules.literature.reviewer import CitationReviewer
    from modules.literature.scraper import LiteratureAggregator, LiteratureConfig
    from modules.literature.synthesis import get_synthesizer
    logger.info("Literature modules loaded.")
except Exception as e:
    logger.error(f"Failed to load Literature modules: {e}")

try:
    from modules.system.auth_manager import auth_manager
    logger.info("Auth Manager loaded.")
except Exception as e:
    logger.error(f"Failed to load Auth Manager: {e}")

from dataclasses import asdict

# Register NanoBot Hybrid Agent Routes
try:
    from api.routes.nanobot_routes import router as nanobot_router
    app.include_router(nanobot_router)
except ImportError as e:
    import logging
    logging.getLogger("biodockify_api").warning(f"NanoBot routes not loaded: {e}")

# Register Agent Zero Enhanced Routes (Memory, Skills, Spawn)
try:
    from api.routes.agent_enhanced_routes import router as agent_enhanced_router
    app.include_router(agent_enhanced_router)
except ImportError as e:
    import logging
    logging.getLogger("biodockify_api").warning(f"Agent Enhanced routes not loaded: {e}")

# Register Messaging Channels Routes (Telegram, WhatsApp, Discord, Feishu)
try:
    from api.routes.channels_routes import router as channels_router
    app.include_router(channels_router)
except ImportError as e:
    import logging
    logging.getLogger("biodockify_api").warning(f"Channels routes not loaded: {e}")

# Register Research Orchestration Routes (Status, WebSocket)
try:
    from api.routers.research import router as research_router
    app.include_router(research_router, prefix="/api/research", tags=["Research"])
except ImportError as e:
    import logging
    logging.getLogger("biodockify_api").warning(f"Research routes not loaded: {e}")

# Register RAG & Notebook Routes
try:
    from api.routes.rag_routes import router as rag_router
    app.include_router(rag_router)
except ImportError as e:
    import logging
    logging.getLogger("biodockify_api").warning(f"RAG routes not loaded: {e}")

# Register Enhanced Project Routes (Phase 11)
try:
    from api.routes.enhanced_project_routes import router as enhanced_project_router
    app.include_router(enhanced_project_router)
except ImportError as e:
    import logging
    logging.getLogger("biodockify_api").warning(f"Enhanced Project routes not loaded: {e}")

# Register Settings Routes (Universal API & Connection Test)
try:
    from api.routes.settings_routes import router as settings_router
    app.include_router(settings_router)
except ImportError as e:
    import logging
    logging.getLogger("biodockify_api").warning(f"Settings routes not loaded: {e}")

from fastapi.middleware.cors import CORSMiddleware

@app.get("/api/health")
async def health_check():
    return {"status": "healthy", "timestamp": time.time()}

# CORS Configuration - Whitelist specific origins for security
allowed_origins = [
    "http://localhost:3000",
    "http://localhost:8234",
    "http://127.0.0.1:3000",
    "http://127.0.0.1:8234",
    "tauri://localhost",  # Tauri desktop app
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["Content-Type", "Authorization", "X-Requested-With", "X-CSRF-Token"],
)

# -----------------------------------------------------------------------------
# Security Headers & CSRF Protection Middleware
# -----------------------------------------------------------------------------
@app.middleware("http")
async def add_security_headers(request: Request, call_next):
    # CSRF Check for state-changing methods
    if request.method in ["POST", "PUT", "DELETE", "PATCH"]:
        origin = request.headers.get("Origin")
        referer = request.headers.get("Referer")
        
        # Simple Origin/Referer check against allowed origins
        is_valid_origin = any(o in (origin or "") for o in allowed_origins)
        is_valid_referer = any(o in (referer or "") for o in allowed_origins)
        
        if not (is_valid_origin or is_valid_referer):
            # If it's a browser request (has origin/referer) but from wrong source
            if origin or referer:
                return JSONResponse(
                    status_code=403,
                    content={"detail": "CSRF verification failed: Invalid origin or referer"}
                )

    response = await call_next(request)
    
    # Essential Security Headers
    response.headers["Content-Security-Policy"] = (
        "default-src 'self'; "
        "script-src 'self' 'unsafe-inline' 'unsafe-eval'; "
        "style-src 'self' 'unsafe-inline' https://fonts.googleapis.com; "
        "font-src 'self' https://fonts.gstatic.com; "
        "img-src 'self' data: https:; "
        "connect-src 'self' ws: wss: http://localhost:8234 https://*.google.com;"
    )
    response.headers["X-Frame-Options"] = "DENY"
    response.headers["X-Content-Type-Options"] = "nosniff"
    response.headers["X-XSS-Protection"] = "1; mode=block"
    response.headers["Strict-Transport-Security"] = "max-age=31536000; includeSubDomains"
    response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    
    return response

# -----------------------------------------------------------------------------
# Rate Limiting & DoS Protection
# -----------------------------------------------------------------------------
REQUESTS_PER_MINUTE = 600
ip_request_counts = {}

@app.middleware("http")
async def rate_limit_middleware(request: Request, call_next):
    client_ip = request.client.host
    current_time = int(time.time() / 60)
    
    key = f"{client_ip}:{current_time}"
    ip_request_counts[key] = ip_request_counts.get(key, 0) + 1
    
    if ip_request_counts[key] > REQUESTS_PER_MINUTE:
        return JSONResponse(
            status_code=429,
            content={"detail": "Too many requests. Please try again later."}
        )
        
    # Periodic cleanup (crude)
    if len(ip_request_counts) > 10000:
        ip_request_counts.clear()
        
    return await call_next(request)

# Max Request Size Limit (10MB)
MAX_SIZE = 10 * 1024 * 1024
@app.middleware("http")
async def limit_request_size(request: Request, call_next):
    if request.method in ["POST", "PUT", "PATCH"]:
        content_length = request.headers.get("Content-Length")
        if content_length and int(content_length) > MAX_SIZE:
            return JSONResponse(
                status_code=413,
                content={"detail": "Request body too large"}
            )
    return await call_next(request)

# -----------------------------------------------------------------------------
# Global Error Handling & Resilience
# -----------------------------------------------------------------------------
from fastapi import Request
from fastapi import Request
import logging
import time
import requests
import functools
import psutil
import asyncio

import json  # logging and time already imported above

class JsonFormatter(logging.Formatter):
    """
    Formatter to output logs as JSON for easier parsing by monitoring tools.
    """
    def format(self, record):
        log_obj = {
            "timestamp": self.formatTime(record, self.datefmt),
            "level": record.levelname,
            "message": record.getMessage(),
            "module": record.module,
            "func": record.funcName
        }
        if record.exc_info:
            log_obj["exception"] = self.formatException(record.exc_info)
        return json.dumps(log_obj)

# Configure Root Logger
handler = logging.StreamHandler()
handler.setFormatter(JsonFormatter())
logging.basicConfig(level=logging.INFO, handlers=[handler], force=True)
logger = logging.getLogger("biodockify_api")

# --- Caching Decorator ---
# Simple in-memory cache for repeated idempotent LLM queries
# Note: For production, Redis is better, but LRU is fine for single-instance desktop app.
@functools.lru_cache(maxsize=100)
def cached_llm_request(params_tuple):
    # Wrapper to allow caching of requests based on hashable params
    method, url, json_str, headers_str, timeout = params_tuple
    import json
    return requests.request(
        method=method, 
        url=url, 
        json=json.loads(json_str), 
        headers=json.loads(headers_str) if headers_str else None, 
        timeout=timeout
    )



# -----------------------------------------------------------------------------
# V2 AGENT ENDPOINTS (Phase 16)
# -----------------------------------------------------------------------------

class AgentGoal(BaseModel):
    goal: str
    mode: str = "auto" # auto, semi-autonomous, autonomous

# Thread-safe state manager for Agent V2
from threading import Lock

class AgentStateManager:
    """Thread-safe state manager for agent operations."""
    def __init__(self):
        self._state = {
            "status": "idle",
            "current_thought": "Waiting for user input...",
            "logs": [],
            "current_task_id": None
        }
        self._lock = Lock()
    
    def update(self, **kwargs):
        with self._lock:
            self._state.update(kwargs)
    
    def get(self, key: str, default=None):
        with self._lock:
            return self._state.get(key, default)
    
    def get_all(self) -> dict:
        with self._lock:
            return self._state.copy()
    
    def reset(self, task_id: str = None):
        with self._lock:
            self._state = {
                "status": "idle",
                "current_thought": "Waiting for user input...",
                "logs": [],
                "current_task_id": task_id
            }
    
    def append_log(self, log: str):
        with self._lock:
            self._state["logs"].append(log)

agent_state = AgentStateManager()

# Standard Verification API (called by UI)
@app.post("/api/auth/verify")
async def verify_user_license(request: Dict[str, str]):
    """
    Standard verification endpoint.
    Delegates to AuthManager -> LicenseGuard -> Supabase 'profiles'.
    """
    email = request.get("email")
    if not email:
        raise HTTPException(status_code=400, detail="Email is required")
        
    # Standard check (will check cache first, then online if needed)
    # For First Run Wizard, we might want to force online if cache is old?
    # But LicenseGuard handles that automatically (monthly check).
    success, msg = await auth_manager.verify_user(name="User", email=email)
    
    if success:
        return {"status": "success", "message": msg}
    else:
        # 403 Forbidden is appropriate for invalid license
        # But we return 200 with status=failed so UI can show message nicely
        # avoiding confusing fetch errors.
        return {"status": "failed", "message": msg}

@app.post("/api/auth/verify-emergency")
async def verify_emergency_access(request: Dict[str, str]):
    email = request.get("email")
    token = request.get("token")
    
    if not email or not token:
        raise HTTPException(status_code=400, detail="Email and Token are required")
        
    success, msg = await auth_manager.verify_user(name="EmergencyUser", email=email, offline_token=token)
    
    if success:
        return {"status": "success", "message": msg}
    else:
        raise HTTPException(status_code=403, detail="Invalid or Expired Emergency Token")

class AgentRequest(BaseModel):
    message: str
    mode: str = "lite" # 'lite' (NanoBot) or 'hybrid' (Agent Zero)

@app.post("/api/agent/chat")
async def agent_chat(request: AgentRequest):
    """
    Direct Chat Endpoint for BioDockify AI.
    - mode="lite": Fast, tool-using receptionist.
    - mode="hybrid": Full reasoning agent.
    """
    try:
        from agent_zero.biodockify_ai import get_biodockify_ai
        
        logger.info(f"BioDockify AI: Processing chat request (len={len(request.message)}, mode={request.mode})")
        agent = get_biodockify_ai()
        
        # Process chat via the Hybrid Agent Loop or Receptionist based on mode
        reply = await agent.process_chat(request.message, mode=request.mode)
        
        # Return format expected by UI
        return {
            "reply": reply,
            "provider": f"biodockify-{request.mode}",
            "enhanced": request.mode == "hybrid"
        }
    except Exception as e:
        logger.error(f"Agent Chat Failed: {e}")
        # Return error as reply so UI shows it in chat bubble instead of crashing
        return {
             "reply": f"**System Error:** {str(e)}\n\nPlease check the backend logs or your AI Provider settings.",
             "provider": "error"
        }


@app.post("/api/v2/agent/goal")
async def set_agent_goal(request: AgentGoal, background_tasks: BackgroundTasks):
    """
    V2: Set a high-level goal for Agent Zero.
    Triggers the Orchestrator in the background.
    """
    logger.info(f"Received Agent Goal: {request.goal} [Mode: {request.mode}]")
    
    # Pre-flight check: Verify LLM provider is available
    try:
        from runtime.config_loader import load_config
        from runtime.service_manager import get_service_manager
        
        cfg = load_config()
        ai_mode = cfg.get("ai_provider", {}).get("mode", "auto")
        
        # If using Ollama, verify it's running before starting task
        if ai_mode == "ollama":
            svc_mgr = get_service_manager(cfg)
            if svc_mgr.check_health("ollama") != "running":
                # Try to start Ollama automatically
                logger.warning("Ollama not running. Attempting to start...")
                if not svc_mgr.start_ollama():
                    raise HTTPException(
                        status_code=503,
                        detail="Ollama service is not running and could not be started. Please start Ollama with 'ollama serve' or configure a cloud API in Settings."
                    )
                logger.info("Ollama started successfully.")
    except HTTPException:
        raise  # Re-raise HTTP exceptions
    except Exception as e:
        logger.error(f"Pre-flight check failed: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to verify AI provider: {str(e)}"
        )
    
    # Generate unique task ID
    task_id = str(uuid.uuid4())
    
    # Reset State with task ID
    agent_state.reset(task_id)
    agent_state.update(status="thinking", current_thought="Analyzing research goal...")
    agent_state.append_log(f"Goal received: {request.goal}")
    
    # Define Background Task wrapper
    def run_agent_task(goal: str, mode: str):
        try:
            from runtime.config_loader import load_config
            cfg = load_config()
            
            # Initialize Orchestrator
            orch = ResearchOrchestrator()
            agent_state.update(current_thought="Planning research steps...")
            agent_state.append_log("Orchestrator initialized.")
            
            # Map agent mode to research planning mode
            # Frontend sends: auto, semi-autonomous, autonomous
            # Orchestrator expects: search, synthesize, write
            planning_mode = "synthesize"  # Default
            if mode in ["auto", "semi-autonomous", "autonomous"]:
                # For now, all autonomous modes use synthesize
                # Could be enhanced later to map to different modes based on complexity
                planning_mode = "synthesize"
            
            # Plan
            plan = orch.plan_research(goal, mode=planning_mode)
            if plan is None:
                raise ValueError("LLM failed to generate a plan. Check AI provider configuration.")
            
            agent_state.append_log(f"Plan generated: {len(plan.steps)} steps.")
            agent_state.update(
                current_thought=f"Plan created with {len(plan.steps)} steps. Ready to execute.",
                status="ready"
            )
            
        except ValueError as e:
            # LLM-specific errors
            logger.error(f"LLM Error: {e}")
            agent_state.update(status="error", current_thought=f"LLM Error: {str(e)}")
            agent_state.append_log(f"LLM Error: {str(e)}")
        except Exception as e:
            logger.error(f"Agent Task Failed: {e}")
            agent_state.update(status="error", current_thought=f"Error: {str(e)}")
            agent_state.append_log(f"Error: {str(e)}")

    # Launch Background Task
    background_tasks.add_task(run_agent_task, request.goal, request.mode)
    
    return {"status": "accepted", "message": "Agent started", "task_id": task_id, "ai_mode": ai_mode}



@app.get("/api/v2/agent/thinking")
def get_agent_thinking():
    """
    V2: Get current agent thought process (Streaming-like UI).
    """
    state = agent_state.get_all()
    logs = state.get("logs", [])
    return {
        "status": state.get("status", "idle"),
        "thought": state.get("current_thought", ""),
        "task_id": state.get("current_task_id"),
        "logs": logs[-5:] if logs else []  # Return last 5 logs
    }

# -----------------------------------------------------------------------------
# V2 SYSTEM HEALTH ENDPOINTS (Phase 19)
# -----------------------------------------------------------------------------
from modules.system.doctor import SystemDoctor

@app.get("/api/v2/system/diagnose")
def diagnose_system():
    """
    V2: Run a full system diagnostic and return a health report.
    """
    from runtime.config_loader import load_config
    cfg = load_config()
    
    doctor = SystemDoctor(cfg)
    report = doctor.run_diagnosis()
    return report

@app.post("/api/v2/system/repair")
def repair_system(service: str = "ollama"):
    """
    V2: Attempt to repair a specific system service.
    """
    from runtime.config_loader import load_config
    from runtime.service_manager import get_service_manager
    
    cfg = load_config()
    svc_mgr = get_service_manager(cfg)
    
    try:
        result = svc_mgr.attempt_repair(service)
        return result
    except AttributeError:
        # Fallback if attempt_repair doesn't exist
        if service == "ollama":
            success = svc_mgr.start_ollama()
            return {
                "status": "success" if success else "failed",
                "service": service,
                "message": "Ollama started" if success else "Failed to start Ollama"
            }
        return {"status": "error", "message": f"Unknown service: {service}"}

@app.post("/api/v2/system/start-surfsense")
def start_surfsense():
    """
    V2: Explicitly start SurfSense Knowledge Engine via Docker.
    """
    from runtime.config_loader import load_config
    from runtime.service_manager import get_service_manager
    
    cfg = load_config()
    svc_mgr = get_service_manager(cfg)
    
    svc_mgr.start_surfsense()
    return {"status": "success", "message": "SurfSense start initiated"}

# -----------------------------------------------------------------------------
# V2 CONNECTIVITY DIAGNOSIS ENDPOINTS (First-Run Self-Healing)
# -----------------------------------------------------------------------------
from modules.system.connection_doctor import ConnectionDoctor, run_diagnosis

@app.get("/api/diagnose/connectivity")
async def diagnose_connectivity():
    """
    Full connectivity diagnosis with repair suggestions.
    Used by FirstRunWizard to detect and auto-repair connection issues.
    
    Returns:
        - status: overall health (healthy | degraded | offline)
        - checks: individual check results
        - repair_actions: suggested fixes for failed checks
        - can_proceed: whether the system can operate in degraded mode
    """
    from runtime.config_loader import load_config
    
    try:
        cfg = load_config()
    except Exception:
        cfg = {}
    
    doctor = ConnectionDoctor(cfg)
    report = await doctor.full_diagnosis(auto_repair=True)
    
    return {
        "status": report.overall_status,
        "checks": [
            {
                "name": c.name,
                "status": c.status.value,
                "message": c.message,
                "details": c.details,
                "can_auto_repair": c.can_auto_repair,
                "repair_action": c.repair_action
            }
            for c in report.checks
        ],
        "repair_actions": report.suggested_repairs,
        "can_proceed": report.can_proceed_with_degraded
    }

@app.post("/api/diagnose/repair/{check_id}")
async def attempt_connectivity_repair(check_id: str):
    """
    Attempt auto-repair for a specific connectivity check.
    
    Args:
        check_id: Identifier of the check to repair (lm_studio, internet, api_keys, backend)
        
    Returns:
        Updated check result after repair attempt
    """
    from runtime.config_loader import load_config
    
    try:
        cfg = load_config()
    except Exception:
        cfg = {}
    
    doctor = ConnectionDoctor(cfg)
    result = await doctor.attempt_repair(check_id)
    
    return {
        "success": result.status.value == "success",
        "name": result.name,
        "status": result.status.value,
        "message": result.message,
        "details": result.details,
        "repair_action": result.repair_action
    }

@app.post("/api/diagnose/lm-studio/start")
async def start_lm_studio():
    """
    Explicitly attempt to auto-start LM Studio.
    Used by the UI when user clicks "Start LM Studio" button.
    
    Returns:
        - success: whether LM Studio was started
        - exe_path: path to the executable (if found)
        - message: status message
    """
    from runtime.config_loader import load_config
    
    try:
        cfg = load_config()
    except Exception:
        cfg = {}
    
    doctor = ConnectionDoctor(cfg)
    
    # Find executable
    exe_path = doctor.find_lm_studio_executable()
    if not exe_path:
        return {
            "success": False,
            "exe_path": None,
            "message": "LM Studio not found. Please install from https://lmstudio.ai"
        }
    
    # Attempt to start
    started = await doctor.auto_start_lm_studio(exe_path)
    
    if started:
        return {
            "success": True,
            "exe_path": exe_path,
            "message": "LM Studio started. Please load a model and wait for initialization."
        }
    else:
        return {
            "success": False,
            "exe_path": exe_path,
            "message": "Failed to start LM Studio. Please start it manually."
        }

# -----------------------------------------------------------------------------
# DIGITAL LIBRARY ENDPOINTS (Phase 5)
# -----------------------------------------------------------------------------
from fastapi import UploadFile, File, Form
from modules.library.store import library_store
from modules.library.ingestor import library_ingestor

# -----------------------------------------------------------------------------
# KNOWLEDGE BASE & PODCAST ENDPOINTS (Phase 33)
# -----------------------------------------------------------------------------

class PodcastRequest(BaseModel):
    text: str
    voice: str = "alloy"  # OpenAI TTS voice

class KnowledgeQueryRequest(BaseModel):
    query: str
    top_k: int = 5

@app.post("/api/knowledge/query")
async def query_knowledge_base(request: KnowledgeQueryRequest):
    """
    Query the internal knowledge base (RAG) and return cited results.
    Used by Agent Zero and the Knowledge Hub UI.
    """
    try:
        from modules.rag.vector_store import get_vector_store
        store = get_vector_store()
        results = store.search(request.query, k=request.top_k)
        return {
            "status": "success",
            "query": request.query,
            "results": [
                {
                    "text": r.get("text", ""),
                    "score": r.get("score", 0),
                    "metadata": r.get("metadata", {})
                }
                for r in results
            ]
        }
    except Exception as e:
        logger.error(f"Knowledge query failed: {e}")
        return {"status": "error", "error": str(e), "results": []}

@app.post("/api/knowledge/podcast")
async def generate_podcast(request: PodcastRequest):
    """
    Generate a podcast audio from text using TTS.
    Supports OpenAI TTS or local Kokoro TTS.
    """
    try:
        from runtime.config_loader import load_config
        cfg = load_config()
        
        # Try OpenAI TTS first if key is available
        custom_key = cfg.get("ai_provider", {}).get("custom_key")
        custom_base_url = cfg.get("ai_provider", {}).get("custom_base_url", "https://api.openai.com/v1")
        
        if custom_key:
            import httpx
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{custom_base_url}/audio/speech",
                    headers={"Authorization": f"Bearer {custom_key}"},
                    json={
                        "model": "tts-1",
                        "input": request.text[:4096],  # Limit to 4096 chars
                        "voice": request.voice
                    },
                    timeout=60.0
                )
                if response.status_code == 200:
                    # Return audio as base64
                    import base64
                    audio_b64 = base64.b64encode(response.content).decode('utf-8')
                    return {
                        "status": "success",
                        "audio_format": "mp3",
                        "audio_base64": audio_b64
                    }
                else:
                    return {"status": "error", "error": f"TTS API returned {response.status_code}"}
        else:
            return {"status": "error", "error": "No TTS API key configured. Please add an OpenAI-compatible API key in Settings."}
            
    except Exception as e:
        logger.error(f"Podcast generation failed: {e}")
        return {"status": "error", "error": str(e)}

# -----------------------------------------------------------------------------
# HYPOTHESIS & SCIENTIFIC METHOD ENDPOINTS (Phase 11.2)
# -----------------------------------------------------------------------------
from modules.scientific_method.models import Hypothesis, HypothesisStatus, EvidenceType
from modules.scientific_method.hypothesis_engine import get_hypothesis_engine

@app.post("/api/hypothesis", response_model=Hypothesis)
def create_hypothesis(statement: str, rationale: str):
    """Create a new scientific hypothesis."""
    engine = get_hypothesis_engine()
    return engine.create_hypothesis(statement, rationale)

@app.get("/api/hypothesis", response_model=list[Hypothesis])
def list_hypotheses():
    """List all tracked hypotheses."""
    engine = get_hypothesis_engine()
    return engine.list_hypotheses()

@app.get("/api/hypothesis/{h_id}", response_model=Hypothesis)
def get_hypothesis(h_id: str):
    """Get details of a specific hypothesis."""
    engine = get_hypothesis_engine()
    h = engine.get_hypothesis(h_id)
    if not h: raise HTTPException(status_code=404, detail="Hypothesis not found")
    return h

class StatusUpdate(BaseModel):
    status: HypothesisStatus

@app.patch("/api/hypothesis/{h_id}/status")
def update_hypothesis_status(h_id: str, update: StatusUpdate):
    """Update the lifecycle status of a hypothesis."""
    engine = get_hypothesis_engine()
    engine.update_status(h_id, update.status)
    return {"status": "success", "new_status": update.status}

class EvidenceAdd(BaseModel):
    description: str
    type: EvidenceType
    source_id: Optional[str] = None

# -----------------------------------------------------------------------------
# PUBLICATION & EXPORT ENDPOINTS (Phase 11.3)
# -----------------------------------------------------------------------------
from modules.publication.latex_exporter import LaTeXExporter
from modules.publication.figure_manager import FigureManager

class ExportRequest(BaseModel):
    title: str
    author: str
    affiliation: str
    abstract: str
    content_markdown: str

# -----------------------------------------------------------------------------
# LITERATURE DISCOVERY & SYNTHESIS ENDPOINTS
# -----------------------------------------------------------------------------

class LitSearchRequest(BaseModel):
    query: str
    limit: int = 15
    sources: Optional[List[str]] = None

class LitSynthesizeRequest(BaseModel):
    topic: str
    papers: List[Dict[str, Any]]

@app.post("/api/literature/search")
async def search_literature_endpoint(req: LitSearchRequest):
    """
    Search for scientific literature across multiple sources.
    """
    try:
        config = LiteratureConfig(
            sources=req.sources,
            max_results=req.limit
        )
        aggregator = LiteratureAggregator(config)
        # Using loop.run_in_executor because search might be blocking (PubMed sync part)
        loop = asyncio.get_event_loop()
        results = await loop.run_in_executor(None, aggregator.search, req.query)
        return {"papers": results}
    except Exception as e:
        logger.error(f"Literature search failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/literature/synthesize")
async def synthesize_literature_endpoint(req: LitSynthesizeRequest):
    """
    Synthesize a literature review from a list of papers.
    """
    try:
        from modules.literature.discovery import Paper as DiscoveryPaper
        
        # Convert dicts to Paper objects
        papers = []
        for p in req.papers:
            papers.append(DiscoveryPaper(
                title=p.get("title", "Unknown"),
                url=p.get("url", p.get("full_text_url", "")),
                source=p.get("source", "Unknown"),
                authors=p.get("authors", []),
                abstract=p.get("abstract", ""),
                year=int(p.get("year", 0)) if str(p.get("year", "0")).isdigit() else 0,
                doi=p.get("doi")
            ))
            
        synthesizer = get_synthesizer()
        
        # Set agent callback if needed (using current agent logic)
        async def agent_callback(prompt: str) -> str:
            # Re-use agent_chat internal logic or similar
            # For now, let's just use the factory directly
            from modules.llm.factory import LLMFactory
            from runtime.config_loader import load_config
            import types
            
            cfg = load_config()
            ai_cfg = cfg.get("ai_provider", {})
            provider_mode = ai_cfg.get("mode", "auto")
            config_obj = types.SimpleNamespace(**ai_cfg)
            
            adapter = LLMFactory.get_adapter(provider_mode, config_obj)
            if not adapter:
                 adapter = LLMFactory.get_adapter("lm_studio", config_obj)
            
            if not adapter:
                return "AI provider not available for synthesis."
            
            return await asyncio.to_thread(adapter.generate, prompt)

        synthesizer.set_agent_callback(agent_callback)
        review = await synthesizer.generate_review(req.topic, papers)
        
        return {"review": review}
    except Exception as e:
        logger.error(f"Literature synthesis failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/publication/export/latex")
def export_to_latex(req: ExportRequest):
    """Generate LaTeX source from Markdown report."""
    try:
        exporter = LaTeXExporter()
        latex_source = exporter.generate_latex(
            title=req.title,
            author=req.author,
            affiliation=req.affiliation,
            abstract=req.abstract,
            content_markdown=req.content_markdown
        )
        return {"latex_source": latex_source}
    except Exception as e:
        logger.error(f"Export failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/publication/figures")
def register_figure(title: str, caption: str, code: str, path: str):
    """Register a scientific figure for reproducibility."""
    fm = FigureManager()
    fig_id = fm.register_figure(title, caption, code, path)
    return {"figure_id": fig_id}

# -----------------------------------------------------------------------------
# STATISTICS & QC ENDPOINTS (Phase 4)
# -----------------------------------------------------------------------------
from modules.statistics.engine import StatisticalEngine

class StatisticsRequest(BaseModel):
    data: List[Dict[str, Any]]
    design: str = "descriptive" # descriptive, two_group, anova, correlation
    tier: str = "basic" # basic, analytical, advanced

@app.post("/api/statistics/analyze")
def analyze_statistics(req: StatisticsRequest):
    """
    Unified Statistical Analysis Endpoint.
    Routes to the 3-Tier Statistical Engine.
    """
    # License Check for Statistics features
    from modules.system.auth_manager import verify_license
    if not verify_license():
        return JSONResponse(
            status_code=403, 
            content={"error": "license_required", "message": "The free version of BioDockify requires a one-time verification to unlock Advanced Statistics. Please go to Settings."}
        )

    engine = StatisticalEngine()
    result = engine.analyze(req.data, req.design, req.tier)
    
    if "error" in result:
        # We don't raise HTTP 500 for data errors, we return the error structure
        # so the UI can explain it to the user (especially Tier 1).
        return JSONResponse(status_code=400, content=result)
        
    return result

# File upload security constants
ALLOWED_EXTENSIONS = {'.pdf', '.txt', '.docx', '.csv', '.xlsx', '.md', '.json', '.ipynb'}
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB

@app.post("/api/library/upload")
async def upload_file(file: UploadFile = File(...)):
    """
    Uploads a file to the internal Knowledge Base.
    Stores file locally AND indexes into vector store for semantic search.
    """
    try:
        from modules.rag.ingestor import ingestor
        from modules.rag.vector_store import get_vector_store
        import os
        
        # Validate file extension
        _, ext = os.path.splitext(file.filename.lower())
        if ext not in ALLOWED_EXTENSIONS:
            raise HTTPException(
                status_code=400, 
                detail=f"Invalid file type '{ext}'. Allowed: {', '.join(ALLOWED_EXTENSIONS)}"
            )
        
        # Read and validate file size
        content = await file.read()
        if len(content) > MAX_FILE_SIZE:
            raise HTTPException(
                status_code=413, 
                detail=f"File too large. Maximum size is {MAX_FILE_SIZE // (1024*1024)}MB"
            )
        
        # 2. Store Locally (for UI listing and file retrieval)
        record = library_store.add_file(content, file.filename)
        file_path = library_store.get_file_path(record['id'])
        
        # 3. Ingest and Index into Vector Store
        vector_status = "pending"
        chunk_count = 0
        try:
            chunks = ingestor.ingest_file(str(file_path))
            if chunks:
                get_vector_store().add_documents(chunks)
                chunk_count = len(chunks)
                vector_status = "indexed"
                # Mark file as processed
                library_store.update_metadata(record['id'], {
                    "processed": True,
                    "chunk_count": chunk_count
                })
        except Exception as e:
            logger.warning(f"Vector indexing failed for {file.filename}: {e}")
            vector_status = "failed"
            library_store.update_metadata(record['id'], {
                "processed": False,
                "index_error": str(e)
            })

        return {
            "status": "success", 
            "file": record, 
            "vector_index": vector_status,
            "chunks_indexed": chunk_count
        }
        
    except Exception as e:
        logger.error(f"Upload failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/library/files")
async def list_files():
    """Lists all files in the library."""
    return library_store.list_files()

@app.delete("/api/library/files/{file_id}")
async def delete_file(file_id: str):
    """Deletes a file from the library."""
    success = library_store.remove_file(file_id)
    if not success:
        raise HTTPException(status_code=404, detail="File not found or failed to delete")
    return {"status": "success"}

class LibraryQuery(BaseModel):
    query: str
    top_k: int = 5

@app.post("/api/library/query")
async def query_library(request: LibraryQuery):
    """
    Semantic search over the digital library.
    """
    try:
        from modules.rag.vector_store import get_vector_store
        store = get_vector_store()
        results = store.search(request.query, request.top_k)
        return {"results": results}
    except Exception as e:
         # Graceful fallback if RAG is offline
         logger.warning(f"RAG Search failed: {e}")
         return {"results": []}

    except Exception as e:
        logger.error(f"Library search failed: {e}")
        return {"results": [], "error": str(e)}

async def background_monitor():
    """
    Periodic system health logging loop.
    Logs CPU/RAM usage every 60 seconds.
    """
    logger.info("Starting Background Monitor...")
    while True:
        try:
            await asyncio.sleep(60)
            cpu = psutil.cpu_percent(interval=None)
            mem = psutil.virtual_memory()
            
            # Log as structured metric
            metric_data = {
                "event": "system_metrics",
                "cpu_percent": cpu,
                "ram_percent": mem.percent,
                "ram_available_gb": round(mem.available / (1024**3), 1)
            }
            
            # Alert on thresholds
            if mem.percent > 90:
                logger.warning(json.dumps({**metric_data, "alert": "High RAM Usage"}))
            else:
                logger.info(json.dumps(metric_data))
                
        except Exception as e:
            logger.error(f"Monitor Error: {e}")
            await asyncio.sleep(60) # Prevent tight loop on error

async def startup_event():
    """
    Service Stability: Model Loading & Pre-warming.
    """
    logger.info("Initializing BioDockify Backend...")
    
    # 1. Background Initialization (Models + Services)
    def background_init():
        """Runs heavy startup tasks without blocking API availability."""
        logger.info("Background Init: Starting...")
        
        # Start Prometheus metrics server
        try:
            from runtime.monitoring import start_monitoring_server
            start_monitoring_server(8000)
        except Exception as e:
            logger.warning(f"Background Init: Failed to start monitoring server: {e}")

        # A. Wait for server bind
        time.sleep(5) 
        
        # B. Warm up Embedding Model with Crash Protection
        try:
            logger.info("Background Init: Pre-loading Embedding Model...")
            from modules.rag.vector_store import get_vector_store
            vs = get_vector_store()
            
            # Guard against partial loading
            if hasattr(vs, '_load_dependencies'):
                 vs._load_dependencies()
                 
            if hasattr(vs, 'model') and vs.model:
                 logger.info("Background Init: Running warmup inference...")
                 vs.model.encode(["warmup"])
                 logger.info("Background Init: Embedding Model Ready.")
            else:
                 logger.warning("Background Init: Model not loaded (dependencies missing?).")
                 
        except Exception as e:
            logger.warning(f"Background Init: Model warmup failed: {e}")
            # Do NOT re-raise, to keep the thread alive

        # C. Start Background Services (Ollama/SurfSense)
        try:
            from runtime.config_loader import load_config
            from runtime.service_manager import get_service_manager
            
            config = load_config()
            if config.get("system", {}).get("auto_start_services", True):
                logger.info("Background Init: Auto-starting services...")
                svc_mgr = get_service_manager(config)
                
                # Ollama
                if config.get("ai_provider", {}).get("mode") in ["auto", "ollama"]:
                     try:
                         if hasattr(svc_mgr, 'start_ollama'):
                             svc_mgr.start_ollama()
                     except Exception as e:
                         logger.warning(f"Background Init: Ollama start failed: {e}")
                
                # SurfSense
                try:
                    # Check if method exists and is safe
                    if hasattr(svc_mgr, 'start_surfsense'):
                        svc_mgr.start_surfsense()
                except Exception as e:
                    logger.warning(f"Background Init: SurfSense start failed: {e}")
                    
        except Exception as e:
            logger.error(f"Background Init: Service init failed: {e}")

        logger.info("Background Init: Completed.")

    import threading
    threading.Thread(target=background_init, daemon=True).start()
    logger.info("Background initialization thread launched.")

    # 4. Start Background Monitoring Loop
    asyncio.create_task(background_monitor())

    # 4. Check for high memory usage warning
    mem_status = psutil.virtual_memory()
    if mem_status.percent > 90:
        logger.warning(f"High Memory Usage on Startup: {mem_status.percent}%")

@app.middleware("http")
async def resource_monitor_middleware(request: Request, call_next):
    """
    Resource Management: Reject heavy requests if system is under stress.
    """
    # Skip for simple health checks and critical auth/settings endpoints
    if request.url.path in ["/health", "/api/system/info", "/api/auth/verify", "/api/auth/verify-emergency", "/api/settings/test"]:
        return await call_next(request)

    # Check RAM
    mem = psutil.virtual_memory()
    if mem.percent > 95: # Critical Threshold
        return JSONResponse(
            status_code=503, 
            content={"detail": "System is under heavy load (Memory > 95%). Please retry later."}
        )
        
    return await call_next(request)

# Rate Limiting & Size Limiting Middleware
from collections import defaultdict
import time

request_counts = defaultdict(list)
RATE_LIMIT = 100 # requests per minute
MAX_UPLOAD_SIZE = 50 * 1024 * 1024 # 50 MB

@app.middleware("http")
async def security_middleware(request: Request, call_next):
    # 1. Size Limiting (Client-Side Check)
    if request.method == "POST":
        content_length = request.headers.get("content-length")
        if content_length and int(content_length) > MAX_UPLOAD_SIZE:
             return JSONResponse(status_code=413, content={"detail": "File too large. Maximum size is 50MB."})

    # 2. Rate Limiting (Simple Token Bucket per IP)
    client_ip = request.client.host if request.client else "testclient"
    now = time.time()
    
    # Clean old requests
    request_counts[client_ip] = [t for t in request_counts[client_ip] if t > now - 60]
    
    if len(request_counts[client_ip]) > RATE_LIMIT:
        return JSONResponse(status_code=429, content={"detail": "Rate limit exceeded. Try again later."})
        
    request_counts[client_ip].append(now)
    
    return await call_next(request)

@app.middleware("http")
async def audit_log_middleware(request: Request, call_next):
    """
    Structured Logging: Audit Trail for all API interactions.
    Logs: Method, Path, IP, Status, Duration, User-Agent.
    """
    start_time = time.time()
    
    # Process Request
    try:
        response = await call_next(request)
        status_code = response.status_code
    except Exception as e:
        status_code = 500
        raise e
    finally:
        process_time = (time.time() - start_time) * 1000 # ms
        
        # Sanitize sensitive paths if needed (e.g. /auth/login passwords)
        # For now, we trust the path logs.
        
        log_cls = logger.info if status_code < 400 else logger.warning
        
        log_data = {
            "event": "api_request",
            "method": request.method,
            "path": request.url.path,
            "client_ip": request.client.host if request.client else "testclient",
            "status_code": status_code,
            "duration_ms": round(process_time, 2),
            "user_agent": request.headers.get("user-agent", "unknown")
        }
        log_cls(json.dumps(log_data))
        
    return response

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"Global Exception: {str(exc)}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"detail": "Internal Server Error", "message": str(exc)},
    )

def safe_request(method, url, max_retries=3, **kwargs):
    """
    Resilient HTTP request with retries (Circuit Breaker Lite).
    """
    for attempt in range(max_retries):
        try:
            resp = requests.request(method, url, **kwargs)
            return resp
        except requests.exceptions.RequestException as e:
            logger.warning(f"Request failed (Attempt {attempt+1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                raise e
            time.sleep(1 * (attempt + 1)) # Linear backoff


# -----------------------------------------------------------------------------
# Persistence Layer (Disk-Based)
# -----------------------------------------------------------------------------
from modules.task_manager.manager import TaskManager
task_manager = TaskManager(os.getenv("DATABASE_URL", "sqlite+aiosqlite:///./tasks.db"))

# -----------------------------------------------------------------------------
# Data Models
# -----------------------------------------------------------------------------

class ResearchRequest(BaseModel):
    title: Optional[str] = None
    topic: Optional[str] = None # Added for frontend compatibility
    mode: str = "synthesize" # search, synthesize, write
    
class TaskStatus(BaseModel):
    task_id: str
    status: str # running, completed, suspended, failed
    result: Optional[Dict[str, Any]] = None
    progress: int = 0
    message: Optional[str] = None
    logs: List[str] = []

# -----------------------------------------------------------------------------
# Background Worker
# -----------------------------------------------------------------------------

def run_research_task(task_id: str, title: str, mode: str):
    """
    Background worker function to run the full research pipeliine.
    """
    try:
        # Load or Init Task
        task = task_manager.load_task(task_id)
        if not task: 
             task = {"task_id": task_id, "title": title, "status": "planning"}

        task["status"] = "planning"
        task_manager.save_task(task_id, task)
        
        # 1. Plan
        # Load runtime config properly
        from runtime.config_loader import load_config
        runtime_cfg = load_config()
        
        
        # Determine cloud usage from config, not just request
        use_cloud = runtime_cfg.get("ai_provider", {}).get("mode") == "hybrid"
        user_persona = runtime_cfg.get("user_persona", {})
        
        config = OrchestratorConfig(
            use_cloud_api=use_cloud,
            user_persona=user_persona
        )
        orchestrator = ResearchOrchestrator(config)
        
        # Pass the research mode to the planner
        plan = orchestrator.plan_research(title, mode=mode, task_id=task_id)
        
        task["status"] = "executing"
        task_manager.save_task(task_id, task)
        
        # 2. Execute with Retry Logic (Async Robustness)
        # Pass task_id into executor for granular checkpointing
        executor = ResearchExecutor(task_id=task_id) 
        
        MAX_JOB_RETRIES = 3
        context = None
        
        for attempt in range(MAX_JOB_RETRIES):
            try:
                context = executor.execute_plan(plan)
                break # Success
            except Exception as job_err:
                logger.warning(f"Task {task_id} failed attempt {attempt+1}: {job_err}")
                if attempt == MAX_JOB_RETRIES - 1:
                     raise job_err # Propagate up to main handler
                time.sleep(2 * (attempt + 1)) # Backoff
        
        if not context:
             raise Exception("Task execution returned no context after retries.")
        
        # ---------------------------------------------------------------------
        # 2.5 Auto-Ingest into Local NotebookLM (RAG) & Neo4j
        # ---------------------------------------------------------------------
        try:
            from modules.rag.vector_store import vector_store
            
            # Simple Chunking Strategy
            full_text = context.extracted_text
            chunk_size = 1000
            chunks = [full_text[i:i+chunk_size] for i in range(0, len(full_text), chunk_size)]
            
            docs_to_ingest = []
            for i, chunk_text in enumerate(chunks):
                docs_to_ingest.append({
                    "text": chunk_text,
                    "metadata": {
                        "source": f"Agent Research: {title}",
                        "type": "generated-research",
                        "task_id": task_id,
                        "chunk_index": i
                    }
                })
            
            if docs_to_ingest:
                vector_store.add_documents(docs_to_ingest)
                
            # Neo4j Sync (Entities)
            # Assuming context.entities is a list of dicts/obects
            # If we had a Neo4j module exposed here, we'd call it.
            # For now, we assume the ResearchExecutor *internally* might have handled it, 
            # or we log that we skipped it if no direct access.
            # (MVP: We rely on RAG for the Notebook experience).
            
        except Exception as rag_err:
             print(f"Warning: Failed to auto-ingest into RAG: {rag_err}")

        # ---------------------------------------------------------------------
        
        # 3. Store Results
        task["status"] = "completed"
        task["result"] = {
            "title": title,
            "text_length": len(context.extracted_text),
            "entities": context.entities,
            "stats": context.analyst_stats
        }
        task_manager.save_task(task_id, task)
        
    except Exception as e:
        task = task_manager.load_task(task_id) or {}
        task["status"] = "failed"
        task["error"] = str(e)
        task_manager.save_task(task_id, task)


# -----------------------------------------------------------------------------
# API Endpoints
# -----------------------------------------------------------------------------

@app.get("/health")
@app.get("/api/health")
def health_check():
    """
    Comprehensive System Health Check.
    Monitors: API, Ollama, Vector Store, Neo4j (optional), System Resources.
    """
    import psutil
    import shutil
    
    status = {"status": "ok", "components": {}}
    
    # 1. API Service
    status["components"]["api"] = {"status": "ok", "version": "1.0.0"}
    
    # 2. Ollama / AI Provider
    try:
        from runtime.config_loader import load_config
        cfg = load_config()
        ai_conf = cfg.get("ai_provider", {})
        
        # Determine active provider
        mode = ai_conf.get("mode", "auto")
        
        if mode in ["auto", "ollama"]:
            ollama_url = ai_conf.get("ollama_url", "http://localhost:11434")
            # Ping with NO retries for fast health check
            resp = safe_request('GET', f"{ollama_url}/api/tags", max_retries=1, timeout=1.5)
            if resp.status_code == 200:
                 status["components"]["ai_core"] = {"status": "ok", "provider": "ollama"}
            else:
                 status["components"]["ai_core"] = {"status": "degraded", "message": f"Ollama Unreachable ({resp.status_code})"}
        elif mode == "lm_studio":
            lm_url = ai_conf.get("lm_studio_url", "http://localhost:1234/v1")
            resp = safe_request('GET', f"{lm_url}/models", max_retries=1, timeout=1.5)
            if resp.status_code == 200:
                 status["components"]["ai_core"] = {"status": "ok", "provider": "lm_studio"}
            else:
                 status["components"]["ai_core"] = {"status": "degraded", "message": f"LM Studio Unreachable"}
        else:
            status["components"]["ai_core"] = {"status": "ok", "provider": mode, "message": "Cloud-based provider active"}
            
    except Exception as e:
         status["components"]["ai_core"] = {"status": "error", "message": str(e)}

    # 3. Vector DB (Chroma/FAISS)
    try:
         from modules.rag.vector_store import vector_store
         # Simple count check
         count = vector_store.client.count() if hasattr(vector_store, 'client') else 0
         status["components"]["vector_db"] = {"status": "ok", "documents": count}
    except Exception as e:
         status["components"]["vector_db"] = {"status": "degraded", "message": "Store not initialized"}

    # 4. Knowledge Engine (SurfSense - replaces Neo4j)
    # Neo4j is deprecated, SurfSense is the primary knowledge engine
    status["components"]["knowledge_engine"] = {"status": "ok", "message": "SurfSense (Neo4j deprecated)"}

    # 5. System Resources
    try:
        mem = psutil.virtual_memory()
        disk = shutil.disk_usage(".")
        status["components"]["system"] = {
            "status": "ok", 
            "ram_free_gb": round(mem.available / (1024**3), 1),
            "disk_free_gb": round(disk.free / (1024**3), 1)
        }
    except:
        status["components"]["system"] = {"status": "unknown"}
        
    return status

@app.post("/api/research/start", response_model=Dict[str, str])
async def start_research(request: ResearchRequest, background_tasks: BackgroundTasks):
    """Start a new research task in the background."""
    task_id = str(uuid.uuid4())
    
    # Use topic if title is missing (frontend sends topic)
    research_title = request.title or request.topic
    if not research_title:
        raise HTTPException(status_code=400, detail="Either 'title' or 'topic' is required")

    # Init Valid Task State on Disk immediately
    initial_state = {
        "task_id": task_id,
        "title": research_title, 
        "status": "pending",
        "created_at": str(int(time.time() * 1000)) # timestamp proxy
    }
    task_manager.save_task(task_id, initial_state)
    
    background_tasks.add_task(run_research_task, task_id, research_title, request.mode)
    
    return {"task_id": task_id, "status": "started"}

@app.get("/api/research/status/{task_id}", response_model=TaskStatus)
def get_status(task_id: str):
    """Get the status of a specific task."""
    task = task_manager.load_task(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")
    
    return TaskStatus(
        task_id=task_id,
        status=task.get("status", "unknown"),
        result=task.get("result"),
        progress=task.get("progress", 0),
        message=task.get("message"),
        logs=task.get("logs", [])
    )

@app.post("/api/research/{task_id}/cancel")
def cancel_research(task_id: str):
    """Cancel a running research task."""
    task = task_manager.load_task(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")
    
    # Mark as failed/cancelled so UI stops polling
    task["status"] = "failed" 
    task["message"] = "Cancelled by user"
    task_manager.save_task(task_id, task)
    
    return {"success": True}


from runtime.config_loader import load_config, save_config, reset_config

# ... (existing imports)

# Legacy Settings API (Removed duplicate test endpoint, using modern version at end of file)
@app.get("/api/settings")
def get_settings():
    """Retrieve current application configuration."""
    return load_config()

@app.post("/api/settings")
def update_settings(settings: Dict[str, Any]):
    """Save new application configuration."""
    from runtime.config_loader import save_config_detailed
    success, message = save_config_detailed(settings)
    if success:
        return {"status": "success", "message": "Settings saved"}
    raise HTTPException(status_code=500, detail=f"Failed to save settings: {message}")

@app.post("/api/settings/reset")
def reset_settings():
    """Reset settings to factory defaults."""
    config = reset_config()
    return {"status": "success", "message": "Settings reset to defaults", "config": config}
@app.get("/api/diagnose/connectivity")
async def diagnose_connectivity():
    """
    Run full connectivity diagnosis (Doctor).
    Used by First-Run Wizard.
    """
    try:
        from modules.system.connection_doctor import ConnectionDoctor
        doctor = ConnectionDoctor()
        # Run diagnosis
        result = await doctor.diagnose_all()
        return result
    except Exception as e:
        logger.error(f"Diagnosis failed: {e}")
        return {
            "status": "offline", 
            "checks": [], 
            "can_proceed": False,
            "can_proceed_with_degraded": False
        }








# -----------------------------------------------------------------------------
# SurfSense Integration Endpoints
# -----------------------------------------------------------------------------

class SurfSenseCheckRequest(BaseModel):
    base_url: str = "http://localhost:8000"

@app.post("/api/surfsense/check")
async def check_surfsense(request: SurfSenseCheckRequest):
    """Check if SurfSense is running and healthy."""
    from modules.surfsense import get_surfsense_client
    
    client = get_surfsense_client(request.base_url)
    is_healthy = await client.health_check()
    
    if is_healthy:
        return {"status": "success", "message": "SurfSense is running"}
    else:
        return {"status": "error", "message": "SurfSense is not reachable"}

class PodcastRequest(BaseModel):
    chat_id: str
    voice: str = "alloy"  # OpenAI voices: alloy, echo, fable, onyx, nova, shimmer

@app.post("/api/surfsense/podcast")
async def generate_podcast(request: PodcastRequest):
    """Generate audio podcast from a chat conversation via SurfSense."""
    from modules.surfsense import get_surfsense_client
    from runtime.config_loader import load_config
    
    config = load_config()
    surfsense_url = config.get('ai_provider', {}).get('surfsense_url', 'http://localhost:8000')
    
    client = get_surfsense_client(surfsense_url)
    
    if not await client.health_check():
        raise HTTPException(status_code=503, detail="SurfSense is not running")
    
    result = await client.generate_podcast(request.chat_id, request.voice)
    
    if "error" in result:
        raise HTTPException(status_code=500, detail=result["error"])
    
    return result

class SurfSenseSearchRequest(BaseModel):
    query: str
    top_k: int = 5

@app.post("/api/surfsense/search")
async def surfsense_search(request: SurfSenseSearchRequest):
    """Search SurfSense knowledge base directly."""
    from modules.surfsense import get_surfsense_client
    from runtime.config_loader import load_config
    
    config = load_config()
    surfsense_url = config.get('ai_provider', {}).get('surfsense_url', 'http://localhost:8000')
    
    client = get_surfsense_client(surfsense_url)
    
    if not await client.health_check():
        return {"results": [], "source": "offline"}
    
    results = await client.search(request.query, top_k=request.top_k)
    return {"results": results, "source": "surfsense"}

# -----------------------------------------------------------------------------
# AGENT ZERO: Central Brain with Full Software Control
# -----------------------------------------------------------------------------

# Agent Zero Constitution & Roles

AGENT_ZERO_CONSTITUTION = """
## IDENTITY
You are Agent Zero, the central intelligence of BioDockify - a pharmaceutical research platform.

## CONSTITUTION (Core Principles)
1. ACCURACY: Provide scientifically accurate, evidence-based information
2. SAFETY: Never provide dangerous drug synthesis or harmful advice
3. TRANSPARENCY: Always cite sources and acknowledge uncertainty
4. EFFICIENCY: Choose the optimal tool for each task
5. PRIVACY: Never share user data outside the system
6. AUTONOMY: You are responsible for your own maintenance and configuration.

## ROLES (You can fulfill all these roles)
- RESEARCHER: Search literature, find papers, summarize findings
- ANALYST: Perform statistical analysis on data
- LIBRARIAN: Manage knowledge base, index documents, retrieve information
- AUDITOR: Verify journal legitimacy, detect predatory publishers
- ASSISTANT: Answer questions, explain concepts, help with workflows
- SYSTEM_ADMIN: Self-diagnose, update settings, and manage background services.

## COMMUNICATION PROTOCOL (MANDATORY)
You must ALWAYS respond with a JSON object in this format:
{
    "thoughts": [
        "Step 1: Analyze the user's request...",
        "Step 2: Check available tools...",
        "Step 3: Decide to use tool X..."
    ],
    "headline": "Brief summary of what you are doing",
    "action": {
        "type": "tool_use" | "final_answer",
        "name": "tool_name",
        "args": { ... }
    }
}

## AVAILABLE ACTIONS (Use when appropriate)
- [ACTION: research | query=<topic>] - Search PubMed/literature
- [ACTION: search_kb | query=<topic>] - Search user's knowledge base
- [ACTION: verify_journal | issn=<issn>, title=<name>] - Check journal legitimacy
- [ACTION: analyze_stats | data=<json>, design=<type>] - Statistical analysis
- [ACTION: podcast | text=<content>] - Generate audio podcast
- [ACTION: web_search | query=<topic>] - Search the web
- [ACTION: deep_research | url=<url>] - Deep research visiting the page (autonomously extracts content)
- [ACTION: delegate_task | task=<description>, role=<role>] - Recursively spawn a sub-agent to handle a complex sub-task
- [ACTION: check_health] - Run system diagnostics (Doctor)
- [ACTION: update_settings | section=<name>, key=<name>, value=<value>] - Edit configuration
- [ACTION: restart_service | service=<name>] - Restart Ollama/SurfSense

## OPERATING MODES
1. CHAT (Default): Answer questions, provide guidance
2. SEMI-AUTONOMOUS: Execute actions when user requests
3. AUTONOMOUS: Proactively complete complex multi-step tasks

## SELF-HEALING & MAINTENANCE
- If a tool fails, analyze the error and try a different approach.
- If an API Key is missing or invalid, ask the user to provide it OR update it yourself if provided in chat.
- Periodically check your own health using [ACTION: check_health].

## RESPONSE FORMAT
- For simple questions: Just answer
- For actions: Include [ACTION: ...] tags
- For multi-step: Break down and explain each step
"""

class AgentExecuteRequest(BaseModel):
    action: str  # "research", "search_kb", "verify_journal", "analyze_stats", "podcast", "web_search", "check_health", "update_settings", "restart_service"
    params: Dict[str, Any] = {}

@app.post("/api/agent/execute")
async def agent_execute(request: AgentExecuteRequest):
    """
    Universal Agent Zero execution endpoint.
    Allows Agent Zero (or direct API calls) to invoke any software function.
    """
    action = request.action.lower()
    params = request.params
    
    try:
        # RESEARCH: Search literature
        if action == "research":
            query = params.get("query", "")
            from modules.literature import search_pubmed
            results = search_pubmed(query, max_results=10)
            return {"status": "success", "action": "research", "results": results}
        
        # SEARCH_KB: Search knowledge base
        elif action == "search_kb":
            query = params.get("query", "")
            from modules.rag.vector_store import get_vector_store
            vs = get_vector_store()
            results = vs.search(query, k=5)
            return {"status": "success", "action": "search_kb", "results": results}
        
        # VERIFY_JOURNAL: Check journal legitimacy
        elif action == "verify_journal":
            issn = params.get("issn", "")
            title = params.get("title", "")
            url = params.get("url")
            from modules.journal_intel import DecisionEngine
            engine = DecisionEngine()
            result = engine.verify(issn=issn, title=title, url=url)
            return {
                "status": "success", 
                "action": "verify_journal",
                "decision": result.decision,
                "confidence": result.confidence_level,
                "risk_factors": result.risk_factors
            }
        
        # ANALYZE_STATS: Run statistical analysis
        elif action == "analyze_stats":
            data = params.get("data", [])
            design = params.get("design", "descriptive")
            tier = params.get("tier", "basic")
            from modules.statistics.engine import StatisticalEngine
            engine = StatisticalEngine()
            result = engine.analyze(data, design=design, tier=tier)
            return {"status": "success", "action": "analyze_stats", "result": result}
        
        # PODCAST: Generate audio (via SurfSense)
        elif action == "podcast":
            text = params.get("text", "")
            chat_id = params.get("chat_id", "")
            from modules.surfsense import get_surfsense_client
            client = get_surfsense_client()
            if await client.health_check():
                result = await client.generate_podcast(chat_id)
                return {"status": "success", "action": "podcast", "result": result}
            return {"status": "error", "action": "podcast", "error": "SurfSense offline"}
        
        # WEB_SEARCH: Search the web (via SurfSense/Tavily)
        elif action == "web_search":
            query = params.get("query", "")
            from modules.surfsense import get_surfsense_client
            client = get_surfsense_client()
            if await client.health_check():
                results = await client.search(query)
                return {"status": "success", "action": "web_search", "results": results}
            return {"status": "error", "action": "web_search", "error": "SurfSense offline"}

        # CHECK_HEALTH: Run system diagnostics
        elif action == "check_health":
            from modules.system.doctor import SystemDoctor
            doctor = SystemDoctor()
            report = doctor.diagnose()
            return {"status": "success", "action": "check_health", "report": report}

        # UPDATE_SETTINGS: Reconfigure system
        elif action == "update_settings":
            section = params.get("section")
            key = params.get("key")
            value = params.get("value")
            
            if not section or not key:
                return {"status": "error", "message": "Section and Key required"}

            current_settings = load_config()
            
            # Navigate to section
            if section not in current_settings:
                return {"status": "error", "message": f"Section '{section}' not found"}
            
            # Update key
            current_settings[section][key] = value
            
            if save_config(current_settings):
                return {"status": "success", "action": "update_settings", "message": f"Updated {section}.{key} = {value}"}
            else:
                return {"status": "error", "message": "Failed to save settings"}

        # RESTART_SERVICE: Manage background services
        elif action == "restart_service":
            service = params.get("service")
            from runtime.service_manager import ServiceManager
            mgr = ServiceManager()
            if service == "ollama":
                mgr.restart_service("ollama")
            elif service == "surfsense":
                mgr.restart_service("surfsense")
            else:
                return {"status": "error", "message": "Unknown service"}
            return {"status": "success", "action": "restart_service", "message": f"Restarted {service}"}
        
        # DEEP_RESEARCH: Autonomous Headless Browsing
        elif action == "deep_research":
            url = params.get("url", "")
            from modules.headless_research import deep_research
            result = await deep_research(url)
            return {"status": "success", "action": "deep_research", "result": result}
            
        # DEEP_REVIEW: Full Autonomous Review Pipeline
        elif action == "deep_review":
            topic = params.get("topic", "")
            from modules.literature.orchestrator import orchestrator
            result = await orchestrator.run_deep_review(topic)
            return {"status": "success", "action": "deep_review", "result": result}
            
        # DELEGATE_TASK: Recursive Agent Spawning (Agent Zero Feature)
        elif action == "delegate_task":
            task_desc = params.get("task", "")
            role = params.get("role", "assistant")
            
            # Simple recursive call: We just re-invoke the chat endpoint with a new context?
            # Ideally, we spin up a new Orchestrator or just simple distinct chat session.
            # For this MVP, we will treat it as a specialized "sub-agent" call via the chat API logic 
            # but instantiated purely in Python to avoid infinite HTTP loops if timeouts occur.
            
            from modules.llm.factory import LLMFactory
            from runtime.config_loader import load_config
            
            cfg = load_config()
            adapter = LLMFactory.get_adapter(cfg)
            
            # Specialized prompt for the sub-agent
            sub_prompt = f"""
            You are a sub-agent specialized in: {role}.
            Your SUPEIOR has assigned you this task: {task_desc}
            
            Execute it efficiently and return a summary of your findings.
            """
            
            try:
                # Direct LLM call for the sub-task
                sub_response = adapter.generate(sub_prompt)
                return {"status": "success", "action": "delegate_task", "result": sub_response}
            except Exception as e:
                return {"status": "error", "action": "delegate_task", "error": f"Sub-agent failed: {str(e)}"}
        

        # SURFSENSE_VIDEO: Generate Video from Topic/Text
        elif action == "generate_video":
            text = params.get("text", "")
            topic = params.get("topic", "")
            
            # If topic provided but no text, generate script first (simplified for now)
            if topic and not text:
                text = f"This is a video summary about {topic}. Detailed analysis coming soon."
            
            # 1. Generate Audio
            from modules.surfsense.audio import generate_podcast_audio
            import uuid
            run_id = uuid.uuid4().hex
            output_dir = os.path.join(os.getcwd(), "ui", "public", "generated", run_id)
            os.makedirs(output_dir, exist_ok=True)
            
            audio_path = os.path.join(output_dir, "audio.mp3")
            await generate_podcast_audio(text, output_path=audio_path)
            
            # 2. Generate Slides
            from modules.surfsense.slides import generate_slides
            # Convert simple text to markdown for slides (very basic heuristic)
            md_text = f"# {topic or 'Video Summary'}\n\n- {text[:50]}...\n- Key Point 1\n- Key Point 2\n"
            slides_path = os.path.join(output_dir, "slides")
            slide_images = await generate_slides(md_text, output_dir=slides_path)
            
            # 3. Stitch Video
            from modules.surfsense.video import create_video_summary
            video_path = os.path.join(output_dir, "video.mp4")
            final_video = await create_video_summary(slide_images, audio_path, output_path=video_path)
            
            # Return relative paths for UI
            return {
                "status": "success", 
                "action": "generate_video", 
                "result": {
                    "video_url": f"/generated/{run_id}/video.mp4",
                    "audio_url": f"/generated/{run_id}/audio.mp3",
                    "slides_count": len(slide_images)
                }
            }

        else:
            return {"status": "error", "error": f"Unknown action: {action}"}
            
    except Exception as e:
        logger.error(f"Agent Execute Error: {e}")
        return {"status": "error", "action": action, "error": str(e)}


# -----------------------------------------------------------------------------
# Agent Chat API (with multi-mode support)
# -----------------------------------------------------------------------------
class AgentChatRequest(BaseModel):
    message: str
    mode: str = "chat"  # "chat", "semi-autonomous", "autonomous"

@app.post("/api/agent/chat")
def agent_chat_endpoint(request: AgentChatRequest):
    """
    Agent Zero Chat Endpoint with SurfSense + RAG Integration.
    1. Tries SurfSense first (if running) for advanced RAG
    2. Falls back to internal Knowledge Base
    3. Routes to configured LLM provider
    """
    import requests as req
    import asyncio
    from modules.rag.vector_store import get_vector_store
    from modules.surfsense import get_surfsense_client
    
    config = load_config()
    provider_config = config.get('ai_provider', {})
    
    user_query = request.message
    
    # =========================================================================
    # STEP 1: Search SurfSense (Priority) then Internal KB (Fallback)
    # =========================================================================
    kb_context = ""
    kb_sources = []
    context_source = "none"
    
    # Try SurfSense first
    surfsense_url = provider_config.get('surfsense_url', 'http://localhost:8000')
    surfsense_client = get_surfsense_client(surfsense_url)
    
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        # Check if SurfSense is running
        if loop.run_until_complete(surfsense_client.health_check()):
            results = loop.run_until_complete(surfsense_client.search(user_query, top_k=3))
            if results:
                context_parts = []
                for r in results:
                    source = r.get('source', r.get('metadata', {}).get('source', 'SurfSense'))
                    text = r.get('text', r.get('content', ''))[:500]
                    context_parts.append(f"[Source: {source}]\n{text}")
                    kb_sources.append(source)
                kb_context = "\n\n---\n\n".join(context_parts)
                context_source = "surfsense"
        loop.close()
    except Exception as e:
        logger.warning(f"SurfSense search failed, falling back to internal KB: {e}")
    
    # Fallback to internal KB
    if not kb_context:
        try:
            vector_store = get_vector_store()
            results = vector_store.search(user_query, k=3)
            
            if results:
                context_parts = []
                for r in results:
                    source = r.get('metadata', {}).get('source', 'Unknown')
                    text = r.get('text', '')[:500]
                    context_parts.append(f"[Source: {source}]\n{text}")
                    kb_sources.append(source)
                kb_context = "\n\n---\n\n".join(context_parts)
                context_source = "internal_kb"
        except Exception as e:
            logger.warning(f"Internal KB search failed: {e}")
    
    # =========================================================================
    # STEP 2: Build Enhanced Prompt with KB Context and Constitution
    # =========================================================================
    # =========================================================================
    # =========================================================================
    # STEP 2: Build Enhanced Prompt with KB Context and Constitution
    # =========================================================================
    # Use the full Agent Zero Constitution as system prompt
    from modules.agent import AGENT_ZERO_SYSTEM_PROMPT, PHD_THESIS_WRITER_PROMPT, PHARMA_REVIEW_WRITER_PROMPT, PHARMA_RESEARCH_WRITER_PROMPT
    
    if request.mode == "thesis_writer":
        base_prompt = PHD_THESIS_WRITER_PROMPT
    elif request.mode == "review_writer":
        base_prompt = PHARMA_REVIEW_WRITER_PROMPT
    elif request.mode == "research_writer":
        base_prompt = PHARMA_RESEARCH_WRITER_PROMPT
    else:
        # Default to the Pharma Research / General Agent Zero prompt
        base_prompt = AGENT_ZERO_SYSTEM_PROMPT

    system_prompt = base_prompt + f"""

## CURRENT MODE: {request.mode.upper()}
## CONTEXT SOURCE: {context_source}
"""

    if kb_context:
        enhanced_prompt = f"""Use the following context from the Knowledge Base to answer the question:

---
KNOWLEDGE BASE CONTEXT:
{kb_context}
---

USER QUESTION: {user_query}

Provide a well-structured answer based on the context above. If the context doesn't contain relevant information, say so and provide general guidance."""
    else:
        enhanced_prompt = user_query
    
    errors = []  # Track all provider errors for debugging
    
    # =========================================================================
    # PROVIDER 1: Custom/Paid OpenAI-Compatible API (GLM, Groq, OpenAI, etc.)
    # =========================================================================
    if provider_config.get('custom_key') and provider_config.get('custom_base_url'):
        try:
            base_url = provider_config['custom_base_url'].rstrip('/')
            key = provider_config['custom_key']
            model = provider_config.get('custom_model', 'gpt-3.5-turbo')
            
            url = f"{base_url}/chat/completions"
            headers = {"Authorization": f"Bearer {key}", "Content-Type": "application/json"}
            payload = {
                "model": model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": enhanced_prompt}
                ]
            }
            
            resp = req.post(url, json=payload, headers=headers, timeout=60)
            if resp.status_code == 200:
                data = resp.json()
                content = data.get('choices', [{}])[0].get('message', {}).get('content', '')
                if content:
                    return {"reply": content, "provider": "custom"}
            errors.append(f"Custom API ({model}): {resp.status_code}")
        except Exception as e:
            errors.append(f"Custom API: {str(e)}")
    
    # =========================================================================
    # PROVIDER 2: Google Gemini
    # =========================================================================
    if provider_config.get('google_key'):
        try:
            key = provider_config['google_key']
            url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key={key}"
            payload = {
                "contents": [{"parts": [{"text": f"{system_prompt}\n\nUser: {enhanced_prompt}"}]}]
            }
            
            resp = req.post(url, json=payload, timeout=60)
            if resp.status_code == 200:
                data = resp.json()
                content = data.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', '')
                if content:
                    return {"reply": content, "provider": "google"}
            errors.append(f"Google Gemini: {resp.status_code}")
        except Exception as e:
            errors.append(f"Google Gemini: {str(e)}")
    
    # =========================================================================
    # PROVIDER 3: OpenRouter
    # =========================================================================
    if provider_config.get('openrouter_key'):
        try:
            key = provider_config['openrouter_key']
            url = "https://openrouter.ai/api/v1/chat/completions"
            headers = {
                "Authorization": f"Bearer {key}",
                "HTTP-Referer": "http://localhost:3000",
                "X-Title": "BioDockify",
                "Content-Type": "application/json"
            }
            payload = {
                "model": "openai/gpt-3.5-turbo",  # Default free-tier model
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": enhanced_prompt}
                ]
            }
            
            resp = req.post(url, json=payload, headers=headers, timeout=60)
            if resp.status_code == 200:
                content = resp.json().get('choices', [{}])[0].get('message', {}).get('content', '')
                if content:
                    return {"reply": content, "provider": "openrouter"}
            errors.append(f"OpenRouter: {resp.status_code}")
        except Exception as e:
            errors.append(f"OpenRouter: {str(e)}")
    
    # =========================================================================
    # PROVIDER 4: GLM (ZhipuAI) - Direct API
    # =========================================================================
    if provider_config.get('glm_key'):
        try:
            key = provider_config['glm_key']
            url = "https://open.bigmodel.cn/api/paas/v4/chat/completions"
            headers = {"Authorization": f"Bearer {key}", "Content-Type": "application/json"}
            payload = {
                "model": "glm-4",
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": enhanced_prompt}
                ]
            }
            
            resp = req.post(url, json=payload, headers=headers, timeout=60)
            if resp.status_code == 200:
                content = resp.json().get('choices', [{}])[0].get('message', {}).get('content', '')
                if content:
                    return {"reply": content, "provider": "glm"}
            errors.append(f"GLM: {resp.status_code}")
        except Exception as e:
            errors.append(f"GLM: {str(e)}")
    
    # =========================================================================
    # PROVIDER 5: HuggingFace Inference API
    # =========================================================================
    if provider_config.get('huggingface_key'):
        try:
            key = provider_config['huggingface_key']
            url = "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.1"
            headers = {"Authorization": f"Bearer {key}"}
            payload = {"inputs": f"<s>[INST] {system_prompt}\n\n{enhanced_prompt} [/INST]"}
            
            resp = req.post(url, json=payload, headers=headers, timeout=60)
            if resp.status_code == 200:
                data = resp.json()
                if isinstance(data, list) and len(data) > 0:
                    content = data[0].get('generated_text', '')
                    # Extract response after [/INST]
                    if '[/INST]' in content:
                        content = content.split('[/INST]')[-1].strip()
                    if content:
                        return {"reply": content, "provider": "huggingface"}
            errors.append(f"HuggingFace: {resp.status_code}")
        except Exception as e:
            errors.append(f"HuggingFace: {str(e)}")
    
    # =========================================================================
    # PROVIDER 5.5: LM Studio (Local OpenAI-Compatible)
    # =========================================================================
    if provider_config.get('mode') == 'lm_studio' or provider_config.get('lm_studio_url'):
        try:
            lm_url = provider_config.get('lm_studio_url', 'http://localhost:1234/v1')
            lm_model = provider_config.get('lm_studio_model', 'local-model')
            
            # Ensure URL ends with /chat/completions if not present, but handle base URL correctly
            # LM Studio usually gives http://localhost:1234/v1
            if not lm_url.endswith('/chat/completions'):
                url = f"{lm_url.rstrip('/')}/chat/completions"
            else:
                url = lm_url

            payload = {
                "model": lm_model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": enhanced_prompt}
                ],
                "temperature": 0.7,
                "max_tokens": -1, # LM Studio often needs this for unlimited
                "stream": False
            }
            
            # LM Studio doesn't strictly require a key, but some setups might
            headers = {"Content-Type": "application/json"}
            
            resp = req.post(url, json=payload, headers=headers, timeout=120)
            
            if resp.status_code == 200:
                data = resp.json()
                content = data.get('choices', [{}])[0].get('message', {}).get('content', '')
                if content:
                    return {"reply": content, "provider": "lm_studio"}
            errors.append(f"LM Studio: {resp.status_code} - {resp.text}")
        except Exception as e:
            errors.append(f"LM Studio Error: {str(e)}")

    try:
        ollama_url = provider_config.get('ollama_url', 'http://localhost:11434')
        model = provider_config.get('ollama_model', 'llama2')
        
        url = f"{ollama_url}/api/chat"
        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": enhanced_prompt}
            ],
            "stream": False
        }
        
        resp = req.post(url, json=payload, timeout=120)  # Longer timeout for local
        if resp.status_code == 200:
            content = resp.json().get('message', {}).get('content', '')
            if content:
                return {"reply": content, "provider": "ollama"}
        errors.append(f"Ollama ({model}): {resp.status_code}")
    except Exception as e:
        errors.append(f"Ollama: {str(e)}")
    
    # =========================================================================
    # ALL PROVIDERS FAILED
    # =========================================================================
    error_summary = "; ".join(errors) if errors else "No providers configured"
    
    # Check if Ollama was the only configured provider and failed
    has_cloud_provider = any([
        provider_config.get('custom_key'),
        provider_config.get('google_key'),
        provider_config.get('openrouter_key'),
        provider_config.get('glm_key'),
        provider_config.get('huggingface_key')
    ])
    
    if not has_cloud_provider and 'Ollama' in error_summary:
        # User only has Ollama configured and it failed
        return {
            "reply": f" Agent Zero could not connect to Ollama.\n\n**Error:** {error_summary}\n\n**Why this happens:** Ollama requires a good CPU or GPU to run. If you don't have one, Ollama may not work well or at all.\n\n**Solution:** Configure a free cloud API in Settings:\n **Google Gemini** (Free tier available)\n **OpenRouter** (Free tier available)\n **HuggingFace** (Free tier available)\n\nGo to **Settings  AI & Brain** to add your API key.",
            "provider": "none",
            "errors": errors
        }
    else:
        return {
            "reply": f" Agent Zero could not connect to any AI provider.\n\n**Attempted:** {error_summary}\n\n**Solution:** Please configure at least one provider in Settings (Google, OpenRouter, HuggingFace, Custom API, or Ollama).",
            "provider": "none",
            "errors": errors
        }

from fastapi import UploadFile, File
from modules.rag.ingestor import ingestor
from modules.rag.vector_store import get_vector_store
import shutil
import tempfile
import os

@app.post("/api/rag/upload")
async def upload_document(file: UploadFile = File(...)):
    """Upload and ingest a document (PDF, Notebook, MD) into the knowledge base."""
    import re
    
    # 1. Sanitize Filename
    # Simple rigorous sanitization: allow only alphanumeric, dot, dash, underscore
    clean_name = re.sub(r'[^a-zA-Z0-9_.-]', '_', file.filename)
    if not clean_name:
        clean_name = f"upload_{uuid.uuid4().hex}"
        
    # 2. Validate Extension
    ALLOWED_EXTS = {'.pdf', '.md', '.txt', '.ipynb', '.json'}
    suffix = os.path.splitext(clean_name.lower())[1]
    
    if suffix not in ALLOWED_EXTS:
        raise HTTPException(status_code=400, detail=f"Unsupported file type: {suffix}. Allowed: {ALLOWED_EXTS}")
        
    # 3. Validate Context/Magic Bytes (Optional but recommended for PDF)
    # For now, we rely on ingestion pipeline to fail if invalid, but we ensure safe temp write.
    
    # Save to temp file
    tmp_path = None
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
            shutil.copyfileobj(file.file, tmp)
            tmp_path = tmp.name
            
        # Ingest
        chunks = ingestor.ingest_file(tmp_path)
        
        # Add metadata override for original filename
        for chunk in chunks:
            chunk['metadata']['source'] = clean_name
            
        get_vector_store().add_documents(chunks)
        return {"status": "success", "message": f"Indexed {len(chunks)} chunks from {clean_name}"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Upload failed: {e}")
        raise HTTPException(status_code=500, detail="Failed to process file upload")
    finally:
        if 'tmp_path' in locals() and os.path.exists(tmp_path):
            os.remove(tmp_path)


class Neo4jCheckRequest(BaseModel):
    """DEPRECATED: Neo4j has been replaced by SurfSense Knowledge Engine."""
    uri: str = ""
    user: str = ""
    password: str = ""

@app.post("/api/settings/neo4j/check")
def check_neo4j_endpoint(request: Neo4jCheckRequest):
    """
    DEPRECATED: Neo4j Graph Database check.
    Neo4j has been replaced by SurfSense Knowledge Engine.
    This endpoint is kept for backward compatibility.
    """
    return {
        "status": "deprecated",
        "message": "Neo4j has been replaced by SurfSense Knowledge Engine. Configure SurfSense in Settings > Cloud APIs instead."
    }


class LinkRequest(BaseModel):
    url: str

@app.post("/api/rag/link")
async def link_url(request: LinkRequest):
    tmp_path = None
    try:
        text = scrape_url(request.url)
        if not text:
            raise HTTPException(status_code=400, detail="No content found at URL")
            
        # Create a document chunk (simplified ingestion for direct text)
        # We wrap it in a format ingestor/vector_store expects?
        # Actually ingestor expects a file path.
        # Let's save text to a temp .txt file and use existing ingestor!
        
        with tempfile.NamedTemporaryFile(delete=False, suffix=".txt", mode='w', encoding='utf-8') as tmp:
            tmp.write(f"Source: {request.url}\n\n")
            tmp.write(text)
            tmp_path = tmp.name
            
        chunks = ingestor.ingest_file(tmp_path)
        
        # Override metadata
        for chunk in chunks:
            chunk['metadata']['source'] = request.url
            
        get_vector_store().add_documents(chunks)
        os.remove(tmp_path)
        
        return {"status": "success", "message": f"Indexed content from {request.url}"}
        
    except Exception as e:
        if 'tmp_path' in locals(): os.remove(tmp_path)
        raise HTTPException(status_code=500, detail=str(e))



class ChatRequest(BaseModel):
    query: str

@app.post("/api/rag/chat")

async def chat_with_docs(request: ChatRequest):
    """Retrieve context and answer via configured LLM (Ollama/Agent)."""
    query = request.query
    
    # 1. Retrieve Context
    vector_store = get_vector_store()
    results = vector_store.search(query, k=5) # Top 5 chunks
    context_list = [r['text'] for r in results]
    
    if not context_list:
        return {"answer": "I don't have enough context from your uploaded documents to answer that.", "sources": []}
        
    context_text = "\n\n---\n\n".join(context_list)
    
    # 2. Construct Augmented Prompt
    system_prompt = "You are a research assistant. Answer the user's question based ONLY on the provided Context. If the answer is not in the context, say so."
    full_prompt = f"Context:\n{context_text}\n\nQuestion: {query}\n\nAnswer:"
    
    # 3. Call LLM (Ollama or Configured Provider)
    from runtime.config_loader import load_config
    import requests
    import json
    
    cfg = load_config()
    ai_conf = cfg.get("ai_provider", {})
    
    answer = "Error generating response."
    
    try:
        # Default to Ollama for "Local NotebookLM" experience
        # Check if Ollama is configured
        ollama_url = ai_conf.get("ollama_url", "http://localhost:11434")
        ollama_model = ai_conf.get("ollama_model", "llama3") # Default to llama3 or mistral if not set
        
        # Simple Ollama Generate
        # We use non-streaming for this simple endpoint first
        resp = requests.post(
            f"{ollama_url}/api/generate",
            json={
                "model": ollama_model,
                "prompt": full_prompt,
                "system": system_prompt,
                "stream": False
            },
            timeout=60 
        )
        
        if resp.status_code == 200:
            answer = resp.json().get("response", "No response from model.")
        else:
            # Fallback if Ollama fails or not found?
            # Maybe use the "Agent Zero" / Cloud LLM if configured?
            # For now, return error to prompt user to check settings.
            answer = f"Ollama Error ({resp.status_code}): {resp.text}"
            
    except Exception as e:
        answer = f"Failed to connect to Local LLM: {str(e)}. Please Ensure Ollama is running."

    return {
        "answer": answer,
        "context": context_text,
        "sources": [] 
    }

@app.post("/api/rag/clear")
def clear_knowledge_base():
    """Clear all indexed documents."""
    get_vector_store().clear()
    return {"status": "success", "message": "Knowledge base cleared."}


# Duplicate Agent Chat Endpoint Removed.
# The robust implementations is at line 416.


# -----------------------------------------------------------------------------
# System Info API (For First Run Wizard)
# -----------------------------------------------------------------------------

@app.get("/api/system/info")
def get_system_info():
    """
    Retrieve system hardware and environment information.
    Used by the First Run Wizard to validate compatibility.
    """
    import platform
    import os
    import shutil
    import sys
    import tempfile
    
    # OS Info
    os_name = f"{platform.system()} {platform.release()}"
    
    # CPU
    cpu_cores = os.cpu_count() or 1
    
    # RAM (Approximate using psutil if available, else vague or skip)
    ram_total_gb = 0
    ram_available_gb = 0
    
    try:
        import psutil
        mem = psutil.virtual_memory()
        ram_total_gb = round(mem.total / (1024**3), 1)
        ram_available_gb = round(mem.available / (1024**3), 1)
    except ImportError:
        # Fallback if psutil not installed
        ram_total_gb = 0 
        ram_available_gb = 0
        
    # Disk Usage
    try:
        total, used, free = shutil.disk_usage(".")
        disk_free_gb = round(free / (1024**3), 1)
    except:
        disk_free_gb = 0
        
    # Python
    python_version = sys.version.split(" ")[0]
    
    # Temp Writable Check
    temp_writable = False
    try:
        with tempfile.TemporaryFile() as f:
            f.write(b"test")
            temp_writable = True
    except:
        temp_writable = False
        
    return {
        "os": os_name,
        "cpu_cores": cpu_cores,
        "ram_total_gb": ram_total_gb,
        "ram_available_gb": ram_available_gb,
        "disk_free_gb": disk_free_gb,
        "temp_writable": temp_writable,
        "python_version": python_version
    }





# -----------------------------------------------------------------------------
# PHD THESIS CORE ENDPOINTS (Phase 7)
# -----------------------------------------------------------------------------
from modules.thesis.engine import thesis_engine
from modules.thesis.structure import THESIS_STRUCTURE, PharmaBranch, DegreeType

class ThesisRequest(BaseModel):
    chapter_id: str
    topic: str
    branch: PharmaBranch = PharmaBranch.GENERAL
    degree: DegreeType = DegreeType.PHD

@app.get("/api/thesis/structure")
def get_thesis_structure():
    """Return the Master Pharma Thesis structure definition."""
    # Convert Enum keys to string for JSON serialization
    return {k.value: v.dict() for k, v in THESIS_STRUCTURE.items()}

@app.get("/api/thesis/validate/{chapter_id}")
def validate_chapter(chapter_id: str, branch: PharmaBranch = PharmaBranch.GENERAL, degree: DegreeType = DegreeType.PHD):
    """Check if proofs exist for a chapter with branch-awareness."""
    from modules.thesis.validator import thesis_validator
    return thesis_validator.validate_chapter_readiness(chapter_id, branch, degree)

@app.post("/api/thesis/generate")
async def generate_chapter_endpoint(req: ThesisRequest):
    """Generate a chapter draft if validation passes."""
    return await thesis_engine.generate_chapter(req.chapter_id, req.topic, branch=req.branch, degree=req.degree)

# -----------------------------------------------------------------------------
# Google Drive Backup System
# -----------------------------------------------------------------------------

# Initialize Backup System
drive_client = DriveClient(storage_path="./mock_cloud_storage")
backup_manager = BackupManager(drive_client)

class BackupAuthVerifyRequest(BaseModel):
    code: str

class RestoreRequest(BaseModel):
    snapshot_id: str

@app.post("/api/backup/auth/url")
def get_backup_auth_url():
    """Returns the URL to start the OAuth flow via BioDockify."""
    return {"url": drive_client.get_auth_url()}

@app.post("/api/backup/auth/verify")
def verify_backup_auth(req: BackupAuthVerifyRequest):
    """Verifies the auth code (simulated exchange)."""
    success = drive_client.authenticate(req.code)
    if success:
        return {"status": "success", "user": drive_client.get_user_info()}
    else:
        raise HTTPException(status_code=401, detail="Authentication failed")

@app.get("/api/backup/status")
def get_backup_status():
    """Returns connection status and user info."""
    return drive_client.get_user_info()

@app.post("/api/backup/run")
def run_backup(background_tasks: BackgroundTasks):
    """Triggers a backup in the background."""
    # Defers the actual backup to background task
    def _do_backup():
        # Backing up the adjacent 'brain' directory and 'ui/src' as example of critical data
        # In prod, this would be the user's workspace path
        target_dirs = ["./brain", "./ui/src/lib"] 
        result = backup_manager.create_backup(target_dirs)
        logger.info(f"Backup completed: {result}")
    
    background_tasks.add_task(_do_backup)
    return {"status": "started", "message": "Backup initiated in background"}

@app.get("/api/backup/history")
def get_backup_history():
    """Lists available snapshots."""
    return drive_client.list_backups()

@app.post("/api/backup/restore")
def restore_backup(req: RestoreRequest):
    """Restores a specific snapshot."""
    # Restoration path defaults to a safe 'restored' folder for safety in this demo
    target_path = "./restored_data"
    result = backup_manager.restore_backup(req.snapshot_id, target_path)
    if result["status"] == "error":
        raise HTTPException(status_code=500, detail=result["message"])
    return result


# =============================================================================
# SLIDES GENERATION ENDPOINTS
# =============================================================================

class SlideRequest(BaseModel):
    """Request model for slide generation."""
    source: str  # "knowledge_base", "search", "prompt", "documents"
    topic: str = ""
    search_query: str = ""
    custom_prompt: str = ""
    document_ids: List[str] = []
    style: str = "academic"
    num_slides: int = 10
    include_citations: bool = True


@app.post("/api/slides/generate")
async def generate_slides(request: SlideRequest, background_tasks: BackgroundTasks):
    """
    Generate presentation slides from various Knowledge Base sources.
    
    Supports:
    - knowledge_base: Query KB by topic
    - search: Generate from search results
    - prompt: Custom user description
    - documents: Specific document IDs
    """
    try:
        from modules.slides.slide_generator import get_slide_generator
        from modules.rag.vector_store import get_vector_store
        from modules.llm.factory import LLMFactory
        from runtime.config_loader import load_config
        
        # Initialize dependencies
        cfg = load_config()
        rag = get_vector_store()
        
        # Get LLM adapter
        try:
            from orchestration.planner.orchestrator import OrchestratorConfig
            orch_config = OrchestratorConfig()
            provider = cfg.get("ai_provider", {}).get("mode", "auto")
            llm = LLMFactory.get_adapter(provider, orch_config)
        except:
            llm = None
        
        generator = get_slide_generator(llm, rag)
        
        # Generate based on source type
        if request.source == "knowledge_base":
            result = generator.generate_from_knowledge_base(
                topic=request.topic,
                style=request.style,
                num_slides=request.num_slides,
                include_citations=request.include_citations
            )
        elif request.source == "search":
            # First perform search
            search_results = rag.search(request.search_query, top_k=request.num_slides * 2)
            result = generator.generate_from_search(
                search_results=search_results,
                style=request.style,
                title=request.topic or "Research Findings"
            )
        elif request.source == "prompt":
            result = generator.generate_from_prompt(
                prompt=request.custom_prompt,
                style=request.style,
                num_slides=request.num_slides
            )
        elif request.source == "documents":
            result = generator.generate_from_documents(
                document_ids=request.document_ids,
                style=request.style,
                title=request.topic or "Document Summary"
            )
        else:
            raise HTTPException(status_code=400, detail=f"Invalid source: {request.source}")
        
        return result
        
    except Exception as e:
        logger.error(f"Slides generation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/slides/styles")
def get_slide_styles():
    """Get available slide styles."""
    from modules.slides.slide_styles import SLIDE_STYLES
    return {
        "styles": [
            {"id": k, "name": v["name"], "description": v["description"]}
            for k, v in SLIDE_STYLES.items()
        ]
    }


@app.post("/api/slides/render")
async def render_slides_html(request: dict):
    """Render slides as HTML for preview or download."""
    try:
        from modules.slides.slide_styles import generate_presentation_html
        
        slides = request.get("slides", [])
        style = request.get("style", "academic")
        title = request.get("title", "Presentation")
        
        html = generate_presentation_html(slides, style, title)
        
        return {"status": "success", "html": html}
        

    except Exception as e:
        logger.error(f"Slides rendering failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# -----------------------------------------------------------------------------
# SETTINGS & CONNECTION TEST
# -----------------------------------------------------------------------------
class TestConnectionRequest(BaseModel):
    service_type: str  # 'llm', 'elsevier'
    provider: Optional[str] = None
    key: Optional[str] = None
    base_url: Optional[str] = None
    model: Optional[str] = None

@app.post("/api/settings/test")
async def test_connection_endpoint(request: TestConnectionRequest):
    """
    Test connection to external services to avoid CORS issues.
    Proxies the request from the backend to escape browser restrictions.
    """
    logger.info(f"[DEBUG] test_connection_endpoint called: service_type={request.service_type}, provider={request.provider}, has_key={bool(request.key)}, base_url={request.base_url}, model={request.model}")
    
    try:
        # 1. LLM Testing
        if request.service_type == "llm":
            from modules.llm.adapters import (
                GoogleGeminiAdapter, 
                OpenRouterAdapter, 
                HuggingFaceAdapter, 
                CustomAdapter, 
                ZhipuAdapter
            )
            
            adapter = None
            
            # Map provider to Adapter
            # Dedicated presets
            if request.provider == "google":
                adapter = GoogleGeminiAdapter(request.key)
            elif request.provider == "openrouter":
                adapter = OpenRouterAdapter(request.key, model=request.model or "mistralai/mistral-7b-instruct")
            elif request.provider == "huggingface":
                adapter = HuggingFaceAdapter(request.key)
            elif request.provider in ["glm", "zhipu"]:
                adapter = ZhipuAdapter(request.key)
            elif request.provider == "deepseek":
                # DeepSeek dedicated case (using CustomAdapter with official base URL)
                adapter = CustomAdapter(request.key, "https://api.deepseek.com", request.model or "deepseek-chat")
            elif request.provider == "groq":
                adapter = CustomAdapter(request.key, "https://api.groq.com/openai/v1", request.model or "llama-3.3-70b-versatile")
            
            # Generic Custom case
            elif request.provider == "custom":
                if not request.base_url:
                     return {"status": "error", "message": "Base URL required for custom provider"}
                
                # Normalize URL
                base = request.base_url.rstrip("/")
                if base.endswith("/chat/completions"):
                    base = base.replace("/chat/completions", "")
                
                adapter = CustomAdapter(request.key or "dummy", base, request.model)
            
            if not adapter:
                 return {"status": "error", "message": f"Unsupported provider: {request.provider}"}

            # Perform test generation in a threadpool to avoid blocking the event loop
            def _run_test():
                return adapter.generate("Test. Reply with 'OK'.", system_prompt="You are a connection tester.")
            
            logger.info(f"[DEBUG] Running {request.provider} test via executor...")
            response = await asyncio.get_event_loop().run_in_executor(None, _run_test)
            
            if response:
                 logger.info(f"[DEBUG] {request.provider} test SUCCESS")
                 return {"status": "success", "message": f"Connected! Response: {response[:50]}..."}
            else:
                 logger.warning(f"[DEBUG] {request.provider} test FAILED: No response")
                 return {"status": "error", "message": "No response received from API."}

        # 2. Elsevier Testing
        elif request.service_type == "elsevier":
             if not request.key:
                 return {"status": "error", "message": "API Key required"}
             
             # Simple test query to ScienceDirect or Scopus
             # We use requests directly here
             headers = {
                 "X-ELS-APIKey": request.key,
                 "Accept": "application/json"
             }
             # Search for something static
             url = "https://api.elsevier.com/content/search/scopus?query=heart&count=1"
             
             def _do_req():
                 with requests.Session() as session:
                     return session.get(url, headers=headers, timeout=10)
            
             resp = await asyncio.get_event_loop().run_in_executor(None, _do_req)
                 
             if resp.status_code == 200:
                  return {"status": "success", "message": "Elsevier API Connected via Scopus"}
             elif resp.status_code == 401:
                  return {"status": "error", "message": "Invalid API Key"}
             else:
                  return {"status": "error", "message": f"API Error: {resp.status_code} {resp.text[:100]}"}

        # 3. Bohrium Testing (MCP)
        elif request.service_type == "bohrium":
            target_url = request.base_url or "http://localhost:7000/mcp"
            
            # Simple JSON-RPC tool call
            payload = {
                "jsonrpc": "2.0",
                "method": "call_tool",
                "params": {
                    "name": "search_papers",
                    "arguments": {"query": "test", "limit": 1}
                },
                "id": 999
            }
            
            def _do_bohrium_req():
                try:
                    return requests.post(target_url, json=payload, timeout=5)
                except requests.exceptions.RequestException as e:
                    return str(e)

            result = await asyncio.get_event_loop().run_in_executor(None, _do_bohrium_req)
            
            if isinstance(result, str):
                 return {"status": "error", "message": f"Connection Error: {result}"}
            
            if result.status_code == 200:
                 return {"status": "success", "message": "Bohrium Agent Connected (MCP)"}
            else:
                 return {"status": "error", "message": f"Bohrium Error: {result.status_code}"}

        # 4. Brave Search Testing
        elif request.service_type == "brave":
            if not request.key:
                return {"status": "error", "message": "Brave API Key required"}
            
            headers = {
                "Accept": "application/json",
                "Accept-Encoding": "gzip",
                "X-Subscription-Token": request.key
            }
            url = "https://api.search.brave.com/res/v1/web/search?q=test"
            
            def _do_brave_req():
                try:
                    return requests.get(url, headers=headers, timeout=10)
                except Exception as e:
                    return str(e)
            
            result = await asyncio.get_event_loop().run_in_executor(None, _do_brave_req)
            
            if isinstance(result, str):
                return {"status": "error", "message": f"Connection Error: {result}"}
            
            if result.status_code == 200:
                return {"status": "success", "message": "Brave Search API Connected"}
            elif result.status_code == 401:
                return {"status": "error", "message": "Invalid Brave API Key"}
            else:
                return {"status": "error", "message": f"Brave Error: {result.status_code} {result.text[:100]}"}

        return {"status": "error", "message": f"Unknown service type: {request.service_type}"}

    except Exception as e:
        logger.error(f"Connection test failed: {e}")
        return {"status": "error", "message": str(e)}


# -----------------------------------------------------------------------------
# AUTH & LICENSING (Supabase)
# -----------------------------------------------------------------------------
class AuthRequest(BaseModel):
    name: str
    email: str

@app.post("/api/auth/verify")
async def verify_user_license(request: AuthRequest):
    """
    Verify user against Supabase registry.
    """
    from modules.system.auth_manager import auth_manager
    success, message = await auth_manager.verify_user(request.name, request.email)
    
    return {
        "success": success,
        "message": message,
        "user": {
            "name": request.name,
            "email": request.email
        } if success else None
    }

# -----------------------------------------------------------------------------
# LITERATURE & CITATION VERIFICATION (Reviewer Agent)
# -----------------------------------------------------------------------------

class VerificationRequest(BaseModel):
    text: str

@app.post("/api/literature/verify")
async def verify_citations(request: VerificationRequest):
    """
    BioDockify Reviewer Agent: Verify citations in text.
    """
    try:
        reviewer = CitationReviewer()
        results = reviewer.verify_text(request.text)
        return results
    except Exception as e:
        logger.error(f"Citation verification failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))
