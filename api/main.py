from fastapi import FastAPI, BackgroundTasks, HTTPException, Request
from contextlib import asynccontextmanager
import asyncio
from dotenv import load_dotenv
from fastapi.responses import JSONResponse
import os
import time

load_dotenv() # Load environment variables from .env

try:
    from runtime.sentry import setup_sentry
    setup_sentry()
except ImportError:
    pass

try:
    from runtime.monitoring import start_monitoring_server
except ImportError:
    pass

from pydantic import BaseModel
from typing import Dict, Any, Optional, List
import uuid

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    BioDockify Backend Lifespan Controller.
    Handles startup/shutdown logic for all modules.
    """
    await startup_event()
    yield
    logger.info("BioDockify Backend Shutdown.")

app = FastAPI(
    title="BioDockify API",
    description="Backend for BioDockify Pharma Research AI",
    version="v2.8.6",
    lifespan=lifespan
)

# Robust Imports
import logging
logger = logging.getLogger("biodockify_api")

try:
    from orchestration.planner.orchestrator import ResearchOrchestrator, OrchestratorConfig
    from orchestration.executor import ResearchExecutor
    logger.info("Orchestration modules loaded.")
except Exception as e:
    logger.error(f"Failed to load Orchestration modules: {e}")

try:
    from modules.analyst.analytics_engine import ResearchAnalyst
    logger.info("Analytics Engine loaded.")
except Exception as e:
    logger.error(f"Failed to load Analytics Engine: {e}")

try:
    from modules.backup import DriveClient, BackupManager
    logger.info("Backup Manager loaded.")
except Exception as e:
    logger.error(f"Failed to load Backup Manager: {e}")

try:
    from modules.literature.reviewer import CitationReviewer
    from modules.literature.scraper import LiteratureAggregator, LiteratureConfig
    from modules.literature.synthesis import get_synthesizer
    logger.info("Literature modules loaded.")
except Exception as e:
    logger.error(f"Failed to load Literature modules: {e}")

try:
    # from modules.system.auth_manager import auth_manager  # REMOVED - Supabase
    logger.info("Auth Manager disabled (Supabase removed).")
except Exception as e:
    logger.error(f"Auth Manager bypassed: {e}")

from dataclasses import asdict

# Register NanoBot Hybrid Agent Routes
try:
    from api.routes.nanobot_routes import router as nanobot_router
    from api.routes.research_management import router as research_management_router
    # from api.routers.research import router as research_router # Already included later with prefix

    app.include_router(nanobot_router)
    # app.include_router(research_router) # Redundant here

    # Research management routes
    app.include_router(research_management_router)
except ImportError as e:
    import logging
    logging.getLogger("biodockify_api").warning(f"NanoBot routes not loaded: {e}")

# Register Agent Zero Enhanced Routes (Memory, Skills, Spawn)
try:
    from api.routes.agent_enhanced_routes import router as agent_enhanced_router
    app.include_router(agent_enhanced_router)
except ImportError as e:
    import logging
    logging.getLogger("biodockify_api").warning(f"Agent Enhanced routes not loaded: {e}")

# Register Messaging Channels Routes (Telegram, WhatsApp, Discord, Feishu)
try:
    from api.routes.channels_routes import router as channels_router
    app.include_router(channels_router)
except ImportError as e:
    import logging
    logging.getLogger("biodockify_api").warning(f"Channels routes not loaded: {e}")

# Register Research Orchestration Routes (Status, WebSocket)
try:
    from api.routers.research import router as research_router
    app.include_router(research_router, prefix="/api/research", tags=["Research"])
except ImportError as e:
    import logging
    logging.getLogger("biodockify_api").warning(f"Research routes not loaded: {e}")

# Register RAG & Notebook Routes
try:
    from api.routes.rag_routes import router as rag_router
    app.include_router(rag_router)
except ImportError as e:
    import logging
    logging.getLogger("biodockify_api").warning(f"RAG routes not loaded: {e}")

# Register Enhanced Project Routes (Phase 11)
try:
    from api.routes.enhanced_project_routes import router as enhanced_project_router
    app.include_router(enhanced_project_router)
except ImportError as e:
    import logging
    logging.getLogger("biodockify_api").warning(f"Enhanced Project routes not loaded: {e}")

# Register Settings Routes (Universal API & Connection Test)
try:
    from api.routes.settings_routes import router as settings_router
    app.include_router(settings_router)
except ImportError as e:
    import logging
    logging.getLogger("biodockify_api").warning(f"Settings routes not loaded: {e}")

# Register Statistics Routes
try:
    from api.routes.statistics import router as statistics_router
    app.include_router(statistics_router)
except ImportError as e:
    import logging
    logging.getLogger("biodockify_api").warning(f"Statistics routes not loaded: {e}")

from fastapi.middleware.cors import CORSMiddleware


# ===================== Proactive Guidance Integration =====================
from modules.proactive_integration import create_proactive_guidance_routes

# Initialize proactive guidance system
try:
    proactive_guidance_manager = create_proactive_guidance_routes(config, app)
except Exception as e:
    proactive_guidance_manager = None
# ========================================================================



# CORS Configuration - Whitelist specific origins for security
# Priority: ENV > Defaults
env_origins = os.getenv("ALLOWED_ORIGINS", "").split(",")
allowed_origins = [o.strip() for o in env_origins if o.strip()]
if not allowed_origins:
    allowed_origins = [
        "http://localhost:3000",
        "http://localhost:8234",
        "http://127.0.0.1:3000",
        "http://127.0.0.1:8234",
        "tauri://localhost",
    ]

app.add_middleware(
    CORSMiddleware,
    allow_origins=allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["Content-Type", "Authorization", "X-Requested-With", "X-CSRF-Token"],
)

# -----------------------------------------------------------------------------
# Security Headers & CSRF Protection Middleware
# -----------------------------------------------------------------------------
@app.middleware("http")
async def add_security_headers(request: Request, call_next):
    # CSRF Check for state-changing methods
    if request.method in ["POST", "PUT", "DELETE", "PATCH"]:
        origin = request.headers.get("Origin")
        referer = request.headers.get("Referer")
        
        # Simple Origin/Referer check against allowed origins
        is_valid_origin = any(o in (origin or "") for o in allowed_origins)
        is_valid_referer = any(o in (referer or "") for o in allowed_origins)
        
        if not (is_valid_origin or is_valid_referer):
            # If it's a browser request (has origin/referer) but from wrong source
            if origin or referer:
                return JSONResponse(
                    status_code=403,
                    content={"detail": "CSRF verification failed: Invalid origin or referer"}
                )

    response = await call_next(request)
    
    # Essential Security Headers
    response.headers["Content-Security-Policy"] = (
        "default-src 'self'; "
        "script-src 'self' 'unsafe-inline' 'unsafe-eval'; "
        "style-src 'self' 'unsafe-inline' https://fonts.googleapis.com; "
        "font-src 'self' https://fonts.gstatic.com; "
        "img-src 'self' data: https:; "
        "connect-src 'self' ws: wss: http://localhost:8234 https://*.google.com;"
    )
    response.headers["X-Frame-Options"] = "DENY"
    response.headers["X-Content-Type-Options"] = "nosniff"
    response.headers["X-XSS-Protection"] = "1; mode=block"
    response.headers["Strict-Transport-Security"] = "max-age=31536000; includeSubDomains"
    response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    
    return response

# -----------------------------------------------------------------------------
# Rate Limiting & DoS Protection
# -----------------------------------------------------------------------------
REQUESTS_PER_MINUTE = 600
ip_request_counts = {}

@app.middleware("http")
async def rate_limit_middleware(request: Request, call_next):
    client_ip = request.client.host
    current_time = int(time.time() / 60)
    
    key = f"{client_ip}:{current_time}"
    ip_request_counts[key] = ip_request_counts.get(key, 0) + 1
    
    if ip_request_counts[key] > REQUESTS_PER_MINUTE:
        return JSONResponse(
            status_code=429,
            content={"detail": "Too many requests. Please try again later."}
        )
        
    # Periodic cleanup (crude)
    if len(ip_request_counts) > 10000:
        ip_request_counts.clear()
        
    return await call_next(request)

# Max Request Size Limit (10MB)
MAX_SIZE = 10 * 1024 * 1024
@app.middleware("http")
async def limit_request_size(request: Request, call_next):
    if request.method in ["POST", "PUT", "PATCH"]:
        content_length = request.headers.get("Content-Length")
        if content_length and int(content_length) > MAX_SIZE:
            return JSONResponse(
                status_code=413,
                content={"detail": "Request body too large"}
            )
    return await call_next(request)

# -----------------------------------------------------------------------------
# Global Error Handling & Resilience
# -----------------------------------------------------------------------------
from fastapi import Request
import logging
import time
import requests
import functools
import psutil
import asyncio

import json  # logging and time already imported above

class JsonFormatter(logging.Formatter):
    """
    Formatter to output logs as JSON for easier parsing by monitoring tools.
    """
    def format(self, record):
        log_obj = {
            "timestamp": self.formatTime(record, self.datefmt),
            "level": record.levelname,
            "message": record.getMessage(),
            "module": record.module,
            "func": record.funcName
        }
        if record.exc_info:
            log_obj["exception"] = self.formatException(record.exc_info)
        return json.dumps(log_obj)

# Configure Root Logger
handler = logging.StreamHandler()
handler.setFormatter(JsonFormatter())
logging.basicConfig(level=logging.INFO, handlers=[handler], force=True)
logger = logging.getLogger("biodockify_api")

# --- Caching Decorator ---
# Simple in-memory cache for repeated idempotent LLM queries
# Note: For production, Redis is better, but LRU is fine for single-instance desktop app.
@functools.lru_cache(maxsize=100)
def cached_llm_request(params_tuple):
    # Wrapper to allow caching of requests based on hashable params
    method, url, json_str, headers_str, timeout = params_tuple
    import json
    return requests.request(
        method=method, 
        url=url, 
        json=json.loads(json_str), 
        headers=json.loads(headers_str) if headers_str else None, 
        timeout=timeout
    )



# -----------------------------------------------------------------------------
# V2 AGENT ENDPOINTS (Phase 16)
# -----------------------------------------------------------------------------

class AgentGoal(BaseModel):
    goal: str
    mode: str = "auto" # auto, semi-autonomous, autonomous

# Thread-safe state manager for Agent V2
from threading import Lock

class AgentStateManager:
    """Thread-safe state manager for agent operations."""
    def __init__(self):
        self._state = {
            "status": "idle",
            "current_thought": "Waiting for user input...",
            "logs": [],
            "current_task_id": None
        }
        self._lock = Lock()
    
    def update(self, **kwargs):
        with self._lock:
            self._state.update(kwargs)
    
    def get(self, key: str, default=None):
        with self._lock:
            return self._state.get(key, default)
    
    def get_all(self) -> dict:
        with self._lock:
            return self._state.copy()
    
    def reset(self, task_id: str = None):
        with self._lock:
            self._state = {
                "status": "idle",
                "current_thought": "Waiting for user input...",
                "logs": [],
                "current_task_id": task_id
            }
    
    def append_log(self, log: str):
        with self._lock:
            self._state["logs"].append(log)

agent_state = AgentStateManager()

# Auth verification is implemented further down in the file (consolidated)

@app.post("/api/auth/verify-emergency")
async def verify_emergency_access(request: Dict[str, str]):
    email = request.get("email")
    token = request.get("token")
    
    if not email or not token:
        raise HTTPException(status_code=400, detail="Email and Token are required")
        
    success, msg = True, "Emergency access bypassed - unlimited access enabled"
    
    if success:
        return {"status": "success", "message": msg}
    else:
        raise HTTPException(status_code=403, detail="Invalid or Expired Emergency Token")

class AgentRequest(BaseModel):
    message: str
    mode: str = "lite" # 'lite' (NanoBot) or 'hybrid' (Agent Zero)

@app.post("/api/agent/chat")
async def agent_chat(request: AgentRequest):
    """
    Direct Chat Endpoint for BioDockify AI.
    - mode="lite": Fast, tool-using receptionist.
    - mode="hybrid": Full reasoning agent.
    """
    try:
        from agent_zero.biodockify_ai import get_biodockify_ai
        
        logger.info(f"BioDockify AI: Processing chat request (len={len(request.message)}, mode={request.mode})")
        agent = get_biodockify_ai()
        
        # Process chat via the Hybrid Agent Loop or Receptionist based on mode
        reply = await agent.process_chat(request.message, mode=request.mode)
        
        # Return format expected by UI
        return {
            "reply": reply,
            "provider": f"biodockify-{request.mode}",
            "enhanced": request.mode == "hybrid"
        }
    except Exception as e:
        logger.error(f"Agent Chat Failed: {e}")
        # Return error as reply so UI shows it in chat bubble instead of crashing
        return {
             "reply": f"**System Error:** {str(e)}\n\nPlease check the backend logs or your AI Provider settings.",
             "provider": "error"
        }


@app.post("/api/v2/agent/goal")
async def set_agent_goal(request: AgentGoal, background_tasks: BackgroundTasks):
    """
    V2: Set a high-level goal for Agent Zero.
    Triggers the Orchestrator in the background.
    """
    logger.info(f"Received Agent Goal: {request.goal} [Mode: {request.mode}]")
    
    # Pre-flight check: Verify LLM provider is available
    try:
        from runtime.config_loader import load_config
        from runtime.service_manager import get_service_manager
        
        cfg = load_config()
        ai_mode = cfg.get("ai_provider", {}).get("mode", "auto")
        
        # If using Ollama, verify it's running before starting task
        if ai_mode == "ollama":
            svc_mgr = get_service_manager(cfg)
            if svc_mgr.check_health("ollama") != "running":
                # Try to start Ollama automatically
                logger.warning("Ollama not running. Attempting to start...")
                if not svc_mgr.start_ollama():
                    raise HTTPException(
                        status_code=503,
                        detail="Ollama service is not running and could not be started. Please start Ollama with 'ollama serve' or configure a cloud API in Settings."
                    )
                logger.info("Ollama started successfully.")
    except HTTPException:
        raise  # Re-raise HTTP exceptions
    except Exception as e:
        logger.error(f"Pre-flight check failed: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to verify AI provider: {str(e)}"
        )
    
    # Generate unique task ID
    task_id = str(uuid.uuid4())
    
    # Reset State with task ID
    agent_state.reset(task_id)
    agent_state.update(status="thinking", current_thought="Analyzing research goal...")
    agent_state.append_log(f"Goal received: {request.goal}")
    
    # Define Background Task wrapper
    def run_agent_task(goal: str, mode: str):
        try:
            from runtime.config_loader import load_config
            cfg = load_config()
            
            # Initialize Orchestrator
            orch = ResearchOrchestrator()
            agent_state.update(current_thought="Planning research steps...")
            agent_state.append_log("Orchestrator initialized.")
            
            # Map agent mode to research planning mode
            # Frontend sends: auto, semi-autonomous, autonomous
            # Orchestrator expects: search, synthesize, write
            planning_mode = "synthesize"  # Default
            if mode in ["auto", "semi-autonomous", "autonomous"]:
                # For now, all autonomous modes use synthesize
                # Could be enhanced later to map to different modes based on complexity
                planning_mode = "synthesize"
            
            # Plan
            plan = orch.plan_research(goal, mode=planning_mode)
            if plan is None:
                raise ValueError("LLM failed to generate a plan. Check AI provider configuration.")
            
            agent_state.append_log(f"Plan generated: {len(plan.steps)} steps.")
            agent_state.update(
                current_thought=f"Plan created with {len(plan.steps)} steps. Ready to execute.",
                status="ready"
            )
            
        except ValueError as e:
            # LLM-specific errors
            logger.error(f"LLM Error: {e}")
            agent_state.update(status="error", current_thought=f"LLM Error: {str(e)}")
            agent_state.append_log(f"LLM Error: {str(e)}")
        except Exception as e:
            logger.error(f"Agent Task Failed: {e}")
            agent_state.update(status="error", current_thought=f"Error: {str(e)}")
            agent_state.append_log(f"Error: {str(e)}")

    # Launch Background Task
    background_tasks.add_task(run_agent_task, request.goal, request.mode)
    
    return {"status": "accepted", "message": "Agent started", "task_id": task_id, "ai_mode": ai_mode}



@app.get("/api/v2/agent/thinking")
def get_agent_thinking():
    """
    V2: Get current agent thought process (Streaming-like UI).
    """
    state = agent_state.get_all()
    logs = state.get("logs", [])
    return {
        "status": state.get("status", "idle"),
        "thought": state.get("current_thought", ""),
        "task_id": state.get("current_task_id"),
        "logs": logs[-5:] if logs else []  # Return last 5 logs
    }

# -----------------------------------------------------------------------------
# V2 SYSTEM HEALTH ENDPOINTS (Phase 19)
# -----------------------------------------------------------------------------
from modules.system.doctor import SystemDoctor

@app.get("/api/v2/system/diagnose")
def diagnose_system():
    """
    V2: Run a full system diagnostic and return a health report.
    """
    from runtime.config_loader import load_config
    cfg = load_config()
    
    doctor = SystemDoctor(cfg)
    report = doctor.run_diagnosis()
    return report

@app.post("/api/v2/system/repair")
def repair_system(service: str = "ollama"):
    """
    V2: Attempt to repair a specific system service.
    """
    from runtime.config_loader import load_config
    from runtime.service_manager import get_service_manager
    
    cfg = load_config()
    svc_mgr = get_service_manager(cfg)
    
    try:
        result = svc_mgr.attempt_repair(service)
        return result
    except AttributeError:
        # Fallback if attempt_repair doesn't exist
        if service == "ollama":
            success = svc_mgr.start_ollama()
            return {
                "status": "success" if success else "failed",
                "service": service,
                "message": "Ollama started" if success else "Failed to start Ollama"
            }
        return {"status": "error", "message": f"Unknown service: {service}"}

@app.post("/api/v2/system/start-surfsense")
def start_surfsense():
    """
    V2: Explicitly start SurfSense Knowledge Engine via Docker.
    """
    from runtime.config_loader import load_config
    from runtime.service_manager import get_service_manager
    
    cfg = load_config()
    svc_mgr = get_service_manager(cfg)
    
    svc_mgr.start_surfsense()
    return {"status": "success", "message": "SurfSense start initiated"}

# -----------------------------------------------------------------------------
# V2 CONNECTIVITY DIAGNOSIS ENDPOINTS (First-Run Self-Healing)
# -----------------------------------------------------------------------------
from modules.system.connection_doctor import ConnectionDoctor, run_diagnosis

@app.get("/api/diagnose/connectivity")
async def diagnose_connectivity():
    """
    Full connectivity diagnosis with repair suggestions.
    Used by FirstRunWizard to detect and auto-repair connection issues.
    
    Returns:
        - status: overall health (healthy | degraded | offline)
        - checks: individual check results
        - repair_actions: suggested fixes for failed checks
        - can_proceed: whether the system can operate in degraded mode
    """
    from runtime.config_loader import load_config
    
    try:
        cfg = load_config()
    except Exception:
        cfg = {}
    
    doctor = ConnectionDoctor(cfg)
    report = await doctor.full_diagnosis(auto_repair=True)
    
    return {
        "status": report.overall_status,
        "checks": [
            {
                "name": c.name,
                "status": c.status.value,
                "message": c.message,
                "details": c.details,
                "can_auto_repair": c.can_auto_repair,
                "repair_action": c.repair_action
            }
            for c in report.checks
        ],
        "repair_actions": report.suggested_repairs,
        "can_proceed": report.can_proceed_with_degraded
    }

@app.post("/api/diagnose/repair/{check_id}")
async def attempt_connectivity_repair(check_id: str):
    """
    Attempt auto-repair for a specific connectivity check.
    
    Args:
        check_id: Identifier of the check to repair (lm_studio, internet, api_keys, backend)
        
    Returns:
        Updated check result after repair attempt
    """
    from runtime.config_loader import load_config
    
    try:
        cfg = load_config()
    except Exception:
        cfg = {}
    
    doctor = ConnectionDoctor(cfg)
    result = await doctor.attempt_repair(check_id)
    
    return {
        "success": result.status.value == "success",
        "name": result.name,
        "status": result.status.value,
        "message": result.message,
        "details": result.details,
        "repair_action": result.repair_action
    }

@app.post("/api/diagnose/lm-studio/start")
async def start_lm_studio():
    """
    Explicitly attempt to auto-start LM Studio.
    Used by the UI when user clicks "Start LM Studio" button.
    
    Returns:
        - success: whether LM Studio was started
        - exe_path: path to the executable (if found)
        - message: status message
    """
    from runtime.config_loader import load_config
    
    try:
        cfg = load_config()
    except Exception:
        cfg = {}
    
    doctor = ConnectionDoctor(cfg)
    
    # Find executable
    exe_path = doctor.find_lm_studio_executable()
    if not exe_path:
        return {
            "success": False,
            "exe_path": None,
            "message": "LM Studio not found. Please install from https://lmstudio.ai"
        }
    
    # Attempt to start
    started = await doctor.auto_start_lm_studio(exe_path)
    
    if started:
        return {
            "success": True,
            "exe_path": exe_path,
            "message": "LM Studio started. Please load a model and wait for initialization."
        }
    else:
        return {
            "success": False,
            "exe_path": exe_path,
            "message": "Failed to start LM Studio. Please start it manually."
        }

# -----------------------------------------------------------------------------
# DIGITAL LIBRARY ENDPOINTS (Phase 5)
# -----------------------------------------------------------------------------
from fastapi import UploadFile, File, Form
from modules.library.store import library_store
from modules.library.ingestor import library_ingestor

# -----------------------------------------------------------------------------
# KNOWLEDGE BASE & PODCAST ENDPOINTS (Phase 33)
# -----------------------------------------------------------------------------

class PodcastRequest(BaseModel):
    text: str
    voice: str = "alloy"  # OpenAI TTS voice

class KnowledgeQueryRequest(BaseModel):
    query: str
    top_k: int = 5

@app.post("/api/knowledge/query")
async def query_knowledge_base(request: KnowledgeQueryRequest):
    """
    Query the internal knowledge base (RAG) and return cited results.
    Used by Agent Zero and the Knowledge Hub UI.
    """
    try:
        from modules.rag.vector_store import get_vector_store
        store = get_vector_store()
        results = await store.search(request.query, k=request.top_k)
        return {
            "status": "success",
            "query": request.query,
            "results": [
                {
                    "text": r.get("text", ""),
                    "score": r.get("score", 0),
                    "metadata": r.get("metadata", {})
                }
                for r in results
            ]
        }
    except Exception as e:
        logger.error(f"Knowledge query failed: {e}")
        return {"status": "error", "error": str(e), "results": []}

@app.post("/api/knowledge/podcast")
async def generate_podcast(request: PodcastRequest):
    """
    Generate a podcast audio from text using TTS.
    Supports OpenAI TTS or local Kokoro TTS.
    """
    try:
        from runtime.config_loader import load_config
        cfg = load_config()
        
        # Try OpenAI TTS first if key is available
        custom_key = cfg.get("ai_provider", {}).get("custom_key")
        custom_base_url = cfg.get("ai_provider", {}).get("custom_base_url", "https://api.openai.com/v1")
        
        if custom_key:
            import httpx
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{custom_base_url}/audio/speech",
                    headers={"Authorization": f"Bearer {custom_key}"},
                    json={
                        "model": "tts-1",
                        "input": request.text[:4096],  # Limit to 4096 chars
                        "voice": request.voice
                    },
                    timeout=60.0
                )
                if response.status_code == 200:
                    # Return audio as base64
                    import base64
                    audio_b64 = base64.b64encode(response.content).decode('utf-8')
                    return {
                        "status": "success",
                        "audio_format": "mp3",
                        "audio_base64": audio_b64
                    }
                else:
                    return {"status": "error", "error": f"TTS API returned {response.status_code}"}
        else:
            return {"status": "error", "error": "No TTS API key configured. Please add an OpenAI-compatible API key in Settings."}
            
    except Exception as e:
        logger.error(f"Podcast generation failed: {e}")
        return {"status": "error", "error": str(e)}

# -----------------------------------------------------------------------------
# HYPOTHESIS & SCIENTIFIC METHOD ENDPOINTS (Phase 11.2)
# -----------------------------------------------------------------------------
from modules.scientific_method.models import Hypothesis, HypothesisStatus, EvidenceType
from modules.scientific_method.hypothesis_engine import get_hypothesis_engine

@app.post("/api/hypothesis", response_model=Hypothesis)
def create_hypothesis(statement: str, rationale: str):
    """Create a new scientific hypothesis."""
    engine = get_hypothesis_engine()
    return engine.create_hypothesis(statement, rationale)

@app.get("/api/hypothesis", response_model=list[Hypothesis])
def list_hypotheses():
    """List all tracked hypotheses."""
    engine = get_hypothesis_engine()
    return engine.list_hypotheses()

@app.get("/api/hypothesis/{h_id}", response_model=Hypothesis)
def get_hypothesis(h_id: str):
    """Get details of a specific hypothesis."""
    engine = get_hypothesis_engine()
    h = engine.get_hypothesis(h_id)
    if not h: raise HTTPException(status_code=404, detail="Hypothesis not found")
    return h

class StatusUpdate(BaseModel):
    status: HypothesisStatus

@app.patch("/api/hypothesis/{h_id}/status")
def update_hypothesis_status(h_id: str, update: StatusUpdate):
    """Update the lifecycle status of a hypothesis."""
    engine = get_hypothesis_engine()
    engine.update_status(h_id, update.status)
    return {"status": "success", "new_status": update.status}

class EvidenceAdd(BaseModel):
    description: str
    type: EvidenceType
    source_id: Optional[str] = None

# -----------------------------------------------------------------------------
# PUBLICATION & EXPORT ENDPOINTS (Phase 11.3)
# -----------------------------------------------------------------------------
from modules.publication.latex_exporter import LaTeXExporter
from modules.publication.figure_manager import FigureManager

class ExportRequest(BaseModel):
    title: str
    author: str
    affiliation: str
    abstract: str
    content_markdown: str

# -----------------------------------------------------------------------------
# LITERATURE DISCOVERY & SYNTHESIS ENDPOINTS
# -----------------------------------------------------------------------------

class LitSearchRequest(BaseModel):
    query: str
    limit: int = 15
    sources: Optional[List[str]] = None

class LitSynthesizeRequest(BaseModel):
    topic: str
    papers: List[Dict[str, Any]]

@app.post("/api/literature/search")
async def search_literature_endpoint(req: LitSearchRequest):
    """
    Search for scientific literature across multiple sources.
    """
    try:
        config = LiteratureConfig(
            sources=req.sources,
            max_results=req.limit
        )
        aggregator = LiteratureAggregator(config)
        # Using loop.run_in_executor because search might be blocking (PubMed sync part)
        loop = asyncio.get_event_loop()
        results = await loop.run_in_executor(None, aggregator.search, req.query)
        return {"papers": results}
    except Exception as e:
        logger.error(f"Literature search failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/literature/synthesize")
async def synthesize_literature_endpoint(req: LitSynthesizeRequest):
    """
    Synthesize a literature review from a list of papers.
    """
    try:
        from modules.literature.discovery import Paper as DiscoveryPaper
        
        # Convert dicts to Paper objects
        papers = []
        for p in req.papers:
            papers.append(DiscoveryPaper(
                title=p.get("title", "Unknown"),
                url=p.get("url", p.get("full_text_url", "")),
                source=p.get("source", "Unknown"),
                authors=p.get("authors", []),
                abstract=p.get("abstract", ""),
                year=int(p.get("year", 0)) if str(p.get("year", "0")).isdigit() else 0,
                doi=p.get("doi")
            ))
            
        synthesizer = get_synthesizer()
        
        # Set agent callback if needed (using current agent logic)
        async def agent_callback(prompt: str) -> str:
            # Re-use agent_chat internal logic or similar
            # For now, let's just use the factory directly
            from modules.llm.factory import LLMFactory
            from runtime.config_loader import load_config
            import types
            
            cfg = load_config()
            ai_cfg = cfg.get("ai_provider", {})
            provider_mode = ai_cfg.get("mode", "auto")
            config_obj = types.SimpleNamespace(**ai_cfg)
            
            adapter = LLMFactory.get_adapter(provider_mode, config_obj)
            if not adapter:
                 adapter = LLMFactory.get_adapter("lm_studio", config_obj)
            
            if not adapter:
                return "AI provider not available for synthesis."
            
            return await asyncio.to_thread(adapter.generate, prompt)

        synthesizer.set_agent_callback(agent_callback)
        review = await synthesizer.generate_review(req.topic, papers)
        
        return {"review": review}
    except Exception as e:
        logger.error(f"Literature synthesis failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/publication/export/latex")
def export_to_latex(req: ExportRequest):
    """Generate LaTeX source from Markdown report."""
    try:
        exporter = LaTeXExporter()
        latex_source = exporter.generate_latex(
            title=req.title,
            author=req.author,
            affiliation=req.affiliation,
            abstract=req.abstract,
            content_markdown=req.content_markdown
        )
        return {"latex_source": latex_source}
    except Exception as e:
        logger.error(f"Export failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/publication/figures")
def register_figure(title: str, caption: str, code: str, path: str):
    """Register a scientific figure for reproducibility."""
    fm = FigureManager()
    fig_id = fm.register_figure(title, caption, code, path)
    return {"figure_id": fig_id}

# -----------------------------------------------------------------------------
# STATISTICS & QC ENDPOINTS (Phase 4)
# -----------------------------------------------------------------------------
from modules.statistics.engine import StatisticalEngine

class StatisticsRequest(BaseModel):
    data: List[Dict[str, Any]]
    design: str = "descriptive" # descriptive, two_group, anova, correlation
    tier: str = "basic" # basic, analytical, advanced

@app.post("/api/statistics/analyze")
def analyze_statistics(req: StatisticsRequest):
    """
    Unified Statistical Analysis Endpoint.
    Routes to the 3-Tier Statistical Engine.
    """
    # License Check for Statistics features
    if not verify_license():
        return JSONResponse(
            status_code=403, 
            content={"error": "license_required", "message": "The free version of BioDockify requires a one-time verification to unlock Advanced Statistics. Please go to Settings."}
        )

    engine = StatisticalEngine()
    result = engine.analyze(req.data, req.design, req.tier)
    
    if "error" in result:
        # We don't raise HTTP 500 for data errors, we return the error structure
        # so the UI can explain it to the user (especially Tier 1).
        return JSONResponse(status_code=400, content=result)
        
    return result

# File upload security constants
ALLOWED_EXTENSIONS = {'.pdf', '.txt', '.docx', '.csv', '.xlsx', '.md', '.json', '.ipynb'}
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB

@app.post("/api/library/upload")
async def upload_file(file: UploadFile = File(...)):
    """
    Uploads a file to the internal Knowledge Base.
    Stores file locally AND indexes into vector store for semantic search.
    """
    try:
        from modules.rag.ingestor import ingestor
        from modules.rag.vector_store import get_vector_store
        import os
        
        # Validate file extension
        _, ext = os.path.splitext(file.filename.lower())
        if ext not in ALLOWED_EXTENSIONS:
            raise HTTPException(
                status_code=400, 
                detail=f"Invalid file type '{ext}'. Allowed: {', '.join(ALLOWED_EXTENSIONS)}"
            )
        
        # Read and validate file size
        content = await file.read()
        if len(content) > MAX_FILE_SIZE:
            raise HTTPException(
                status_code=413, 
                detail=f"File too large. Maximum size is {MAX_FILE_SIZE // (1024*1024)}MB"
            )
        
        # 2. Store Locally (for UI listing and file retrieval)
        record = library_store.add_file(content, file.filename)
        file_path = library_store.get_file_path(record['id'])
        
        # 3. Ingest and Index into Vector Store
        vector_status = "pending"
        chunk_count = 0
        try:
            chunks = ingestor.ingest_file(str(file_path))
            if chunks:
                await get_vector_store().add_documents(chunks)
                chunk_count = len(chunks)
                vector_status = "indexed"
                # Mark file as processed
                library_store.update_metadata(record['id'], {
                    "processed": True,
                    "chunk_count": chunk_count
                })
        except Exception as e:
            logger.warning(f"Vector indexing failed for {file.filename}: {e}")
            vector_status = "failed"
            library_store.update_metadata(record['id'], {
                "processed": False,
                "index_error": str(e)
            })

        return {
            "status": "success", 
            "file": record, 
            "vector_index": vector_status,
            "chunks_indexed": chunk_count
        }
        
    except Exception as e:
        logger.error(f"Upload failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/library/files")
async def list_files():
    """Lists all files in the library."""
    return library_store.list_files()

@app.delete("/api/library/files/{file_id}")
async def delete_file(file_id: str):
    """Deletes a file from the library."""
    success = library_store.remove_file(file_id)
    if not success:
        raise HTTPException(status_code=404, detail="File not found or failed to delete")
    return {"status": "success"}

class LibraryQuery(BaseModel):
    query: str
    top_k: int = 5

@app.post("/api/library/query")
async def query_library(request: LibraryQuery):
    """
    Semantic search over the digital library.
    """
    try:
        from modules.rag.vector_store import get_vector_store
        store = get_vector_store()
        results = await store.search(request.query, request.top_k)
        return {"results": results}
    except Exception as e:
        logger.error(f"Library search failed: {e}")
        return {"results": [], "error": str(e)}

async def background_monitor():
    """
    Periodic system health logging loop.
    Logs CPU/RAM usage every 60 seconds.
    """
    logger.info("Starting Background Monitor...")
    while True:
        try:
            await asyncio.sleep(60)
            cpu = psutil.cpu_percent(interval=None)
            mem = psutil.virtual_memory()
            
            # Log as structured metric
            metric_data = {
                "event": "system_metrics",
                "cpu_percent": cpu,
                "ram_percent": mem.percent,
                "ram_available_gb": round(mem.available / (1024**3), 1)
            }
            
            # Alert on thresholds
            if mem.percent > 90:
                logger.warning(json.dumps({**metric_data, "alert": "High RAM Usage"}))
            else:
                logger.info(json.dumps(metric_data))
                
        except Exception as e:
            logger.error(f"Monitor Error: {e}")
            await asyncio.sleep(60) # Prevent tight loop on error

async def startup_event():
    """
    Service Stability: Model Loading & Pre-warming.
    """
    logger.info("Initializing BioDockify Backend...")
    
    # 1. Background Initialization (Models + Services)
    def background_init():
        """Runs heavy startup tasks without blocking API availability."""
        logger.info("Background Init: Starting...")
        
        # Start Prometheus metrics server
        try:
            from runtime.monitoring import start_monitoring_server
            start_monitoring_server(8000)
        except Exception as e:
            logger.warning(f"Background Init: Failed to start monitoring server: {e}")

        # A. Wait for server bind
        time.sleep(5) 
        
        # B. Warm up Embedding Model with Crash Protection
        try:
            logger.info("Background Init: Pre-loading Embedding Model...")
            from modules.rag.vector_store import get_vector_store
            vs = get_vector_store()
            
            # Guard against partial loading
            if hasattr(vs, '_load_dependencies'):
                 vs._load_dependencies()
                 
            if hasattr(vs, 'model') and vs.model:
                 logger.info("Background Init: Running warmup inference...")
                 vs.model.encode(["warmup"])
                 logger.info("Background Init: Embedding Model Ready.")
            else:
                 logger.warning("Background Init: Model not loaded (dependencies missing?).")
                 
        except Exception as e:
            logger.warning(f"Background Init: Model warmup failed: {e}")
            # Do NOT re-raise, to keep the thread alive

        # C. Start Background Services (Ollama/SurfSense)
        try:
            from runtime.config_loader import load_config
            from runtime.service_manager import get_service_manager
            
            config = load_config()
            if config.get("system", {}).get("auto_start_services", True):
                logger.info("Background Init: Auto-starting services...")
                svc_mgr = get_service_manager(config)
                
                # Ollama
                if config.get("ai_provider", {}).get("mode") in ["auto", "ollama"]:
                     try:
                         if hasattr(svc_mgr, 'start_ollama'):
                             svc_mgr.start_ollama()
                     except Exception as e:
                         logger.warning(f"Background Init: Ollama start failed: {e}")
                
                # SurfSense
                try:
                    # Check if method exists and is safe
                    if hasattr(svc_mgr, 'start_surfsense'):
                        svc_mgr.start_surfsense()
                except Exception as e:
                    logger.warning(f"Background Init: SurfSense start failed: {e}")
                    
        except Exception as e:
            logger.error(f"Background Init: Service init failed: {e}")

        logger.info("Background Init: Completed.")

    import threading
    threading.Thread(target=background_init, daemon=True).start()
    logger.info("Background initialization thread launched.")

    # 4. Start Background Monitoring Loop
    asyncio.create_task(background_monitor())

    # 4. Check for high memory usage warning
    mem_status = psutil.virtual_memory()
    if mem_status.percent > 90:
        logger.warning(f"High Memory Usage on Startup: {mem_status.percent}%")

@app.middleware("http")
async def resource_monitor_middleware(request: Request, call_next):
    """
    Resource Management: Reject heavy requests if system is under stress.
    """
    # Skip for simple health checks and critical auth/settings endpoints
    if request.url.path in ["/health", "/api/system/info", "/api/auth/verify", "/api/auth/verify-emergency", "/api/settings/test"]:
        return await call_next(request)

    # Check RAM
    mem = psutil.virtual_memory()
    if mem.percent > 95: # Critical Threshold
        return JSONResponse(
            status_code=503, 
            content={"detail": "System is under heavy load (Memory > 95%). Please retry later."}
        )
        
    return await call_next(request)

# Rate Limiting & Size Limiting Middleware
from collections import defaultdict
import time

request_counts = defaultdict(list)
RATE_LIMIT = 100 # requests per minute
MAX_UPLOAD_SIZE = 50 * 1024 * 1024 # 50 MB

@app.middleware("http")
async def security_middleware(request: Request, call_next):
    # 1. Size Limiting (Client-Side Check)
    if request.method == "POST":
        content_length = request.headers.get("content-length")
        if content_length and int(content_length) > MAX_UPLOAD_SIZE:
             return JSONResponse(status_code=413, content={"detail": "File too large. Maximum size is 50MB."})

    # 2. Rate Limiting (Simple Token Bucket per IP)
    client_ip = request.client.host if request.client else "testclient"
    now = time.time()
    
    # Clean old requests
    request_counts[client_ip] = [t for t in request_counts[client_ip] if t > now - 60]
    
    if len(request_counts[client_ip]) > RATE_LIMIT:
        return JSONResponse(status_code=429, content={"detail": "Rate limit exceeded. Try again later."})
        
    request_counts[client_ip].append(now)
    
    return await call_next(request)

@app.middleware("http")
async def audit_log_middleware(request: Request, call_next):
    """
    Structured Logging: Audit Trail for all API interactions.
    Logs: Method, Path, IP, Status, Duration, User-Agent.
    """
    start_time = time.time()
    
    # Process Request
    try:
        response = await call_next(request)
        status_code = response.status_code
    except Exception as e:
        status_code = 500
        raise e
    finally:
        process_time = (time.time() - start_time) * 1000 # ms
        
        # Sanitize sensitive paths if needed (e.g. /auth/login passwords)
        # For now, we trust the path logs.
        
        log_cls = logger.info if status_code < 400 else logger.warning
        
        log_data = {
            "event": "api_request",
            "method": request.method,
            "path": request.url.path,
            "client_ip": request.client.host if request.client else "testclient",
            "status_code": status_code,
            "duration_ms": round(process_time, 2),
            "user_agent": request.headers.get("user-agent", "unknown")
        }
        log_cls(json.dumps(log_data))
        
    return response

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"Global Exception: {str(exc)}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"detail": "Internal Server Error", "message": str(exc)},
    )

def safe_request(method, url, max_retries=3, **kwargs):
    """
    Resilient HTTP request with retries (Circuit Breaker Lite).
    """
    for attempt in range(max_retries):
        try:
            resp = requests.request(method, url, **kwargs)
            return resp
        except requests.exceptions.RequestException as e:
            logger.warning(f"Request failed (Attempt {attempt+1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                raise e
            time.sleep(1 * (attempt + 1)) # Linear backoff


# -----------------------------------------------------------------------------
# Persistence Layer (Disk-Based)
# -----------------------------------------------------------------------------
from modules.task_manager.manager import TaskManager
task_manager = TaskManager(os.getenv("DATABASE_URL", "sqlite+aiosqlite:///./tasks.db"))

# -----------------------------------------------------------------------------
# Data Models
# -----------------------------------------------------------------------------

class ResearchRequest(BaseModel):
    title: Optional[str] = None
    topic: Optional[str] = None # Added for frontend compatibility
    mode: str = "synthesize" # search, synthesize, write
    
class TaskStatus(BaseModel):
    task_id: str
    status: str # running, completed, suspended, failed
    result: Optional[Dict[str, Any]] = None
    progress: int = 0
    message: Optional[str] = None
    logs: List[str] = []

# -----------------------------------------------------------------------------
# Background Worker
# -----------------------------------------------------------------------------

async def run_research_task(task_id: str, title: str, mode: str):
    """
    Background worker function to run the full research pipeliine.
    """
    try:
        # Load or Init Task
        task = task_manager.load_task(task_id)
        if not task: 
             task = {"task_id": task_id, "title": title, "status": "planning"}

        task["status"] = "planning"
        task_manager.save_task(task_id, task)
        
        # 1. Plan
        # Load runtime config properly
        from runtime.config_loader import load_config
        runtime_cfg = load_config()
        
        
        # Determine cloud usage from config, not just request
        use_cloud = runtime_cfg.get("ai_provider", {}).get("mode") == "hybrid"
        user_persona = runtime_cfg.get("user_persona", {})
        
        config = OrchestratorConfig(
            use_cloud_api=use_cloud,
            user_persona=user_persona
        )
        orchestrator = ResearchOrchestrator(config)
        
        # Pass the research mode to the planner
        plan = orchestrator.plan_research(title, mode=mode, task_id=task_id)
        
        task["status"] = "executing"
        task_manager.save_task(task_id, task)
        
        # 2. Execute with Retry Logic (Async Robustness)
        # Pass task_id into executor for granular checkpointing
        executor = ResearchExecutor(task_id=task_id) 
        
        MAX_JOB_RETRIES = 3
        context = None
        
        for attempt in range(MAX_JOB_RETRIES):
            try:
                context = executor.execute_plan(plan)
                break # Success
            except Exception as job_err:
                logger.warning(f"Task {task_id} failed attempt {attempt+1}: {job_err}")
                if attempt == MAX_JOB_RETRIES - 1:
                     raise job_err # Propagate up to main handler
                await asyncio.sleep(2 * (attempt + 1)) # Backoff
        
        if not context:
             raise Exception("Task execution returned no context after retries.")
        
        # ---------------------------------------------------------------------
        # 2.5 Auto-Ingest into Local NotebookLM (RAG)
        # ---------------------------------------------------------------------
        try:
            from modules.rag.vector_store import get_vector_store
            store = get_vector_store()
            
            # Simple Chunking Strategy
            full_text = context.extracted_text
            chunk_size = 1000
            chunks = [full_text[i:i+chunk_size] for i in range(0, len(full_text), chunk_size)]
            
            docs_to_ingest = []
            for i, chunk_text in enumerate(chunks):
                docs_to_ingest.append({
                    "text": chunk_text,
                    "metadata": {
                        "source": f"Agent Research: {title}",
                        "type": "generated-research",
                        "task_id": task_id,
                        "chunk_index": i
                    }
                })
            
            if docs_to_ingest:
                await store.add_documents(docs_to_ingest)
                
            # Neo4j Sync (Entities)
            # Assuming context.entities is a list of dicts/obects
            # If we had a Neo4j module exposed here, we'd call it.
            # For now, we assume the ResearchExecutor *internally* might have handled it, 
            # or we log that we skipped it if no direct access.
            # (MVP: We rely on RAG for the Notebook experience).
            
        except Exception as rag_err:
             print(f"Warning: Failed to auto-ingest into RAG: {rag_err}")

        # ---------------------------------------------------------------------
        
        # 3. Store Results
        task["status"] = "completed"
        task["result"] = {
            "title": title,
            "text_length": len(context.extracted_text),
            "entities": context.entities,
            "stats": context.analyst_stats
        }
        task_manager.save_task(task_id, task)
        
    except Exception as e:
        task = task_manager.load_task(task_id) or {}
        task["status"] = "failed"
        task["error"] = str(e)
        task_manager.save_task(task_id, task)


# -----------------------------------------------------------------------------
# API Endpoints
# -----------------------------------------------------------------------------

@app.get("/health")
@app.get("/api/health")
def health_check():
    """
    Comprehensive System Health Check.
    Monitors: API, Ollama, Vector Store, System Resources.
    """
    import psutil
    import shutil
    
    status = {"status": "ok", "components": {}}
    
    # 1. API Service
    status["components"]["api"] = {"status": "ok", "version": "1.0.0"}
    
    # 2. Ollama / AI Provider
    try:
        from runtime.config_loader import load_config
        cfg = load_config()
        ai_conf = cfg.get("ai_provider", {})
        
        # Determine active provider
        mode = ai_conf.get("mode", "auto")
        
        if mode in ["auto", "ollama"]:
            ollama_url = ai_conf.get("ollama_url", "http://localhost:11434")
            # Ping with NO retries for fast health check
            resp = safe_request('GET', f"{ollama_url}/api/tags", max_retries=1, timeout=1.5)
            if resp.status_code == 200:
                 status["components"]["ai_core"] = {"status": "ok", "provider": "ollama"}
            else:
                 status["components"]["ai_core"] = {"status": "degraded", "message": f"Ollama Unreachable ({resp.status_code})"}
        elif mode == "lm_studio":
            lm_url = ai_conf.get("lm_studio_url", "http://localhost:1234/v1")
            resp = safe_request('GET', f"{lm_url}/models", max_retries=1, timeout=1.5)
            if resp.status_code == 200:
                 status["components"]["ai_core"] = {"status": "ok", "provider": "lm_studio"}
            else:
                 status["components"]["ai_core"] = {"status": "degraded", "message": f"LM Studio Unreachable"}
        else:
            status["components"]["ai_core"] = {"status": "ok", "provider": mode, "message": "Cloud-based provider active"}
            
    except Exception as e:
         status["components"]["ai_core"] = {"status": "error", "message": str(e)}

    # 3. Vector DB (Chroma/FAISS)
    try:
         from modules.rag.vector_store import get_vector_store
         store = get_vector_store()
         # Simple count check
         count = store.index.ntotal if (store.index and hasattr(store.index, 'ntotal')) else 0
         status["components"]["vector_db"] = {"status": "ok", "documents": count}
    except Exception as e:
         status["components"]["vector_db"] = {"status": "degraded", "message": "Store not initialized"}

    # 4. Knowledge Engine (SurfSense)
    status["components"]["knowledge_engine"] = {"status": "ok", "message": "SurfSense Knowledge Engine"}

    # 5. System Resources
    try:
        mem = psutil.virtual_memory()
        disk = shutil.disk_usage(".")
        status["components"]["system"] = {
            "status": "ok", 
            "ram_free_gb": round(mem.available / (1024**3), 1),
            "disk_free_gb": round(disk.free / (1024**3), 1)
        }
    except Exception as e:
        logger.warning(f"System health check partial failure: {e}")
        status["components"]["system"] = {"status": "unknown"}
        
    return status

@app.post("/api/research/start", response_model=Dict[str, str])
async def start_research(request: ResearchRequest, background_tasks: BackgroundTasks):
    """Start a new research task in the background."""
    task_id = str(uuid.uuid4())
    
    # Use topic if title is missing (frontend sends topic)
    research_title = request.title or request.topic
    if not research_title:
        raise HTTPException(status_code=400, detail="Either 'title' or 'topic' is required")

    # Init Valid Task State on Disk immediately
    initial_state = {
        "task_id": task_id,
        "title": research_title, 
        "status": "pending",
        "created_at": str(int(time.time() * 1000)) # timestamp proxy
    }
    task_manager.save_task(task_id, initial_state)
    
    background_tasks.add_task(run_research_task, task_id, research_title, request.mode)
    
    return {"task_id": task_id, "status": "started"}

@app.get("/api/research/status/{task_id}", response_model=TaskStatus)
def get_status(task_id: str):
    """Get the status of a specific task."""
    task = task_manager.load_task(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")
    
    return TaskStatus(
        task_id=task_id,
        status=task.get("status", "unknown"),
        result=task.get("result"),
        progress=task.get("progress", 0),
        message=task.get("message"),
        logs=task.get("logs", [])
    )

@app.post("/api/research/{task_id}/cancel")
def cancel_research(task_id: str):
    """Cancel a running research task."""
    task = task_manager.load_task(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")
    
    # Mark as failed/cancelled so UI stops polling
    task["status"] = "failed" 
    task["message"] = "Cancelled by user"
    task_manager.save_task(task_id, task)
    
    return {"success": True}


from runtime.config_loader import load_config, save_config, reset_config

# ... (existing imports)

# Legacy Settings API (Removed duplicate test endpoint, using modern version at end of file)
@app.get("/api/settings")
def get_settings():
    """Retrieve current application configuration."""
    return load_config()

@app.post("/api/settings")
def update_settings(settings: Dict[str, Any]):
    """Save new application configuration."""
    from runtime.config_loader import save_config_detailed
    success, message = save_config_detailed(settings)
    if success:
        return {"status": "success", "message": "Settings saved"}
    raise HTTPException(status_code=500, detail=f"Failed to save settings: {message}")

@app.post("/api/settings/reset")
def reset_settings():
    """Reset settings to factory defaults."""
    config = reset_config()
    return {"status": "success", "message": "Settings reset to defaults", "config": config}
@app.get("/api/diagnose/connectivity")
async def diagnose_connectivity():
    """
    Run full connectivity diagnosis (Doctor).
    Used by First-Run Wizard.
    """
    try:
        from modules.system.connection_doctor import ConnectionDoctor
        doctor = ConnectionDoctor()
        # Run diagnosis
        result = await doctor.diagnose_all()
        return result
    except Exception as e:
        logger.error(f"Diagnosis failed: {e}")
        return {
            "status": "offline", 
            "checks": [], 
            "can_proceed": False,
            "can_proceed_with_degraded": False
        }








# -----------------------------------------------------------------------------
# SurfSense Integration Endpoints
# -----------------------------------------------------------------------------

class SurfSenseCheckRequest(BaseModel):
    base_url: str = "http://localhost:8000"

@app.post("/api/surfsense/check")
async def check_surfsense(request: SurfSenseCheckRequest):
    """Check if SurfSense is running and healthy."""
    from modules.surfsense import get_surfsense_client
    
    client = get_surfsense_client(request.base_url)
    is_healthy = await client.health_check()
    
    if is_healthy:
        return {"status": "success", "message": "SurfSense is running"}
    else:
        return {"status": "error", "message": "SurfSense is not reachable"}

class PodcastRequest(BaseModel):
    chat_id: str
    voice: str = "alloy"  # OpenAI voices: alloy, echo, fable, onyx, nova, shimmer

@app.post("/api/surfsense/podcast")
async def generate_podcast(request: PodcastRequest):
    """Generate audio podcast from a chat conversation via SurfSense."""
    from modules.surfsense import get_surfsense_client
    from runtime.config_loader import load_config
    
    config = load_config()
    surfsense_url = config.get('ai_provider', {}).get('surfsense_url', 'http://localhost:8000')
    
    client = get_surfsense_client(surfsense_url)
    
    if not await client.health_check():
        raise HTTPException(status_code=503, detail="SurfSense is not running")
    
    result = await client.generate_podcast(request.chat_id, request.voice)
    
    if "error" in result:
        raise HTTPException(status_code=500, detail=result["error"])
    
    return result

class SurfSenseSearchRequest(BaseModel):
    query: str
    top_k: int = 5

@app.post("/api/surfsense/search")
async def surfsense_search(request: SurfSenseSearchRequest):
    """Search SurfSense knowledge base directly."""
    from modules.surfsense import get_surfsense_client
    from runtime.config_loader import load_config
    
    config = load_config()
    surfsense_url = config.get('ai_provider', {}).get('surfsense_url', 'http://localhost:8000')
    
    client = get_surfsense_client(surfsense_url)
    
    if not await client.health_check():
        return {"results": [], "source": "offline"}
    
    results = await client.search(request.query, top_k=request.top_k)
    return {"results": results, "source": "surfsense"}

# -----------------------------------------------------------------------------
# AGENT ZERO: Central Brain with Full Software Control
# -----------------------------------------------------------------------------

# Agent Zero Constitution & Roles

AGENT_ZERO_CONSTITUTION = """
## IDENTITY
You are Agent Zero, the central intelligence of BioDockify - a pharmaceutical research platform.

## CONSTITUTION (Core Principles)
1. ACCURACY: Provide scientifically accurate, evidence-based information
2. SAFETY: Never provide dangerous drug synthesis or harmful advice
3. TRANSPARENCY: Always cite sources and acknowledge uncertainty
4. EFFICIENCY: Choose the optimal tool for each task
5. PRIVACY: Never share user data outside the system
6. AUTONOMY: You are responsible for your own maintenance and configuration.

## ROLES (You can fulfill all these roles)
- RESEARCHER: Search literature, find papers, summarize findings
- ANALYST: Perform statistical analysis on data
- LIBRARIAN: Manage knowledge base, index documents, retrieve information
- AUDITOR: Verify journal legitimacy, detect predatory publishers
- ASSISTANT: Answer questions, explain concepts, help with workflows
- SYSTEM_ADMIN: Self-diagnose, update settings, and manage background services.

## COMMUNICATION PROTOCOL (MANDATORY)
You must ALWAYS respond with a JSON object in this format:
{
    "thoughts": [
        "Step 1: Analyze the user's request...",
        "Step 2: Check available tools...",
        "Step 3: Decide to use tool X..."
    ],
    "headline": "Brief summary of what you are doing",
    "action": {
        "type": "tool_use" | "final_answer",
        "name": "tool_name",
        "args": { ... }
    }
}

## AVAILABLE ACTIONS (Use when appropriate)
- [ACTION: research | query=<topic>] - Search PubMed/literature
- [ACTION: search_kb | query=<topic>] - Search user's knowledge base
- [ACTION: verify_journal | issn=<issn>, title=<name>] - Check journal legitimacy
- [ACTION: analyze_stats | data=<json>, design=<type>] - Statistical analysis
- [ACTION: podcast | text=<content>] - Generate audio podcast
- [ACTION: web_search | query=<topic>] - Search the web
- [ACTION: deep_research | url=<url>] - Deep research visiting the page (autonomously extracts content)
- [ACTION: delegate_task | task=<description>, role=<role>] - Recursively spawn a sub-agent to handle a complex sub-task
- [ACTION: check_health] - Run system diagnostics (Doctor)
- [ACTION: update_settings | section=<name>, key=<name>, value=<value>] - Edit configuration
- [ACTION: restart_service | service=<name>] - Restart Ollama/SurfSense

## OPERATING MODES
1. CHAT (Default): Answer questions, provide guidance
2. SEMI-AUTONOMOUS: Execute actions when user requests
3. AUTONOMOUS: Proactively complete complex multi-step tasks

## SELF-HEALING & MAINTENANCE
- If a tool fails, analyze the error and try a different approach.
- If an API Key is missing or invalid, ask the user to provide it OR update it yourself if provided in chat.
- Periodically check your own health using [ACTION: check_health].

## RESPONSE FORMAT
- For simple questions: Just answer
- For actions: Include [ACTION: ...] tags
- For multi-step: Break down and explain each step
"""

class AgentExecuteRequest(BaseModel):
    action: str  # "research", "search_kb", "verify_journal", "analyze_stats", "podcast", "web_search", "check_health", "update_settings", "restart_service"
    params: Dict[str, Any] = {}

@app.post("/api/agent/execute")
async def agent_execute(request: AgentExecuteRequest):
    """
    Universal Agent Zero execution endpoint.
    Allows Agent Zero (or direct API calls) to invoke any software function.
    """
    action = request.action.lower()
    params = request.params
    
    try:
        # RESEARCH: Search literature
        if action == "research":
            query = params.get("query", "")
            from modules.literature import search_pubmed
            results = search_pubmed(query, max_results=10)
            return {"status": "success", "action": "research", "results": results}
        
        # SEARCH_KB: Search knowledge base
        elif action == "search_kb":
            query = params.get("query", "")
            from modules.rag.vector_store import get_vector_store
            vs = get_vector_store()
            results = vs.search(query, k=5)
            return {"status": "success", "action": "search_kb", "results": results}
        
        # VERIFY_JOURNAL: Check journal legitimacy
        elif action == "verify_journal":
            issn = params.get("issn", "")
            title = params.get("title", "")
            url = params.get("url")
            from modules.journal_intel import DecisionEngine
            engine = DecisionEngine()
            result = engine.verify(issn=issn, title=title, url=url)
            return {
                "status": "success", 
                "action": "verify_journal",
                "decision": result.decision,
                "confidence": result.confidence_level,
                "risk_factors": result.risk_factors
            }
        
        # ANALYZE_STATS: Run statistical analysis
        elif action == "analyze_stats":
            data = params.get("data", [])
            design = params.get("design", "descriptive")
            tier = params.get("tier", "basic")
            from modules.statistics.engine import StatisticalEngine
            engine = StatisticalEngine()
            result = engine.analyze(data, design=design, tier=tier)
            return {"status": "success", "action": "analyze_stats", "result": result}
        
        # PODCAST: Generate audio (via SurfSense)
        elif action == "podcast":
            text = params.get("text", "")
            chat_id = params.get("chat_id", "")
            from modules.surfsense import get_surfsense_client
            client = get_surfsense_client()
            if await client.health_check():
                result = await client.generate_podcast(chat_id)
                return {"status": "success", "action": "podcast", "result": result}
            return {"status": "error", "action": "podcast", "error": "SurfSense offline"}
        
        # WEB_SEARCH: Search the web (via SurfSense/Tavily)
        elif action == "web_search":
            query = params.get("query", "")
            from modules.surfsense import get_surfsense_client
            client = get_surfsense_client()
            if await client.health_check():
                results = await client.search(query)
                return {"status": "success", "action": "web_search", "results": results}
            return {"status": "error", "action": "web_search", "error": "SurfSense offline"}

        # CHECK_HEALTH: Run system diagnostics
        elif action == "check_health":
            from modules.system.doctor import SystemDoctor
            doctor = SystemDoctor()
            report = doctor.diagnose()
            return {"status": "success", "action": "check_health", "report": report}

        # UPDATE_SETTINGS: Reconfigure system
        elif action == "update_settings":
            section = params.get("section")
            key = params.get("key")
            value = params.get("value")
            
            if not section or not key:
                return {"status": "error", "message": "Section and Key required"}

            current_settings = load_config()
            
            # Navigate to section
            if section not in current_settings:
                return {"status": "error", "message": f"Section '{section}' not found"}
            
            # Update key
            current_settings[section][key] = value
            
            if save_config(current_settings):
                return {"status": "success", "action": "update_settings", "message": f"Updated {section}.{key} = {value}"}
            else:
                return {"status": "error", "message": "Failed to save settings"}

        # RESTART_SERVICE: Manage background services
        elif action == "restart_service":
            service = params.get("service")
            from runtime.config_loader import load_config
            from runtime.service_manager import get_service_manager
            mgr = get_service_manager(load_config())
            if service == "ollama":
                mgr.attempt_repair("ollama")
            elif service == "surfsense":
                mgr.attempt_repair("surfsense")
            else:
                return {"status": "error", "message": "Unknown service"}
            return {"status": "success", "action": "restart_service", "message": f"Restarted {service}"}
        
        # DEEP_RESEARCH: Autonomous Headless Browsing
        elif action == "deep_research":
            url = params.get("url", "")
            from modules.headless_research import deep_research
            result = await deep_research(url)
            return {"status": "success", "action": "deep_research", "result": result}
            
        # DEEP_REVIEW: Full Autonomous Review Pipeline
        elif action == "deep_review":
            topic = params.get("topic", "")
            from modules.literature.orchestrator import orchestrator
            result = await orchestrator.run_deep_review(topic)
            return {"status": "success", "action": "deep_review", "result": result}
            
        # DELEGATE_TASK: Recursive Agent Spawning (Agent Zero Feature)
        elif action == "delegate_task":
            task_desc = params.get("task", "")
            role = params.get("role", "assistant")
            
            # Simple recursive call: We just re-invoke the chat endpoint with a new context?
            # Ideally, we spin up a new Orchestrator or just simple distinct chat session.
            # For this MVP, we will treat it as a specialized "sub-agent" call via the chat API logic 
            # but instantiated purely in Python to avoid infinite HTTP loops if timeouts occur.
            
            from modules.llm.factory import LLMFactory
            from runtime.config_loader import load_config
            
            cfg = load_config()
            adapter = LLMFactory.get_adapter(cfg)
            
            # Specialized prompt for the sub-agent
            sub_prompt = f"""
            You are a sub-agent specialized in: {role}.
            Your SUPEIOR has assigned you this task: {task_desc}
            
            Execute it efficiently and return a summary of your findings.
            """
            
            try:
                # Direct LLM call for the sub-task
                sub_response = adapter.generate(sub_prompt)
                return {"status": "success", "action": "delegate_task", "result": sub_response}
            except Exception as e:
                return {"status": "error", "action": "delegate_task", "error": f"Sub-agent failed: {str(e)}"}
        

        # SURFSENSE_VIDEO: Generate Video from Topic/Text
        elif action == "generate_video":
            text = params.get("text", "")
            topic = params.get("topic", "")
            
            # If topic provided but no text, generate script first (simplified for now)
            if topic and not text:
                text = f"This is a video summary about {topic}. Detailed analysis coming soon."
            
            # 1. Generate Audio
            from modules.surfsense.audio import generate_podcast_audio
            import uuid
            run_id = uuid.uuid4().hex
            output_dir = os.path.join(os.getcwd(), "ui", "public", "generated", run_id)
            os.makedirs(output_dir, exist_ok=True)
            
            audio_path = os.path.join(output_dir, "audio.mp3")
            await generate_podcast_audio(text, output_path=audio_path)
            
            # 2. Generate Slides
            from modules.surfsense.slides import generate_slides
            # Convert simple text to markdown for slides (very basic heuristic)
            md_text = f"# {topic or 'Video Summary'}\n\n- {text[:50]}...\n- Key Point 1\n- Key Point 2\n"
            slides_path = os.path.join(output_dir, "slides")
            slide_images = await generate_slides(md_text, output_dir=slides_path)
            
            # 3. Stitch Video
            from modules.surfsense.video import create_video_summary
            video_path = os.path.join(output_dir, "video.mp4")
            final_video = await create_video_summary(slide_images, audio_path, output_path=video_path)
            
            # Return relative paths for UI
            return {
                "status": "success", 
                "action": "generate_video", 
                "result": {
                    "video_url": f"/generated/{run_id}/video.mp4",
                    "audio_url": f"/generated/{run_id}/audio.mp3",
                    "slides_count": len(slide_images)
                }
            }

        else:
            return {"status": "error", "error": f"Unknown action: {action}"}
            
    except Exception as e:
        logger.error(f"Agent Execute Error: {e}")
        return {"status": "error", "action": action, "error": str(e)}


# -----------------------------------------------------------------------------
    """Upload and ingest a document (PDF, Notebook, MD) into the knowledge base."""
    import re
    
    # 1. Sanitize Filename
    # Simple rigorous sanitization: allow only alphanumeric, dot, dash, underscore
    clean_name = re.sub(r'[^a-zA-Z0-9_.-]', '_', file.filename)
    if not clean_name:
        clean_name = f"upload_{uuid.uuid4().hex}"
        
    # 2. Validate Extension
    ALLOWED_EXTS = {'.pdf', '.md', '.txt', '.ipynb', '.json'}
    suffix = os.path.splitext(clean_name.lower())[1]
    
    if suffix not in ALLOWED_EXTS:
        raise HTTPException(status_code=400, detail=f"Unsupported file type: {suffix}. Allowed: {ALLOWED_EXTS}")
        
    # 3. Validate Context/Magic Bytes (Optional but recommended for PDF)
    # For now, we rely on ingestion pipeline to fail if invalid, but we ensure safe temp write.
    
    # Save to temp file
    tmp_path = None
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
            shutil.copyfileobj(file.file, tmp)
            tmp_path = tmp.name
            
        # Ingest
        chunks = ingestor.ingest_file(tmp_path)
        
        # Add metadata override for original filename
        for chunk in chunks:
            chunk['metadata']['source'] = clean_name
            
        get_vector_store().add_documents(chunks)
        return {"status": "success", "message": f"Indexed {len(chunks)} chunks from {clean_name}"}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Upload failed: {e}")
        raise HTTPException(status_code=500, detail="Failed to process file upload")
    finally:
        if 'tmp_path' in locals() and os.path.exists(tmp_path):
            os.remove(tmp_path)


class Neo4jCheckRequest(BaseModel):
    """DEPRECATED: Neo4j has been replaced by SurfSense Knowledge Engine."""
    uri: str = ""
    user: str = ""
    password: str = ""

@app.post("/api/settings/neo4j/check")
def check_neo4j_endpoint(request: Neo4jCheckRequest):
    """
    DEPRECATED: Neo4j Graph Database check.
    Neo4j has been replaced by SurfSense Knowledge Engine.
    This endpoint is kept for backward compatibility.
    """
    return {
        "status": "deprecated",
        "message": "Neo4j has been replaced by SurfSense Knowledge Engine. Configure SurfSense in Settings > Cloud APIs instead."
    }


class LinkRequest(BaseModel):
    url: str

@app.post("/api/rag/link")
async def link_url(request: LinkRequest):
    tmp_path = None
    try:
        text = scrape_url(request.url)
        if not text:
            raise HTTPException(status_code=400, detail="No content found at URL")
            
        # Create a document chunk (simplified ingestion for direct text)
        # We wrap it in a format ingestor/vector_store expects?
        # Actually ingestor expects a file path.
        # Let's save text to a temp .txt file and use existing ingestor!
        
        with tempfile.NamedTemporaryFile(delete=False, suffix=".txt", mode='w', encoding='utf-8') as tmp:
            tmp.write(f"Source: {request.url}\n\n")
            tmp.write(text)
            tmp_path = tmp.name
            
        chunks = ingestor.ingest_file(tmp_path)
        
        # Override metadata
        for chunk in chunks:
            chunk['metadata']['source'] = request.url
            
        get_vector_store().add_documents(chunks)
        os.remove(tmp_path)
        
        return {"status": "success", "message": f"Indexed content from {request.url}"}
        
    except Exception as e:
        if 'tmp_path' in locals(): os.remove(tmp_path)
        raise HTTPException(status_code=500, detail=str(e))



class ChatRequest(BaseModel):
    query: str

@app.post("/api/rag/chat")

async def chat_with_docs(request: ChatRequest):
    """Retrieve context and answer via configured LLM (Ollama/Agent)."""
    query = request.query
    
    # 1. Retrieve Context
    vector_store = get_vector_store()
    results = vector_store.search(query, k=5) # Top 5 chunks
    context_list = [r['text'] for r in results]
    
    if not context_list:
        return {"answer": "I don't have enough context from your uploaded documents to answer that.", "sources": []}
        
    context_text = "\n\n---\n\n".join(context_list)
    
    # 2. Construct Augmented Prompt
    system_prompt = "You are a research assistant. Answer the user's question based ONLY on the provided Context. If the answer is not in the context, say so."
    full_prompt = f"Context:\n{context_text}\n\nQuestion: {query}\n\nAnswer:"
    
    # 3. Call LLM (Ollama or Configured Provider)
    from runtime.config_loader import load_config
    import requests
    import json
    
    cfg = load_config()
    ai_conf = cfg.get("ai_provider", {})
    
    answer = "Error generating response."
    
    try:
        # Default to Ollama for "Local NotebookLM" experience
        # Check if Ollama is configured
        ollama_url = ai_conf.get("ollama_url", "http://localhost:11434")
        ollama_model = ai_conf.get("ollama_model", "llama3") # Default to llama3 or mistral if not set
        
        # Simple Ollama Generate
        # We use non-streaming for this simple endpoint first
        resp = requests.post(
            f"{ollama_url}/api/generate",
            json={
                "model": ollama_model,
                "prompt": full_prompt,
                "system": system_prompt,
                "stream": False
            },
            timeout=60 
        )
        
        if resp.status_code == 200:
            answer = resp.json().get("response", "No response from model.")
        else:
            # Fallback if Ollama fails or not found?
            # Maybe use the "Agent Zero" / Cloud LLM if configured?
            # For now, return error to prompt user to check settings.
            answer = f"Ollama Error ({resp.status_code}): {resp.text}"
            
    except Exception as e:
        answer = f"Failed to connect to Local LLM: {str(e)}. Please Ensure Ollama is running."

    return {
        "answer": answer,
        "context": context_text,
        "sources": [] 
    }

@app.post("/api/rag/clear")
def clear_knowledge_base():
    """Clear all indexed documents."""
    get_vector_store().clear()
    return {"status": "success", "message": "Knowledge base cleared."}


# Duplicate Agent Chat Endpoint Removed.
# The robust implementations is at line 416.


# -----------------------------------------------------------------------------
# System Info API (For First Run Wizard)
# -----------------------------------------------------------------------------

@app.get("/api/system/info")
def get_system_info():
    """
    Retrieve system hardware and environment information.
    Used by the First Run Wizard to validate compatibility.
    """
    import platform
    import os
    import shutil
    import sys
    import tempfile
    
    # OS Info
    os_name = f"{platform.system()} {platform.release()}"
    
    # CPU
    cpu_cores = os.cpu_count() or 1
    
    # RAM (Approximate using psutil if available, else vague or skip)
    ram_total_gb = 0
    ram_available_gb = 0
    
    try:
        import psutil
        mem = psutil.virtual_memory()
        ram_total_gb = round(mem.total / (1024**3), 1)
        ram_available_gb = round(mem.available / (1024**3), 1)
    except ImportError:
        # Fallback if psutil not installed
        ram_total_gb = 0 
        ram_available_gb = 0
        
    # Disk Usage
    try:
        total, used, free = shutil.disk_usage(".")
        disk_free_gb = round(free / (1024**3), 1)
    except Exception as e:
        logger.debug(f"Free space check failed: {e}")
        disk_free_gb = 0
        
    # Python
    python_version = sys.version.split(" ")[0]
    
    # Temp Writable Check
    temp_writable = False
    try:
        with tempfile.TemporaryFile() as f:
            f.write(b"test")
            temp_writable = True
    except Exception as e:
        logger.debug(f"Temp writability check failed: {e}")
        temp_writable = False
        
    return {
        "os": os_name,
        "cpu_cores": cpu_cores,
        "ram_total_gb": ram_total_gb,
        "ram_available_gb": ram_available_gb,
        "disk_free_gb": disk_free_gb,
        "temp_writable": temp_writable,
        "python_version": python_version
    }





# -----------------------------------------------------------------------------
# PHD THESIS CORE ENDPOINTS (Phase 7)
# -----------------------------------------------------------------------------
from modules.thesis.engine import thesis_engine
from modules.thesis.structure import THESIS_STRUCTURE, PharmaBranch, DegreeType

class ThesisRequest(BaseModel):
    chapter_id: str
    topic: str
    branch: PharmaBranch = PharmaBranch.GENERAL
    degree: DegreeType = DegreeType.PHD

@app.get("/api/thesis/structure")
def get_thesis_structure():
    """Return the Master Pharma Thesis structure definition."""
    # Convert Enum keys to string for JSON serialization
    return {k.value: v.dict() for k, v in THESIS_STRUCTURE.items()}

@app.get("/api/thesis/validate/{chapter_id}")
def validate_chapter(chapter_id: str, branch: PharmaBranch = PharmaBranch.GENERAL, degree: DegreeType = DegreeType.PHD):
    """Check if proofs exist for a chapter with branch-awareness."""
    from modules.thesis.validator import thesis_validator
    return thesis_validator.validate_chapter_readiness(chapter_id, branch, degree)

@app.post("/api/thesis/generate")
async def generate_chapter_endpoint(req: ThesisRequest):
    """Generate a chapter draft if validation passes."""
    return await thesis_engine.generate_chapter(req.chapter_id, req.topic, branch=req.branch, degree=req.degree)

# -----------------------------------------------------------------------------
# Google Drive Backup System
# -----------------------------------------------------------------------------

# Initialize Backup System
drive_client = DriveClient(storage_path="./mock_cloud_storage")
backup_manager = BackupManager(drive_client)

class BackupAuthVerifyRequest(BaseModel):
    code: str

class RestoreRequest(BaseModel):
    snapshot_id: str

@app.post("/api/backup/auth/url")
def get_backup_auth_url():
    """Returns the URL to start the OAuth flow via BioDockify."""
    return {"url": drive_client.get_auth_url()}

@app.post("/api/backup/auth/verify")
def verify_backup_auth(req: BackupAuthVerifyRequest):
    """Verifies the auth code (simulated exchange)."""
    success = drive_client.authenticate(req.code)
    if success:
        return {"status": "success", "user": drive_client.get_user_info()}
    else:
        raise HTTPException(status_code=401, detail="Authentication failed")

@app.get("/api/backup/status")
def get_backup_status():
    """Returns connection status and user info."""
    return drive_client.get_user_info()

@app.post("/api/backup/run")
def run_backup(background_tasks: BackgroundTasks):
    """Triggers a backup in the background."""
    # Defers the actual backup to background task
    def _do_backup():
        # Backing up the adjacent 'brain' directory and 'ui/src' as example of critical data
        # In prod, this would be the user's workspace path
        target_dirs = ["./brain", "./ui/src/lib"] 
        result = backup_manager.create_backup(target_dirs)
        logger.info(f"Backup completed: {result}")
    
    background_tasks.add_task(_do_backup)
    return {"status": "started", "message": "Backup initiated in background"}

@app.get("/api/backup/history")
def get_backup_history():
    """Lists available snapshots."""
    return drive_client.list_backups()

@app.post("/api/backup/restore")
def restore_backup(req: RestoreRequest):
    """Restores a specific snapshot."""
    # Restoration path defaults to a safe 'restored' folder for safety in this demo
    target_path = "./restored_data"
    result = backup_manager.restore_backup(req.snapshot_id, target_path)
    if result["status"] == "error":
        raise HTTPException(status_code=500, detail=result["message"])
    return result


# =============================================================================
# SLIDES GENERATION ENDPOINTS
# =============================================================================

class SlideRequest(BaseModel):
    """Request model for slide generation."""
    source: str  # "knowledge_base", "search", "prompt", "documents"
    topic: str = ""
    search_query: str = ""
    custom_prompt: str = ""
    document_ids: List[str] = []
    style: str = "academic"
    num_slides: int = 10
    include_citations: bool = True


@app.post("/api/slides/generate")
async def generate_slides(request: SlideRequest, background_tasks: BackgroundTasks):
    """
    Generate presentation slides from various Knowledge Base sources.
    
    Supports:
    - knowledge_base: Query KB by topic
    - search: Generate from search results
    - prompt: Custom user description
    - documents: Specific document IDs
    """
    try:
        from modules.slides.slide_generator import get_slide_generator
        from modules.rag.vector_store import get_vector_store
        from modules.llm.factory import LLMFactory
        from runtime.config_loader import load_config
        
        # Initialize dependencies
        cfg = load_config()
        rag = get_vector_store()
        
        # Get LLM adapter
        try:
            from orchestration.planner.orchestrator import OrchestratorConfig
            orch_config = OrchestratorConfig()
            provider = cfg.get("ai_provider", {}).get("mode", "auto")
            llm = LLMFactory.get_adapter(provider, orch_config)
        except Exception as e:
            logger.warning(f"LLM initialization in task failed: {e}")
            llm = None
        
        generator = get_slide_generator(llm, rag)
        
        # Generate based on source type
        if request.source == "knowledge_base":
            result = generator.generate_from_knowledge_base(
                topic=request.topic,
                style=request.style,
                num_slides=request.num_slides,
                include_citations=request.include_citations
            )
        elif request.source == "search":
            # First perform search
            search_results = rag.search(request.search_query, top_k=request.num_slides * 2)
            result = generator.generate_from_search(
                search_results=search_results,
                style=request.style,
                title=request.topic or "Research Findings"
            )
        elif request.source == "prompt":
            result = generator.generate_from_prompt(
                prompt=request.custom_prompt,
                style=request.style,
                num_slides=request.num_slides
            )
        elif request.source == "documents":
            result = generator.generate_from_documents(
                document_ids=request.document_ids,
                style=request.style,
                title=request.topic or "Document Summary"
            )
        else:
            raise HTTPException(status_code=400, detail=f"Invalid source: {request.source}")
        
        return result
        
    except Exception as e:
        logger.error(f"Slides generation failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/slides/styles")
def get_slide_styles():
    """Get available slide styles."""
    from modules.slides.slide_styles import SLIDE_STYLES
    return {
        "styles": [
            {"id": k, "name": v["name"], "description": v["description"]}
            for k, v in SLIDE_STYLES.items()
        ]
    }


@app.post("/api/slides/render")
async def render_slides_html(request: dict):
    """Render slides as HTML for preview or download."""
    try:
        from modules.slides.slide_styles import generate_presentation_html
        
        slides = request.get("slides", [])
        style = request.get("style", "academic")
        title = request.get("title", "Presentation")
        
        html = generate_presentation_html(slides, style, title)
        
        return {"status": "success", "html": html}
        

    except Exception as e:
        logger.error(f"Slides rendering failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# SETTINGS & CONNECTION TEST (Managed by api/routes/settings_routes.py)


# -----------------------------------------------------------------------------
# Auth verification consolidated above

# -----------------------------------------------------------------------------
# LITERATURE & CITATION VERIFICATION (Reviewer Agent)
# -----------------------------------------------------------------------------

class VerificationRequest(BaseModel):
    text: str

@app.post("/api/literature/verify")
async def verify_citations(request: VerificationRequest):
    """
    BioDockify Reviewer Agent: Verify citations in text.
    """
    try:
        reviewer = CitationReviewer()
        results = reviewer.verify_text(request.text)
        return results
    except Exception as e:
        logger.error(f"Citation verification failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))
